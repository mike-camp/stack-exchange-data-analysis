title,content,tags
"Localization within a map with gyro, accelerometer and magnetometer","My goal is to locate precision within a known map. 
If the size of the map is known, a rectangle 4 x 3 m, and I know the initial position and orientation of the robot, after a certain period of time here moved randomly, can you know at what point of the map I am? 
I use the MPU 9250 sensor, gyro, accelerometer and magnetometer sensor.
","kinematics, raspberry-pi, mapping, python"
What battery capacity would be required to run a component for an hour?,"I have a 9.8kW Component that runs at 51.8 Volts. That means I need a battery that has about 9.8kWh of energy to run the component for an hour
For the sake of simplicity, say I have some batteries that just happened to match that voltage (51.8V) simply laying around. How many mAh would that battery need to be to run this for an hour? My instinct is to simply say 98 Ah, but this doesn't seem right..
","battery, power"
How can I use an RC servo to rotate more than 360 deg and still maintain positional accuracy?,"RC Servos are great because they are low cost, widely available, easy to control, and pretty accurate.  One disadvantage is that they usually have very limited range of motion.  This is because they are mostly for use on actuating RC control surfaces that rarely move more than 120 degrees.

Our FTC robotics teams uses them a lot; but, often we need to rotate more than 120 deg and often would like 360 deg or more.  Robot design requires the solution to be small and light weight as it will usually be at the end of an extension arm.  The game rules (and practicality) requires the solution to use a 3-wire RC servo.  Also, space and alignment issues usually make using external gear sets problematic.
Last season we needed a large ""grabber"" at the end of our arm and resorted to using a continuous rotation servo the rotated until it torque limited.  This worked but was far from ideal as it over-stressed the servo and we had minimal control on the ""grabber"" - we could open it or close it.
Our ideal solution would be small, light-weight, and inexpensive (add less than 50% to the weight, cost, or the length of any dimension of motor).
Given our constraints, how can we rotate an axis more than 360 deg and still maintain positional accuracy?
","servomotor, servos, rcservo, first-robotics"
Lego Mindstorms EV3 gyro sensor offset fix,"I am currently working on a project for a Lego Mindstorms EV3 autopilot using Matlab/Simulink. Basically, I am using a closed loop control system with a PID controller for the control of the control surfaces. I'm using Simulink to construct the autopilot block diagram. The feedback loop consists of the gyro sensor. Now the gyro sensor is not accurate in the sense that it has some offset. It does not have any bias or noise. I need to get rid of the offset to give me the actual angle of the device. How could I solve this problem? I could use a low pass filter but how do I know what transfer function to use in Simulink?
","matlab, gyroscope"
Quadrapod vs Hexapod,"I want to build a robot, but I don't know if I want to build a quadrapod or a hexapod. I would like to use three servos/leg. 

Can you tell me what are the pros and cons of quadrapods compared to hexapods? 

","mobile-robot, servos, legged, hexapod"
DC motor pid control,"Would a 12V geared DC motor (about 140rpm) (with an encoder) PID controlled as a servo give a study response when subjected to a load?
","control, motor"
Quadcopter flight controller - black smoke?,"I bought a qx95 and after a few flights, maybe 3, there was black smoke that fizzled out of the what looks like the flight controller. And since then, anytime I give throttle - the fpv video feed cuts signal. 
I'm assuming what I fizzled was some sort of limiter / power converter for the video? 
My question is - what do I need to replace? Was that the flight controller or camera? Any thoughts on what I need to replace or repair? 
",quadcopter
Sensing an actuator using vision,"I am looking forward for a method to sense in real time, using computer vision for a control application.
For this I am unsure of which is the right platform . I am well versed with MATLAB, but I am wondering if it will be able to do the image processing in real time or not.
If there are other platforms which are much quicker, you can please recommend me.
","control, computer-vision"
Controllling an industrial robotic arm,"How can you control a servo driver (delta, ...) with an industrial PC?
To control position and velocity of a servo you need PWM signal to the servo drive (amplifier), but how do we create the signal and using which component?  Would a 555 timer be sufficient?
","robotic-arm, precise-positioning"
What stepper motor driver to use?,"I want to control a DVD drive's stepper motor with an arduino uno, but at the moment I'm not sure what kind of motor driver will work for that.
I am currently looking at this one
I have no idea about the specs of a motor driver, so any help would be greatly appreciated!
","arduino, stepper-motor, stepper-driver"
What is the typical weight distribution for a brushless motor?,"Brushless motors, like regular motors, have different parts (commutator, stater, etc).
What do these various parts weigh?
","motor, brushless-motor"
Servo coupler size needed,"Hey all I would like to know what type of coupler I would need for this type of servo?

The specs are:

I'm guessing that the 25T means it has 25 teeth in the part that holds the adapters?
So I would guess I would need something like this?

But I also would like to know what size those are called for my servo. I've seen them advertised as 5mm to 8mm. 6mm to 8mm, etc etc so I don't know if those are going to work? Some also just have a smooth round hole without the teeth and I'm not sure if that's something I could use as well?
","arduino, servomotor, stepper-motor, servos, rcservo"
Do the distance function and steering function in an RRT have to be related?,"I am developing an RRT (rapidly exploring random tree) for car-like robots in SE2 space using Dubins steering function and have a question that has implications on the performance of RRTs.
In order for an RRT to be performant, an efficient nearest neighbor data structure needs to be used. There are efficient nearest neighbor data structures for metric spaces (like Euclidean space), however, none that I know of for a non-metric space (like the Dubins space).
This leads me to wonder if I can use a different distance function than the Dubins curve length in my RRT despite using the Dubins steering function to connect states.
","wheeled-robot, motion-planning, path-planning, rrt, steering"
function of PIDControl #pragma config() directive in robotC,"I am trying to sync motors on a VEX Cortex based robot and have had mixed success using the encoders with position control.  I noticed that the motor setup directive
#pragma config(Motor, port2, motorA, tmotorVex393, PIDControl, encoder, encoderPort, I2C_1, 1000)

has a parameter ""PIDControl"" but I cannot find any documentation as to what it actually does.  
I see on the encoder documentation page here that the encoder provides velocity output, but it is not apparently built into the API.  So my question is two fold:
1) What does the ""PIDControl"" directive actually do?
2) How can I use the encoder to control the speed of the motors?
",robotc
Algorithm for Determining Mobile Base Positioning Given End Effector Pose,"I have a 6 DOF arm robot with a mobile base, and a given x/y/z/quaternion vector that the end effector must match. I am to determine the most optimal position of the mobile base such that an IK solution for the arm can be constructed from the end effector vector. I already have an IK method for the arm alone, but not one that includes the mobile base. 
Not to mention, there is a collision aspect to this too. There is collision that the arm must avoid around the vector, which can easily be filtered with my simulator, but is just something that must be considered. Could anybody point out any algorithms that could possibly help? Thank you for your time.
","mobile-robot, inverse-kinematics"
Hubsan x4 drone camera recording black,"I'm unsure if this is the correct community to ask this question (vs. Electronics or Aviation Stack Exchange, for example), but I recently purchased a Hubsan x4 HD video drone from Amazon.
This is my second Hubsan drone so I am already familiar with using the recording feature. However, after every recording, the recordings are the correct length, with the correct audio, but the image is black. I tried formatting the micro SD, using different micro SDs, reading up on forums, etc. but nothing seems to do the trick.
Is mine defective, or has someone had this issue and has been able to solve it?
","quadcopter, cameras"
Error in simmechanics matlab while doing inverse dynamics using custom joint,"I am doing inverse dynamics in Matlab simmechanics in which position and orientation will be input to the end effector. 
But it shows error as below.
Error originates in Mechanical block rob33/Subsystem2/Subsystem1/Custom Joint. The coordinate systems attached to this joint must lie on the prismatic axis (for 1 axis) or in the plane of the prismatic axes (for 2 axes). If joint has no prismatic axes, the attached coordinate systems must be collocated, within tolerances.
Model is given below.
 
In subsystem1, I used custom joint to give orientaion input. Susystem1 model and  custom joint block parameters is given below figure.

Parameters of link1-1 and parallel constaints is given in below figure. 

How do I correct this error?
","inverse-kinematics, matlab, simulation, simulator"
"What is the correct name for ""servo brackets""?","I refer to these types of brackets as servo brackets, or robot brackets:

I know that the two specific brackets, shown above, are known as a short-U (some vendors refer to them as ""C"", en lieu of ""U"") and a multi-function bracket, respectively, and that there are other types available, namely:

Long U bracket
Oblique U bracket
i bracket
L bracket
etc.

However, I am sure that there is a correct name for these types of bracket (or this range of bracket, if you will), rather than just servo brackets - either a generic name or a brand name. I have seen the term once before, on a random web page, but the name escapes me. They are either named after their creator, or, if I recall correctly, the institution where they were developed.
Does anyone have a definitive answer, preferably with a citation or web reference, or a little historical background?
",mechanism
Controlling a quadrotor from a PC,"I need to control quadrotor from a PC, without using a joystick.
I have got a mini-beetle quad V929 Beetle 4-Axis  and also have this NRF24L01+ Wireless Transceiver Module Chip (2.4 GHz transceiver)
Is it possible to write an Arduino program to make them speak to each other?
I did some research and found that the quad V929 model uses FlySky protocol and only works with A7105 NRF24L01 2.4 GHz transmitter chip not the one which I mentioned above.
Are there any other better ways of controlling the quad from PC or Arduino board?
","arduino, quadcopter, radio-control"
Is there a way to combine and sync two 2K cameras @ 90fps with ICs,"I am searching for a way to minimize the size of a stereo vision module and cannot find any ICs that will combine and sync two MIPI CSI-2 (4 lane) data streams without an FPGA and too much code.  there was one online (MAX7366A 3D Video Combiner/Synchronizer with two MIPI CSI-2 Input and one MIPI CSI-2 output) but the product is not publicly available.   Does anyone have knowledge of an arrangement of ICs that I could try?.
",computer-vision
Why do my quad copter motors slowly decrease RPM with full throttle?,"The flight controller is setup using LibrePilot, no propellers are fitted, when I put the throttle of the transmitter to full the motors go to full RPM and then slowly decrease to zero RPM with no change in the position of the throttle. This repeats over time.
Check this video for visual symptoms: Quad motors slowly decrease RPM over time with full throttle.
This also happens when all the props are fitted.
",quadcopter
Can RC servo motors continually rotate?,"I know that RC servo motors are designed for precise movement, rather than a D.C. motor's continual rotation. Are most RC servo motors limited to movement within one rotation or can they actually be made to continually rotate? That is to say, is their movement limited to a specific arc? Or does it depend on the type of RC servo motor?
I have seen videos of industrial size steppers rotating constantly, but, more specifically, I was wondering whether a MG995 can.

I don't own any RC servo motors yet, so I can't actually test it myself. I just want to make sure before I make a purchase. I keep seeing conflicting information, for example the instructable, How to modify a RC servo motor for continuous rotation (One motor walker robot), implies that a RC servo motor will not continually rotate, else otherwise, why would there be a need to modify it? 
Addendum
I have just realised, after further digging about on google, and as HighVoltage points out in their answer, that I have confused steppers and servos.
In addition, I found out how to hack the TowerPro MG995 Servo for continuous rotation.
","stepper-motor, rcservo"
Using 2x UARTs on STM32F072RB,"I am trying to use 2x UARTs with ChibiOS on the STM32F072RB Nucleo Board. 
I initialized UART2 but I am still getting output on UART1 pins, which is totally weird.
#include ""ch.h""
#include ""hal.h""


/*
 * UART driver configuration structure.
 */
static UARTConfig uart_cfg_1 = {
    NULL,   //txend1,
    NULL,   //txend2,
    NULL,   //rxend,
    NULL,   //rxchar,
    NULL,   //rxerr,
    800000,
    0,
    0,      //USART_CR2_LINEN,
    0
};

static UARTConfig uart_cfg_2 = {
    NULL,   //txend1,
    NULL,   //txend2,
    NULL,   //rxend,
    NULL,   //rxchar,
    NULL,   //rxerr,
    800000,
    0,
    0,
    0
};

/*
 * Application entry point.
 */
int main(void) {

  /*
   * System initializations.
   * - HAL initialization, this also initializes the configured device      drivers
   *   and performs the board-specific initializations.
   * - Kernel initialization, the main() function becomes a thread and the
   *   RTOS is active.
   */
  halInit();
  chSysInit();

  /*
   * Activates the serial driver 1, PA9 and PA10 are routed to USART1.
   */
  //uartStart(&UARTD1, &uart_cfg_1);
  uartStart(&UARTD2, &uart_cfg_2);

  palSetPadMode(GPIOA, 9, PAL_MODE_ALTERNATE(1));  // USART1 TX.
  palSetPadMode(GPIOA, 10, PAL_MODE_ALTERNATE(1)); // USART1 RX.
  palSetPadMode(GPIOA, 2, PAL_MODE_ALTERNATE(1));  // USART2 TX.
  palSetPadMode(GPIOA, 3, PAL_MODE_ALTERNATE(1));  // USART2 RX.

  /*
   * Starts the transmission, it will be handled entirely in background.
   */
  //uartStartSend(&UARTD1, 13, ""Starting...\r\n"");
  uartStartSend(&UARTD2, 13, ""Starting...\r\n"");

  /*
   * Normal main() thread activity, in this demo it does nothing.
   */
  while (true) {
    chThdSleepMilliseconds(500);
    uartStartSend(&UARTD2, 7, ""Soom!\r\n"");
    //uartStartSend(&UARTD1, 7, ""Boom!\r\n"");
  }
}

The line uartStartSend(&UARTD2, 7, ""Soom!\r\n""); gives output on UART1. 
Is there anything else I need to do?
mcuconfig.h reads
#define STM32_UART_USE_USART1               TRUE
#define STM32_UART_USE_USART2               TRUE
#define STM32_UART_USART1_IRQ_PRIORITY      3
#define STM32_UART_USART2_IRQ_PRIORITY      3
#define STM32_UART_USART1_DMA_PRIORITY      0
#define STM32_UART_USART2_DMA_PRIORITY      0

","microcontroller, serial, c"
Biped State Space Implementation,"Currently I working on a humanoid robot using inverted pendulum model and use LQR for walking stabilizer. Input u is Torque, State x is angle and angular velocity and y output is angle.

I got the gain K value that meet my control specification (rise time, steady state error, etc) and the feedback control law like this

So now I got u(torque) value, but I don't know how to use u(torque) to move my actuator (to control 2 angkle servo) because my servo only move using angle as the command input, not torque. Is there any step that must be done to convert torque to angle? Or something? Thank You for the help, need this for my final project.
","control, dynamics, rcservo"
Why I'm getting very long terms in the inertia matrix (or dynamics model) of the robot using matlab script?,"I'm working on the dynamics model of a RRRR articulated robot, I'm following Euler-Lagrange approach and developing my code in m-file in matlab; I'm looking for dynamic model of this form: $$
D(q) \ddot{q} + C(q,\dot{q})\dot{q}+ g(q) = \tau
$$
where $D$ and $C$ are $4 \times4$ matrices and $g$ and $\tau$ (torque) are $4\times1$ vectors; by formulating the kinetic and potential energies;;
The problem is that, I'm getting very long equations, and the term in $D$ matrix are very huge and nonlinear, involving sin and cos; I'm talking about a several pages per equation;
After I published the code - 7 pages - and the output I got around 45 pages in total;
I searched around there was some guy he faced the same problem before, but there was no helpful proposal.
Any Suggestions ?? 
","robotic-arm, kinematics, dynamics, matlab"
Estimating yaw angle and yaw rate for a frontal vehicle,"I would like to know if it is possible (and if it is possible how can it be done) to estimate the yaw angle and yaw rate of a vehicle in front of me knowing the following information:
-my speed (x,y,z),my position(x,y,z),my yaw angle and my yaw rate
-the relative speed to me(on x and y) of the vehicle/robot to which I want to find the yaw and yaw rate
-the position(x,y,z) of a point on the vehicle( corner of a vehicle) the length, width and height of the vehicle
","mobile-robot, control, dynamics, motion"
Linear State Space of a 6DOF robot,"I'm new to the field of robotics, so this is a pretty basic question:
I'm assigned with the task of constructing a linear state space model of a 6DOF robotic arm that moves in 3D space. I believe the goal is to transform the original nonlinear problem to a piecewise linear problem for the purpose of designing a controller based on gain scheduling for example, and thus making use of the powerful linear control techniques and design tools. 
I'm in the process of linearizing the dynamics using the small disturbance theory, but it is a lengthy, time-consuming process that I feel is not very practical considering the alternatives. 
My question is: Is this a common practice in robotics? If not, is it practical and has it ever been done before, or should I consider an alternative approach?
","robotic-arm, dynamics"
How to balance a flying quadcopter?,"I'm using my own code to create a quadcopter robot. The hardware part is done but I need to balance the copter. 
The original video demonstrating the problem was shared via dropbox and is no longer available.

I have tried to play with the speed of each motor to get it balanced. It didn't go. 
I actually have a gyro and accelerometer onboard. But how shall I adjust the motor speed based on these values? What are the rules that I should beware of?
Is there any better solution other that try and error? Where shall I begin? Any tips? 
","balance, quadcopter"
Multicopter: What are Euler angles used for?,"I hope you can help me and this is the right forum to ask.
In the process of building and programming my own Quadcopter, I encountered the term Euler angles. I took some time to understand them and then wondered why they are used in multicopter systems.
In my understanding Euler angles are used to rotate a point or vector in a coordinate system/ to express that rotation. I now wonder why i should use Euler angles to compute the orientation of the quadcopter as I could easily(at least i think so) compute the angles by themself, like
$$
\theta = arctan(y/z)
$$
$$
\phi = arctan(x/z)
$$
(just using accelerometer, where $x, y, z$ are axis accelerations and $\theta, \phi$ are pitch and roll, respectively. In the actual implementation I do not only use the accelerometer, this is just simplified to make the point clear).
Where exactly are Euler angles used?
Are they only used to convert desired trajectory in the earth frame to desired trajectory in the Body frame?
I would be very glad if anyone could point this out and explain the concept/ why and where they are used further.
To clarify: I do know that Euler angles encounter gimbal lock, that they are three rotations about $x, y, z$ axis and 
how they generally work(I think). @Christo gave a very good explanation.
My question now is, why are they used? Isn't it counterproductive to apply the yaw rotation, then pitch, and then roll?
-Earth frame X,Y,Z
rotation about Z(psi)
->Frame 1 x', y', z'
rotation about y'(theta)
->Frame 2 x'', y'', z''
rotation about x''(phi)
->Body frame x, y, z

and vice versa.
Why? I would just have said:
pitch = angles between X and x
roll = angle between Y and y
yaw = angle between x-y-projection of the magnetic field-vector and the starting vector(yaw is kinda different).

(Notice the difference between uppercase and lowercase, look at the Earth-to-Body-Frame for notation).
Tied with this i wonder why the correct formula for pitch($\theta)$ should be $$\theta = \tan^{-1}\left(-f_x/\sqrt{f_y^2+f_z^2}\right)$$
I would have thought $$\theta = \tan^{-1}\left(-f_x/f_z\right)$$ suffices.
Maybe I have some flaw in my knowledge or a piece of the puzzle is still missing.
I hope this is understandable, if not feel free to ask. If this gets too crowded, I can always ask another question, just make me aware of it.
If anyone could explain how to use quaternions to express orientation I would be very thankful, but I can also just ask another time. I get the concept of Quaternions, just not how to use them to express orientation not rotation.
","quadcopter, algorithm, multi-rotor, orientation"
High Res Rotary Encoders,"As part of a project I need an encoder to determine the angle that this, Continuous Rotation Servo - FeeTech FS5103R, has rotated through. The resolution needs to be fairly high as I will be using it to automate a process and so I want to make sure it's accurate.
The shaft it will be mounted to will be custom built so I'm just looking for standalone encoders right now. What are the pros and cons of different styles of rotary encoders?
","arduino, sensors, servomotor"
How to find the Adjoint matrix of multiple twists,"So let's say I have a three degrees-of-freedom robot with twists ${\xi}_{1}$ ${\xi}_2$ and ${\xi}_3$. The spatial Jacobian is given by $$J=[{\xi}_1 \space  Ad_{g1}{\xi}_2 \space Ad_{g12}{\xi}_3]$$ 
I know that $$Ad_{g1} = [\begin{matrix}R_1 \space \space p \times R_1\\0 \space \space \space R1 \end{matrix}]$$
However I am not sure how to calculate $Ad_{g12}$. Do I multiply $Ad_{g1} *Ad_{g2}$ or do I get the Transformation matrix of $\xi_1$ and $\xi_2$ and then use the formula for the adjoint?
",jacobian
Drone control from the computer,"I'm doing a project with autonomous quadrotors and want to control drone from my PC. The drone is going to be Eachine Falcon with FlySky i6 transmitter and Naze32 control board. It has a training port, any suggestions how to use it to send control data? The only project I found  is dead (http://www.endurance-rc.com/). Or any step-by-step setup to make a pc-to-copter controller?
","quadcopter, automation"
Mapping and localization using Ultrasound sensor,"In my end of studies project, I have to make a robot that should do mapping and localization using only ultrasounds sensor. So for doing this i should convert sensor_msgs/LaserScan message to sensor_msgs/Range message.
My question: How can i do this ??
","slam, ros, navigation, mapping, sonar"
BeagleBone - PRU questions,"I will be using at least one programmable real-time unit (PRU) to send pulses to a stepper motor driver but before I begin, I am trying to lay out the structure of my programs.
I am using this library PRU Linux API 
for loading assembly code into the PRU instruction memory but there doesn't seem to be much documentation other then whats at that wiki and the source:
github-pru-packageh
My c program will be calculating the position of the sun using an algorithm and executing the assembly/writing a pulse count to the PRU(s) data memory so they can just switch on/off a gpio at my desired frequency and for the number of pulses required to turn a stepper the appropriate number of steps. I am not even sure if this is an acceptable method but I am pretty new at this and it seems like a simple way to accomplish my task
My Questions regarding the library functions are:

Is there a significant performance difference between using prussdrv_map_prumem or prussdrv_pru_write_memory to give the PRU(s) access to the pulse count?
Would it be better halt the PRU assembly program after has completed the tasks for each pulse count then re-execute it with new values, or keep the PRU program running and poll for a new pulse count to be written in?

I plan to send a pulse count every 10 seconds or so.
Any suggestions on revisiting the whole structure and logic are welcome as well.
","c, beagle-bone"
Mapping using a number of ultrasound sensor,"In my end of studies project, I have to make a robot that should do mapping and localization using only ultrasounds sensor and MPU 6050.
How can I create an Occupancy grid mapping using range between the robot and the obstacle ? 
","mobile-robot, localization, slam, ros, mapping"
Stepper Motor Torque,"Tried searching for this answer in a simple way but can't seem to find the answer..
I have a 3 axis Bipolar Nema 17(Took off my 1st 3d printer =) ) robotic arm(Homemade) and provides 28oz-57oz of Torque as per datasheet,  now i am trying to figure out how much that can lift in Grams/lbs
Can anybody help?? Or if someone can give me a REALLY Simplified version on how to calculate and get the answer myself, which would be useful for future projects.
Thanxs
P
","robotic-arm, stepper-motor, torque, first-robotics"
Driving a 2213 Size motor with a tiny ESC?,"I've got a Quadrotor with a DJI F450 frame, naturally one of the most popular motors for this frame is EMAX 2213 935KV motor, I need to get some new ESCs but am confused as to what to get. 
Racing drones have saturated the quadrotor market that when I see small tiny ESCs such as Spedix ES30 HV ESC or Sunrise Cicada 20A 4-in-1 BB2 ESC I get very confused, these ESCs have the appropriate Amps rating but I do not know whether they are ok for my use?
To clarify are tiny Racing Drone ESCs with appropriate Amp rating (e.g Motor at 18A and ESC at 20A) going to be able to handle the Motors with a rating of 935KV even though racing drone ESCs are designed to work with 3000+KV
","quadcopter, esc"
PTAM CameraCalibrator error,"I am trying to run the cameracalibrator.launch using PTAM according to this Camera Calibration tutorial. However, when I do so, I get the following error:
ERROR: cannot launch node of type [ptam/cameracalibrator]: can't locate node [cameracalibrator] in package [ptam]

I source my devel/setup.bash before I run the code as well and it still does not work. Here is my launch file:
<launch>
    <node name=""cameracalibrator"" pkg=""ptam"" type=""cameracalibrator"" clear_params=""true"" output=""screen"">
        <remap from=""image_raw"" to usb_cam/image_raw"" />
        <remap from=""pose"" to=""pose""/>
        <rosparam file=""$(find ptam)/PtamFixParams.yaml""/>
    </node>
</launch>

Here is what I get for rostopic list:
/rosout
/rosout_agg
/svo/dense_input
/svo/image
/svo/image/compressed
/svo/image/compressed/parameter_descriptions
...
/tf
/usb_cam/camera_info
/usb_cam/image_raw
/usb_cam/image_raw/compressed
...
/usb_cam/image_raw/theora
/usb_cam/image_raw/parameter_descriptions
/usb_cam/image_raw/parameter_updates

The path where the cameracalibration.launch file is catkin_ws/src/ethzasl_ptam/ptam/launch. 
I am not sure why this error keeps coming up because when I run roslaunch ptam cameracalibrator.launch, it says:
NODES
  /
    cameracalibrator (ptam/cameracalibrator)

So I'm thinking that PTAM does include cameracalibrator. If someone could please point out my error, that would be really helpful. I've been using this post as a guide, but it's not been helping me much: Ros Dynamic Config file.
As it says in the above link, I tried find . -executable and I could not find cameracalibrator. I could only find the below. How do I proceed?
./include
./include/ptam
./cfg
...
./launch
./src
./src/ptam
./src/ptam/cfg
...

","ros, cameras, calibration"
Can I implement continuous time feedback regulator in an Arduino microcontroller?,"Arduino is a digital mikrocontroller. But I wonder if it's possible to implement an continuous time feedback regulator in an Arduino microprocessor?
Continuous time feedback regulators such as PID:
$$
K = P(e(t)-D\frac{\mathrm{d} }{\mathrm{d} x}e(t) + I\int_{0}^{\infty} e(t) dt)
$$
Or LQG regulator (this is a LQR with kalmanfilter only, not the model):
$$
\dot{\hat{x}} = (A - KC)\hat{x} + Bu + Ky + Kn - KC\hat{x}
$$
$$
u = r - K\hat{x} $$
Or do it need to be a digital feedback regulator?  I mean....those feedback regulators works exellent by using operational amplifiers. 
I know that operational amplifier works in real time. But an Arduino working in 16 Mhz speed, and that's very fast too. 
","arduino, control, microcontroller, kalman-filter, matlab"
Should the kalmanfilter have disturbance as input?,"This is a kalmanfilter

As you can see, the process noise(disturbance) is not going to the kalmanfilter. But the state space model for the state feedback system is written as this: https://youtu.be/H4_hFazBGxU?t=5m43s
So what is right and what is wrong? Should a kalmanfilter have disturbance as input too?
Like this. With disturbance as input:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} I & 0\\ I & K \end{bmatrix}\begin{bmatrix} d\\ n \end{bmatrix}\
$$
Or this. Without disturbance as input:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} I & 0\\ 0 & K \end{bmatrix}\begin{bmatrix} d\\ n \end{bmatrix}\
$$
","control, robotic-arm, kalman-filter, matlab, noise"
Visualizing raw accelerometer and gyro data,"I have an arduino wired to an MPU6050 breakout board. The arduino continuously collects accelerometer and gyroscope data from the MPU6050 and calculates angle and velocity. 
Simply plotting the vector components (x,y,z) of this data does not allow one to reason about the motion of the sensor or robot. It's possible, though not easy, to do sanity checks (Is the sensor oriented as expected? Is gravity working?). But it's very difficult to look at a x,y,z plot of accelerometer log data and imagine what the robot did for instance. 
I was wondering if there is some sort of tool or Python library to visualise accelerometer and gyro, or IMU data? (I'm looking for something like this- https://youtu.be/6ijArKE8vKU)
","arduino, accelerometer, gyroscope, visualization"
RobotC graphical raise arms while reversing,"I am trying to help my son program his robot using RobitC graphical. He has it doing most of what he wants it to do but the code is stuck and won't continue past the arm raise/ hold. What it is supposed to be doing at the point where it gets stuck is raise the arms (to lift an object) Hold the arms up while reversing Turn 90 degrees Reverse (This is as far as he's created as he realized it isn't working) Turn 90 degrees Reverse to the basket and raise the arms again
His code is trying to say to hold the arms up until he creates a new set point
He has tried both of these codes
I know it says (left drive) instead of (arm) on one bit that was corrected and still doesn't work correctly
I was asked in the comments on my other question to post which line it fails. I believe it fails at 18. The arms stay up but the bot does not continue the program (reverse). I could not respond in the comments as I do not have a high enough rating. 
","mobile-robot, robotic-arm, programming-languages"
What is the function of inductors in series between the terminals and the brushes of a DC motor?,"I'm repairing (hopefully) a 12V DC motor (Johnson Electric HC971(2)LG-101).
The motor has a coil in the motor cap between each terminal and the corresponding brush - two coils (inductors) one on each side of the armature electrically speaking. It also has what I think is a resister on one side and what I think is a (broken) capacitor between the two terminals.
What is the function of the two inductors in series between the terminals and brushes?
I'm sure this info it's out there somewhere but I haven't found it online after several days of searching. Thanks.
","motor, electric"
Mounting on servo horn,"I'm trying to mount a rod on a micro servo's horn. The horn has several 1mm wide through holes. Does anyone know what sort of (tapping?) screws should be used for tapping 1mm plastic holes? Are M1.2 too small since the threads will be just 0.1mm deep?
Are there canonical approaches to choosing the appropriate self-tapping screw size given hole diameter?
","servomotor, servos, rcservo"
"Driving a robot in a ""straight"" line while intercepting a waypoint","Goal: Have a robot with three wheels to drive a set distance while intercepting a waypoint.
Background: I want the vehicle to have three wheels because I believe it would be the best way to have the vehicle drive the straightest.  I would have two front wheels with DC motors and encoders connected to an Arduino and have a caster wheel (the ones with a ball) on the back of the robot.  Thoughts/suggestions?
Background (cont'd): Anyways, to the main concern of the project.  I have attached a rough diagram below of my goal for this robot but I want it to drive a set distance (y) with no variance in the x-direction between the starting and ending positions.  However, I'd like it to drive through a set of obstacles (like posts) that are set at a known position.  The only form of position feedback that the robot can have are wheel encoders (for which I will be utilizing two Sparkfun Photo Interrupters on 3D printed wheels).  I shouldn't say that this can be the only form of position feedback, I just can't have any sensors that rely on external sources such as a GPS or IR rangefinder.

Request: What I need help in accomplishing is an outline of an algorithm (in simple terms as I'm only 17 and have not had too advanced of Math courses yet) that I can use to control the direction of the robot.  I know that I will use differential steering either using a dedicated DC motor driver board or a plethora of relays by varying the speeds of the motors.
Any questions/suggestions/critisisms are welcomed!
","arduino, quadrature-encoder"
Consitent Scale in Monocular pose estimation,"I just wanted to know if scale is conserved across multiple images, when doing a process such as monocular odometry. I know in reality the scale tends to drift over time due to accumulation of small errors, mismatching of features and other problems. However, if I had a perfect system with perfect correspondences and perfect relative pose estimation would the scale also drift over time. 
While working through the problem I believe it should not as long as each pair of images has correspondences in common with the previous image pair. However, I would like if someone could just confirm this theory of mine. Also if you have any proof as to why this is or a paper that explains it in depth that would also be very helpful.
","computer-vision, geometry, visual-odometry, monocular"
Electronic Spring Vibrating Spheres,"For a science project, I'm looking for a way to actuate a spring, connecting two ~ 1 cm Styrofoam spheres to each other, at frequencies around 1-60 Hz. 
In other words, I want to be able vibrate 2 connected spheres around their center of mass.
What is this spring I'm looking for?
","kinematics, actuator"
"Robotic cell simulation software, PLC","I need to simulate robotic cell where cartesian robot trims a PCB arriving on conveyor, picks it up with vacuum cup and and places in another device. After receiving signal from device the robot would pick it up and place on another belt. I want to make the cartesian robot myself using servomotors and control cell using a PLC. Would there be software that can simulate all this? I would also need to integrate sensors and possibly machine vision.
","computer-vision, simulation, manufacturing, plc"
Facing problems in Mask vector in Robotics toolbox,"I am working on a 7 DOF serial manipulator and was trying to use ikine to get the joint coordinates for simple 2 DOF robot. 
Even though I am using the masking vector as [ 1 1 0 0 0 0 ], I am getting error stating: 

Number of robot DOF must be >= the same number of 1s in the mask matrix

This is my 2 DOF robot 
L1 = Link('d', 0, 'a', 1, 'alpha', 0);
L1.m=50;
L1.r=[0.5,0,0];
L1.I=[0,0,0;0,0,0;0,0,10];
L1.G=1;
L1.Jm=0;

L2=Link(L1);

r2=SerialLink([L1,L2]);

r2.name='POLIrobot';
r2.gravity=[0;9.81;0];

q0=r2.ikine([eye(3),[0.2;0;0];[0,0,0,1]],[0,0],[1 1 0 0 0 0]);

Can anyone please help and explain why is it happening?
","robotic-arm, inverse-kinematics, robotics-library"
Motor Controller Power Supply,"I have an Orange Pi which I would like to use as the computer for a robotic project. It supplies around 15 mAh power directly from the board, barely enough for a single LED so I will have to use a motor controller and an external power supply. 
My question is, is there a limit to how big of a motor I use with this small orange Pi zero? As long as I have the appropriate power supply and controller for the motor I feel it shouldn't be an issue.
What are some things to keep in mind?
","mobile-robot, control, motor, microcontroller"
Compensation for IMU mounting misalignment,"I have an MPU6050 IMU and I would like to mount it on an FSAE car and use it to measure the yaw, pitch, roll, and angular velocities as it drives. As it's impossible to mount it perfectly flat and align the IMU axes with the axes of the car, I am looking for a way to calibrate and compensate for the rotational offset of the car's frame and the IMU's frame. 
From the IMU I can get quaternions, Euler angles, raw acceleration and angular velocity data, or yaw, pitch, and roll values. I imagine the solution will involve matrix and trig calculations, but I didn't pay nearly enough attention in multivariable calc to figure this out.
","kinematics, imu, accelerometer, gyroscope, motion"
Control a robotic gripper,"I have created a robotic gripper. However, I need help in the control circuit:

There are two buttons, the upper and lower one (connected to the timer):

the upper one is two states timer either up or down (will be replaced with an active low pin in PCB design).
the lower one is a push button that must be pushed and  let back to its initial position to give a pulse to the monostable timer to make the servo rotate long enough to just close the gripper (calculate with the RC circuit). 

My problem: I want to replace the down button with some component so as to give a quick pulse when the upper button change state.
For example, the button was 0 and went 1 -> Component -> a small pulse to drive the timer (not a pulse that will last until the state changes):

Where,

Green: is button state
Red: trigger pulse needed.

","control, robotic-arm"
Permanent Magnet DC Motor Calcuations,"I have to determine speed and torque suitable for my combat robot.
I've done some calculations and I need to know whether they're right or not (because they don't seem to be right).
Suppose I have 10kg robot, I want to push other 10kg robot to the arena walls. Let's first assume opposite robot is not opposing my push. Suppose I want it to move it with acceleration of 10cm/sec/sec (also lemme know if acceleration assumed is suitable for robowar or not).
Then force required will be,
F = (total mass to be moved) * (acceleration) = (10+10)*(10e-2) = 2 N.
Assume opposite robot is also pushing with me same force, in that case I will need another 2 N for opposing his push, Now taking into account frictional retardation, suppose I need another 2 N so total would be 6 N.
Assuming velocity = 10cm/sec, then Power required will be 6 * velocity = 6*10e-2 = 600mW. For 12V motor even with 20% efficiency it would cause current of 250mA only. Also output power = speed X torque.
Now I've heard that robowars usually require motor with high full load current otherwise they'll be damaged.
So it looks like I am somewhere wrong in my calculations. If so, lemme know where,am I wrong where I assumed that opposing robot will cause just 2 N,  because generally they'd be fitted with more high torque motors. But is it the only thing which goes wrong in above calculations? If opposite bot had to just exert 2N on my bot, are calculations right?
PS: By side-shaft geared motors, it is meant that it is internally geared with horizontal shaft, right? Or ""side-shaft"" means something more?
","mobile-robot, motor, wheeled-robot, design, torque"
How to update the cartesian target when the robotic arm has a communication delay?,"My robot system uses a 3D mouse to teleoperate the robotic arm's TCP. The robot returns its TCP's position every 10-20 milliseconds to the remote PC. The remote PC returns the new destination based on the current position and an input from the 3D mouse (new destination = current position + delta input from the mouse).
The problem is that the communication between the robot and the remote PC has delay around 100 milliseconds. Because of this condition, the robot can't generate a smooth trajectory, but it goes back and forth (jittered motion), or it repeats stop and start to move. I understand that this is because the current position is not updated in real-time and that leads to generating a wrong destination (even backward when the robot moves forward). 
One thing I already tried was to filter out the target if the length between it and the current position was smaller than the one between the previous destination and the current position. However, I couldn't succeed because the robot requires the target update every 10-20 msec, and if not sending (or if sending the previous destination again), the robot stopped to move. So, this solution didn't work.
Does anyone know how to calculate/update the new destination every cycle in this condition? Do I need to forecast the new destination based on the past trajectory?
","control, robotic-arm, industrial-robot, forward-kinematics, communication"
Ways to create a Telescoping Lift,"What different types of mechanical linkages can be used to create a lift that extends to at least three times its original height?  Note that I am intending on a motor driven system.
The most common designs I am aware of are:

Scissor Lift
Winch driven cascading linear slide


What other ideas are out there?
Extra bonus if the design can be driven by a single motor.
","mechanism, movement"
Programming Forward and Inverse Kinematics of PUMA 560?,"I am currently coding a Forward and Inverse Kinematics solver for a PUMA 560 robot. For the Inverse Kinematics part I am using the closed for solution given in this paper. But my issue is, my solution for IK for a given set of (x,y,z) does not return the same values returned by my FK values. The reason I am doing this is to verify my code accurately computes the FK and IK.
These are the DH parameters for my robot (These are Python code, since I was testing my algorithm on Spyder IDE before implementing on C++).
DH Parameters 

Link lengths 
a = [0, 650, 0, 0, 0, 0]
Link offsets
d = [0, 190, 0, 600, 0, 125]
Link twist angle
alpha = [-pi/2, 0, pi/2, -pi/2, pi/2, 0]

So basically I finding the T transformation matrix for each link from the the base frame {B} to wrist frame {W}. This is my code;
 Function to compute Forward Kinematics
def forwardK(q):

#T06 is the location of Wrist frame, {W}, relative to Base frame, {B} 
T01 = genT(q[0],0,d[0],0)
T12 = genT(q[1],a[0],d[1],alpha[0])
T23 = genT(q[2],a[1],d[2],alpha[1])
T34 = genT(q[3],a[2],d[3],alpha[2])
T45 = genT(q[4],a[3],d[4],alpha[3])
T56 = genT(q[5],a[4],d[5],alpha[4])

#Tool frame {T}
#T67 = genT(0,0,d[5],0)

T03 = matmul(T01,T12,T23)
T36 = matmul(T34,T45,T56)
T06 = matmul(T03,T36)    
#T07 = matmul(T06,T67)

x = T[0][3]
y = T[1][3]
z = T[2][3]

print(""X: "",x)
print(""Y: "",y)
print(""Z: "",z,""\n"")
print(""T: "",T,""\n"")

return T06  

 The function to compute  T Matrix 
def genT(theta, a, d, alpha):
T =  array([[cos(theta), (-sin(theta)), 0, a],
    [sin(theta)*cos(alpha), (cos(theta)*cos(alpha)), -sin(alpha), (-   d*sin(alpha))],
    [sin(theta)*sin(alpha), cos(theta)*sin(alpha), cos(alpha), cos(alpha)*d],
    [0, 0, 0, 1]])

return T

from the T Matrix relating the {B} frame to the {W} frame position vector of the {w} [x y z] is extracted. R Matrix (orientation) of the {W} relative to the {B} is obtained by the following piece of code;
T = forwardK([30,-110,-30,0,0,0])
x = T[0][3]
y = T[1][3]
z = T[2][3]
R = T[0:3,0:3]

Where T is the transformation matrix relating {W} to {B}. Then this information is fed in to the invK(x,y,z,R,ARM,ELOBOW,WRIST) function to check if the algorithm returns the same set of angles fed to the forwardK(q1,q2,q3,q4,q5,q6) function. 

In the invK(x,y,z,R,ARM,ELOBOW,WRIST)
      - ARM, ELBOW, WRIST are orientation specifiers to describe various possible configurations of the manipulator. Each of these parameters are either {+1,-1}. These values are then used in the closed form geometrical solution presented by the afore-mentioned paper.

I did not post the code for the invK(x,y,z,R,ARM,ELOBOW,WRIST) since it is a direct implementation of the closed form solution presented in the paper and also it significantly long hence making it highly unreadable. 
What do you think I am doing wrong? I am quite sure the way I am computing the FK is correct but I could be wrong. The matrix multiplications of my Python code are correct since I double checked them with Matlab. Any advice is appointed. 
","robotic-arm, inverse-kinematics, industrial-robot, forward-kinematics"
Sigma gate (std deviation) plot Inference,"So i plot some standard deviation plots (std deviation obtained from covariance matrices generated) around mean, to see if the ground truth lies in it or not. 
It is an localization estimation of an UAV, so i have 3 plot; estimation of x (north), estimation of y (east) and estimation of z (downwards towards earths center). 
http://imgur.com/a/1jHED
As it can seen from the plots that the covariance estimation looks fine in the case of x and y as the ground truth lies just within it, not leaving too much residual. However in the estimation of z the covariance estimation is not that good.
I believe, it is because the uncertainty in attitude estimation is not modelled efficiently, am i correct? 
What my question here is what can i infer from the plots, a little detail will help me a lot. 
Thanks.
","slam, uav"
How to use scan command in Arduino WifiBee,"We want to find available WiFi networks near.
So in tutorial there is command scan, which is send from CoolTerm program from PC.
Now we want to write program to Arduino which will do same operation, how it can be done?
","arduino, wifi"
How quickly can a driverless vehicle generate control commands?,"Can anyone give me a typical frequency that a driverless vehicle could generate steering and motor thrust commands? 
I am trying to model a driverless vehicle in MatLab. Right now the vehicle is generating commands at 50Hz and the integrator is solving the dynamics at a 0.01s timestep (100Hz). I just want to know if this is realistic. 
I would be even more grateful if you could point to a published source for this number.
","control, navigation, matlab, self-driving"
Stabilising a quadcopter using YPR,"I'm using the MPU-6050 accelerometer + gyro with the library I2Cdev which outputs: quaternion, euler angles and YPR angles. The equations used for calculating the YPR are:
uint8_t MPU6050::dmpGetYawPitchRoll(float *data, Quaternion *q, VectorFloat *gravity) {
  // yaw: (about Z axis)
  data[0] = atan2(2 * q -> x * q -> y - 2 * q -> w * q -> z, 2 * q -> w * q -> w + 2 * q -> x * q -> x - 1);
  // pitch: (nose up/down, about Y axis)
  data[1] = atan(gravity -> x / sqrt(gravity -> y * gravity -> y + gravity -> z * gravity -> z));
  // roll: (tilt left/right, about X axis)
  data[2] = atan(gravity -> y / sqrt(gravity -> x * gravity -> x + gravity -> z * gravity -> z));
  return 0;
}

I want to stabilize a quadcopter with these values and 3 PID regulators like this:

FL = Throttle + (-PitchPID) + (-RollPID) + (+YawPID)
FR = Throttle + (-PitchPID) + (+RollPID) + (-YawPID)
RL = Throttle + (+PitchPID) + (-RollPID) + (+YawPID)
RR = Throttle + (+PitchPID) + (+RollPID) + (-YawPID)

The pitch and roll values are between -90 and +90 degrees (0 degrees is horizontal and +-90 is vertical). The problem is that when the quad starts tipping over, the error will start decreasing and will stabilize upside down.
","quadcopter, pid, imu"
Why doesn't this kalmanfilter filter anything? Reduce disturbances and noises with LQG controller,"I have a state space model which look like this:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ 0 & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
$$
\begin{bmatrix} y\\ e \end{bmatrix} = \begin{bmatrix} C &0 \\ 0& C \end{bmatrix}\begin{bmatrix} x\\ \tilde{x} \end{bmatrix} + \begin{bmatrix} 0 & 0 &0 \\ 0 & 0 &1 \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix} 
$$
This state space model represents this picture:

If you still not understand how. Look at this video:
https://youtu.be/H4_hFazBGxU?t=2m13s
Notice the estimation error: $\tilde{x} = x - \hat{x}$
So I've made som Octave code which are very similar to MATLAB code if you have the MATLAB control package installed. Octave have a free control package and symbolic package to use.
clc
clear

% ladda bibliotek - Load GNU octave library - Free to download 
pkg load symbolic
pkg load control


% Parametrar 
m1 = 10; m2 = 7; M = 1000;
Ap = 40; Am = 20; 
Pp = 20; Pm = 10;
b1 = 3000; b2 = 1000;
L = 0.1; g = 9.82; mu = 0.3;

% Tillstånd vid statiskt - When this model are a statical point 
% the state are:
x1 = 0.65;
x2 = 0; 
x3 = 0.2;
x4 = 0;
x5 = 5*pi/180;
x6 = 0;

% Symboliska variabler
syms k1 k2 k3

% Statisk beräkning - Statistical calculations using symbolic solve
Equation1 = -k1/m1*x1 + k1/m1*x3 - b1/m1*x2 + b1/m1*x4 + Ap*10/m1*Pp - Pm*Am*10/m1*x2;
Equation2 = k1/M*x1 - k1/M*x3 + b1/M*x2 - b1/M*x4 - g*mu*x4 - k2/M*x3 + k2*L/M*x5;
Equation3 = 3*k2/(m2*L)*x3 - 3*k2/m2*x5 - 3*k3/(m2*L^2)*x5 - 3*b2/(m2*L^2)*x6 + 3*g/(2*L)*x5;


[k1, k2, k3] = solve(Equation1 == 0, Equation2 == 0, Equation3 == 0, k1, k2, k3);
k1 = double(k1);
k2 = double(k2);
k3 = double(k3);

% Dynamisk beräkning - Dynamical calculations for the state space model
A = [0 1 0 0 0 0;
     -k1/m1 (-b1/m1-Pm*Am*10/m1) k1/m1 b1/m1 0 0;
     0 0 0 1 0 0;
     k1/M b1/M (-k1/M -k2/M) (-b1/M -g*mu) k2*L/M 0;
     0 0 0 0 0 1;
     0 0 3*k2/(m2*L) 0 (-3*k2/m2 -3*k3/(m2*L^2) + 3*g/(2*L)) -3*b2/(m2*L^2)];

B = [0; Ap*10/m1; 0 ; 0; 0; 0]; % Input matrix
C = [0 1 0 0 0 0]; % Output matrix
I = [0; 1; 0 ; 0; 0; 0]; % Disturbance matrix. 

% LQR
Q = diag([0 0 0 40 0 0]);
R = 0.1;
L = lqr(A, B, Q, R); % The control law - LQR gain matrix

% LQE
Vd = diag([1 1 1 1 1 1]);
Vn = 1;
K = (lqr(A',C',Vd,Vn))'; % A way to use LQR command to compute the Kalman gain matrix

% LQG
a = [(A-B*L) B*L; zeros(6,6) (A-K*C)];
b = [B I zeros(6,1); zeros(6,1) zeros(6,1) K];
c = [C zeros(1,6); zeros(1,6) C];
d = [0 0 0; 0 0 1];
sysLQG = ss(a, b, c, d);

% Simulate the LQG with disturbance and white gaussian noise
t = linspace(0, 2, 1000);
r = linspace(20, 20, 1000);
d = 70*randn(size(t));
n = 0.1*randn(size(t));
x0 = zeros(12,1);
lsim(sysLQG, [r' d' n'], t, x0)

This will result if I got noise $0.1*randn(size(t))$ . $y1 = y$ and $y2 = e$

But let say I got no noise at all! I got this: 

That means that $\tilde{x}$ has no function at all! Something is wrong. I have tried diffrent values at the $C$ matrix, but have not get an estimation error to look at. 
Question: $$ $$
What is wrong with my model? I want to control this so the model and stand against the disturbance and noise. But now, the model just accepting the disturbance and noise. 
Does the observer(kalmanfilter) need to have disturbance too? 
EDIT: $$ $$
If I simulate this new state space model with the same options and code I had before:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} B & B & 0\\ 0 & B & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
$$
\begin{bmatrix} y\\ e \end{bmatrix} = \begin{bmatrix} C &0 \\ 0& C \end{bmatrix}\begin{bmatrix} x\\ \tilde{x} \end{bmatrix} + \begin{bmatrix} 0 & 0 &0 \\ 0 & 0 &1 \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix} 
$$
I get this. All I did was to add disturbance to the kalmanfilter too. And insted of $I$ matrix, I replace it with $B$ matrix. 

But the problem is that if I simulate an LQR with only state feedback, not the kalmanfilter (observer). I get this:

There are both red and green color in this picture. You have to zoom in. The red color is the LQG simulation and the green color is the LQR simulation(the simulation wihout the kalmanfilter). You can se that the kalmanfilter does not filter anything. Why?
Here is a short code sample:
% LQG
a = [(A-B*L) B*L; zeros(6,6) (A-K*C)];
b = [B B zeros(6,1); zeros(6,1) B K];
c = [C zeros(1,6); zeros(1,6) C];
d = [0 0 0; 0 0 1];
sysLQG = ss(a, b, c, d);

% Simulate the LQG with disturbance and no white gaussian noise
t = linspace(0, 2, 1000);
r = linspace(20, 20, 1000);
d = 2*randn(size(t));
n = 0*randn(size(t));
x0 = zeros(12,1);
[yLQG, t, xLQG] = lsim(sysLQG, [r' d' n'], t, x0);

% Simulera LQR med störning 
sysLQR = ss(A-B*L, [B B], C, [0 0])
x0 = zeros(6,1);
[yLQR, t, xLQR] = lsim(sysLQR, [r' d'], t, x0);

plot(t, yLQR, 'g', t, yLQG(:,1), 'r');

","control, robotic-arm, kalman-filter, matlab, noise"
Type of differential equations of motion of a space robot,"This is probably more of a mathematics question but here it is anyway: 
The typical equations of motion of a space robot take the form: $M(q)\ddot{q}+C(q,\dot{q})\dot{q}= \tau$, which is essentially similar to the general form of ground-based robots after dropping the gravity terms, $g(q)$, since micro-gravity conditions are assumed in space. For robotic arms mounted on satellites, the vector of generalized coordinates, $q$, is given by:
$q=\left(
\begin{array}{c}
 r_b \\
 \theta _b \\
 \theta_m \\
\end{array}
\right)$  
Where $r_b$ is the base position vector, $\theta_b$ is the base orientation vector and $\theta_m$ is the vector of the manipulator joint variables (assuming all are of revolute type). Analogous to ground-based robots, space robots' equations of motion of this form are nonlinear, second order, $\textbf{ODEs}$. 
However, for free-floating and attitude-controlled space manipulators, assuming no external forces act on the system, the linear momentum of the system is conserved. If it is further assumed that the initial linear velocity of the system is zero, the system's center of mass becomes fixed in inertial space, and the inertial position vector of the base, $r_b$, becomes a nonlinear function of the base and arm variables, $\theta_b$ and $\theta_m$.
My question is: when substituted in the equations of motion, will this dependency cause the system of equations to be $\textbf{DAE's}$ instead of ODEs?    
","robotic-arm, dynamics"
Balancing robot tuning approach,"I'm at the stage where I assembled a balancing robot and it's not maintaining a stable position. This is not a surprise I just started testing last night.  
My code is here, views of the device are here. 
Briefly, it's based on a teensy 3.2, a brushless motor controller that receives I2C commands that drives brushless gimbal motors. It uses an MPU9250 for angle measurement. I'm using PID control, and I made a tkinter-based interface that allows me to send it P/I/D values for realtime testing. I plan on implementing a bluetooth based serial to reduce wires going to the device. 
At this stage I'm not asking people for specific help on debugging what's going wrong, I'm asking about a general strategy for testing. I have used the RAM on the teensy before to record PID response time and then send that data to pyplot, which was very informative before. I was wondering if it would be a good idea to detach my wheels and mount the motors to a rigid pedestal - and to do some PID tuning using that system to tweak the wobbliness/stability. My reasoning being ""hey if I can't get this thing to stay upright when it's rigidly mounted to the bench, why would it work when it's got wheels on it?"" 
Are there any comments on this strategy, and would anyone want to offer other ways to go at the problem at this point? Yes, I've read the many posts on PID tuning, I'll follow them as best I can. 
I can post pictures and other code examples but newbies only get to put two links into the OP.
","arduino, pid, balance"
How to remove sound comming from servo motor?,"When ever i move the servo motors from the arduino, it produces sound. I don't want the servos to make this sound. How can i remove this sound. As i think this sound must be coming from the gears used inside the servo motor, but how can i silent those sound?
","arduino, servomotor, servos"
Can I write the seperation principle for LQG controllers in this state space form?,"Look at this picture. This is the seperation principle diagram. 

It is an LQG controller which going to control the real life process. What I want to do, is to create a state space model for this seperation principle system, including the real life process. A LQG controller is a LQR controller together with the Kalmanfilter. Kalmanfilter is also called an observer. 
The LQR controler is a feedback gain matrix L and the kalmanfilter is just a mathematical description of the real life system with a gain matrix K.
r(t) is the reference signal vector which describe how the system's states should hold e.g. temperature or pressure. 
y(t) is the output from the real life process.
$\hat{y}$ is the estimated output from the kalmanfilter.
d(t) is the disturbance vector for the input. That's a bad thing, but the Kalman filter are going to reduce the disturbance and noise. 
u(t) is the in signal vector to the real life system and the kalmanfilter.
n(t) is the noise vector from the measurement tools.
x(t) is the state vector for the system.
$\dot{x}$ is the state vector derivative for the system.
$\hat{x}$ is the estimated state vector for the system.
$\dot{\hat{x}}$ is the estimated state vector derivative for the system.
A is the system matrix. B is the in signal matrix. C is the output matrix. L is the LQR controler gain matrix. K is the kalmanfilter gain matrix. 
So....a lot of people create the state space system as this: $$ $$
For the real life system:
$$ \dot{x} = Ax + Bu + d$$
For the kalmanfilter:
$$\dot{\hat{x}} = A\hat{x} + Bu + Ke$$
But $u(t)$ is:
$$u = r - L\hat{x}$$
And $e(t)$ is:
$$e = y + n - \hat{y} = Cx + n - C\hat{x} $$
And then...for some reason, people says that the state space model should be model by the state estimation error:
$$\dot{\tilde{x}} = \dot{x} - \dot{\hat{x}}  = (Ax + Bu + d) - (A\hat{x} + Bu + Ke)
$$
$$ \dot{\tilde{x}}  = (Ax + Bu + d) - (A\hat{x} + Bu + K(Cx + n - C\hat{x})$$
$$ \dot{\tilde{x}} = Ax - A\hat{x} + d - KCx - Kn + KC\hat{x} $$
And we can say that:
$$\tilde{x} = x - \hat{x} $$
Beacuse:
$$\dot{\tilde{x}} = \dot{x} - \dot{\hat{x}}$$
The kalmanfilter will be:
$$ \dot{\tilde{x}} = (A - KC)\tilde{x} + Kn$$
The real life process will be:
$$ \dot{x} = Ax + Bu + d = Ax + B(r - L\hat{x}) + d = Ax + Br - BL\hat{x} + d$$
But:
$$\tilde{x} = x - \hat{x} \Leftrightarrow  \hat{x} = x - \tilde{x}$$
So this will result for the real life process:
$$ \dot{x} =  Ax + Br - BL(x - \tilde{x}) + d = Ax + Br - BLx + BL\tilde{x} + d$$
So the whole state space model will then be:
$$
\ \begin{bmatrix} \dot{x} \\ \\\dot{\tilde{x}} \end{bmatrix} =\begin{bmatrix} A - BL& BL \\ 0 & A-KC \end{bmatrix} \begin{bmatrix} x\\ \tilde{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ 0 & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
Youtube example:
https://youtu.be/H4_hFazBGxU?t=2m13s
$I$ is the identity matrix. But doesn't need to be only ones on digonal form. 
$0$ is the zero matrix.
The Question: $$ $$
If I write the systems on this forms:
$$ \dot{x} = Ax + Bu + d = Ax + B(r - L\hat{x}) + d$$
For the kalmanfilter:
$$\dot{\hat{x}} = A\hat{x} + Bu + Ke = A\hat{x} + B(r - L\hat{x}) + K(y + n - \hat{y})$$
Beacuse $u(t)$ and $e(t)$ is:
$$u = r - L\hat{x}$$
$$e = y + n - \hat{y} = Cx + n - C\hat{x} $$
I get this:
$$\dot{x} = Ax + Br - BL\hat{x} + d$$
$$\dot{\hat{x}} = A\hat{x} + Br - BL\hat{x} + KCx + Kn - KC\hat{x}$$
Why not this state space form:
$$
 \begin{bmatrix} \dot{x} \\ \\\dot{\hat{x}} \end{bmatrix} =\begin{bmatrix} A & -BL \\ KC & [A-BL-KC] \end{bmatrix} \begin{bmatrix} x\\ \hat{x} \end{bmatrix}+\begin{bmatrix} B & I & 0\\ B & 0 & K \end{bmatrix}\begin{bmatrix} r\\ d\\ n \end{bmatrix}\
$$
Youtube example:
https://youtu.be/t_0RmeSnXxY?t=1m44s
Who is best? Does them both works as the LQG diagram shows? Which should I use?
","control, robotic-arm, kalman-filter, matlab, noise"
Default settings controllers in AR.Drone 2.0 Parrot,"I need information about structure of control system in AR.Drone 2.0 Parrot to my thesis. Do any of you have the exact values of default settings controllers (e.g. time constants, gains) used in the device?
","quadcopter, control, pid"
Is this way correct for getting linear state vector from nonlinear states?,"I have written a code for the linear model of quadrotor vehicle and a linear model predictive controller to control it.
The state vector for linear model is;
$x=[x\quad y\quad z\quad \phi\quad \theta\quad \psi\quad \dot{x}\quad \dot{y}\quad \dot{z}\quad \dot{\phi}\quad \dot{\theta}\quad \dot{\psi}]^T$
Also, the controller uses this state vector to calculate the system inputs as squared speeds of 4 rotors. This works as expected with the linearized system model around hover condition which approximates Euler rates to body rates as;
$[\dot{\phi}\quad \dot{\theta}\quad \dot{\psi}]^T=[p\quad q\quad r]^T$
Now I want to use this linear controller with nonlinear model of the vehicle whose state vector is;
$x=[x\quad y\quad z\quad p\quad q\quad r\quad \dot{x}\quad \dot{y}\quad \dot{z}\quad \phi\quad \theta\quad \psi]^T$
In order to use this state vector and realize the system I am using 4th order Runge-Kutta with fixed step size to solve the following differential equations;
$[\dot{x}\quad \dot{y}\quad \dot{z}]^T=R[u\quad v\quad w]^T$ where R is rotation matrix from body to inertial frame.
$\dot{p}=\frac{(I_y-I_z)rq+\tau _x}{I_x}\qquad \dot{q}=\frac{(I_z-I_x)pr+\tau _y}{I_y}\qquad \dot{p}=\frac{(I_x-I_y)pq+\tau _z}{I_z}$
$\ddot{x}=\frac{f_t}{m}[sin(\phi)sin(\psi)-cos(\phi)cos(\psi)sin(\theta)$
$\ddot{y}=\frac{f_t}{m}[cos(\phi)sin(\psi)sin(\theta)+cos(\psi)sin(\phi)$
$\ddot{x}=-g+\frac{f_t}{m}[cos(\phi)cos(\theta)]$
$\dot{\phi}=p+r[cos(\phi)tan(\theta)]+q[sin(\phi)tan(\theta)]$
$\dot{\theta}=qcos(\phi)-r(\sin(\phi))$
$\dot{\psi}=r\frac{cos(\phi)}{cos(\theta)}+q\frac{sin(\phi)}{cos(\theta)}$
after each iteration in the Runge-Kutta, I am using the last 3 formulas to convert body rates to Euler rates to generate linear state vector. Is this the correct approach?
This is my differential function; 
function dxdt = RunnableSys(t, x)
global g Q sqrd_wi;

%x -> x y z p q r u v w phi theta psi  
%xdot -> u v w pdot qdot rdot udot vdot wdot phidot thetadot psidot

T = body2inert([x(12) x(11) x(10)]);%%body rate to euler rate
R = eul2rotm([x(12) x(11) x(10)]);%%euler to rotation matrix

mapMatrix = [Q.b*ones(1,4); 0 -Q.L*Q.b 0 Q.L*Q.b; -Q.L*Q.b 0 Q.L*Q.b 0; Q.d*[-1 1 -1 1]];%%rotor speeds to external forces (1vertical thrust 3 Moments)
u = mapMatrix*sqrd_wi(1:4);

dxdt = zeros(12,1);
%%
dxdt(1:3,1) = R*x(7:9);%xdot ydot zdot
dxdt(10:12,1) = T*x(4:6);%phidot thetadot psidot

%%
dxdt(7:9)     = 
[(sin(x(12))*sin(x(10))+cos(x(12))*sin(x(11))*cos(x(10)))*u(1)/Q.m;%xddot
             (-
cos(x(12))*sin(x(10))+sin(x(12))*sin(x(11))*cos(x(10)))*u(1)/Q.m;%yddot
             -g+(cos(x(11))*cos(x(10)))*u(1)/Q.m];%zddot
%%             
dxdt(4:6,1)   = [((Q.J(2)-Q.J(3))*x(5)*x(6)+u(2))/Q.J(1);%pdot
           ((Q.J(3)-Q.J(1))*x(4)*x(6)+u(3))/Q.J(2);%qdot
           ((Q.J(1)-Q.J(2))*x(4)*x(5)+u(4))/Q.J(3)];%rdot
end

and in the Runge-Kutta section;
for i = 2:N
  ti = tspan(i-1); 
  hi = h(i-1);%0.001 0.1
  yi = Y(:,i-1);




 rpms(end+1,:) = sqrd_wi;   


 T = body2inert([yi(12) yi(11) yi(10)]);

  Q.xReal(end+1,:) = [yi(1:3); yi(10:12); yi(7:9); T*yi(4:6)];%% careful
  Q.yReal(end+1,:) = Q.xReal(end, [1 2 3 6]);




  %% RK4


  F(:,1) = feval(odefun,ti,yi,varargin{:});
  F(:,2) = feval(odefun,ti+0.5*hi,yi+0.5*hi*F(:,1),varargin{:});
  F(:,3) = feval(odefun,ti+0.5*hi,yi+0.5*hi*F(:,2),varargin{:});
  F(:,4) = feval(odefun,tspan(i),yi+hi*F(:,3),varargin{:});

  Y(:,i) = yi + (hi/6)*(F(:,1) + 2*F(:,2) + 2*F(:,3) + F(:,4));




  %% controller activated in each Ts for contorller update speed
  if ti == 0

%     sqrd_wi = u0;
  else
   %%
  if mod(ti, Ts) == 0
    %%
    %Y -> x y z p q r u v w phi theta psi  
    %x -> x y z phi theta psi xd yd zd phid thetad psid
    x = Y(:,i);


    T = body2inert([x(12) x(11) x(10)]);
    Q.x = [x(1:3); x(10:12); x(7:9); T*x(4:6)];%% careful used for linear controller
    Q.y = Q.x([1 2 3 6]);

    Q.Controller.UpdateState;
    Q.UpdateStateHistory;

    if ~isempty(Q.Controller.PF)
       ref = Q.Controller.PF.CalculateReference;
    else
       ref = Q.Controller.ref;
    end

    sqrd_wi = Q.Controller.FindInput(ref);      
    end
  end
end
Y = Y.';

my controller does not respond as expected when using this functions for nonlinear model. And I can't find why.
","quadcopter, control"
Simple sensor to detect motion of inanimate objects,"This is for a high school project that I'm doing. The sensor should be able to detect the movement of a moving inanimate object and convert into electrical energy, and be fairly cheap to buy.
I have looked into a few sensors but most of them seem to be geared towards detecting HUMAN movement, eg. PIR sensors. I am looking for the type of sensor that is able to detect non-human movement at close range (about 1/2 metres). Any suggestions? 
",sensors
Understanding inverse kinematics with the Jacobian,"I'm learning about inverse kinematics with Jacobians, and getting a little confused. So, let's say I have a robot arm with two joints with angles Y = (a, b) whose tip I want to move along a certain direction in 2D space X = (u, v). The Jacobian J tells me how much the arm will move in 3D space, with respect to rotations of the joints: J = dY/dX. Then, in order to move the arm in a certain direction, I can find the inverse of the Jacobian J-inv, and then multiply this by the 3D direction I want the tip to move in: X = J_inv * Y.
However, let's say that at a particular joint configuration, the first joint (with angle a) is much more able to move the tip in the desired direction, than the second joint. So, dY/da >> dY/db. Intuitively, it would therefore make sense that greater velocity is given to the first joint than the second joint, to take advantage of this.
But this does not seem to be the case. If X = J_inv * Y, then J_inv will cause a greater response from the joint which finds it harder to move the tip in the desired direction, i.e. the second joint, because J_inv is effectively finding db\dY, which is greater than da\dY.
So why would the joint which finds it harder to move the tip along the desired direction, actually be given a higher velocity than the joint which finds it easier?
","kinematics, inverse-kinematics, jacobian"
IRobot Create 2 Dashboard,"I am trying make an irobot Dashboard for my project. I saw a sample Project in the irobot STEM Website and they have a sample program for it. I am using raspberry pi 3 for my project too. I copied the sample codes to raspberry pi 3. however, i couldn't run it as there is a syntax error popping out. I tried using Python 2 and Python 3 too. How do I fix this Problem?

This what I want to achieve. below are the programming codes used. 
#include ""mbed.h""

Serial device(p9, p10);  // tx, rx

DigitalOut led1(LED1);
DigitalOut led2(LED2);
DigitalOut led3(LED3);
DigitalOut led4(LED4);

// Definitions of iRobot Create OpenInterface Command Numbers
// See the Create OpenInterface manual for a complete list

//                 Create Command              // Arguments
const char         Start = 128;
const char         SafeMode = 131;
const char         FullMode = 132;
const char         Sensors = 142;              // 1:    Sensor Packet ID
const char         SensorStream = 148;         // x+1: [# of packets requested] IDs of requested packets to stream
const char         QueryList = 149;            // x+1: [# of packets requested] IDs of requested packets to stream
const char         StreamPause = 150;          // 1:    0 = stop stream, 1 = start stream
const char         LED_Color = 139;

/* iRobot Create Sensor Paqcket IDs */
const char         BumpsandDrops = 7;
const char         Distance = 19;
const char         Angle = 20;
/* Global variables with sensor packet info */
char Sensor_byte_count = 0;
char Sensor_Data_Byte = 0;
char Sensor_ID = 0;
char Sensor_Num_Bytes = 0;
char Sensor_Checksum = 0;

void start();
void receive_sensor();
//
// Demo to read in sensor data with serial interrupts
//
int main() {
    char Color = 128;
    char count = 0;
// wait for Create to power up to accept serial commands
    wait(5);
// set baud rate for Create factory default
    device.baud(57600);
// Start command mode and select sensor data to send back
    start();
// Setup a serial interrupt function to receive data
    device.attach(&receive_sensor);
// Main program keeps running - it loops sending out commands to change Create LEDs color
// ...Add code to control robot here using sensor data and sending commands
    while (1) {
// Send out a command for different colors on the create LED
        device.putc(LED_Color);
        device.putc(char(0));
        device.putc(char(Color));
        device.putc(char(200));
        Color +=64;
// Send a real command periodically to avoid a safe mode timeout
        if (count==30) {
            device.putc(SafeMode);
            count = 0;
        } else count++;
        wait(1);
    }
}


// Start  - send start and safe mode, start streaming sensor data
void start() {
    // device.printf(""%c%c"", Start, SafeMode);
    device.putc(Start);
    device.putc(SafeMode);
    wait(.5);
    //  device.printf(""%c%c%c"", SensorStream, char(1), BumpsandDrops);
    device.putc(SensorStream);
    device.putc(1);
    device.putc(BumpsandDrops);
    wait(.5);
}

// Interrupt Routine to read in serial sensor data packets - BumpandDrop sensor only
void receive_sensor() {
    char start_character;
// Loop just in case more than one character is in UART's receive FIFO buffer
    while (device.readable()) {
        switch (Sensor_byte_count) {
// Wait for Sensor Data Packet Header of 19
            case 0: {
                start_character = device.getc();
                if (start_character == 19) Sensor_byte_count++;
                break;
            }
// Number of Packet Bytes
            case 1: {
                Sensor_Num_Bytes = device.getc();
                Sensor_byte_count++;
                break;
            }
// Sensor ID of next data value
            case 2: {
                Sensor_ID = device.getc();
                Sensor_byte_count++;
                break;
            }
// Sensor data value
            case 3: {
                Sensor_Data_Byte = device.getc();
                Sensor_byte_count++;
                break;
            }
// Read Checksum and update LEDs with sensor data
            case 4: {
                Sensor_Checksum = device.getc();
                // Could add code here to check the checksum and ignore a bad data packet
                led1 = Sensor_Data_Byte &0x01;
                led2 = Sensor_Data_Byte &0x02;
                led3 = Sensor_Data_Byte &0x04;
                led4 = Sensor_Data_Byte &0x08;
                Sensor_byte_count = 0;
                break;
            }
        }
    }
    return;
}

",irobot-create
how to move my robot to the assigned coordinates,"I am working on an ground surveillance robot using an Arduino mega for programming, am using components like the HMC5883L compass, Adafruit GPS for assigning of coordinates (latitude and longitude) which are the way points, I have written up the code for both the compass and the GPS and am able to get information from them, but now what I want for my robot to move to those specified coordinates (latitude and longitude waypoints) which I don't know how to do if anyone could just write an example code for me or push me to the write place I could get a sample code please do pardon me for asking such question cause I am new to coding GPS and compass and I would appreciate it if anyone could help me out or explain a bit in details what I need to do please find my code here http://textuploader.com/drqwv
","arduino, c++, programming-languages, c"
Pioneer 3D-X Simulator,"Can I get the Pioneer 3D-X simulator more precisely Mobilesim to work without me having and connecting the robot to the laptop? if yes, how can I do that? 
Thank you!
","mobile-robot, simulation, simulator"
Which is the best visual fiducial marker (2D barcode)?,"Which is the best visual fiducial marker (2D barcode) for detection and robust and accurate pose estimation? 
Im not looking for a fiducial marker which can store lot of information. The main goal is just to get the pose of the marker with respect to the camera as accurate as possible. 
","computer-vision, opencv, pose"
Robot Veering to the Left,"I'm currently programming in RobotC, for a Vex 2.0 Cortex. I'm using encoders to make my robot go straight.
This is my code:
#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, dgtl2,  ,               sensorDigitalIn)
#pragma config(Sensor, dgtl7,  ,               sensorDigitalOut)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port1,           RM,            tmotorVex393_HBridge, openLoop, reversed, encoderPort, I2C_2)
#pragma config(Motor,  port10,          LM,            tmotorVex393_HBridge, openLoop, encoderPort, I2C_1)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

/* Port 1 is right motor*/

//all functions expect no reverse motors (in port menu)
//**GLOBAL VARIABLES**
int buttonSTATE = 0;

//**MOVE FUNCTIONS**
void goforwards(int time)
{
    int Tcount = 0;
    int speed1 = 30;
    int speed2 = 30;
    int difference = 5;


    motor[LM] = speed1;
    motor[RM] = speed2;
    while (Tcount < time)
    {
        nMotorEncoder[RM] = 0;
        nMotorEncoder[LM] = 0;

        while(nMotorEncoder[RM]<3000)
        {
            int REncoder = -nMotorEncoder[RM];
            int LEncoder = -nMotorEncoder[LM];

            if (LEncoder > REncoder)
            {
                motor[LM] = speed1 - difference;
                motor[RM] = speed2 + difference;    
            }
            if (LEncoder < REncoder)
            {
                motor[LM] = speed1 + difference;
                motor[RM] = speed2 - difference;
            }
            wait1Msec(100);
        }
        Tcount ++;
    }
}

//**CONTROL STRUCTURE:**
task main()
{

    goforwards(1);
}

When I execute the code, the Robot's encoder values are very close, but the robot quickly starts to veer to the left. What are possible causes of this? Is it something in the code?
","mobile-robot, movement, robotc"
UDP data reception in ROS,"I am new to ROS programming, I need quick fix for a small problem.
I need to send data about position of a object from a Non-ROS computer(Raspberry Pi 3) to a ROS enabled computer(Lenovo pc) over a Wi-Fi network.
So I wrote a program in Rpi to send data over UDP, but I don't how to write a 
ROS node to receive and publish this data.
Can anyone pointout  resources that will help me.
Thanks in advance
",ros
How to program a continuous servo motor? (Arduino),"I need to use continuously rotating servo for a camera stabilization system. My professor bought servos that have already been modified for continuous motion--there's no stop in the gears, and the potentiometer allows it to spin 360+ degrees.
I am currently using PWM with an Arduino Uno. The servo does spin continuously, but not in a stable way. I've also taken out the potentiometer in another one of the servo, and on a third servo I used a voltage divider in place of the potentiometer. 
I've tried static values and a ""sweep"" from 0% duty cycle to 100% to get a feeling for how they work, but I just cannot figure it out. I greatly would appreciate any tips on this.
Here is my code:
 //PWM test for continious motion Servo

 int servoPin = 9; // connect servo to pin 10
 int pwmVal = 0; // declare pulse width modulation value

void setup(void) {
  pinMode(servoPin, OUTPUT); //set up the servoPin as an output pin
  Serial.begin(9600); // begin serial monitor
}

void loop(void) {

  //for loop that sweeps values from 0 to 255
  for (pwmVal = 0; pwmVal <= 253; pwmVal += 1) {
    analogWrite(servoPin, pwmVal);
    Serial.println(pwmVal);
    delay(100);
  }
  for (pwmVal = 253; pwmVal >= 0; pwmVal -= 1) {
    analogWrite(servoPin, pwmVal);
    Serial.println(pwmVal);
    delay(100);
  }


  //assign a static pwm value
  pwmVal = 0;
  analogWrite(servoPin, pwmVal);
}

","arduino, servomotor"
Thermal Imaging camera activation upon detection,"So I am planning on building a robot that turns on when it detects some kind of heat source, I am currently looking at thermal imaging cameras, but am not sure as to how to go about writing code to send a ping or some sort of message when the camera detects a heat source.
Does anyone know of any way to do this?
Thanks 
",wheeled-robot
MPU6050: Changing Gyro Output Range Throws off Positional Reading,"I am using the MPU6050 in conjunction with an Arduino, and I would like for the gyro rate outputs be more precise than the default setting, which is 1/16.4 of a degree (+/-2000 deg/sec range). The gyro outputs can be changed in setup(){} using mpu.setFullScaleGyroRange(uint8_t range). To get the more precise values, I passed in MPU6050_GYRO_FS_500 to get a range of +/-500 deg/sec, and a precision of 1/131 of a degree.
In this project, I also need the YPR position, which I obtain through mpu.dmpGetQuaternion(&q, fifoBuffer) and mpu.dmpGetGravity(&gravity, &q) and mpu.dmpGetYawPitchRoll(ypr, &q, &gravity).
That's all easy, but the problem is that when I change the gyro output range, it messes up the YPR output. What happens is the YPR position changes drastically when the MPU is being rotated, and once the MPU is held still again the YPR catches up slowly and levels off to the actual value. I think there is an error in the filter that combines the GYRO and ACCEL data to get the rotational position. Maybe the DMP is dividing the GYRO rate data by the default sensitivity factor (16.4) when it should be dividing by the new one (131)? It just looks to me like the gyro data is too sensitive when position is being determined.
How can I get accurate YPR readings without delay?
Here is a screenshot of a program I wrote. The blue line is the gyro rate data, and the pink line is the Roll Position of the MPU. The graph shows two rotations of the MPU.

","quadcopter, arduino, imu"
bounded deviation for straight line motion,"I came across the paper(link given below) which discusses about bounded deviation joint path for straight line motion. 
Planning and Execution of Straight Line Manipulator Trajectories (RH Taylor)
https://pdfs.semanticscholar.org/e01a/58608f4e68f31c7b9e7cdbddceae645727bb.pdf
In this method, the assumption is that the maximum deviation happens at or near the midpoint between the start and end point. 
1) Is this assumption true in all cases? 
2) Even if the assumption may not be true, will resulting trajectory be a straight line if this method is used for trajectory planning?
I hope someone shed some light on this. Thank you.
",motion-planning
Drift in Integrating Angular Acceleration to Angular Velocity using two accelerometer data,"I am trying to integrate Angular acceleration obtained from a set of accelerometers positioned specifically at opposite corners of a cube, based on ""EcoIMU: A Dual Triaxial-Accelerometer Inertial Measurement Unit for Wearable Applications"" paper.
I am getting the angular acceleration on each Axis 
This signal is quite noisy .
Then after integrating angular acceleration to angular velocity using trapezoidal rule , I get signals which drifts heavily and randomly.
I understand that noise and also numerical integration is causing the effect. Other than low pass filtering the data, is there any other methods to reduce noise. 
And the major factor for the drift is numerical integration, how can this be handled.
Please help me out with this. 
","accelerometer, integration"
What does Simultaneous Localization And Mapping (SLAM) software do?,"I took a course to have a better understanding of drones and their design. At the end of the course there was a test question that I got wrong and I would like to understand why. 
I was supposed to select the choices that best describe SLAM.
and the possible answers were:

Estimates the location of features in the environment? 
Controls the robot's flight through the environment?
Causes the robot to avoid obstacles in the environment?
Navigate in a cluttered environment?
Estimates the position and orientation of the robot with respect to
    the environment?

At first I knew that at least 3 and 4 were right because I watched a drone doing these things. I also thought that the last answer was linked to these two so I said yes to it too. Finally, I thought that the only thing that was still controlled by the user would be the flight...
Yet I failed again... Therefore what does Simultaneous Localization And Mapping (SLAM) software do?
",slam
Using the Create2 Tethered Driving program- inputting a string of keyboard presses,"I'm attempting to do a quick and dirty autonomous path with the create2. I'm using the tethered driving program seen here.
I set my own buttons just to make it rotate 90° and go forward one ""pulse"". I'd like to know, does anyone have any ideas on how to trick the attached Create2_TetheredDrive.py into thinking it's seeing a series of keyboard entries?
","irobot-create, path-planning"
Finding the Center Field of View for a UAV,"I am working on a project involving a UAV where I am given a 3D space with terrain information as well as the location of the UAV in the 3D space and the orientation of the UAV camera. My job is to locate the point on the terrain where the UAV camera is looking. I am looking for help determining what type of algorithm to use to project the ray from the camera onto the terrain to determine where they intersect.
","cameras, uav"
"Cleanflight : 4 first channels not working and ""Failed to open serial port""","I just finished assembling my new 250 quad which is equipped with an SP F3 Flight controller.
I think that I kinda bricked my FC in the first 10 minutes of configuring it:
I first plugged it in, installed a few drivers and opened cleanflight
Cleanflight recognized it, no problem for now.
I then tried to test the RX channels and discovered that apart from the 4 AUX channels, none of them worked, I swapped the plugs on the transmitter and saw that the RX was OK, it's the FC that didn't show anything on the first 4 channels
After reading some stuff about upgrading the firmware I did something completely wrong, I followed the first 10 seconds of this guide, using baseflight... though my board isn't baseflight it's cleanflight, and I did the mistake of clicking the ""Flash firmware"" button which then made it impossible to use it and flash it.
After some trial and error I finally had a problem where cleanflight tells me that it cannot flash the firmware on it because ""Failed to open serial port""
I used Process Explorer to check if any program was using and holding the COM3 port but no
No Program was using it.
Thank you for reading this very long and probably stupid question, do you have anything that I can try?
","quadcopter, serial"
"Detecting ""zero"" when turning a potentiometer from a stepper motor","We're using a stepper motor to control a knob on a piece of audio equipment.  The stepper is coupled to the potentiometer shaft with a simple coupler. 
Right now, in order to get to ""zero"" on the potentiometer, we ""crank"" the stepper motor -3600 steps (about one full rotation).  
This creates unnecessary torque on the pot. I'm looking for a hardware solution to avoid this.  Here are a few ideas I had:

Some sort of zero detection - Know when the knob gets to 0 and stop the stepper from turning when it reaches that point.
A coupler that can detect excessive force on the shaft.  Basically some sort of spring loaded coupler that will ""click"" when the rotation gets to ""zero"" and close a circuit.

Also open to other ideas...
We're currently using Arduinos with Firmata firmware connected to Node.JS, but this is mostly a hardware issue.
",stepper-motor
What's wrong in controlling Roll+Roll rate in a quad-copter instead of using 2 separate loops for each?,"I am curious to know why we can't apply control algorithm like PID on the weighted signal of Roll and Roll rate in a quad-copter instead of using two loops to control them independently. Fundamentally PID will make the input signal to approach $0$. In the case of (Roll + Roll-Rate), which would be $ w_1\theta + w_2\dot\theta$, the sum becomes $0$ when individually both tend to $0$; since we have an exponentially decaying curve.
So why do people generally use sequential loops to control each of them separately? (Roll is just for example)
","quadcopter, control, pid"
How to create Matlab Simulation for Kinematics of Differential Drive,"I studied the forward and inverse Kinematics of the robot and got a clear understanding. I am in the progress of developing my matlab simulation for a two wheeled differential drive robot. The robot moves in a straight line and has been integrated with PID. I want to show by the animation the movement of the robot.
Equation is, 
The Vector in Initial Frame = Rotation Matrix Inverse x Vector in Robot Frame

My Rotational Matrix is, [0 -1 0; 1 0 0; 0 0 1] since the angle is 90.

The Robot Frame is [a; b; c] 
where a = Total translational Speed = (r x w1)/2 + (r x w2)/2
      b = In y direction = 0
      c = Total Rotational Speed = (r x w1)/2l + (r x w2)/2l

where l = 0.12 and r = 0.033 and w1 and w2 are angular velocities of wheel 1 and 2.

I have w1 and w2 data in a file as 
w1 1 2 3 4 5 6 8 9
w2 1 3 4 5 6 7 8 9

I want to run an algorithm in such a way, Mat lab runs the equation and calculate the values of Total translational Speed and Total angular speed in the world frame and plot the graph. I also want to make an animation in such a way a box moves according to this equation. How can I do that? I can run it for one time if I input one value for w1 and w2, But not continuously. Help. Thanks Much.

","kinematics, inverse-kinematics, matlab, simulation, forward-kinematics"
No luck communicating with my Create 2 using an Arduino,"I'm trying to simply read values from my Create 2 using an Arduino Uno.  Using the code below that I found online from a workshop, Arduino and iRobot Create - Prof. Fabian Winkler.   
Connection to Create2 is using the recommended 'Diode Drop/Serial Cable' from the Arduino Tutorial.  The Arduino seems to be behaving fine but, no matter what I do, I get 0 for any sensor values I try to read.  I have changed the Create baud rate to 19200 using the hold on button down procedure.    
Any help would be greatly appreciated.
#include <SoftwareSerial.h>

#define rxPin 10
#define txPin 11
// set up a new software serial port:
SoftwareSerial softSerial = SoftwareSerial(rxPin, txPin);

int inByte = 0; // incoming serial byte

void setup()
{
  delay(2000); 

  pinMode(rxPin, INPUT);
  pinMode(txPin, OUTPUT);
  pinMode(12,OUTPUT);  

  softSerial.begin(19200);

  Serial.begin(19200);
  softSerial.write(128); // This command starts the OI.
  softSerial.write(131); // set mode to safe (see p.7 of OI manual)
 }

 void checkBumpSensors() {
   char sensorbytes[10]; 


 softSerial.write((byte)142); // get sensor packets
 softSerial.write((byte)1); // sensor group packet ID 1, size 10

 delay(64);
 // wipe old sensor data
 char i = 0;

 while (i < 10) {
    sensorbytes[i++] = 0;
 }

 i = 0;
 while(softSerial.available()) {
   int c = softSerial.read();
   sensorbytes[i++] = c;
  }

 int bumpRight = sensorbytes[0] & 0x01;

 int bumpLeft = sensorbytes[0] & 0x02;

 Serial.print(""Right "" + bumpRight);
 Serial.print("" "");
 Serial.println(""Left "" + bumpLeft);
}


void loop()
{
 checkBumpSensors();  

 softSerial.write(142); // requests the OI to send a packet of

 softSerial.write(9); // request cliff sensor value specifically
 delay(250); // poll sensor 4 times a second

 if (softSerial.available() > 0) {
  inByte = softSerial.read();
}

 Serial.println(inByte);

 digitalWrite(12, HIGH);   // turn the LED on (HIGH is the voltage level)
 delay(500);              // wait for a second
 digitalWrite(12, LOW);    // turn the LED off by making the voltage LOW
}

Update to original.  I got frustrated trying to use #include  since nothing seemed to be communicating and so I simply used pins 0 and 1 on the arduino.  good news is that using my modified code below I have now began to see some response.  Bad news... I don't know what or why I'm getting the output I am.  Once I hit the one button on the create2 the Serial.available returns 63 and decrements after each iteration of the loop.  The inbyte variable returns a decimal value and seems to be a data packet based on the serial.available.  Is this a data packet?   Also note that I commented out Serial.write(128) and Serial.write(131) and it still runs??  Here is what the serial monitor looks look when I hit the on key on the create2
100    63     120
100    62      9
100    61      0
100    60      26
.....
int analogValue;
   int inByte = 0;  
void setup()
{
  pinMode(4, OUTPUT);  

  delay(2000); // Needed to let the robot initialize
  // start hardware serial port 
  Serial.begin(19200);

 //Serial.write(128); // This command starts the OI. 
  //Serial.write(131); // set mode to safe (see p.7 of OI manual)

}
void loop()
   {
 // read the analog input on pin 0:
 analogValue = analogRead(A0);

 digitalWrite(4, HIGH);
 delay(250);  
 digitalWrite(4, LOW);
 delay(250);  

 // print it out in many formats:
 Serial.print(analogValue);         // print as an ASCII-encoded decimal
 Serial.print(""\t"");                // print a tab character

 //Serial.write(142);  // requests the OI to send a packet of 
                      // sensor data bytes
 //Serial.write(9);  // request cliff sensor value specifically
  delay(250); // poll sensor 4 times a second

 Serial.print(Serial.available());  

  Serial.print(""\t"");                // print a tab character

 if (Serial.available() > 0) {
   inByte = Serial.read();
   digitalWrite(4, HIGH); 
   delay(2000);
   digitalWrite(4, LOW); 
   delay(1000);

 } 
Serial.println(inByte);
Serial.println();  

// delay 10 milliseconds before the next reading:
delay(10); 

}
",arduino
Grid based SLAM using BeagleBone Green Wireless and RPLIDAR,"I've written a grid based DFS algorithm with a PID-based steering system to maneuver a 30cm2 square-grid maze all in Python. The robot is a 4 wheel drive with an approximate size of 20 cm. The robot has a BeagleBone Green Wireless controller which is connected by USB to the RPLIDAR A1. 
At this current moment, the robot is underutilizing the LIDAR and I want to begin to learn SLAM. However, the environment is highly predictable which I think makes a full SLAM counterintuitive. I would also like the code to be low CPU strain. 
I've seen people converting a conventional SLAM into a grid based but only after the calculations are complete. I was wondering if there is a way to do a Grid Based SLAM right from the start (assume its position and map with a grid). 
Accuracy isn't hugely important here as long as it understands a tile and the robot is able to avoid walls.
Any advice, tips or suggestion is appreciated. How would you store the map? How would you locate the position of the robot? How would you map the LIDAR's values?
","mobile-robot, localization, slam"
Communication between Intel Edison and Open Mv7,"I'm trying to do a simple communication between an Intel Edison and Open Mv 7 Camera through UART (Tx/Rx). I assumed this would be a simple task but both sides receive ""�"". Both are using python, the Intel Edison using  Pyserial to communicate while OpenMv7 is using PYB library. 
Open Mv7.
import time
from pyb import UART

uart = UART(3, 9600)


while(True):
    if(uart.any() > 0):
        print(uart.read())

Intel Edison
import serial
import time 

ser = serial.Serial(port = ""/dev/ttyO1"", baudrate=9600)
ser.close()
ser.open()

print(""Online"")

ser.write(""Hello World!"")

","serial, communication"
Compatibilty of my setup?,"I'm building my first quadcopter, and these are the components I intend to buy:

Motor: EMAX BL2212 1400 KV Brushless Outrunner Motor around 0.9 kg thrust: 
Flight Controller: Multiwii V2.5 Flight Controller 
Propellers: I don't know which one to get: fut-electronics propellers collection 
GPS: Skylab UART GPS Module SKM58   (Small Form Factor)
Radio Communication: Radio Telemetry 915 Mhz (3DR), is there an affordable alternative to buying a radio telemetry maybe using Wi-Fi?  
ESCs: 4x1 ESC (4x25A) - Speed Controller for Quadcopter 
Battery: I don't know which one to choose

My questions are:

Are the components compatible?
What battery to choose?
If I'm not planning to do GPS planned missions, would the GPS be important for anything else?

By the way I intend to attach a camera or a smart-phone to it for video capturing I think it is about an extra 200 grams.
","quadcopter, multi-rotor, uav"
Open-source software that supports multi robot simulation and navigation capabilities,"I am interested in knowing a software platform which deals with multi-robot navigation and:-

has implemented local  collision  avoidance  algorithms for each robot to prevent collisions between robots
has independent controllers such as PID for each robot

Most likely such a framework will be in ROS or python. I am interested in open source codes such as those in github. The goal is to have a platform where a set of way-points can be assigned to each robot and they can avoid collisions and reach their way-points. At least this much ability should be already implemented.
","control, ros, motion-planning, simulation, multi-agent"
Why does the motor only run between 170-180?,"I'm using a brushless motor of 1000kv and an ESC simon30A. I power the motor using arduino adapter AC/DC 9v. When I try to run the motor using sweep example. The motor will only run when the value is between 170-180. How come other values below 170 won't run the motor? Is my ESC broken?? 
The model of bldc is A212/3T. 1000kv and ESC is Simon 30A . I'm using servo Library to control the speed.
",arduino
Would the quad fly better with a 3S or 4S based on the weights of the batteries?,"So the questions for the title is a bit hard to explain, so here is a better explanation of my question. 
So I want to get into the FPV quad flying hobby so I decided to pick up a very starter quad, it's the Eachine Wizard X220. From the specs of the quad its lift off weight is 535 grams and the quad itself without the battery weighs in at 364 grams. So I just need to pickup a battery everything else is accounted for. I want to get a 4S battery 1500 mAh which is 160 grams but that brings the total weight to 524. 
So, my question is would the quad not be as efficient with this weight. Or should I just stick a 3S 1300 mAh and have a total weight of 484 grams? 
",quadcopter
Is there an open source autonomous vehicle simulator with control over traffic?,"As a part of my research work I'm supposed to build or use an existing autonomous vehicle simulator, in which I can also control the behaviour of the traffic in the system, I was searching on Google and posted on reddit to check if there is any such simulator (open source), After all the exhaustive attempts, I haven't found anything yet.
I'm trying to build this in V-rep or any other ROS compatible simulators. The end product has to look something like the one shown here, YouTube - Webots: The autonomous vehicle simulator, and would also give control over the traffic behaviour. Any help/suggestion is greatly appreciated.
PS: I do know that there's this one written in Unity, Github:A self-driving car simulator built with Unity (An Open Source Self-Driving Car), but we are looking to import OSM data and also Unity doesn't have a good ROS integration mechanism.
","motion-planning, navigation, simulation, autonomous-car"
Length and Width of a Line Following Robot,"I'm building a line following robot. I have made different chassis designs. The main prototype I'm using is a rectangle base. At one side motors are placed. On the other side of the rectangle caster wheel is placed in the middle. Look at the following image.

By varying the values of distance, I have seen that the stability of the robot is varying rapidly. 
I'm driving the robot using PID. I have seen that for some chassis designs it is very hard(sometimes impossible) to calculate correct constant values. And for some chassis it is very easy. By the word stability I meant this. I have a feeling that the robot dimensions, distance values and that stability has a relationship..
Is there an equation or something that can be used to estimate the value of the distance when the width of the robot is known..?
Other than that is there a relationship between robot weight and diameter of the wheel or robot dimensions and the diameter..?
Thanks for the attention!!
","arduino, motor, pid, line-following"
iCreate 2 with Arduino (Just getting going),"I am new to the iRobot Create 2 but I do know a thing or two about the Arduino (don't assume too much though). However, in this case, I am beyond stumped over what I am sure is something simple but is somehow not obvious to me. Three people have confirmed my wiring from the Create 2 to the Arduino to be correct and the code I have looks similar to many examples that I have seen on this forum. However, I cannot get my Create 2 to do ANYTHING. I am not at all sure what is wrong and I am starting to wonder if the robot is even receiving commands let alone doing anything with them. Is there anything wrong with this code and can anybody suggest a way to verify that the robot is receiving data (since it does not beep or provide return messages)? Thank you.
EDIT (06/24 01:10 EST); Updated code (with a few notes).
#########################
#include <SoftwareSerial.h>
#include <SPI.h>

int baudPin = 17;
int i;
int ledPin = 13;
int rxPin = 19;
int txPin = 18;

unsigned long baudTimer = 240000; // 4 minutes
unsigned long thisTimer = 0;
unsigned long prevTimer = 0;

SoftwareSerial Roomba(rxPin, txPin);

void setup() {
  pinMode(baudPin, OUTPUT);
  pinMode(ledPin, OUTPUT);
  pinMode(rxPin, INPUT);
  pinMode(txPin, OUTPUT);

  // I have tired communicating with both baud rates (19200 and 115200).
  // When trying the 115200 baud, I set ""i<=0;"" in the loop below since
  // the pulse does not need to be sent.
  Roomba.begin(19200);
  Serial.begin(115200);
  delay(2000);

  // I hooked up an LED in series with the baudPin so that it would turn
  // off when low thus giving me some kind of visual confirmation that a
  // pulse is being sent. See additional note in loop() below.
  for (i = 1; i <= 3; i++) {
    digitalWrite(baudPin, HIGH);
    delay(100);
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
  }

  // I know this might not be the right way to send data to the robot,
  // but I was fiddling with this while trying to figure out a separate
  // problem regarding the TX/RX lines which I am putting off until I
  // get the baud issue straightened out.
. /*
    int sentBytes = Roomba.write(""128"");
    Serial.print(sentBytes);
    Serial.print(""\n"");
  */

  i = 0;
}

void loop() {
  thisTimer = millis();

  // The LED that I have hooked up in series with the baudPin blinks
  // when the pulse is low, thus indicating that a pulse is being sent.
  // However, it only seems to wake the robot when it is asleep. If the
  // robot is already awake when the pulse is sent, it has no affect and
  // the robot will fall asleep a minute later.
  if (thisTimer - prevTimer > baudTimer) {
    prevTimer = thisTimer;
    i = 10;
    Serial.print(""Sending pulse...\n"");
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
  }

  /*
    i++;
    Serial.print(prevTimer);
    Serial.print("" --> "");
    Serial.print(thisTimer);
    Serial.print("" --> "");
    Serial.print(i);
    Serial.print(""\n"");
    delay(1000);
  */
}
#########################

","arduino, irobot-create"
Can we specify the working zone for the robot using SLAM approach for navigation?,"I am using SLAM for the autonomous vehicle navigation.
I want to the specify the working zone for the vehicle before the start of navigation, or can we restrict the working zone of the vehicle using SLAM approach for navigation.
",slam
Kuka KR16L-2 robot simulation base and wrist rotation inconsistent with original robot,"The issue is regarding simulation of Kuka robot KR16L6-2 in MATLAB using Robotics toolkit by Peter Corke. I wish to simulate  the kinematic before passing a command to real robot for motion.
I have attached the DH-Parameters. Apart from this I have also tried many other combination of orientations but to no useful effect.

The problems is that the robot base rotates counter-clockwise by default for positive increases in Joint1, while the original robot moves in the opposite direction. Similarly for the wrist roll, i.e. Joint 4, the direction of simulation is reversed.
In order to confirm that it's not my mistake only, I searched for similar ready made simulation software. Although it did not include the same KUKA robot, a similar variant (KUKA_KR_5_sixx_R650) was available. Hence, KUKA_KR_5_sixx_R650 had one set of motions for base and wrist in RoKiSim v1.7 for positive increases in joint angle and reverse motion in roboanalyzerv7  .

NOTE: Only the rotation of J1 (base) and J4 (Wrist Roll) are reversed
and I want to recreate the results of RoKiSim v1.7 in Matlab where rotations are similar to the real world robot spec provided by KUKA.
","matlab, industrial-robot, simulation, kuka"
Error when using Matlab Function block in Simechanics (nested functions and parsing of Matlab function both error),"I create a simple simulink model using Matlab Function block to calculate angles of 4 dof robotic arm using Robotic toolbox Matlab while input is x, y and z values. I used forward kinematics to calculate angles. Model and error is shown in below images:


My code is given below (which is put in matlab funcrion block):
function [theta1,theta2,theta3,theta4]=invkin(px,py,pz)
% code to find inverse kinematics solution for 4 dof rootic arm
preach = [px py pz]; % reach point of end-effector
theta1 = 0;  theta2 = 0; theta3 = 0; theta4 = 0;
% create links using D-H parameters
L(1) = Link([ 0 0 0 pi/2 0], 'standard');
L(2) = Link([ 0 .15005 .4318 0 0], 'standard');
L(3) = Link([0 .0203 0 -pi/2 0], 'standard');
L(4) = Link([0 .4318 0 pi/2 0], 'standard');
%define link mass
L(1).m = 4.43;
L(2).m = 10.2;
L(3).m = 4.8;
L(4).m = 1.18;
%define center of gravity
L(1).r = [ 0 0 -0.08];
L(2).r = [ -0.216 0 0.026];
L(3).r = [ 0 0 0.216];
L(4).r = [ 0 0.02 0];
%define link inertial as a 6-element vector
%interpreted in the order of [Ixx Iyy Izz Ixy Iyz Ixz]
L(1).I = [ 0.195 0.195 0.026 0 0 0];
L(2).I = [ 0.588 1.886 1.470 0 0 0];
L(3).I = [ 0.324 0.324 0.017 0 0 0];
L(4).I = [ 3.83e-3 2.5e-3 3.83e-3 0 0 0];
% set limits for joints
 L(1).qlim=[deg2rad(-160) deg2rad(160)];
 L(2).qlim=[deg2rad(-125) deg2rad(125)];
 L(3).qlim=[deg2rad(-270) deg2rad(90)];
 L(4).qlim=[deg2rad(-170) deg2rad(110)];
   %build the robot model
  rob = SerialLink(L, 'name','Puma56');
  qready = [0 -pi/4 pi/4 0]; % initial position of robot
T1= transl(preach); % convert of reach point of 4x4 homogenous matrix
[qreach,err,exitflag] =  rob.ikcon(T1, qready); % find inverse kinematics with error
%rob.fkine(qreach);
theta1 = qreach(1,1);  theta2 = qreach(1,2); theta3 = qreach(1,3); theta4 = qreach(1,4);
end

How to solve these errors. 
Thanks.
","robotic-arm, matlab, simulation"
Modified DH Parameters for Puma 560 - Wrong Results,"In continuation of my question on modified parameter for Puma 560 posted here Modified DH Parameters for Puma 560. Further I used a available dimension for Puma 560 here (FYI: the figure shows dimension in inches, but all following dimensions in DH parameters for length are converted to mm), [ and trial version of RoboDK simulator to check my result. I assigned the frame as show in the figure from first link, the last link is placed to the flange in the figure below with z6 pointing downward while keeping x6 in same direction as x5. 
So the DH parameter looked liked, 
    double alpha[6] = { 0, -90, 0, -90, 90, -90 };
    double a[6] = { 0, 0, 431.80, 0, 0, 0 };
    double d[6] = { 0, 0, 139.7, 433.07, 0, 55.88 };
I started with joint angles (theta) 
double theta[6] = { 0, 0, 0, 0, 0, 0 };
My calculations give me same position as Puma 560 simulator except z values are negatives. Correct position for joint angles of zeroes is x = 431.800, y = 139.700, z = 489.580. I get x = 431.800, y = 139.700, z = -489.580.
But if I put double d[6] = { 0, 0, 139.7, -433.07, 0, -55.88 }; then I get the correct value, x = 431.800, y = 139.700, z = 489.580.
I tested with other joint angles, for which I am getting correct values with negative -433.07 and -55.88 in above. So, they must be negatives.
My question is, why I have to take negative values for d to get correct result? Does this is because the value in this case should be assigned as per base frame (all value above base frame is assigned positive and all below should be assigned negative irrespective of convention.)
Base frame is located at same position as frame 1 (refer to the top most link) I used the same procedure as described in ""Introduction to Robotics"" by J.J. Craig. 
EDIT: below is the codes I am using for the computation. Alpha and theta is converted to radians.
    ////// Craig Matrix - Modified DH Parameters Convention
    mat[0] = cos(theta);
    mat[1] = -1 * sin(theta);
    mat[2] = 0;
    mat[3] = a;

    mat[4] = sin(theta) * cos(alpha);
    mat[5] = cos(theta) * cos(alpha);
    mat[6] = -1 * sin(alpha);
    mat[7] = -1 * sin(alpha) * d;

    mat[8] = sin(theta) * sin(alpha);
    mat[9] = cos(theta) * sin(alpha);
    mat[10] = cos(alpha);
    mat[11] = cos(alpha) * d;

    mat[12] = 0;
    mat[13] = 0;
    mat[14] = 0;
    mat[15] = 1;

Above codes will give a 4 x 4 DH transformation matrix for each frame as per Modified DH conventions (Refer to Modified DH parameters on Wikipedia or J.J. Craig book, the matrix is defined there.)
Now, we multiply all matrices as per, 
starting with matrix for frame 6 on the right hand side and multiplying it with preceding joint from left hand side. Repeating the same sequence as noted above. This should give as the location of origin of frame 6 with respect to the base frame of the robot.
",dh-parameters
Denavit-Hartenberg Humanoid robot,"In humanoid robot, Which way we determine DH Parameter:

DH parameter for each leg separability.
Or, DH-Parameter for two leg together.

and if I can calculate from any way, what about trunk how can insert it in DH calculation?
","forward-kinematics, dh-parameters"
Electronic speed controllers and total current draw,"Do we have to take Electronic Speed Controllers (ESC) into account when calculating the total current draw of the system?
As far as I can understand, the motor and ESC are connected in series, so we take the component that draws the most current, which is the motor in this case. Then, all combinations of a motor and ESC are connected in parallel, thus the currents add up. Am I thinking right? Or am I missing something here?
","esc, current"
6DOF Robot Dynamics from Newton-Euler Iterative Algorithm,"I don't know if someone can help me with this but I'm calculating the dynamics of a 6DOF robot using the Newton-Euler iterative dynamics algorithm. I'm following the recursive method (inwards and outwards) explained in the book Introduction to Robotics Mechanics and Control (Pages 175-176). After putting down on MATLAB the calculations, I started to check if the gravity compensation, g term, made sense. I had calculated the gravity term from the Lagrange approach before so I knew the set of torques had to be the same for a specific pose. Although the values are almost similar (one actuator has some considerable deviation, still unknown to me as to why). Now, here's the thing: the robot is the Kinova JACO v2 arm, and if one assumes that only the gravity effect is taking place,  no torque is assumed for the first actuator (its associated link is at the base). Indeed this is visually clear, and the Lagrangian approach based on the potential energy corroborates that, giving me a torque vector with no torque being sent to the first actuator. 
My problem is just that... The Newton-Euler iterative algorithm is based on the balance of the forces between the links. And since the contributions of the forces are summed up (when performing the outwards calculations) the torque sent for the actuator 1 is not zero and has actually the value that would be sent to the actuator 2. Basically a ""shift"" was made, and the torque for actuator 1 should've been for actuator 2, and so on.
I don't know if you can get any insight from this... But I've tried to recheck my calculations and I can't seem to find any problem with them... Please if you have any suggestions I'll be grateful.
Thanks.
","control, robotic-arm, dynamics"
Extended Kalman Filter for IMU,"I am trying to use a multilayer perceptron to make a flight controller for a ROV. I have a MPU-9250 IMU and I need to remove the noise from the sensor before I can train my MLP. The IMU has a accelerometer, gyro and a magnetometer. I know my state vector is supposed to be 
[acc_x, Acc_y, Acc_z, gyro_x, gyro_y, gyro_z, mag_x, magy, mag_z]

I am not sure about the rest. Since I want the clean Acc, gyro and mag's x, y and z values can I just use an identity matrix for everything in EKF?
I want to pass the IMU data to the MLP after cleaning it and try to predict the control signal for all my thrusters. For my dataset I recoded the IMU information from a ROV2 and I also recorded the pwm signals for each thruster. If it does work I won't be able to control things like PID values of the thrusters but for now I don't really care about it.
",ekf
How to go on designing the water thruster system for water surface vehicle?,"I am an electronics student so I am new to this sort of mechanical stuff. Any help will be appreciated.I am building a water surface vehicle. I am facing a problem/challenge on how to make it move in water. What I have is dys 1100kv brushless motor as shown below and a 2200 mah 25 C lipo battery.


So I have following questions for you :

How to choose the kv rating for brushless motor? (I choose 1100kv motor because I have read somewhere lower kv rating give more torque and I thought I will require more torque as it has to run in water)
It seems to me the propeller (the green one) as shown above are not good for generating thrust in water(although they through enough air outside). I looked online and found these propellers. Don't know if I need something like this?

Also do I need to enclose the propellers inside a cylinder like this? Why they use this type of enclosing? Does it concentrate the thrust in one direction?

How to choose the size of propellers? Do their size effect torque and current consumption?

","mobile-robot, brushless-motor, underwater"
Why the bldc motor not able to move my water boat?,"I have a 1100kv brushless motor from dys with a propeller as shown.
 
I have attached this motor to two pvc pipes joined together as shown and packed the electronics inside polyform to protect from water.

When I did the test run of this model the boat didn't move. The motor was just creating the turbulence in the water.
So I your help of where I am going wrong
 1. Is it wrong to place brushless motor in water(although have seen videos of
people placing their motors in water) ?
 2. Is the propeller design/size is wrong (maybe its a simple fan propeller not made for water, really no idea)?
 3. Or the motor rating is not enough (thought lower kv rating would generate more torque and therefore more thrust) ?
I used a 11.2V 2200 mah 25C lipo battery.
","mobile-robot, brushless-motor, underwater"
Inverse kinematics showing incorrect results for 4 dof robot in MATLAB using Robotics Toolbox,"I'm doing inverse kinematics for 4 dof robot using robotics toolbox matlab. The code is given below:
    preach = [0.326 0.223 0.342]; % reach point of end-effector
% create links using D-H parameters
% Link('d', 0.15005, 'a', 0.0203, 'alpha', -pi/2)
L(1) = Link([0  0     0.15   pi/2    0], 'standard');
L(2) = Link([0  0     0.15   0       0], 'standard');
L(3) = Link([0  0     0.15   0       0], 'standard');
L(4) = Link([0  0     0.15   0       0], 'standard');
  % set limits for joints
 L(1).qlim=[deg2rad(-160) deg2rad(160)];
 L(2).qlim=[deg2rad(-45) deg2rad(45)];
 L(3).qlim=[deg2rad(-60) deg2rad(60)];
 L(4).qlim=[deg2rad(-50) deg2rad(50)];
 %build the robot model
rob = SerialLink(L, 'name','rob');
qready = [0 0 0 0]; % initial position of robot
plot(rob,qready,'noname');
T1= transl(preach); % convert of reach point of 4x4 homogenous matrix
[qreach,err] =  rob.ikcon(T1, qready); % find inverse kinematics with error

Matlab shows results like this(using robotics toolbox ):
    >> [qreach,err] =  rob.ikcon(T1, qready)
qreach =
    2.7925    0.7854    1.0472    0.8727
err =
    9.6055

I'm not taking preach = [0.326 0.223 0.342]; randomly. Infact, first I do forward kinemtics to get these points. code is below:
    % to find forward kinemtics
qreadyrr = [0.6 0.45 0.63 0.22]; % setting the four angles randomly within range to get preach
T0 = fkine(rob, qreadyrr);

then, I got T0 as
     >> T0
T0 =
    0.2208   -0.7953    0.5646    0.3267
    0.1510   -0.5441   -0.8253    0.2235
    0.9636    0.2675    0.0000    0.3421
         0         0         0    1.0000

Also, when I put this T0 in place of T1 in inverse kinematics code as given above, the values I got is very accurate with negligible error.
    >> [qreach,err] =  rob.ikcon(T0, qready)
qreach =
    0.6002    0.4502    0.6296    0.2204
err =
   4.6153e-07

The point is, in my case, I have only px, py and pz values for transformation matrix but with this, inverse kinematics is not solving it correctly. I want to do inverse kinematics px, py and pz values. how can I do it correctly.
Thanks.
","robotic-arm, inverse-kinematics, matlab, forward-kinematics"
How to send a new Mavlink message from Ardupilot?,"I'm trying to add a new message to the MAVLink interface. Following this page, there are the steps I took:

Added the message to ardupilotmega.xml. Right at the end of the file:
<message name=""TESTING_TESTING_TESTING"" id=""182"">
  <description>A testing message</description>
  <field type=""int16_t"" name=""placeholder"">Does nothing. Simply a placeholder</field>
</message>

Regenerated the mavlink messages headers using ./libraries/GCS_MAVLink/generate.sh. It worked okay and the new headers appeared.
Then I added a function to the GCS class to make sure I'm sending on the right channel:
void GCS_MAVLINK::send_testing_testing_testing()
{
    mavlink_msg_testing_testing_testing_send(chan, 0);
}

Now it's time to send the message, I added my own function to the scheduler (on last priority). I made sure the function is called by sending text first and seeing it on the mission planner console. Here is the function I added:
static void a_testing_loop(void)
{
    for (uint8_t i=0; i<num_gcs; i++) 
    {
        if (gcs[i].initialised)
        {
            // gcs[i].send_text_P(SEVERITY_HIGH,PSTR(""Testing String""));
            gcs[i].send_testing_testing_testing();
        }
    }
}


My message, however, isn't received on the mission planner end. It might have been received and ignored by the mission planner, but anyway it doesn't appear on the console window (even with 'Mavlink Message Debug' on)
Is there configuration to be made to the Mission Planner for it to receive new messages? Or am I sending the message wrong?
Also, is there a way to filter out messages from the console when using 'Mavlink debug mode'?
I'm using SITL for testing
(I don't have enough reputation - But this should be under the tag 'mavlink')
","ardupilot, mavlink"
Theory behind wheeled inverted pendulum robots,"I'm trying to get a deeper understanding of the theory behind wheeled self-balancing robots. Can anyone point me to a text on dynamic modelling and control theory that might help?
","control, wheeled-robot, dynamics, theory, balance"
Matching 3D models in a 2D scene using PCL or OpenCV,"I have a 3D model obtained with a 3D scanner and I want to match it in a 2D scene (simple 2D video which contains the model).
I know Point Cloud Library (PCL) deals only with point clouds, and OpenCV with 2D images. Is it possible, though, to use any of them to extract the keypoints from the 3D model, and then use them to find the model in a 2D image?
","c++, opencv, 3d-model, point-cloud"
What is are digitally-mated pairs,"I am considering competing in the robogames, but the rules require that R/C bots have  digitally-mated pairs. I am not sure what these pairs are. For reference here are the rules for robot sumo and combots.
Robot Sumo Rules

Combot Rules
",wheeled-robot
SLAM system for LIDAR and Stereo camera for cone detection for Autonomous driving,"The purpose of the SLAM system is very specific, for detecting cones in an image and triangulate their position to create a map.
The data input would be the camera data, odometry and the LIDAR data.
I have been going through SLAM algorithms on openSLAM.org and through other implementations of SLAM systems.
I would like to know if there are a set of SLAM algorithms specific for the problem I have and what are the most efficient and least time consuming SLAM algorithms available. Any leads would be helpful. 
","slam, autonomous-car"
Guessing the K matrix gain for the Optimal Control LQR?,"I'm are going to create a LQR to control a system. The problem is to choose the Q and R weighting matrices for the cost function. The Q and R matrices are going to minimize the cost function so the system are going to be optimal.
I'm using Scilab and Scilab have a good library for optimal control. Scilab have a built function named lqr() to compute the gain matrix K, which is the LQ regulator. 
But the problem is still to choose those weighting matrices. I don't know here to start. I just might start with the identity matrix as Q and just a constant as R. But the gain matrix K does not make my model go smooth. No one can say that this is the real Q and R weighting matrices for the system. As a developer, I choose the weighting matrices. But why should I do that when I can choose the K gain matrix directly?
So I just made up my own numbers for the gain matrix K and now my model is very smooth. All I did just do was to guess some numbers for the gain matrix K and simulate and look at the result. Was it still bad, I might change the first element for the gain matrix to increase the position, or change the second element in the gain matrix K, to speed up the velocity for the position. 
This works great for me! Guessing and simulate and look at the results.
I choosing the LQ-technique for two main reasons: It gives multivariable action and can reduce noise by using a kalman filter. A PID cannot do that. 
But here is my question:
Will this method give me an optimal control just by guessing the gain matrix K and changing the values depending how the simulation results looks like?
I'm I happy with the results, I might quit there and accept the gain matrix K as the optimal LQR for the system.
","control, pid, microcontroller, kalman-filter, dynamics"
Calibrating a laser scanner to a line camera,"We have a high-resolution Riegl laser scanner and mounted atop it a Resonon Pika L, which is a hyperspectral camera which records one spatial column at a time, using the second dimension of the sensor for the wavelength spectrum from 400 to 1000 nanometers.
Now we would like to label each scan point with hyperspectral data. For this, we 

record a fixed angular window with the laser scanner
stitch together the individual spatial columns from the hyperspectral camera rotating with the scanner 
Identify chessboard corners in the point cloud 
also find them in the stitched hyperspectral panorama (reduced to rgb).
Using OpenCV's calibrateCamera we try to get a camera matrix, which I assume will only be valid for this particular panorama size (if at all) 

and then we could in theory obtain rotation and translation between scanner and panorama coordinates using solvePnP.
Is this a valid way to go about solving this problem? There doesn't seem to be much prior art in this regard.
","mobile-robot, sensor-fusion, calibration, laser, point-cloud"
PI-controller and selective periodic disturbance rejection,"This question related to the another question I posted here, however I think its best to start a different post.
I have a 1-DOF electromechanical application whereby I am controlling the contact-force at the tip of the end-effector. The sensor unfortunately only samples at an incredible painful 20 Hz. With a simple PI controller, I have poor disturbance rejection.
The end-effector is in contact with a compliant surface that is moving back-and-forth periodically with 2 frequency components (0.3 and 1.25 Hz). I am interested in only rejecting the slower frequency (0.3 Hz). Through Simulink, I was able to verify that I can design a PI-controller to reject this frequency with acceptable performance - I also evaluated it on a lab bench. 
However, in the real system both frequencies exists, and the PI controller attempts to compensate for the 1.25 Hz disturbance causing instability. Is it possible to add a low-pass filter somewhere in the loop so controller doesn't care about the 1.25 Hz disturbance.
All of this is implemented on an Arduino Due.
Thank you,
","arduino, control, pid"
PI with poor sampling rate,"I have a 1-DOF electromechanical application whereby I am controlling the contact-force at the tip of the end-effector.
The force sensor on the end-effector unfortunately gives me force data and an incredibly low sampling rate at 20 Hz, and I cannot do anything about it.
I generated a linear model for my plant and determined I will probably need a sampling rate of at least 320 Hz to follow the rule of thumb (Ts is 1/10th of your time constant). With the current setup, I of course have poor disturbance rejection (i.e., when the surface moves into the end-effector) with a PI controller . 
Fortunately, the wall motion is periodic and fairly predictable. I implemented a modified Smith predictor that can reject periodic disturbances. This works fairly well but I am interested in exploring other options. Is there anything else I can take a look into that may help with this situation? For instance, I was considering taking a look into implementing a Kalman filter, since I can use it to predict the next sample in the loop, which may improve performance - I am not sure if that is right. 
Regards,
","arduino, pid, dynamics"
Find the robot pose from three beacon measurement,"At every timestep my robot gets sensor measurements from a scanner that finds three beacons with known poses $B_1 = (B_{1x}, B_{1y})^T, B_2 = (B_{2x}^T, B_{2y}) , B_3 = (B_{3x}, B_{3y})^T$ these measurements include the distance and angle to the beacon, the measuremnt for $B_1$ would be $m_{1t} = (d_{it}, \omega_{1t})^T$. and equivalently for the other beacons.
From these measurements i want to calculate the robots pose containing its position and orientaion $x_t = (p_{xt}, p_{yt}, \Theta_{xt})^T$. 
Calculating the position can be done by trilateration, but I can't seem to find a way to get the orientation of the robot from these measurements. Is there possible a model to calculate both in a single calculation?   If not a solution for finding the orientation of the robot would be great.
","mobile-robot, localization"
MPU9255 gyro data to roll pitch?,"So I have -32768 to +32768 coming out from MPU9255 (gx,gy,gz) which is converted to 0-250 dps(degrees per second) using 131 which is Gyro's sensitivity. 
My question would be how do you use this data to convert it into Roll and Pitch? 
I am trying to make a stabiliser. I have tried using 
$$
\theta = \sum^n_n\omega*\delta(t) 
$$
where n = infinity
I don't if my equation is wrong or not here is my code:
dt = now_c - pr_dt;
pr_dt = dt;
Pitch_gyro += Gxyz[1]*(dt/1000000.0);
Roll_gyro += Gxyz[0]*(dt/1000000.0); 

This is my function:
Gxyz[0] = (double)(gx/131);//131;
Gxyz[1] = (double)(gy/131);//131;// 250/32768.0;
Gxyz[2] = (double)(gz/131);//131;// 250/32768.0;

Any guide as to how to solve this I have looked into euler angles. I still don't understand how you get the angles given angular velocity which is from gyro.
","arduino, quadcopter, imu, gyroscope, c"
Image registration with ground plane for surround view,"I am currently working on a project which which involves surround view.
I have 4 fish eye cameras and are fixed at 4 sides of a car.The fish eye cameras are corrected for radial distortion.
After radial correction,Each camera sees a pattern in the Common FOV of adjacent camera to get the points on the ground plane.
Pattern in the common FOV of adjacent camera

Now for each camera I need to warp those points to a plane which is the birds eye view. 
Right now I choose those 8 red points, map first 4 points on a square and the other 4 points on another square on the same line,since I know that both squares lie parallel to each other at some distance,for front and back images and use the same points for left and right also appropriately so that left image comes left and right image comes right of the result.
Then I calculate Homography Matrix for each image(front,left,right and back) using the points in the Image and the birds eye plane.
I map the points such that warped front image sits at the top, left sits at the left side and right at the right side and then back sits at bottom of image .
Example, Front sits at the top of result image

Left sits at the Left of result image
I do this so that I can stitch properly,forming a composite view.
My final stitched image looks like below.

As you can see, the geometrical alignment is not proper.
QUESTIONS

What is the right way of registering each image with the ground plane.
From the object points shown as red dots, what is the proper way to get the corresponding points in the bird's eye plane.

","cameras, opencv"
Obstacle avoiding by Pi camera,"I designed a mobile robot with my colleagues for our graduation project, the purpose of it is to detect mines (metals) in a specific area
We are programming the robot using Python on Raspberry Pi 3
I want it to avoid obstacles using the Pi camera, is that possible through computer vision?
I searched a lot but I can't find a full reference that guides me to do it. 
If it is too hard we'll use an ultrasonic sensor but can it, at least, make a graphical updated map, that marks the position of mines and obstacles on it?
","mobile-robot, raspberry-pi, mapping, python, opencv"
Issue using Digit LEDs Raw (op code 163) on Create2,"If I understand the manual, each leg in each of the 7 segment displays is labeled with a letter A-G.  These letters then map to specific bits in a byte - 1 byte for each of the 4 displays.  Setting a bit turns on the corresponding leg while not setting it leaves it off.
With this understanding, I tried to turn on all the A segments by sending
[163][1][1][1][1]

Instead of the A segment in each display turning on, the displays all showed a 1. Further testing shows that if I send the numbers 1-9 for any of the displays, they will display the number sent.  Sending the number 10 or greater turns on various combinations of the segments.
I was able to activate individual segments with the following numbers:
63 G
64 A
65 B
66 C
67 D
68 E
69 F

However, I haven't been able to determine how the bytes sent affect the individual segments. Either I don't understand the manual or Digit LEDs Raw does not work as the manual specifies.
UPDATE 03JUNE2016
I have confirmed this behavior exists in the following firmware versions:

r3-robot/tags/release-3.4.1:5858 CLEAN
r3_robot/tags/release-3.2.6:4975 CLEAN

",irobot-create
iRobot Create2- Path Following,"I'm working with the iRobot Create2, and modifying the Create2_TetheredDrive Python script as seen here: Using the Create2 Tethered Driving program- inputting a string of keyboard presses
Right now, my modifications look like this:
""if event.type == '2':
...(Other button press commands)
elif k=='1':
cmds=['self.callbackKeyRight', 'self.callback ...' (listing of commands I want the robot to follow)]
for i in range(len(cmds)):
motion change=True
if cmds[i]='self.callbackKeyRight'
self.callbackKeyRight=True
...(Same structure for all of the 4 direction options)
...
""if event.type == '3':
...(ending commands for the Create2 movement)
elif k=='1':
for i in range(len(cmds)):
if cmds[i]='self.callbackKeyRight'
t_max=time.time()+0.05
while time.time()

I have a similarly structured program that makes the Create2 either go forward a specified distance, or turn 90 degrees. However, when I try to use this command, the robot doesn't react at all.
So it looks like it must be due to the string of commands I'm giving it. But I'm not exactly sure where I must be messing up. Any ideas?
","irobot-create, path-planning"
Position Controller for a Quadrotor,"I have a question regarding the implementation of a quadrotor's position controller. 
In my Matlab model the quadrotor takes 4 inputs: a desired altitude ($Z_{des}$) and desired attitude angles($\Phi_{des}$, $\Theta_{des}$, $\Psi_{des}$) which reflects the motion described by the differential equations of the model (see last picture). 

Here an insight into the implemented Matlab dynamic model. As you can see it has a structure like an inner loop controler:

Anyway...it ""hovers"" perfectly on the starting point. (perfect graphs :) )
Now I just need to go over and implement a sort of position controller to let the quadrotor to get from a start to a goal point, defined as usual through 3 coordinates $[X_d, Y_d, Z_d]$. 
That's tricky because I don't have the same space state variables as input and output of the system. So the controller must take a vector of three coordinates and be able to output 3 different angles to get there. The only exception is the height because it will be simply bypassed by the controller and doesn't need another calculation loop. A different story is for the three angles... 
My first idea was to simply create a feedback between the position given at the output of the simulated system and the desired position as in the figure above.
But that rises another question: my quadrotor model solves the following equation system:
$$
\large \cases{ 
 \ddot X = ( \sin{\psi} \sin{\phi} + \cos{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m}  \cr
 \ddot Y = (-\cos{\psi} \sin{\phi} + \sin{\psi} \sin{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \ddot Z = (-g + (\cos{\theta} \cos{\phi}) \frac{U_1}{m} \cr
 \dot p  = \frac{I_{YY} - I_{ZZ}}{I_{XX}}qr - \frac{J_{TP}}{I_{XX}} q \Omega + \frac{U_2}{I_{XX}} \cr
 \dot q  = \frac{I_{ZZ} - I_{XX}}{I_{YY}}pr - \frac{J_{TP}}{I_{YY}} p \Omega + \frac{U_3}{I_{YY}} \cr
 \dot r  = \frac{I_{XX} - I_{YY}}{I_{ZZ}}pq - \frac{U_4}{I_{ZZ}}
}
$$
that means that they expect (as in the matlab model above) the desired angles and height. 
But now I need right the inverse: given a desired position calculate the right angles!!! 
For the direction is the solution really simple, since I can write something like:
Psi = atan2( (yd - yactual), (xd - xactual) );

where y and x lies on the horizontal plane. This is not so simple for the other two angles. So what can I do at this point? Just ""invert"" the given equations to get the desired angles?
Another idea could be to implement a simple PD or PID controller. This is much more easier given the fact that I can experiment very quickly using Simulink and get very good results. But the problem is here again: how get I the desired angles from a desired position?
","pid, quadcopter"
Technology behind Kiva Systems mobile robots,"What kind of sensors and algorithms are the mobile robots of Kiva Systems equipped with? 
","mobile-robot, industrial-robot"
What is the thread/screw size for the iRobot Create 2 internal screw bosses described in the Open Interface Spec doc,"In the “iRobot_Roomba_600_Open_Interface_Spec.pdf” provided for the iRobot Create 2, there is a section titled “Roomba Internal Screw Boss Locations”.  It states that “Screws may be replaced with threaded standoffs.”
Does anyone know what screw/thread size of standoffs should be used to match the screw threads?
(I saw another similar thread but the only solution listed was to re-thread the holes, which I would like to avoid if at all possible.)
Thanks!
",irobot-create
Reward Function for q learning on a robot,"I have 2 wheeled differential drive robot which I use pid for low level control to follow line. I implemented q learning which uses samples for 16 iterations then uses them to decide the best position to be on the line so car takes the turn from there. This allows PID to setup and smooth fast following. My question is how can I setup a reward function that improves the performance i.e. lets the q learning to find the best
Edit
What it tries to learn is this, it has 16 inputs which contains the line positions for the last 15 iterations and this iteration. Line position is between -1 and 1 which -1 means only left most sensor sees the line and 0 means the line is in the center. I want it to learn a line position that when it faces this input again it will set that line position like its the center and take the curve according to that line position. For example error is required position - line position so let say I had 16 0 as input then I calculated the required as 0.4. So after that the car will center itself at 0.4 I hope this helps :)
You asked for my source code i post it below
void MainController::Control(void){

    float linePosition = sensors->ReadSensors();

    if(linePosition == -2.0f){       
      lost_line->FindLine(lastPos[1] - lastPos[0]);
    } 
    else{
        line_follower->Follow(linePosition);
        lastPos.push_back(linePosition);
        lastPos.erase(lastPos.begin());   
    }
}

My Sensor reading returns a value between -1.0f and 1.0f. 1.0f means Outer Sensor on the right is only the line. I have 8 sensors.
void LineFollower::Follow(float LinePosition){

    float requiredPos = Qpredictor.Process(LinePosition,CurrentSpeed);
    float error = requiredPos - LinePosition;

    float ErrorDer = error -LastError;

    float diffSpeed = (KpTerm * error + (KdTerm * ErrorDer));

    float RightMotorSpeed = CurrentSpeed - diffSpeed;
    float LeftMotorSpeed = CurrentSpeed + diffSpeed;

    LastError = error;

    driver->Drive(LeftMotorSpeed,RightMotorSpeed);
}

Here is the logic for the value for QPredictor(I call the learning part as this). And Finally QPredictor
float Memory[MemorySize][DataVectorLength] =
{
  {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0},
  {0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3},
  {0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6},
  {0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8},

  {0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.012, 0.050, 0.113, 0.200, 0.312, 0.450, 0.613, 0.800, 1.000},

  {0.000, 0.025, 0.100, 0.225, 0.400, 0.625, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},

  {0.000, 0.050, 0.200, 0.450, 0.800, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},


  {0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000, 1.000},
  {0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100, 0.400, 0.900, 1.000}
};


QPredictor::QPredictor(){
    for(int i=0;i<MemorySize;i++){
        output[i]=0.0f;
        input[i]=0.0f;
    }

    state = 0;
    PrevState = 0;
}

float QPredictor::Process(float linePosition,float currentBaseSpeed){
    for(int i=1;i<DataVectorLength;i++){
        input[i] = input[i-1];
    }

    input[0] = m_abs(linePosition);

    int MinIndex = 0;
    float Distance = 10000.0f;

    float sum = 0.0f;

    for(int i=0;i<MemorySize;i++){
        sum = 0.0f;
        for(int j=0;j<DataVectorLength;j++){
            sum +=m_abs(input[j] - Memory[i][j]);
        }

        if(sum <= Distance){
            MinIndex = i;
            Distance = sum;     
       }
    }

    sum = 0.0f;
    for(int i=0;i<DataVectorLength;i++){
        sum += input[i];
    }

    float eta = 0.95f;

    output[MinIndex] = eta * output[MinIndex] + (1 - eta) * sum;

    return -m_sgn(linePosition) * output[MinIndex];
}


float QPredictor::rewardFunction(float *inputData,float currentBaseSpeed){
    float sum = 0.0f;

    for(int i=0;i<DataVectorLength;i++){
        sum += inputData[i];
    }

    sum /= DataVectorLength;

    return sum;
}

I now only have average Error and currently not using learning because it's not complete without reward function. How can I adjust it according to the dimensions of my Robot?
","machine-learning, differential-drive, two-wheeled"
"Stepper Motor For ""Cable Car"" System","I am a programmer with a lot of experience in IoT but somewhat new to actuators and robotics.
I would like to create a ""Cable Car"" which consists of two stepper motors and a pulley. The motors would be located one next to the other, and the pulley on the opposite wall. Each stepper motor would have a spool and be connected with fishing wire. In the middle would be a styrofoam boat. The idea would be to move the boat along the wire from one wall to another based on different metrics and calculations. Ideally it will travel from one end to the other in a months time and in slow increments.
Diagram using ASCII:

|___________________|
|x                 o|
|x_________________o|
|      |  |         |
|  <-- BOAT -->     |
|                   |
|                   |
x = stepper motor
o = pulley
_ = fishing wire (starts at top motor, goes to pulley(s), continues and ends at bottom motor)
| = wall

My question is: What type of stepper motors should and controller should I use? I estimate that the wire + the styrofoam boat will not weigh more than 3 pounds total. It will it hang at a height of 10 feet from the floor.
I am hoping to achieve this with a Rasperry Pi I have lying around.
I am confused about various torque requirements, voltage & amperage ratings etc.
If someone has a recommendation for hardware I would be extremely grateful.
","raspberry-pi, stepper-motor, stepper-driver"
How can reading a quadrature encoder fail?,"
I am trying to the read the quadrature encoder on a ServoCity 624 RPM Premium Planetary Gear Motor w/Encoder with a SparkFun ESP32 Thing.
I am supposed to see 228 counts per revolution, but I see 230-232 instead.
Here is my Arduino code:
// Global variables modified by ISR
int state;
unsigned int count;

int error;

// ISR called on both edge of quadrature signals
void handleInterrupt()
{
  // Shift old state into higher bits
  state = ( state << 2 ) & 15;

  // Get current state
  if( digitalRead(34) ) state |= 2;
  if( digitalRead(35) ) state |= 1;

  // Check state change for forward or backward quadrature
  // Flag any state change errors
  switch( state )
  {
    case 1:
    case 7:
    case 14:
    case 8:
      count--;
      break;
    case 11:
    case 13:
    case 4:
    case 2:
      count++;
      break;
    default:
      error++;
  }
}

void setup()
{
  pinMode(33, OUTPUT); // PWM
  pinMode(32, OUTPUT); // DIR

  pinMode(34, INPUT_PULLUP); // QB
  pinMode(35, INPUT_PULLUP); // QA

  attachInterrupt( digitalPinToInterrupt(34), handleInterrupt, CHANGE );
  attachInterrupt( digitalPinToInterrupt(35), handleInterrupt, CHANGE );

  Serial.begin(74880);
}

void loop()
{
  int new_state;
  int old_state;

  // Start motor
  digitalWrite(32, HIGH); // clockwise
  digitalWrite(33, HIGH);

  // 57*4-17 found by trial and error to make a complete revolution
  for( int i = 0; i < (57*4-17); i++ )
  {
    // Busy wait for change
    do
    {
      // Get current state
      new_state = 0;
      if( digitalRead(34) ) new_state |= 2;
      if( digitalRead(35) ) new_state |= 1;
    }
    while( old_state == new_state );
    old_state = new_state;
  }

  // Stop motor
  digitalWrite(33, LOW);
  delay( 1000 );

  Serial.print( "" state="" );
  Serial.print( state );
  Serial.print( "" count="" );
  Serial.print( count % 228 );
  Serial.print( "" error="" );
  Serial.println( error );
}


On my scope, quadrature signals are very clean
I instrumented the code to look at interrupts and they appear to be in the right places and not too close together.
I don't see mechanical slipping, and it would be obvious because
over 100 revs, the count slips an entire revolution.

But the bottom line is: How can this be failing?  If the CPU is missing transitions, I would get illegal transitions and errors.  Noise would also cause errors.  But I am not getting any errors at all (except a single error at startup).
",quadrature-encoder
Enable Bluetooth communication with iRobot Create 2,"I just got a new iRobot Create 2. I used to use an Element Direct BAM (Bluetooth Adapter Module) for iRobot Create previously.
How can I communicate with a Create 2 using Bluetooth? What accessories do I need?
",irobot-create
What is the Name of the part I'm describing,"I'm looking for a part that will do a particular function.  It has to be able to move along one axis and tell me the force that is being exerted on it.
Kind of how like a piston moves inside an engine (one axis of movement) except that something will be pushing at the top of the piston and I need to know how hard its pushing.  Another difference is that the piston won't be constantly moving back in forth, but needs to be able to receive commands like.  move x centimeters forward and then remain stationary at its new position.
I know to make this it would involved a sensor and something that can exert force but what is the name of the described machine?
Edit #1 - Response to Matthew Gordon
The piston would have to move between 0-6 centimeters.  The form factor would be small, ideally smaller than the palm of your hand.  (Smaller=better) The forces it would have to deal with are comparable to the forces exerted on a bicycle by its chain.  I'm a math/cs person not engineering so I don't know technical terms for these kinds of things off the top of my head.  It would have to be real time sensor reading, but the volume of data could be processed by a phone.  Would have to working in conjunction with wireless communication, probably Bluetooth, but I'd have to look into the latency requirements to be sure.
","sensors, mechanism, force-sensor, identification"
"Converting Image Coordinates to (x, y) Position for Robotic Arm","I have a Robotic Arm with a camera mounted above it looking down at a slight angle. 
Assuming I know the height of the camera, the angle of tilt and the small distance from the center of the robot which is considered (0, 0) what else would I need to convert the image coordinates to the distance from the center of the robotic arm?
I am also assuming all the objects will have a z=0 because they will be sitting on the same platform as the arm.
I have the inverse kinematics worked out to control the arm, I just need to give it coordinates to move to.  
If it helps I am using Python and OpenCV. 
Edit: Clarification
","control, robotic-arm, computer-vision, python, opencv"
I need help figuring out what motor is used in this design,"I am learning how to make rigs for high speed motion control. Basically, whatever action needs to be executed must be done by a rig that is triggered at an exact moment.
In this particular case, I need something that pulls out extremely fast, so that the element that is on top falls without any alteration.
In these two pictures I've highlighted the rig that is being used:


Zoomed in on the relevant sections:

This action happens in 1/2 a second, give or take. 
What is the motor capable of pulling out that fast, without altering the object on top? 
In this case the burger bun falls intact, it lands perfectly.
","motor, kinematics, mechanism, identification"
Name of the linkage (or carriage) in video,"I am trying to find the name (nomenclature) of the linkage (or carriage) that is being driven by the dual linear servo (actuator) arrangement in the following Youtube videos:

Servo Basic Concepts
YouTube - 4 X Linear Servo Application

The linkage (carriage) appears to be able to rotate about a 180 degree arc.
What is this metal linkage (or carriage) system called? 

","mechanism, arm, identification"
What is the name of this mechanical linkage?,"
I am trying to find a joint like these for a robot I'm building. It is often called a swivel joint or a universal joint, but with a modified spider. I can't find one anywhere and would prefer not to make it. Searching for 'universal joint' returns the standard automotive type. Any help would be appreciated
","joint, identification"
What type of mechanism is this?,"
Held and rotated by the knurled ends, one in each hand, the silver spokes rise and fall in order for the assembly to rotate. What is it, some companies' salesmen show tool? Found in an old building, unit has no markings.
","design, joint, identification"
What the name of this kind of computer vision?,"Whats the name of the technique used to read symbols with computer vision?
Like the one used for this wood cutter:

I want to use something like this to measure distance from the camera to a piece of tape with symbols, or read QR-like symbols to recognize areas, but I can't find information on this because I don't know how this technique or method is called.
What is it called?
","computer-vision, identification"
Can you identify the construction material/system used in the pic?,"What's the name of the ""big meccano"" used in the photo below to construct all the cabinets and racks?
It appears to be an aluminium cut-to-length system of 4-way rails. I've seen it used many times and assume it has a well-known brand-name to those that know.

Photo taken from theverge.com and was a feature about how Audi are building a new car.
","frame, identification"
How to evaluate 3D Occupancy grid maps,"I would like to know how to go about evaluating 3D occupancy grid maps.
I have a SLAM system that produces a 3D OGM (in .bt format using octomap/octovis)
I also have a ground truth OGM in same .bt format.
How do I compare the accuracy of my map to the ground truth map in a qualitative and quantitative way?
Important notes:

The two maps may not be the same scale.
One map may be less dense than the other.

One method I have thought about using is MRPT's occupancy grid matching application
This would require me to send both 3d maps as a message to the octomap_server node in ROS, get the resulting map in Rviz, save the image 2D image of each separately, and then somehow convert the images to MRPT's .simplemap file format, and then run MRPT's grid matching program on the two files.
Surely there is a better/more accurate way?
EDIT:
So I did more research and another route I could go is Matthew's Correlation Coefficient (MCC). I could compare two maps and iterate over each cell to compare my result to a ground truth, counting the True and False Positives and Negatives. 
Only problem with this is that I have to assume that the two maps are the same scale, and also in the same orientation.
If you have any ideas on solving these scale and orientation issues don't be shy.
","mapping, occupancygrid"
Monobrick Communication Library-reading nxt sensors,"In my program, I'm need to detect if the NXT touch sensor is pressed.
var nxt = new Brick<Sensor, Sensor, Sensor, Sensor>(""usb"");
nxt.Connection.Open();
nxt.Sensor1 = new TouchSensor();
nxt.Sensor1.Reset(false);
nxt.Sensor1.Initialize();
Console.WriteLine(nxt.Sensor1);

When I start the program, the sensor value always reads 0. But I discovered that if I go into the ""View"" menu in the NXT and see the touch sensor value, the program value reads 1. I can't do that for my setup. Also, I can't use Bluetooth; my computer doesn't have it. Can someone help me?
EDIT: my full code
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using MonoBrick.NXT;
using MonoBrick;
using System.Windows.Forms;
using System.Reflection;
using System.Threading;
namespace MonoBrick
{
    class Program
    {
        [STAThread]
        static void Main(string[] args)
        {
            try
            {
                var nxt = new Brick(""usb"");
                nxt.Connection.Open();
                nxt.Sensor1 = new TouchSensor();
                nxt.Sensor1.Reset(false);
                nxt.Sensor1.Initialize();
                Console.WriteLine(nxt.Sensor1.ReadAsString());
                nxt.Beep(500);
                System.Windows.Application application = new System.Windows.Application();
                application.Run(new Window1());
                nxt.Connection.Close();
            }
        catch (Exception e)
        {
            Console.WriteLine(""Error: "" + e.Message);
            Console.WriteLine(""Press any key to end..."");
            Console.ReadKey();
        }
    }
}

}
","nxt, mindstorms"
Inertia Tensors in 3D objects for robot dynamics,"I'm trying to obtain the dynamics of a 6DOF robot. Firstly, I calculated the combined centres of mass (between each link and the respective actuator) in order to calculate the gravity term, since it only depends on the combined centre of mass the the angles of the joints (Yes, I know there's another way which basically takes into account the centre of mass of the links and the actuators, but I just didn't follow that path). My calculations are correct since the robot is effectively compensating the gravity, but now I want to calculate the remaining terms (mass and coriolis).
Since I have the combined centre of mass between each ""system"" (link + actuator), and the datasheet of this robot only gives me the Inertia tensors at the center of mass of each part, I need to know the equivalent inertia tensor in the ""new"" centre of mass (the combined one).
Now, I've done my research and found little information on the web about this. I did found out that I could probably follow the parallel axis theorem, but I've seen people saying that it is based on the premise that the object is planar (which means it would only be applied to 2D objects). My question is: can I apply this theorem in 3D? If so, please explain to me what exactly do I have to do, and if not, what other options I have to follow.
Let me know if you understood what I'm after and thanks in advance.
","robotic-arm, dynamics"
Using an Xbox controller to fly a Quadrocopter,"So I have a quadrocopter, it does come with a remote but I intend to run certain modifications to the copter, like installing a camera, a mechanical manipulator, and other random modifications. The remote that comes with the copter isn't flexible enough to help with such functions and plus it lacks any more buttons. 
I was wondering if I could somehow program the quadrocopter to respond to my Xbox controller. I was planning on using my laptop's Bluetooth connection to talk to copter. The Xbox controller which is connected to the computer would be then used to control the quadrocopter. So my question is, how exactly do I program the controller? How do I go about making all of this possible? 
I understand this question is really vague and that there are too many options out there, but I do need help figuring this out. 
",quadcopter
Software for mission planning in multi-robot systems,"I am interested in mission planning for multi-robot systems. Given multiple robots and multiple tasks and an environment, I need to specify missions and software should plan for the robot team to accomplish the mission. 
To be more precise, tasks are just a bunch of waypoints with or without time-stamps or deadlines. More elaborately, a task in abstract sense is something like patrol location A using two robots which essentially can be coded up as two sets of way-points or trajectories for the robots. hence the assertion above that tasks can just be viewed as a bunch of waypoints.
So there are multiple tasks and what tasks have to be executed in which order has to be planned by user or software so as to fulfil a mission. I am looking for Github repositories where people have tackled such problems to take inspiration. I am open to any software framework.
As a prime example of the kind of work or software I am looking for, FLYAQ - An open source platform for mission planning of autonomous quadrotors is an example. Please share any code or PDF links, if possible.
","mobile-robot, motion-planning, algorithm, multi-agent, reference-request"
What's the difference between feedback and feedforward control?,"I'm reading from Astrom & Murray (2008)'s Feedback Systems: An introduction for scientists and engineers about the difference between feedback and feedforward.  The book states:

Feedback is reactive: there must be an error before corrective actions are taken.  However, in some circumstances, it is possible to measure a disturbance before the disturbance has influenced the system.  The effect of the disturbance is thus reduced by measuring it and generating a control signal that counteracts it.  This way of controlling a system is called feedforward.

The passage makes it seem that feedback is reactive, while feedforward is not.  I argue that because feedforward control still uses sensor values to produce a control signal, it is still reactive to the conditions that the system finds itself in.  So, how can feedforward control possibly be any different from feedback if both are forms of reactive control?  What really separates the two from each other?
A illustrative example of the difference between the two would be very helpful.
",control
Why the IMU measurements only accumulate drift in 4DOF,"I'm studying the Technical Report of VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator available at: https://github.com/HKUST-Aerial-Robotics/VINS-Mono/blob/master/support_files/paper/tro_technical_report.pdf
In section V, where Loop Closure is discussed is writen:
""The IMU measurements render roll and pitch angle fully
observable, so the accumulated drift only occurs in four
degrees-of-freedom (x, y, z and yaw angle). To avoid importing
spurious information, we directly optimize pose graph on these
four degrees-of-freedom.""
I'm having trouble to understand this, in my conception, imu noise should propagate in all degrees of freedom. This might have something to do with the gravity vector?
Thank You All
","slam, imu, odometry"
DH parameters for PRP manipulator,"Can anyone share their DH parameters for a simple PRP manipulator. I have some confusion in setting up the axes and obtaining a solution.
",dh-parameters
2D quadcopter simulation position controller and PD controller,"I'm a beginner in controls system, so if there is a nice tutorial on this, please let me know. 
I have a Simulink model like the following:

It takes roll and pitch commands and output velocities and positions. I want to create a PD controller so that the quadcopter in the simulation can move in a 10x10 meter shaped square(starting from bottom left corner) in a clock-wise direction.
I'm not sure how to go about this so I've been watching Vijay Kumar's Ariel Robotics lectures on Coursera and it seems like I need to create desired position, velocity, and acceleration at every time step which I think I can do (make a trapezoidal velocity profile and compute the rest). 
Then Kumar talks about a nested control structure where there are two controllers: a position controller that takes in the desired position, velocity, and acceleration; and there is attitude controller which I believe is the one I have. 
My question is how do I create a position controller? And is the position controller the one that converts my desired position,velocity, and acceleration into a roll/pitch command?

","quadcopter, pid"
Telescoping linear actuator,"I need to find a linear actuator that can extend to multiple times it's length. I am going to be fixing the actuator horizontally and it will carry a light vertical load. 
So far I have thought of using something similar to the mechanism in a scissor lift. However, this is intended for a CNC application and needs high precision, and I'm not sure a scissor lift design would be rigid enough vertically when place horizontal. The system will eventually be feedback controlled so some give can be tolerated. 
Is there something other than a scissor lift design that would be better suited to this application?
","motor, actuator, cnc"
Two-wheeled self-balancing robot - Choosing the control system,"Would an Arduino Uno have the precision-timing required (using only firmware) to control a two-wheeled inverted pendulum robot, or would it need a RTOS?
Note: Thanks for both answers, they both helped a lot.  I just chose the last answer as the accepted answer.
","arduino, control, microcontroller, wheeled-robot, balance"
Starting and stopping a heavy rotating disc,"I am currently working at a project involving a heavy disc rotating around its center. The disc weights around 2 kg and has a radius of 0,25 m. At every angle π/6, there exists smaller discs of masses from range 0,1 kg to 1 kg 0,15 m from the center. The radius for these are 0,05 m.
A picture to illustrate:

I have roughly calculated the moment of inertia when all smaller discs weight 1 kg. Using the formula for a circular plate and Steiner's theorem, the result is:
$$ Inertia = \frac{0,25^2}{2} + 12(\frac{0,05^2}{4} + 0,15^2) = 0,34 kgm^2$$
Now I want the disc to be able to spin and stop at these specific angles. Say for instance I want the disc to rotate from 0 to π. This means I need a precise way to control my disc. My plan is to use a servo and some gears to drive this. I need the disc to turn 180° in at least 3 seconds (preferably less). With this angular velocity and inertia, I have realized it might not be the easiest thing to stop this spinning wheel, let alone accelerate it. Here is another image:
 
The motor does not need to be positioned like that, it would also be possible to drive the disc by positioning the motor on the edge of the disc for example.
What kind of motor should I be looking for, how would I handle stopping the disc? I am looking for general tips on how to accomplish this.
","motor, torque"
Handling changing frames in ROS,"I have a robot that is inherently symmetric in nature. Sometimes one side is the base while the other is the end-effector and vice versa. This 'mode' can change while the robot is moving around. 
Judging from the URDF tutorials wiki, it looks like the commonly used URDF package in ROS is static and so the base of the robots is assumed to stay the base. Are there ways to get around this so that I can still use the TF package?
",ros
How to call remote ros node on mobile robot through laptop using wifi,"I've recently tried to get the remote ros node on mobile robot through wifi. This is as below picture.

I've run roscore command on both laptop and mobile robot and when running the roscore command I've got the following warning message on my laptop server

WARNING: ROS_MASTER_URI [http://192.168.7.2:11311] host is not set to this machine

auto starting new master
process[master]: started with pid[2719]
ROS_MASTER_URI=http://192.168.7.2:11311/

setting /run_id to 4ef6c0f8-bfdf-11d3-a450-4e699f75a6e7
process[rosout-1]:started with pid[2732]
started core service[/rosout]

but when run rosparam list command on laptop as below I've got the following results
root@duminda-laptop:~#rosparam list
/rosdistro
/roslaunch/uris/host_192_168_7_2_35078
/rosversion
/run_id
root@duminda-laptop:~#rosparam get /rosdistro
hydro
root@duminda-laptop:~#rosparam get /roslaunch/uris/host_192_168_7_2_35078
http://192.168.7.2:35078
root@duminda-laptop:~#rosparam get /run_id
4ef6c0f8-bfdf-11d3-a450-4e699f75a6e7

These results shows that both laptop and mobile robot connected ok .

Is my setup to receiving the mobile robots data to my laptop is ok?
Why there is a warning message like above when starting the roscore?

","slam, ros"
Angular Velocity from dual tri axial accelerometers,"Can anyone throw some light on using accelerometers to measure  angular acceleration and hence angular velocity. This approach is to avoid gyroscopes due to drifting errors. Any links for this also would be very helpful.
Thank you.
","accelerometer, gyroscope"
Cannot disable sleep in passive mode for iRobot Create 2,"I tried to disable sleep by pulsing the BRC pin low for one second every minute as suggested in the OI, but my Create 2 still goes to sleep after 5 minutes.
My firmware is r3_robot/tags/release-3.2.6:4975 CLEAN
The Create 2 is connected to an Arduino, and the BRC is driven by one of the Arduino pins.  I verified on a DMM that the voltage is indeed toggling.  I am able to both send and receive serial data between the Arduino and Create2.
Pseudo-code:

Initialize roomba.  Connect serial at 115200 baud.  Toggle BRC: high for 200 ms, low for 200 ms, then high again.  Leave it high.
Ask roomba to stream sensor data in passive mode.  Wait 1 second after BRC toggle to give some extra time to wake-up.  Then send opcode 7 (reset), wait for reset message to complete by looking for the last few characters, then wait another second for good measure.  Next, send opcode 128 (start into passive mode), wait 100 ms to let opcode stick, then ask for stream of data (opcode 148 followed by number of packet IDs and the packet IDs themselves).
Main loop: Echo data from Create2 to the serial-USB output of the Arduino so that I can view the Create2 data.  The data sent by the Create2 look valid (good checksum) and are sent in the expected time interval of ~15 ms.  The main loop also toggles the BRC low for 1 second every minute.

For the full gory details, the complete Arduino sketch is shown below
const uint8_t brcPin = 2; // Must keep this low to keep robot awake
long last_minute = 0;
long minute = 0;

// Initialize roomba
void roomba_init()
{
  Serial3.begin(115200); // Default baud rate at power up
  while (!Serial3) {}    // Wait for serial port to connect

  // BRC state change from 1 to 0 = key-wakeup
  // keep BRC low to keep roomba awake
  pinMode(brcPin, OUTPUT);
  Serial.println(""BRC HIGH"");
  digitalWrite(brcPin, HIGH);
  delay(200);  // 50-500 ms

  Serial.println(""BRC LOW"");
  digitalWrite(brcPin, LOW);
  delay(200);

  Serial.println(""BRC HIGH"");
  digitalWrite(brcPin, HIGH);
  last_minute = millis()/60000;

  delay(1000);  // give some extra time to wake up after BRC toggle.

  Serial.println(""Opcode 7: reset robot"");
  Serial3.write(7);      // Reset robot
  // Discard roomba boot message
  // Last part of reset message has ""battery-current-zero 257""
  char c = 'x';
  Serial.println(""Gimme a z!"");
  while (c != 'z') {
    if (Serial3.available() > 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a e!"");
  while (c != 'e') {
    if (Serial3.available() > 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a r!"");
  while (c != 'r') {
    if (Serial3.available() > 0) {c = Serial3.read(); Serial.write(c);}
  }
  Serial.println(""Gimme a o!"");
  while (c != 'o') {
    if (Serial3.available() > 0) {c = Serial3.read(); Serial.write(c);}
  }
  // Flush remaining characters: 32 50 53 54 13 10 or "" 257\r\n""
  Serial.println(""Gimme a newline!"");
  while (c != 10) {
    if (Serial3.available() > 0) {c = Serial3.read(); Serial.write(c);}
  }
  delay(1000);  // allow extra time for opcode 7 to stick

  Serial.println(""\nOpcode 128: start OI in passive mode"");
  Serial3.write(128);   // Start the Open Interface.  Passive mode. 
  delay(100);           // Allow some time for opcode 128 to stick (not sure if this is needed)
  Serial.println(""Opcode 148: stream data packets"");
  Serial3.write(148);   // Stream data packets (every 15 ms)
  Serial3.write(16);    //   Number of packet IDs
  Serial3.write(8);     //   Packet ID 8 = wall                       1 byte
  Serial3.write(9);     //   Packet ID 9 = cliff left                 1
  Serial3.write(10);    //   Packet ID 10 = cliff front left          1
  Serial3.write(11);    //   Packet ID 11 = cliff front right         1
  Serial3.write(12);    //   Packet ID 12 = cliff right               1
  Serial3.write(13);    //   Packet ID 13 = virtual wall              1
  Serial3.write(27);    //   Packet ID 27 = wall signal               2
  Serial3.write(28);    //   Packet ID 28 = cliff left signal         2
  Serial3.write(29);    //   Packet ID 29 = cliff front left signal   2
  Serial3.write(30);    //   Packet ID 30 = cliff front right signal  2
  Serial3.write(31);    //   Packet ID 31 = cliff right signal        2
  Serial3.write(41);    //   Packet ID 41 = velocity right            2
  Serial3.write(42);    //   Packet ID 42 = velocity left             2
  Serial3.write(43);    //   Packet ID 43 = encoder counts left       2
  Serial3.write(44);    //   Packet ID 44 = encoder counts right      2
  Serial3.write(45);    //   Packet ID 45 = light bumper              1
}

void setup() {
  // Open serial communications (through USB interface)
  // The serial output of the Create 2 is echoed from Serial3 to Serial
  // so that we can observe the Create 2 serial output on a computer.
  Serial.begin(115200);
  while (!Serial) {}   // Wait for serial port to connect
  Serial.println(F(""Starting roomba test...\n""));

  // Roomba serial commmunications
  Serial.println(F(""Initializing comm to Roomba\n""));
  roomba_init();
}

long low_start_time;
boolean brc_is_low;
void loop() {
  // Read from Serial3 and echo results to Serial
  if (Serial3.available()) {
    uint8_t b = Serial3.read();
    uint8_t checksum = 19;
    if (b==19) { // First byte of reply stream is 19
      Serial.print(""\nStart at "");
      Serial.println(millis());
      Serial.print(b); Serial.print("" "");
      while (Serial3.available() < 43) {}  // Wait for rest of data (buffer is 64 bytes)
      for (int I=0; I<43; I++) {
        b = Serial3.read();
        Serial.print(b); Serial.print("" "");
        checksum += b;
      }
      Serial.print(""Chksum "");
      Serial.println(checksum);  // 0 is good
    } else {
      // Probably an ascii message
      //Serial.write(b);
      Serial.print(b); Serial.print("" "");
    }
  }

  // Pulse BRC low every minute for 1 second
  long now = millis();
  long minute = now/60000;
  if (minute != last_minute) {
    Serial.println(""\n\nBRC LOW"");
    Serial.println(millis());
    digitalWrite(brcPin, LOW);

    last_minute = minute;
    low_start_time = now;
    brc_is_low = true;
  }

  // 1 s low pulse width
  if ((now > low_start_time + 1000) && brc_is_low) {
      Serial.println(""\n\nBRC HIGH"");
      Serial.println(millis());
      digitalWrite(brcPin, HIGH);
      brc_is_low = false;
  }  
}

",irobot-create
"For the second Paden-Kahan Sub-problem, how is 'r' determined?","When using Paden-Kahan Sub-problems to solve the inverse kinematics of manipulators, 'r' is described as the intersection point between the first and second twist axes. But how is this r actually found?
Referencing Murray (here), on page 122.
","inverse-kinematics, dh-parameters"
Recognizing a line from three r-theta ultrasonic distance readings?,"Anyone know of sample Python code, or a tutorial, on the polar coordinate math for recognizing that three ultrasonic distance readings form a straight line?
Deg  Distance

-10°  20 cm  
  0°  18 cm
+10°  16 cm

Once I understand the math, I'll have to deal with the lack of precision.
I want my bot to recognize a wall, and eventually recognize a corner.
","ultrasonic-sensors, geometry"
I-Robot Create 2 reset after sleep issue?,"I am having issues with bringing the robot out of its sleep or off mode. Seems it goes into sleep mode when there is no activity for about 4 minutes. I am using the i-Robot Create 2 serial cable. When it is in its sleep mode I try removing the cable end plugged into robot and connect jumper wire between pins 5 and 6 on the robot 7 pin connector for a brief time period. This effectively shorts the BRC pin to GND for a short period of time ( less than 1 second). Then I reconnect the serial port cable into the robot 7 pin connector and try giving the robot a command but no go. I have also read that commands 173 and 173 173 can help with this issue but I may be mistaken. Any help on this is very much appreciated !!!! Rick
",irobot-create
Orocos KDL issue with Rotation (matrix) - Inverse Kinematics,"I have a problem with the Orocos IK solver, especially with the KDL::Rotation matrix input. 
I try to call my KDL IK solver with a normal vector and a rotation. If I use the example values for the rotation everything went well. But as I tried to use my ""own"" orientation, the IK solver didn't find a solution.
 KDL::Vector vektor(0.457101 , -0.629513, -0.627317); 
 KDL::Rotation rotation(1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0); //myValues
 KDL::Frame end_effector_pose(rotation, vektor);

 rc=kdl_solver.CartToJnt(origin,end_effector_pose,result);

As you can see it's a simple identity matrix -> no rotation -> in my opinion it should work. Anyway, if I try to call it with any other rotation matrix, it doesn't find a solution.
Just in case of 
KDL::Rotation rotation(-0.0921467,0.939391, 0.330232, 0.925128, 0.203433,-0.32055,-0.368302,0.275969,-0.887803);

it terminates with a valid solution. These values are test values from my robot.
Do I have a wrong comprehension of the rotation matrix ?
Thank you very much for your time
Devel
EDITE:
With .msg communication it works. I have no idea why ?
Does anyone know how the following lines construct the rotation matrix ??
geometry_msgs::PoseStamped pose_msg_in = pose_stamp; 
tf::Stamped<tf::Pose> transform;     
tf::Stamped<tf::Pose> > transform_root;
tf::poseStampedMsgToTF(pose_msg_in, transform); 


tf_listener.transformPose(root_name, transform, transform_root);

KDL::Frame F_dest;  tf::transformTFToKDL(transform_root, F_dest);

int ik_valid = ik_solver_pos->CartToJnt(jnt_pos_in, F_dest, jnt_pos_out);

Or is the matrix related to something? I get the feeling that I miss a important information.

SOLVED: Sorry, for the delay. Shahbaz answer was totally right. I simply overestimated the capabilities of my robot. The position was not reachable. After using Moveit(visualisation), it became clear that the orientation is not possible for the robot at that position (x,y,z). THANKS
","robotic-arm, ros, kinematics, inverse-kinematics, rotation"
Driving DC motor using Arduino through commands sending from MATLAB.,"I have a servo motor (http://robokits.download/documentation/RMCS220x_DCServo_Driver.pdf). I wrote a code in Arduino to rotate it in some defined angles and position. Code is given below:
#include <SoftwareSerial.h>
// software serial #1: RX = digital pin 10, TX = digital pin 11
// I have Arduino Uno so I created extra RX and TX to send and receive data. 
// Becuase using inbuilt RX0 and TX0, I was unable to transfer data to motor 
// and get feedack in PC
SoftwareSerial serial1(10, 11); 

void setup()
{
  Serial.begin(9600);
  serial1.begin(9600);
}

void loop()
{

  if (Serial.available() > 0)
  {
    delay(5);
    serial1.println(Serial.readString());
  }


  if (serial1.available() > 0)
  {
    delay(5);
    Serial.println(serial1.readString());
    delay(5);
  }
}

Using this code what I am able to do is, in terminal I enter some value say ""G400"" or ""R821"" etc. and motor rotate accordingly. But, this is not my aim. I don't want to put values manually, instead I used matlab script which give me some angle after calculations. I have to send this value to motor. Say, after calculations, matlab gives 26.4 degree, then I have input value to motor 26.4/0.2 = 132 counts i.e. ""G132"". Value changes every time for next calculation it may be 40 degree. What should be the  coding for this in Arduino as well as in MATLAB. 
Thanks.
","arduino, matlab"
Wheel Odometry Covariance Matrix for Custom Robot,"I have been looking around in the forums, but could not find any answer for how to go about getting the wheel odometry covariance matrix for a custom built planar robot (I found some posts related to EKF, but could not find any clear solution). I need this in order to for e.g. fuse wheel odometry with other types of odometry, etc... I really would only need the covariance for the global planar velocities (which are directly related to the encoders/inputs), and determining the position or acceleration covariances seem to be able to be derived from there. 
I will try posting the answers to this as I find them, and perhaps this could also help future roboticists (hobbysts!?) that want to build a new mobile robot but may be confused about this.
Let's say I have a custom planar robot with N number of motors/wheels/encoders, and a defined kinematic model.
That is, I have a mapping:

(Vx, Vy, AngVel) -> ( W1, W2, ..., Wn)

where W's are each motor's angular velocities. I am not sure if the inverse mapping always exists, but I could assume that it does just for now.
By reading around the forums, I found that first we should calibrate for the systematic errors (e.g., due to unequal wheel diameters, etc..). For differential wheeled robots, this can be done with the UMBMark algorithm. This still does not give any specific information on how to get the covariance matrix tho.
I imagine there are two options, using a static covariance matrix (predetermined by calibration), or dynamically adjusting them (let's say through a Kalman filter).
A static covariance matrix is probably less accurate, but simpler to determine. However, I have no idea how to go about choosing these values (should I make the robot move back and forth several times and use the error as the Vx variance?). Are there any basic guidelines for filling up the covariance matrix statically?
Another (more difficult) option seems to be to use a Kalman filter, and update the covariance matrices dynamically. But I am unsure what to choose as the inputs, nor white gaussian noise values for process/observation. 
Imagine there's some sort of local controller that I just give desired angular velocities and it tries to produce them. Should I go as low level as defining my inputs as the currents, and then go through the motor model? Or should I just choose the inputs to be my commanded angular velocities? 
But if my inputs are the desired angular velocities, then the state doesn't seem to depend on the previous state, and doesn't follow the kalman filter convention for the process (i.e. the new state would only depend on the input, as I am controlling the planar velocities directly from the wheels, and it is not affected by the previous state)!?
At least the sensor model seems to be quite easy to derive from the kinematic model.
As you can probably see, I am extremely confused for something that most likely has to be determined for most mobile robots out there. I am finding little to no clear documentation, which is weird for such a (very common!?) problem. If anyone could point me in the right direction I'd be extremely happy! 
Thanks!!
","mobile-robot, odometry, calibration, wheel, sensor-error"
Stepper motor control,"I am trying to understand the stepper motor to Mach3 type software control interfaces, mostly from a logical perspective, deducing most of everything because I have no concrete resource to refer. 
So basically I have purchased a ""LinkSprite"" 3 axis engraver. It says it comes with Arduino GRBL board which interprets G-code. Sends it to stepper driver shields. 
Bottom
I can see the drivers need to send a approximate sign wave of some sort to one or both windings to actuate them. 
Top
Mach3 I read essentially transmits two signals per axis to the motion controller/breakout board (which in turn is connected to the drivers), one being the number of steps the other the direction and probably via parallel ports GPIO or something.

So what underlying transmission protocol carries mach3 signals, like i2c or something via the parallel poet? How is steps and direction, and axis encoded?
What does the motion controller do exactly? Minimally does it just breakout out the signal to the drivers? What is the drivers inputs?
My Arduino GRBL board I read interprets G-code, but isn't that what Mach3 does?

How can I connect from stepper motor waveform on windings to some interface like Mach3 the encodings and concrete information of this logical path of the workings of the control of the stepper motors?
","control, microcontroller, stepper-driver"
Retrieving distance and angle of roobma create2,"The goal of my project is to get a real-time location information of roobma. The information include the x position, y position and the angle(12 o'clock direction is 0 degree, 3 o'clock is 90 degree and so on)
I use c++ to program the roomba. I want to use ""Stream""(Opcode: 148) to get the date, but it doesn't work for me. Please help me out thanks.
this is the part I get stuck
unsigned char streamCommand[3];
streamCommand[0]=148;
streamCommand[1]=1;
streamCommand[2]=19;
write(robot,&streamCommand,3);
signed char readStream[6];


bool indicator=true;
while(indicator){
    string command="""";
    cout<<""Please input your command""<<endl;
    cin>>command;
    action(command);
    read(robot,&readStream,6);
    cout<<""[0]=""<<readStream[0]<<endl;
    cout<<""[1]=""<<readStream[1]<<endl;
    cout<<""[2]=""<<readStream[2]<<endl;
    cout<<""[3]=""<<readStream[3]<<endl;
    cout<<""[4]=""<<readStream[4]<<endl;
    cout<<""[5]=""<<readStream[5]<<endl;
}

return 0;

}
I use streamCommand is a read command to ask roomba to send back data in stream.
readStream is the array that I store my retreive data.
it doesn't work for me. Please help me out thanks.
","control, sensors, roomba, programming-languages"
Plan robot trajectory in real time under highly dynamic environment,"Are there any open-source and light-weight robotics libraries or software to plan robot (mobile wheeled robot) trajectories in real time under highly dynamic environment (e.g. in RoboCup competitions)? 
I have found many similar stuff but they seem to be more applicable to trajectory planning of robot arms or SLAM research, which are obviously too complicated for me. So it would be great if the open source robotics software is light-weight and easy to use.
","software, path-planning, robotics-library"
unstable quad with naza m lite,"I almost finished my first quad and first time it just flipped(the back over the front 360 degrees)...the motors and  props are installed correctly..i triple checked but i'm still a  noob in this...my quad specs are q330 frame with naza m lite (with gps),hobbywing 30a esc,rs 2205 motors ,flysky fs i6 with ia6b,3s battery ... maybe someone can help with my problem...i don't want to loose to many props or even worse...thanks
",quadcopter
Dji Wookong-M - To unstable to take off,"I've built a quadcopter using the Dji Wookong-M. As of a couple of weeks ago I have been able to get everything to work except for one small thing. When I throttle up the Drone tends to flip to the side. I have tested all the motors over and over again and I know that they are spinning the right direction and that I have the right props on the right motors. I tested on both grass and concrete but both times it flipped. It starts to flip once the throttle is past 50%. I don't know if it is catching or if something is off balance although I don't think this is the problem since the quadcopter tips different directions almost every time. If any one could tell me what is wrong I would appreciate it a lot since my project is due in 2 1/2 weeks.
Thanks in Advance
",quadcopter
Problems using syskit monitors -> failed emission of the foo event of,"I had just tested my first monitor, which results in the following error
regarding the suggestion in How to define conditions for state-machines in roby?
unfortunately i ran into a runtime error, i don't know whether this is a bug or if i misuse the monitor...
16:28:27.564 (Roby) = failed emission of the weak_signal event of Pipeline::Detector:0x71f5cf0
16:28:27.564 (Roby) = Backtrace
16:28:27.564 (Roby) |
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/task.rb:663:in `emitting_event'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/task_event_generator.rb:46:in `emitting'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/event_generator.rb:628:in `emit_without_propagation'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1017:in `block (2 levels) in event_propagation_step'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:648:in `propagation_context'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1015:in `block in event_propagation_step'
16:28:27.564 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:559:in `block in gather_propagation'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:648:in `propagation_context'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:559:in `gather_propagation'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1014:in `event_propagation_step'
16:28:27.565 (Roby) |/home/auv/dev/tools/roby/lib/roby/execution_engine.rb:783:in `block in event_propagation_phase'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:761:in `gather_errors'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:779:in `event_propagation_phase'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1426:in `process_events'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1940:in `block (2 levels) in event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/support.rb:176:in `synchronize'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1939:in `block in event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1917:in `loop'
16:28:27.565 (Roby) /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1917:in `event_loop'
16:28:27.565 (Roby) | /home/auv/dev/tools/roby/lib/roby/execution_engine.rb:1797:in `block (3  levels) in run'
16:28:27.565 (Roby) =

Don't know whether this is a bug, or if i had miss-used the monitor... 
here is the action_state_machine i'm using:
183     describe(""Find_pipe_with_localization"").
184         optional_arg(""check_pipe_angle"",false)
185     action_state_machine ""find_pipe_with_localization"" do
186         find_pipe_back = state target_move_def(... some long stuff here ... )
187         pipe_detector = state pipeline_detector_def
188         pipe_detector.depends_on find_pipe_back, :role => ""detector""
189         start(pipe_detector)
190
191         pipe_detector.monitor(
192             'angle_checker', #the Name
193             pipe_detector.find_port('pipeline'), #the port for the reader
194             :check_pipe_angle => check_pipe_angle). #arguments
195             trigger_on do |pipeline|
196                 angle_in_range = true
197                 if check_pipe_angle
198                     angle_in_range = pipeline.angle < 0.1 && pipeline.angle > -0.1
199                 end
200                 state_valid = pipeline.inspection_state ==  :ALIGN_AUV || pipeline.inspection_state == :FOLLOW_PIPE
201                 state_valid && angle_in_range #last condition
202             end. emit pipe_detector.success_event
# for non-monitor use, this works if the above is commented out
203 #        forward pipe_detector.align_auv_event, success_event
204 #        forward pipe_detector.follow_pipe_event, success_event
205
206         forward pipe_detector.success_event, success_event
207         forward pipe_detector,find_pipe_back.success_event,failed_event #timeout here on moving
208     end

","rock, syskit"
Kalman filter prediction questions,"I have a dataset where measurements were taken at 1 Hz, and I am trying to use a Kalman filter to add predicted samples in between the measurements, so that my output is at 10 Hz. I have it working ok when the velocity is linear, but when the direction changes, the filter takes a while to catch up. I am new to Kalman models, so am very likely making some mistakes in my settings. What can I do to improve this? See attached image for an example, the red is measured data, with stepping in between measurements. The blue is the Kalman corrected. 
std::vector<double> measurements is a dummy data array I am testing with.
The main Kalman code is based on Github: hmartiro/kalman-cppkalman.cpp
My code is:
int main(int argc, char* argv[]) {

  int n = 3; // Number of states
  int m = 1; // Number of measurements

  double dt = 1.0/30; // Time step

  Eigen::MatrixXd matA(n, n); // System dynamics matrix
  Eigen::MatrixXd matC(m, n); // Output matrix
  Eigen::MatrixXd matQ(n, n); // Process noise covariance
  Eigen::MatrixXd matR(m, m); // Measurement noise covariance
  Eigen::MatrixXd matP(n, n); // Estimate error covariance

  // Discrete motion, measuring position only
  matA << 1, dt, 0, 0, 1, dt, 0, 0, 1;
  matC << 1, 0, 0;

  // Reasonable covariance matrices
  matQ << 0.001, 0.001, .0, 0.001, 0.001, .0, .0, .0, .0;
  matR << 0.03;
  matP << .1, .1, .1, .1, 10000, 10, .1, 10, 100;

  // Construct the filter
  KalmanFilter kf(dt,matA, matC, matQ, matR, matP);

  // List of noisy position measurements (yPos)
  std::vector<double> measurements = {
     10,11,13,13.5,14,15.2,15.6,16,18,22,20,21,19,18,17,16,17.5,19,21,22,23,25,26,25,24,21,20,18,16
  };

  // Best guess of initial states
  Eigen::VectorXd x0(n);
  x0 << measurements[0], 0, 0;
  kf.init(dt,x0);

  // Feed measurements into filter, output estimated states
  double t = 0;
  Eigen::VectorXd y(m);


  for(int i = 0; i < measurements.size(); i++) { //ACTUAL MEASURED SAMPLE

      yPos << measurements[i];

      kf.update(yPos);

      for (int ji = 0; ji < 10; ji++)  // TEN PREDICTED SAMPLES
      {
          t += dt;       

         kf.update(yPos);


          yPos << kf.state().transpose(); //USE PREDICTION AS NEW SAMPLE

      }
  }

  return 0;
}

","kalman-filter, c++"
Rotate DC servo motor with ARDUINO,"I have this servo motor (http://robokits.co.in/motors/high-torque-encoder-dc-servo-motor-60rpm-with-uart-i2c-ppm-drive?gclid=CLHf9_fCqNQCFVAeaAodhM0Ddg&). Generally, I have seen servo motor with three wires only but this servo motor have 6 wires. I want to rotate this using PPM signal and accordingly I made connection as described in motor manual. Arduino Code to rotate motor is:
// Include the Servo library 
#include <Servo.h> 
// Declare the Servo pin 
int servoPin = 10; 
// Create a servo object 
Servo Servo1; 
void setup() { 
   // We need to attach the servo to the used pin number 
   Servo1.attach(servoPin); 
}
void loop(){ 
   // Make servo go to 0 degrees 
   Servo1.write(0); 
   delay(1000); 
   // Make servo go to 90 degrees 
   Servo1.write(90); 
   delay(1000); 
   // Make servo go to 180 degrees 
   Servo1.write(180); 
   delay(1000); 
}

But what the motor does is it continuously turning. 
How to control motor position using PPM.
Thanks.
","arduino, servomotor"
Using Gazebo installed on same machine in MATLAB,"I am planning to use MATLAB and Gazebo for one of my course projects. 
However all the tutorials I have seen till now use Gazebo by using a virtual machine which has ROS and Gazebo installed. I have already installed ROS and Gazebo on this machine (OS Ubuntu). I also have MATLAB installed on it. 
Is it possible to use the Gazebo on this machine itself with the MATLAB toolbox? 
","ros, matlab, gazebo"
difference between sabertooth motor controller and rc esc?,"i am building a rc/robot mower.
most of the youtube videos show the sabertooth motor controllers being used to connect the rc receiver to the dc motors. 
But here in Australia, the sabertooth i need cost about 200 dollars. RC esc sell for about 10 dollars. 
What is the difference between the esc and the sabertooth and can i use an esc instead? 
the specs i have on the motor and battery are 12v 10amp normal, 35amp stall. and my rc is a flysky
","motor, esc"
How to implement RANSAC and kalman filter or particle filter algorithms with ROS packages?,"I'd like to implement those algorithms by using ROS packages to solve one way the SLAM problem. I know that gmapping, Rviz, slam_gmapping and robot_pose_ekf (for extended kalman filter) could be useful packages, but I'm kind of lost. I don't ask a tutorial because the next days I'm going to start studying deeper this subject, but I need orientation in the procedure.
""a possible way to implement RANSAC algorithm"" http://pointclouds.org/documentation/...
Note: I'm planning to do something like this indoors. https://www.youtube.com/watch?v=17W8dkzkvWA.
I'm working on ubuntu. I have the ""kinect"" (xbox360), and for now I don't know what kind of cheap 2WD robotic platform choose in pages as amazon, robotshop and ebay (although Canakit 2WD, DFrobot 2WD and alphabot seems to be a good option). More than anything, I need orientation about how to mix everything to solve the SLAM problem with a 2WD robotic platform, the kinect and ROS packages :)
Thanks in advance
","slam, ros, kinect, filter"
Granite devices IONI servo drive,"The IONI drive can accept a new setpoint using an analog input.  How many bits is the ADC on the IONI drive?  Is it 12, or 16 bit?
",servomotor
How to communicate Ardupilot with Arduino,"Can anyone tell me if it is possible to send data through the analogue pins of the Ardupilot to the analog pins of the Arduino? 
For example, I would like to trigger a button on a channel from my radio control and Ardupilot should send a specific number to the Arduino. Would anyone have any idea how I can do this?
Thank you in advance.
","arduino, ardupilot"
Which sensors will best suited for indoor/outdoor SLAM system with high accuracy of localization?,"We need a SLAM system for mobile platform which will be used in on-site construction.
We are to build a 3D map from clouds of points and need localization in it.
We don’t need fast localization, but we need high accuracy of localization, error of localization should be around 1mm at a distance up to 5 meters.
Which sensor will suit for this - maybe it will be enough only a stereo vision (can it give us required accuracy?), or only LiDAR sensor (what’s about accuracy and usage in direct sunlight?) or we need to use something different and combine several type of sensors?
Can you advise the best solution for this task?
","mobile-robot, localization, slam"
How to convert raw Pitot tube readings to air speed in m/s?,"I'm building a kite flying robot, for which I want to measure air speed. So I combined a Pitot tube with an ADC and connected it to my Raspberry Pi using this tutorial. 
It seems to work perfect, in that it receives data, but I'm not sure what this data tells me. 
I get a constant stream of numbers, mostly between 502 and 504 when nothing happens. When I blow on the tube the number increases to 550-600, or even up to 1000 when I put my lips on the tube and blow with force. 
My question is now; what does this tell me about the air speed in meters per second? 500 seems to be zero air speed, but what does 550 or 600 tell me? Is there some kind of conversion table for this? 
All tips are welcome!
","sensors, python, uav, pitot-tube"
Fetching encoder value with SimpleMotion library by Granite Devices,"Is it possible to fetch the current encoder position of a drive with the old SimpleMotion library (not V2) and a Granite Devices VSD-E through the smRawCommand(...) function?
",servomotor
Is there an online RobotC IDE?,"I'm on a Linux chromebook, and am using a Vex robot, and am wondering if there is an online IDE for RobotC that will run my computer?
","robotc, vex"
Can a state matrix have a row of zeros?,"While formulating a state matrix of a system, say a system of a typical cruise controller,
\begin{equation}
\begin{bmatrix}
\dot{v}
\end{bmatrix} = \begin{bmatrix} -\frac{b}{m} \end{bmatrix} \begin{bmatrix} v \end{bmatrix} + \begin{bmatrix} -\frac{1}{m} \end{bmatrix} \begin{bmatrix} u \end{bmatrix}
\end{equation}
\begin{equation}
y = \begin{bmatrix} 1 \end{bmatrix} \begin{bmatrix} v \end{bmatrix}
\end{equation}
If we consider $b = 0$ (negligible), then does this state space matrix make sense any more and is it viable? A state matrix without state variable influence in equation?
","control, design, dynamics, self-driving"
Granite Devices Ioni Pro. PWM set point mode,"Inspecting the PWM set point mode it seems HSIN2 is the PWM input, and HSIN1 is the PWM direction input.  Is it possible to reverse this, so that it's more consistent with step and direction mode wiring?  I would like HSIN1 as the PWM input, and HSIN2 as the PWM direction.
Could the function of HSIN1, and HSIN2 be selected in Granity, once the setpoint type is chosen?
Most grateful for your help,
Mark
",motor
Help !!Homogeneous transformation matrix,"I came across many good books on robotics. In particular I am interested in Inverse kinematic of 6dof robot. All books have example which goes on like this ""given homogeneous transformation matrix as below, find the angles ?"".. Problem, is how do I find components of a homogeneous transformation. matrix in real world? i.e. how do i practically derive 9 components of the rotation matrix embeded in homogeneous transformation matrix?.....
",inverse-kinematics
Odometry motion model with PID controller in SLAM,"I wrote a MATLAB code with a PID controller to force the robot to go on a square trajectory. Actually it has simple kinematics with constant velocity, which I control the angular velocity to force the robot to go on a square. So I have many points (on the square trajectory) and the PID controller also have some noise in the control command that gave to plant at each iteration and the robot tries to move between these points. 
I want to do SLAM, in the end, but I'm little confused about the prediction step that uses the odometry information. I know that we must use the odometry information which have noises in it in Kalman filter but I can't figure out what model to use in MATLAB. I mean, should I use the kinematics that I used for simulation, and add some noise to it and use it in the filter?
I tried to use an odometry model in a probabilistic robotic book but I can't figure out the algorithm.
It uses 3 deltas and it uses 3 delta hats.
I don't know how to find them.
I know the formulas but I don't know which points in my square trajectory should I use to compute them. Which points are the query points?

","pid, slam, kalman-filter"
Estimate Disparity Error for Depth Accuracy Estimation,"I would like to get a rough estimate of the depth accuracy / uncertainty of a stereo camera system. For this I would like to use the basic formula in the attached image. What is still unclear to me is what's a reasonable choice for the disparity error delta_d. Unfortunately, for example this answer (in point ""Resolution"") describes the assumption of the size of the disparity error only very quickly. 
Is a reasonable assumption just the width of one pixel? Why yes, why not?

",stereo-vision
Problems connecting to the Granite Devices USB6AX motion controller,"I am working on a setup from 2008 with a 6 axis USB motion controller and 4 vsd-e drives very similar to the setup shown here.
The motion controller board is exactly the same as in the first post of these two threads, it also says ""USB-SPI 6 axis Rev 0"" on the PCB:

Motion controller board example 1.
Motion controller board example 2.

I have installed GDtool and SimpleMotion library. When trying to connect to the device with GDtool, the first step produces the following output in the event log:
USB mode configured.

Enabling configure mode.
Shell command: OPEN NORMAL

Running OPEN...
Normal mode enabled
> 
Got input from SPI shell
Saatiin viesti:  0

Parametreja: 2
 0

USB mode configured.
isBootloaderMode

The second step fails with the message ""Connection failed. Please check connections."" and the following output in the event log:
Configuring connection
Shell command: OPEN

Running OPEN...
cDevice::sendCommandOnly( 2, 0 )
0x200008a -> 0x0
cDevice::sendCommandOnly( 2, 0 )
0x200008a -> 0x0
cDevice::sendCommandOnly( 2, 0 )
0x200008a -> 0x0
Connection failed.
> 
Got input from SPI shell
Saatiin viesti:  0
 0

Parametreja: 3
 0

setConnectionStatus
connected
Connection failed
 0

setConnectionStatus
connected
Connection failed

I played around with the example program ""SimpleMotionTest"" and ""FT_Prog"" by FTDI to manipulate the USB-controllers Product Description string. The best I could do with SimpleMotionTest was ""Communication error. Possibly drive not in SPI mode.""
It seems that the axis names are given by the USB Product Description string, because if axis name in SimpleMotionTest and the descriptor string do not match, it says ""USB device with given axis not found."" This makes sense with the Granite Devices tuning cables but not with a 6 axis controller which can only have one Product Descriptor string.
Is it possible to configure the USB6AX with GDtool? Is it possible to control it with the SimpleMotion library? If yes what am I doing wrong and if no what is the suitable configuration utility and how can I interface the controller with my LabView/C++/... application?
If anyone is out there who still uses the same controller thanks you for sharing you experiences!
","servomotor, usb"
Granite Devices IONI current (torque) spikes,"I am trying to tune the IONI Pro for a Maxon brushless motor and things look find when I have the shaft locked but when I am finished and let the motor spin I see current drop out spikes at a high frequency. Is this normal?
At first I thought it might be because of the extremely lower motor inductance, around 0.13mH but I added a 1mH series inductor on each motor lead, re-tuned and still see the same thing.
This seems to cause an over current fault because as the motor runs the current continues to increase beyond the target and will trip the limit and stop. If this is true current control then why is the current increasing until the limit is tripped?

",brushless-motor
hector SLAM in ROS with RPLIDAR,"Trying to set up Hector SLAM with the RPLidar A2.  I downloaded both rplidar_ros-master and hector_slam-catkin and extracted them into my catkin_ws/src folder and ran catkin_make. Then I edited the mapping_default.launch file and changed the next to last line:
<node pkg=""tf"" type=""static_transform_publisher"" name=""base_to_laser_broadcaster""
 args=""0 0 0 0 0 0 base_link laser 100"" />

Then, after running the roslaunch command on rplidar.launch and on tutorial.launch  RVIZ starts but fails to generate a map and gives the warning:
No tf data. Actual error: Fixed Frame [map] does not exist

Do I need to add the 'map' Fixed Frame to the tf node in my 'mapping_default.launch' file?
","ros, raspberry-pi, lidar"
.launch file associated while using a sensor,"What is the general structure of .launch file for using a sensor?
For example:
1.Following code is example for using a JoyStick to control TurtleSim
<launch>
  <node pkg=""turtlesim"" type=""turtlesim_node"" name=""sim""/>
  <node pkg=""chapter4_tutorials"" type=""example1"" name=""example1"" />
  <param name=""axis_linear"" value=""1"" type=""int"" />
  <param name=""axis_angular"" value=""0"" type=""int"" />
  <node respawn=""true"" pkg=""joy""type=""joy"" name=""teleopJoy"">
    <param name=""dev"" type=""string"" value=""/dev/input/js0"" />
    <param name=""deadzone"" value=""0.12"" />
  </node>
</launch>

2.Using a Laser Range finder
<launch>
  <node pkg=""hokuyo_node"" type=""hokuyo_node"" name=""hokuyo_node""/>
  <node pkg=""rviz"" type=""rviz"" name=""rviz""
        args=""-d $(find chapter4_tutorials)/example2.vcg""/>
  <node pkg=""chapter4_tutorials"" type=""example2"" name=""example2"" />
</launch>

What exactly is the syntax?
Why do we have two <node pkg>?
","sensors, ros"
Create 2 packet group 100,"I tried getting pack sensors for packet group 100. What I noticed was the order of the packets did not match what's in the document. Anyone notice the same problem?
For example here is the output of packet group 6 & 101 - which should be the same at 100 but that's not the case. It would seem like the order of the packets are not the same in group 100.
Here is packet group 6:
2017/05/31 193538 PACKET GROUP 6 len:52
        Bump & Wheel Drop:  0  - 1
                     Wall:  0  - 1
               Cliff Left:  0  - 1
         Cliff Front Left:  0  - 1
        Cliff Front Right:  0  - 1
              Cliff Right:  0  - 1
             Virtual Wall:  0  - 1
        Wheel Overcurrent:  0  - 1
              Dirt Detect:  0  - 1
                 Unused 1:  0  - 1
             Omni IR Code:  161  - 1
                  Buttons:  0  - 1
                 Distance:  0 - 2
                    Angle:  0 - 2
                 Charging:  0  - 1
                  Voltage:  15140 - 2
                  Current:  -211 - 2
              Temperature:  25  - 1
           Battery Charge:  2234 - 2
         Battery Capacity:  2696 - 2
              Wall Signal:  0 - 2
        Cliff Left Signal:  2938 - 2
  Cliff Front Left Signal:  2047 - 2
 Cliff Front Right Signal:  1399 - 2
       Cliff Right Signal:  2232 - 2
                 Unused 2:  0  - 1
                 Unused 3:  0 - 2
          Charging Source:  0  - 1
                  OI Mode:  1  - 1
              Song Number:  0  - 1
             Song Playing:  0  - 1
       Num Stream Packets:  0  - 1
            Req. Velocity:  0 - 2
              Req. Radius:  0 - 2
      Req. Right Velocity:  0 - 2
       Req. Left Velocity:  0 - 2

Here is packet group 101:
2017/05/31 193859 PACKET GROUP 101 len:28
             Left Encoder:  10 - 2
            Right Encoder:  6 - 2
                   Bumper:  0  - 1
       Bumper Left Signal:  11 - 2
 Bumper Front Left Signal:  6 - 2
Bumper Center Left Signal:  8 - 2
Bumper Center Right Signal:  0 - 2
Bumper Front Right Signal:  11 - 2
      Bumper Right Signal:  0 - 2
             IR Code Left:  0  - 1
            IR Code Right:  0  - 1
       Left Motor Current:  0 - 2
      Right Motor Current:  0 - 2
       Main Brush Current:  0 - 2
       Side Brush Current:  0 - 2
                   Stasis:  0  - 1

And here is packet group 100:
2017/05/31 193654 PACKET GROUP 100 len:80
        Bump & Wheel Drop:  0  - 1
                     Wall:  0  - 1
               Cliff Left:  0  - 1
         Cliff Front Left:  0  - 1
        Cliff Front Right:  0  - 1
              Cliff Right:  0  - 1
             Virtual Wall:  0  - 1
        Wheel Overcurrent:  0  - 1
              Dirt Detect:  0  - 1
                 Unused 1:  0  - 1
             Omni IR Code:  161  - 1
                  Buttons:  0  - 1
                 Distance:  0 - 2
                    Angle:  0 - 2
                 Charging:  0  - 1
                  Voltage:  15140 - 2
                  Current:  -219 - 2
              Temperature:  25  - 1
           Battery Charge:  2230 - 2
         Battery Capacity:  2696 - 2
              Wall Signal:  0 - 2
        Cliff Left Signal:  2950 - 2
  Cliff Front Left Signal:  2198 - 2
 Cliff Front Right Signal:  0 - 2
       Cliff Right Signal:  3072 - 2
                 Unused 2:  0  - 1
                 Unused 3:  0 - 2
          Charging Source:  0  - 1
                  OI Mode:  0  - 1
              Song Number:  0  - 1
             Song Playing:  0  - 1
       Num Stream Packets:  0  - 1
            Req. Velocity:  0 - 2
              Req. Radius:  0 - 2
      Req. Right Velocity:  0 - 2
       Req. Left Velocity:  0 - 2

             Left Encoder:  8 - 2
            Right Encoder:  4 - 2
                   Bumper:  0  - 1
       Bumper Left Signal:  11 - 2
 Bumper Front Left Signal:  6 - 2
Bumper Center Left Signal:  9 - 2
Bumper Center Right Signal:  0 - 2
Bumper Front Right Signal:  0 - 2
      Bumper Right Signal:  0 - 2
             IR Code Left:  0  - 1
            IR Code Right:  0  - 1
       Left Motor Current:  0 - 2
      Right Motor Current:  0 - 2
       Main Brush Current:  0 - 2
       Side Brush Current:  0 - 2
                   Stasis:  0  - 1

",irobot-create
Live Streaming video via Arduino with limited space,"I am currently undertaking a competition called CanSat, in which you build a satellite in the form of a standard soda can. It will in turn then be launched 30 meters, 100 meters and then finally 500 meters into the sky. I'm stuck on something and I need a little bit of help.
Basically I am looking at streaming low quality video (480p-10fps/ 240p-30fps), using an Arduino Uno/Mega, inside of a can. The spacial limitations I mentioned above are 115 mm in height and 65 mm in diameter. I will not be able to use a conventional IP camera, due to the limitations.
I was thinking about using a Bluetooth V2 Chip in order to achieve the transfer rate to stream to a PC. I am looking for the following help:

What camera to use
What software to use to receive the video on a PC 
And finally any other relevant information about the limitations of the Arduino and streaming.

","arduino, cameras"
How to choose a motor for water surface vehicle?,"I am about to build a water surface vehicle (a kind of boat). 
I was looking for different type motors and ruled out the stepper and servo motors. But I got confused between brushed and brushless dc motors. Brushless motors on one hand are more efficient but brushed are low cost and less complex. 
Can you explain to me or get me some resource on how to find the best motor for low as well as for high budget? I am expecting the load to be around 12 kg. Please draw my attention to the necessary requirements that need to be keep in mind while choosing the motors. Also suggest me link where I can study about water thrusters and the propellers.  
","mobile-robot, motor, underwater"
Multiple robots in ROS Gazebo SITL with separate MAVlink/MAVproxy codes,"Background:
This question is about simulating one ErleCopter and one ErleRover simultaneously for a non-commercial research. I would like to have the quadcopter follow a rover, which in turn is to be tasked with following a line. I am trying to spawn the vehicles in Gazebo and control them using MAVProxy.
The Problem:
Any time I try this, I run into one of two problems:

Spawning the second vehicle terminates the first MAVProxy instance, or
The second vehicle spawned cannot be linked to the second instance of MAVProxy. 

I'm not sure what to do about this because I am not sure if this problem is one problem or if it's composed of two sub-problems. The first problem is in spawning the robots and the second one is in controlling both independently (and obtain the state parameters of both vehicles and use that as feedback).
I believe a contributing factor to this problem is that I'm trying to do the simulation and both MAVProxy instances on one computer, a Lenovo-y50-70 is being used with Ubuntu 14.04. Two computers are not easy to obtain immediately and there are network stability issues where I am.
The Question:
The entire question probably reduces to ""How to link second robot spawned by rosrun to second MAVproxy instance?"".
Desired Outcome:
I would like help either getting the simulation to run as desired (two vehicles co-simulated in one virtual world with two MAVProxy instances, one linked to each vehicle), OR official documentation somewhere that this is not possible.
What I've Attempted:
Initial attempts can be seen in earlier edits of this question. For clarity, that information has been removed, but again, if interested, see an earlier version of this question. Fast-forwarding to the relevant:
The second robot has been spawned successfully as mentioned in this video by using the commands mentioned below.
cd path_to_urdf_model_files
rosrun gazebo_ros spawn_model -file rover.urdf -urdf -model rover_object

Note: ROS Indigo is different; gazebo_ros must be used instead of gazebo_worlds
The weird behaviour of the robot rotating about itself is probably because of MAVproxy; I have experienced this before. Attempt to establish separate network connections to Copter and Rover has been successful so far. The original structure is as shown below: 

rover_circuit.launch -> apm_sitl.launch -> node.launch (node name:
  ""mavros"")

The current architecture is as shown below:

copter_circuit.launch -> apm_sitl_copter.launch -> node_copter.launch
  (""mavros_copter"")
rover_circuit1.launch -> apm_sitl_rover.launch -> node_rover.launch  (""mavros_rover"")

rover_ciruit1.launch is as shown below:
<launch>

  <include file=""$(find mavros)/launch/apm_sitl_rover.launch""></include>
  <arg name=""enable_logging"" default=""true""/>
  <arg name=""enable_ground_truth"" default=""true""/>
  <arg name=""log_file"" default=""rover""/>
  <arg name=""tf_prefix"" default=""$(optenv ROS_NAMESPACE)""/>
  <arg name=""model"" default=""$(find ardupilot_sitl_gazebo_plugin)/urdf/rover.urdf""/>

  <param name=""robot_description"" command=""
    $(find xacro)/xacro.py '$(arg model)'
    enable_logging:=$(arg enable_logging)
    enable_ground_truth:=$(arg enable_ground_truth)
    log_file:=$(arg log_file)""
  />
  <param name=""tf_prefix"" type=""string"" value=""$(arg tf_prefix)"" />

  <node name=""spawn_rover"" pkg=""gazebo_ros"" type=""spawn_model""
    args=""-param robot_description -urdf -model 'rover' "" respawn=""false"" output=""screen""></node>

</launch>

This is the minimal launch file, and it works. I had thought of rosrun apm_sitl_rover.launch followed by rosrun rover.urdf, but I have been unable to find a suitable package which launches apm_sitl_rover.launch directly. It is easy to roslaunch a launch file that has an  method appended. 
Naming issues and other network errors have been resolved. 
Outstanding Problem Remaining:
I'm still having issues launching and linking the second vehicle, but now it seems like I've narrowed it down such that the only problem is with UDP bind port, which is 14555 by default, and this is crashing Gazebo for the second instance because the second instance is using the same bind port.
It looks like libmavconn is getting called somehow, particularly interface.cpp which has url_parse_host(bind_pair, bind_host, bind_port, ""0.0.0.0"", 14555); and the udp.h included in interface.cpp has a function MAVConnUDP() which has bind_port=14555, and this has resulted in ""udp1: Bind address: 14555"" and ""GCS: DeviceError:udp:bind: Address already in use"".
Trying to assess the connection between sim_vehicle.sh, libmavconn, and Gazebo, I was able to figure out that sim_vehicle.sh calls mavproxy.py in one of the ending lines which in turn uses pymavlink. I have been unable to find further relationship currently.
Leading Questions / An Approach to the Solution
As I have a strong intuition that this is the final stage, I currently resolve to fix this by using interface_copter.cpp and interface_rover.cpp. I think, if I could get answers to the following questions, I can work out where the failure is in successfully launching and link the second (or subsequent) vehicles:

How does sim_vehicle.sh trigger the libmavconn package and ultimately Gazebo?
Is there a software architecture diagram which describes the complete structure from sim_vehicle.sh to joints and controllers?

","quadcopter, ros, software, simulation, gazebo"
iRobot Create 2 to Vacuum (2)?,"Does the Create 2 have firmware support (navigation, brush & fan control, ...) to support the vacuum functionality? 
Can I use parts from my 655 or 805 Roomba to convert the Create 2 back to a vacuum?
",irobot-create
Coordinate transform of accelerometer on rigid body,"I have a question about something that seems like it would be pretty basic, but so fair I haven't been able to find a whole lot of discussion on the issue. It's possible I'm not not familiar enough with the terminology. 
I have a rigid body with an accelerometer/gyro IC dev board nailed to it. I would like to know what the accelerometer would measure at another point on this board, in this case, the sensor of a camera that is also nailed to it. 
My thinking is that I can use the accelerator, gyroscope and differentiated gyroscope data and the equation $a_t = a_m + \omega' \times r + \omega \times (\omega \times r)$, where 
$a_t$ = transformed acceleration
$a_m$ = measure acceleration
$\omega$ = measured gyroscope reading
$\omega'$ = first derivative of the gyroscope reading
$r$ = the vector between the accelerometer/gyro and the point I want transformed to. 
My plan is to get $\omega'$ with a Savitzky-Golay filter, though this makes implementation a lot less convenient, because I have to buffer my data, and try to figure out how the filter effects the noise variance of the sensor. 
Does this plan make sense? Is there a better accepted way that I don't know about? I'm surprised that ROS or tf2 doesn't have a built in function for this. Is there something I am missing? Thanks!
",accelerometer
"What are good, low cost, actuators for a braille tablet to be controlled by arduino?","I want to basically make a pin matrix controlled either by spring, electromagnets or small motors(spring being the most viable option), something like what’s shown in the image. I'm pretty new to arduino and hardware in general so any input would be appreciated.
I mostly know the arduino end but don't have clue about the hardware part. Plus I don't have the technical expertise, as in I know electromagnets won't be a good option as I have to control individual pins and not clusters. Plus springs have the disadvantage of pushing them back in but other than that a very option. And its not viable to have individual motors for so many pins.

","arduino, motor, electronics"
Making a robot move straight between two maze walls,"Using a PID with encoders, I can make the robot move straight but there is a 0.5 degrees drift and it eventually hits a wall so I need to adjust to center it between the two walls. I have a sensor on each side that gives me the distance from the wall, so What's the best approach to make the robot adjust when it comes to close to one wall?
","arduino, sensors, pid"
IMU in a gimbal system,"I have short question about the alexmos gimbal controller. The Controller receives the gyroscope and accelerometer sensor data from the IMU, that is mounted on the camera. In optimal case, the camera should stay in perfect position, which means, that there would be no gyroscope data, since there is no movement. So the only data for positioning would be the accelerometer. Is there a second IMU onboard that receives the gyrometer data?
All the gimbal controllers from aliexpress seems not to have an IMU onboard, but it that case the controller can only use the accelerometer, right?
","motor, imu, cameras"
Differential GPS,"I want to use DGPS on a robot. I understand how DGPS works but I am having trouble figuring out what specific hardware I need. Is there a good resource for how to actually setup DGPS? Thanks for your help
","localization, gps"
Ardupilot on Raspberry Pi,"I'm making a quadcopter for the first time and want to be as low cost as possible, and when it comes to flight controllers APM is the best open-source project, and is compatible with Raspberry Pi.
My question is, will it perform as good as of the shelf APMs, if I add GPS, IMUs, and all of the other sensors, the same as in ready-made?
Consider that APM is clone from ebay.
","quadcopter, imu, gps, multi-rotor, ardupilot"
Calibrating a 2D LiDAR and a camera,"I am trying to fuse the camera data and the LiDAR data to get the distance of points along a row in a camera. I've read a paper that proposes an algorithm for the calibration by using 8 point line correspondences. It says

Let $l = [a,b,c]^T$denote the homogeneous coordinate of the image of
  L in the image plane.

L is a spatial line that can be seen both by the camera and LiDAR.
How do I get the homogeneous coordinates of the image of line L in the image plane?
","cameras, calibration, lidar, geometry"
Irobot Create 2 Open Interface Communication,"I am able to connect the create 2 robot to my laptop with a serial cable.  I am using putty_beta terminal to run. I am not able to key in commands in the terminal. but I am able to receive information from the robot. I want to achieve two way serial connection. What can I do?

","irobot-create, serial"
Do robots usually have databases?,"I am a junior web developer working mainly with Bash, Javascript, and Drupal. I admit I am much more fascinated writing scripts and programs doing certain concrete actions instead of querying and manipulating databases. I do have the desire to step into robotics in the future, after completions and I while thinking about this the following question came to my mind:
Do robots usually have databases and if so, please give a practical example what are they using for? Maybe in the context of machine vision or machine motion.
",data-association
Calculate Transformation between two point sets - but with constrained Degrees of Freedom,"How can I calculate the rigid-body transformation [R|t] between two 3d triangles, but restricted to a given N degrees of freedom (for N = 1..6) ?
I know for N=6 I can get a least-squares solution via SVD of a certain matrix, but how can I integrate further constraints (fewer DOF) into the system?
","computer-vision, kinematics, inverse-kinematics, rotation"
How to model transition matrix in indirect kalman filter with external orientation estimate,"I am trying to implement an indirect/error state kalman filter following http://www.iri.upc.edu/people/jsola/JoanSola/objectes/notes/kinematics.pdf.
However, instead of modelling the orientation and error in orientation I have chosen to utilize Madwick (http://x-io.co.uk/res/doc/madgwick_internal_report.pdf) to estimate the orientation.
The problem is that when I create the transition matrix from the first paper it expects the orientation error which it multiplies with the skew matrix of the measured acceleration and the accelerometer bias (page 40, equation 204). Since I have removed that from my states I can't use it, but then the measured acceleration is never considered (which I assume makes the filter worse). Is there any change I can make to the transition matrix so that it accounts for the acceleration?
","kalman-filter, sensor-fusion"
Create a simple C++ client Application to control KUKA's Robot-arm LBR iiwa via FRI,"Until now I have been programming the robot using Java on KUKA's IDE ""KUKA Sunrise.Workbench"", what I want to do is control the robot arm via my C++.Net application (I would use a camera or Kinect to get commands). 
I'm reading the documents provided by Kuka, but as I'm a bit in hurry, I want to understand how a C++ client application (running on my laptop) can send/receive information to/from the robot's controller ""KUKA Sunrise Cabinet"" (running the server application) via FRI. I still have issues grasping the whole mechanism.
A simple application (Server/Client) source code with explanation (or a schematic) would be more than helpful .
","robotic-arm, c++"
Correct Fish eye camera radial distortion and blurry stretch at the image edges,"There are a lot of questions regarding this topic, but I am trying to get a more clear picture from these questions.
I am trying to calibrate a fish eye camera and I am using OpenCV cv::omnidir class functions to find the camera intrinsics.
I am getting fair results. The problem is, at the image edges, the objects get stretched and I am, also, losing some information at the edges.
Here is my input image:

Here is my output image:

As you can see, I am losing some information at the edges (left and right) and also the images start stretching at the edges.
My questions are as follows:

How I can I include more FOV in the corrected image at the edges, where the information is lost?
How can I reduce the blur effect at the edges?
During calibration, should I cover the entire FOV of the camera so that the corners are present at the edges also?
What is the correct way of showing the patterns while calibration?
Are there any online tool boxes which provides fish eye calibration.

Here is my code snippet for calibration and testing:
//Calibration
Mat K, xi, D, idx;
int flags=0|omnidir::CALIB_FIX_SKEW | omnidir::CALIB_FIX_K1 | 
 omnidir::CALIB_FIX_K2;

TermCriteria critia(cv::TermCriteria::COUNT + cv::TermCriteria::EPS, 200, 
0.0001);

vector<cv::Mat> rvecs, tvecs;

double rms = cv::omnidir::calibrate(obj_points, image_points, image_size, K, 
xi, D, rvecs, tvecs, flags, critia, idx);


//Testing
Mat R = Mat::eye(3, 3, CV_32F);
Mat Mapx, Mapy;
Mat New_camera_mat(3,3,CV_32F);

//New_camera_mat tries to get entire FOV,but it is losing some information 
  at edges

New_camera_mat.at<float>(0, 0) = 100; New_camera_mat.at<float>(0, 1) = 0; 
New_camera_mat.at<float>(0, 2) = 1280/2;

New_camera_mat.at<float>(1, 0) = 0; New_camera_mat.at<float>(1, 1) = 100; 
New_camera_mat.at<float>(1, 2) = 720/2 ;

New_camera_mat.at<float>(2, 0) = 0; New_camera_mat.at<float>(2, 1) = 0; 
New_camera_mat.at<float>(2, 2) = 1;

cv::omnidir::initUndistortRectifyMap(K, D, xi_Right, R, New_camera_mat, 
image_size, CV_32F, Mapx, Mapy, cv::omnidir::RECTIFY_PERSPECTIVE);

remap(distorted_frame, undistorted_out_frame, Mapx, Mapy, INTER_CUBIC);

","computer-vision, cameras, calibration"
What is the intuitive explanation of using Jacobian of observation model while calculating Kalman gain in EKF SLAM?,"The idea of using Kalman gain in EKF SLAM is to figure out how much we trust our motion model and sensor/observation model. As explained in The Extended Kalman Filter: An Interactive Tutorial for Non-Experts - Part 5: Computing the Gain, the Kalman gain can be calculated as,
$$K_t = \frac{p}{(p +r)}$$
where $p$ denotes prediction error and $r$ denotes sensor noise. 
Now, if we look into the equation in the image, 
$$K_t = \bar{\Sigma_t}H_t^T(H_t\bar{\Sigma_t}H_t^T +Q_t)^{-1}$$
we can see that Kalman gain is calculated using Covariance matrix ($\Sigma$), Jacobian of observation model ($H$) and Sensor noise ($Q$). Comparing with earlier equation, $p$ can be considered equivalent of $\Sigma$, while $r$ can be equivalent of $Q$. 
How does $H$ fit in, in this equation? What would be an intuitive explanation?
","mobile-robot, slam, kalman-filter, navigation, ekf"
Unmanned guard boat,"I need an urgent help in my graduation project on unmanned guard boat. 
I need to control the boat autonomously and manually using ArduPilot and Arduino Uno.
I just ask if anyone has the code, that can be written on the ArduPilot, that enables control both:

autonomously, using way-points, and;
manually, using LabVIEW,

and then sends the data through the Arduino to complete the movement of the boat as the motors are connected to the Arduino. 
Furthermore I need to know to send GPS coordinates using a U-blox NEO-7M connected to ArduPilot running on an Arduino to be displayed through the serial port of PC on the software. 
","simulation, simulator"
Robot end effector sensor query,"I am currently in the process of implementing a Fanuc robotic arm at our company and have just received the pneumatic gripper end effector to go on the end of the arm (which we won't have for about a week). 
The end effector has come with both NPN and PNP sensors which attach into machined slots in the side of the gripper.
I have very little experience or understanding of electronics so my question is really, what is the purpose of these sensors when used in conjunction with an end effector. Do they enable and disable the gripper or is that done through controlling the air supply?
Many thanks for any help!
","robotic-arm, sensors"
PLCs: Values for peripherally addressed outputs copied to process image?,"When a PLC uses peripheral addressing for an output (i.e. the value is written straight to the output as opposed to the output image), is the value also copied to the output image? The Norton Stuxnet Dossier (http://www.symantec.com/content/en/us/enterprise/media/security_response/whitepapers/w32_stuxnet_dossier.pdf) states that ""When the peripheral output is written to [by Stuxnet], the attack code intercepts the output and ensures it is not written to the process image output"" (paraphrased).  I can only reason that it must be addressing the peripheral device via peripheral addressing, and that this value would also be copied to the process image did Stuxnet not intercept it.
Thanks.
",plc
Will two 8-bit microcontrollers running in parallel perform similarly to a single 16-bit microcontroller?,"Let's say I have multiple (8 bit) sensors which are sending signals to two microcontrollers simultaneously. In this case, I am looking to harness the parallel processing capability of the two microcontrollers to process the signals at the same time.
However, I am curious whether I can substitute the above setup by using only one 16-bit microcontroller? Then, would sending two 8-bit signals simultaneously be possible, using the 16-bit bus on the microcontroller?
Assuming in both cases above, they are running at the same clock speed (MHz).
Edited
Sorry, this is all still new to me. 
One aspect that I'd like to understand is: Can we make two 8-bit signals from two different 8-bit sensors share the 16-bit bus at the same time?
","arduino, mobile-robot, microcontroller"
Why are some motor shafts stiff while others spin freely when not powered?,"Most servos and steppers I've worked with are able to spin manually when no power is applied, but I've come across a couple which are stuck in their position. I'm not sure if they are just very difficult to spin manually, or if they would break if forced. 
For example, I have this stepper and servo which seem impossible to turn manually.
Is there a specification that would tell you whether or not the motor can spin without power?


","stepper-motor, servos"
Wind flow diagram of a quadcopter,"I'm trying to determine the wind flow diagram around a quadcopter when it is in action. I looked up on internet but couldn't find any reliable source.
By wind flow diagram, what I mean is when my quadcopter is in mid-air, hovering at some fixed position, how the air is moving around it? All the directions are needed to be kept in mind, from top to bottom (vertical direction) and also the horizontal direction.
","quadcopter, dynamics"
Why do the motors of my quadcopter not deliver sufficient power?,"I am building a quadcopter using the tutorial The Ultimate PVC Quadcopter.
When ever I go to lift off, at full throttle, the motors spin but the quadcopter doesn't go anywhere. I have checked again and again and the motors are spinning in the right direction, and have the right propellers. Does anybody know why my quad won't fly?
I am using a KK2.1.5 flight controller, propellers marked with 1045r on the counter clockwise motors and the propellers just marked with 1045 on the clockwise motors.
If I switch the propellers, 1045r clockwise and 1045 counter clockwise, then my quadcopter flips over.
The layout of the motors is:
1. CW     2. CCW
4. CCW    3. CW

I am a beginner and this is the first drone I have built/owned.
P.S. My quadcopter weighs 3.2 pounds, is using 980 kv motors, 10"" propellers, 20 A ESCs, and a 3S 50C 2200 mAh Li-Po battery.
","quadcopter, radio-control, multi-rotor"
Using a six wire stepper motor with L298n,"I am using a L298n IC and (not a driver shield) and an Arduino.
I would like to know how to use the IC with the Arduino to run a six wire stepper motor.
Could I have a detailed explanation for wiring the IC connections on the breadboard and the Arduino?
",stepper-motor
Robots with less than 6 degrees of freedom,"Is there any possibility to have a robot with less than six degrees of freedom & still be able to achieve any position & orientation around end effector?
","inverse-kinematics, first-robotics"
BeagleBone to Arduino analog communication,"
I have a BeagleBone Black, an Arduino Duemilanove and a WS2812b LED strip driven with 5 V.
I have no level shifters, nor I intend to order one.
The Arduino works perfectly with a LED strip, and I need the BeagleBone Black just to send simple commands like 0, 1, 2, 3, 4, 5... to make the Arduino perform one of five modes.
The BeagleBone Black's USB port is taken by other device that I need, and I do not intend to use USB splitters.

What is the best way to communicate between the BeagleBone Black and the Arduino in this situation? If I do not want to use level shifter I2C and RS232 TTL communication is impossible? 
Is it OK to use for example just one wire (plus ground) and communicate via the analog port using analogWrite() on the BeagleBone Black and analogRead() on the Arduino, and is there some additional advice on communicating this way? 
","arduino, beagle-bone"
Dashboard for IRobot Create 2,"In case anyone is interested, I build a dashboard for the icreate 2.(http://blog.mindfront.net/2017/06/roomba-dashboard-cli-dashboard-for.html)
","control, irobot-create"
Confusion regarding set point calculation in presence of non-zero yaw in quad-rotor,"I have recently started working on a project to fly a quadrotor through a Matlab interfaced software and I was going through their sample controller code.
A function called ""adjusted pitch and roll command"" is used which modifies the given roll and pitch setpoint if any non-zero yaw is present (I think they assumed reference yaw as 0).
function adj_roll_cmd = fcn(roll_cmd, pitch_cmd, heading)

% Given the roll and pitch commands computed for the global frame, this 
function
% rotates the roll command into the local frame, allowing the control
% command to work even for non-zero yaw.

adj_roll_cmd = roll_cmd*cos(heading) + pitch_cmd*sin(heading); %% Eqn-1

For the pitch command:
function adj_pitch_cmd = fcn(pitch_cmd, roll_cmd, heading)

% Given the roll and pitch commands computed for the global frame, this 
function
% rotates the pitch command into the local frame, allowing the control
% command to work even for non-zero yaw.

adj_pitch_cmd = -roll_cmd*sin(heading) + pitch_cmd*cos(heading); %% Eqn-2

It's pretty clear that they have rotated the roll and pitch commands by an angle of ""heading"". However, I am unable to understand the logic behind it. What is the meaning of rotating ""angles"" (roll and pitch)? Shouldn't the commanded angles roll, pitch and yaw represent unique orientation irrespective of current heading? There shouldn't be any intermediate function like above involved according to me. 
I am unable to visualize what's actually happening. Please clarify.
EDIT: I found a similar question.
Compensating for Yaw in Lateral Quadcopter Movement
But I am still not clear. Please explain from the basics.
","quadcopter, gyroscope, dynamics, orientation"
Calculating the singular configuration of a 3 revolute joint manipulator,"I would really appreciate it if somebody could help me calculate the singular configuration of this simple manipulator
I am confused since J is a 2x3 matrix and I cannot simply calculate the derivative.
Thanks in advance.
","inverse-kinematics, manipulator, jacobian, singularity"
How to change blades on eachine e010 mini?,"I have broken one blade on the quadcopter Eachine e010 Mini. Does anybody know how to remove old blades to install new ones? It seems like I can neither simply pull it off nor remove it by rotating.
",quadcopter
How to derive the Cubic Interpolation algorithm?,"I have a data point array. Which is recorded at 20Hz(0.05 second. It can be 30Hz, 40Hz, 50Hz. 20Hz is an example value)

I want to interpolate this data to bigger frequency for example 1kHz(0.001 second) with cubic interpolation to get smooth data set. 
y(t) = at^3 + bt^2 + ct + d

But I can't figure out how can I derive the function and implement with C.
",motion-planning
Mobile robot speed synchronization for straight line moving,"I would like to build a simple mobile robot with differential wheels, and I am currently design the wheel speed controller. After reading some papers, I noticed that to realize a straight line moving, the linear speed and angular speed of the mobile robot have to be controlled at the same time, which makes the system a Multi-input-multi-output (MIMO) system. I plotted two different controller structures I came across while reading materials. Both of them have angular speed feedback and control, but one with linear speed feedback and control, the other without. In the picture, Gl(s) and Gr(s) refer to the motor transfer function, and vl and vr are the measured wheel speed.
Would anyone please suggest which controller structure is more reasonable and can realize better straight line moving?

Updates
v* and w* are linear speed reference and angular speed reference respectively that could either be fixed values or come from trajectory generation; corrected a typo in the motor block in structure 2, as @Chuck pointed out, and changed G1(s) and G2(s) to Gl(s) and Gr(s) for better illustration. 
","mobile-robot, motor"
Control of WMR (Wheeled Mobile Robot) in 3D,"I've implemented SMC (Sliding Mode Controller) on WMR in both X-Y and X-Z plane. 
Now i want to combine both of these to control WMR in 3D. For this purpose I'm trying to use resultant vector of simulation in XY plane and track that resultant vector in XZ plane as value of X in previously designed code.  Tracking control of resultant vector is shown in figure 1 while Vector sum decomposed in rectangular coordinates after simulation is shown in figure 2. 
Am I going wrong?  
What other tecniques can I apply to do 3D control of vehicle using Sliding Mode Controller.
Can i reduce the time delay offset? I've implemented right equations for SMC tracking controller equations but simulation does not gives exact results.These equations work well for control of vehicle in two dimensions (X-Z plane).


","wheeled-robot, matlab, simulation"
Accuracy of 3D scanners in featureless environments,"I am trying to 3D reconstruct a room that has been freshly constructed but the walls have not yet been either plastered or painted/wallpapered. So far I have tried using mapping techniques(like rtab map) on KinectV2 but they don't work as these techniques rely on features to stitch point clouds.
I am currently looking at buying either Structure sensor or Google's Project Tango on Phab2Pro. Since, these are a little expensive (post shipping and customs) I want to be sure of a few things before I begin experimenting.

Do these sensors use something other than features to register point clouds (the phone's accelerometer, for example)?
Is one sensor better at it than the other at this job? If so, why?

If any one of you could somehow attach a point cloud or an image captured using these sensors, it would be of plenty help. Also, feel free to suggest better alternatives.
Thanks!
","sensors, 3d-reconstruction"
iRobot Create 2 -language set error type 2,"I have an iRobot Create 2 and I am connected to it with the 7-pin connector to a desktop machine via USB.  Serial communication is working fine under that configuration.  I can tell it to restart with ctrl-G and read the messages it sends back.  On power up the serial port outputs the following:
2015-08-24-1648-L
r3-robot/tags/release-3.5.x-tags/release-3.5.4:6058 CLEAN
bootloader id: 4701 5652 7E52 3FFF
assembly: 3.5-lite
revision: 2
flash version: 10
flash info crc passed: 1
battery-current-zero 257

When I plug in the raspberry pi 3 board with a 7-pin connector to the on-board UART, I can communicate with the iRobot without issue.  However, the iRobot starts to continuously sound ""uh-oh"" every few seconds (no other beeps follow)  after about 20 or so seconds of powering up the raspberry pi until the robot enters sleep mode.  The iRobot serial port outputs the following message to the raspberry pi every time the robot sounds ""uh-oh""
4701 5652 7E52 3FFF
ERROR: language set error type 2
Does anyone know what this error means?
My last resort is to scope the serial port from the raspberry pi to check for noise; unfortunately I don't have access to my scope at the moment.
","raspberry-pi, irobot-create, serial"
Confirm my understanding on BLDC motors,"This question assumes an ideal system with 100% efficiency.
Lets say I have this propeller which if spun at 1000 RPM in water will need a 48 Watt motor to drive it.
So I get a 1000 Kv 16V motor and a gear reduction of 1:16 so now it will spin at 1000 RPM and thus will use 48 watts of power.
That means that the ESC will be using 3 Amp to power the motor.
Now if I power the ESC with 48V instead of 16V and set it to 33% instead of 100% 
The effective voltage on the motor is still 16V and the motor will still use 48W, but since the ESC is getting 48V it will use 1 Amp not 3.
So I want to choose a brush-less motor for an ROV propeller that will need 200W of power at 1000RPM and the voltage applied to the ESC varies between 12V and 48V.
So I choose a motor and gearbox that when 10V (headroom to allow the motor to slow down a little and still maintain enough speed to drive the prop) is applied to it, it spins at 1000RPM and it has max power of 300W.
Then in the control system I have Voltage and current sensors in the ESC and I drive the motor at constant power mode.
The system would now what voltage is the ESC running at and vary the PWM signal till reaching wanted power.
Is this right or am I missing something?
",brushless-motor
How to best design a rotation mechanism?,"I have a device which needs to rotate 150-180°, using 3 inches of linear motion. I'll add a hall effect sensor to establish fully extended state. It is probably easiest to do this at the base of the device (it is a solar tracker), where it will not require much energy to rotate an arbitrary number of degrees. It is mounted on a standard tablet holder, which at its base can move 360°.
What is the best practice for making a sturdy mechanism to provide this rotation? 
I am unsure if EE is a better forum for this, but robotics is certainly going to yield some good answers.
Cluebats please - thanks in advance. 
Edit: here's an offsite link to the 'device': 
https://postimg.org/image/8669cl8l5/
","arduino, design, actuator, rotation"
Weird behaviour with a Create2 connected via XBEE,"This is not really a problem but something strange is going on.
When Create2 is connected to a PC via the USB original connector lead, when you start-up the computer the Create2 is activated by the Baud Rate Change (BRC) pulling to ground. If I understand correctly, normal behaviour.
My Create2 is connected to a XBEE via a buck converter, I added a switch so the buck converter and the XBEE should not drain the battery continuously so as mentioned in the specs.
I followed the Bluetooth pdf for the connections, its working well for sending commands but I still just have a few problems with streaming the return data but that will be resolved.
But now, with the XBEE switched off my Create2 still activates when I start-up my PC, how is that possible, how can the BRC be pulled to ground?
There can be no communication between the PC XB and the Create2 XB since the Create2 XB is switched off, only the PC XB is switched on when starting the computer.
Its not a problem, its just that I am puzzled. Can anyone explain why this is happening?
",irobot-create
Does Inverse Kinematics need the current joint angles?,"I had a discussion with a study colleague about the IK solver. The question was: Does IK need the current joint values to calculate the requested position.
I think it doesn't need it. From my understanding, IK only needs a translation and rotation respected to the e.g. base frame, but no information about the current joint values or e.g. gripper frame for the calculation itself. My study colleague argue that we always give the IK Solver the current joint values through our .msg file and that's right. However, I'm sure that the IK solver doesn't use it for the calculation. Maybe, in case of optimization if it finds more than one possible solution.
Would be nice if anyone can help
Greetings
R.Devel
","kinematics, inverse-kinematics"
Gazebo ROS JointVelocityController,"I followed along with the Gazebo and ROS JointPositionController tutorial for RRBot here, and got it working just fine. However, when I tried modifying it to control velocity instead of position the setup immediately broke. The only change I made to the original was replacing 
type: effort_controllers/JointPositionController

with
type: effort_controllers/JointVelocityController

in my .yaml parameter file. 
Loading the controller through a service call returns success (I am using rospy and the return value of the call prints ok: True), but the visualization in gazebo immediately breaks and the joint velocities are set to ""nan"". Any ideas what I am doing wrong? Perhaps there are additional steps one must take when creating a velocity controller? I've been looking online for some kind of explanation but haven't had much luck so far. 
Any help is appreciated.
Cheers,
Raghav Malik
","control, ros, gazebo"
Problem with Coordinate Transformation,"So I have a quad with a black-box estimator on it. The black box estimates the pose of the quad. I also have a Vicon system that I'm using to get the ground truth pose of the quad. I'm trying to transform the output from the black-box system into the coordinate frame of the Vicon system so I can compare the two.
I have two series' of points recorded using the whole setup that I am trying to use to compute this transformation (from one world frame to the other world frame).
If you're not familiar, it is possible to compute a transformation between two frames given a set of points in each frame.
I have implemented the method described in the paper Least-Squares Rigid Motion Using SVD
But I'm getting wonky results:

If it's not clear, if the transformation were working correctly, the points labeled 'transformed' displayed on the graph below would roughly overlap with those labeled 'Vicon'. As you can see, they only overlap for less than half a second.
Any suggestions? Ideas about what could be wrong?
","quadcopter, frame"
Is MoveIt! suitable for fixed-wing aircraft path planning and obstacle avoidance?,"I have seen that it is possible to use MoveIt! for path planning and obstacle avoidance for quadcopters/quadrotors.
For example:

https://www.youtube.com/watch?v=NRzeQD_Etog
https://www.youtube.com/watch?v=VlBQLbmc03g

Is MoveIt! as suitable for path planning and obstacle avoidance for fixed-wings aircraft as it is for quadrotors? 
The main difference I can think of is that quadrotors can stop at will, while fixed-wing aircraft have to continue moving to stay in the air. Will this be a problem for path-planning and obstacle avoidance in MoveIt?
","ros, uav, path-planning"
Motion planning and robot controlling,"I am stuck with understanding how can I make my robot move along planned path. For instance, if we have a grid map of an environment and applied, for example, A* to plan a path then after that we have to make our robot move through each cell in our path. Assuming that we know center coordinates of cells, the task is to generate control commands which will lead robot along the trajectory. 
I have two differential wheeled robot so equations of motion are going to be like these, where b is a distance between wheels:
$v = \frac{1}{2}(v_{1}+v_{2})\\
\dot\theta=\frac{1}{b}(v_{2}-v_{1})\\
\theta = \frac{\delta t}{b}(v_{2}-v_{1}) + \theta_{0}\\
\dot x = v\cos(\theta)\\
\dot y = v\sin(\theta)\\
x = x_{0} + \frac{b(v_{1}+v_{2})}{2(v_{2}-v_{1})}(\sin(\theta)-\sin(\theta_{0}))\\
y = y_{0} - \frac{b(v_{1}+v_{2})}{2(v_{2}-v_{1})}(\cos(\theta)-\cos(\theta_{0}))
$
Suppose that we can control speeds of both wheels therefore we are able to set any possible angular and linear velocities. So what actually I have to do with these equations to make robot move through each cell?
Moreover there may be different constraints like moving with constant linear speed etc. I understand that I have to solve these equations somehow.
Will appreciate practical advises, certain names of algorithms and etc. Thanks!  
","mobile-robot, control, wheeled-robot, motion-planning, motion"
"In ROS, how to avoid 'buffer exceeded' error when using rosbag record?","I am recording topics from a high-resolution stereo camera (30 Hz), IMU (100 Hz) and a GPS(1 Hz). I run into issues after recording around 4 bag files when there is a warning message 'buffer exceed - dropping oldest data'. My RAM size is 16 Gb and I allocated as much memory as I could. But still, I get the error. Will increasing the RAM size and consequently the buffer size help? 
following is the command I used to record the sensors

rosbag record -O < path to data file > < topics >  --buffsize=15000
  --split --duration=5s

I wonder why the buffer overflows as the rosbag files that get created are together less than 10 Gb (when I gave 15 Gb as buffer).
","mobile-robot, sensors, ros"
"I am making a Ping pong-playing robot using two cameras to trace the ball (3D vision),what is the minimum Frame rate of the cameras?","I am making a Ping pong-playing robot, like the one in this video, and i am intending to use two cameras to trace the ball in 3D space. Supposing that the robot is playing against a beginner player, the ball takes about 0.7 second to travel from one side of the table to the other. but during this time the robot should process number of frames and predict the rest of the ball track, and the robotic arm should  move to the required position.
I read some papers talking about the same project, but I found a big difference from one to another(some paper used 30 FPS, while other one used 120 FPS). I didn't order the cameras yet, and I can't try more than once because it is a graduating project, so the time and the budget are limited. 
So is there any way to predict the minimum Frame rate of the cameras?
I also heard that some projects are using Kinect instead of using two cameras or 3D vision,
Is the Kinect fast and precise enough for my project?
","robotic-arm, cameras, kinect"
What is the difference between conventional and unconventional path planning methods?,"Is there an exact definition for conventional and unconventional path planning methods? What are the features that help distinguish conventional and unconventional path planning methods? What is an example of conventional and unconventional path planning methods? 
","motion-planning, path-planning"
Why EKF-SLAM uses EKF instead of linear Kalman Filter?,"I am writing a bachelor's diploma on vSLAM. I learned and programmed EKF-SLAM like it's written in MonoSLAM paper, and I was going to write, that I can't use KF and have to use EKF because of non-linearity of observation function, but wait, how is it possible, if everything is linear?! 
I understand, that if I store direction of a camera in form of axis-angle vector or quaternion then there will be something non-linear, but what if I will store it directly as values of rotation matrix? Then my observation function is just going to be a multiplication of matrices, which is a linear operator, and therefore linear. Am I wrong?
","slam, kalman-filter, ekf"
Motor for DIY Electric skateboard,"I've been planning to make an electric skateboard for a while now. All the tutorials that I have seen use brushless motors. However, where I live, its not possible to get the exact ones as shown in those tutorials. 
The ones I can buy are somewhat like this.
The one shown in the tutorials is this.
I am new to brushless motors. The one in the tutorial in 270kv and the one I can buy is 1000kv (and above) but they both differ greatly in size. Plus there's this ""outrunner"" thing. The ones I can buy seem to be made for quadcopters and similar stuff.
So am I okay using the ones I can buy? The little 1000 kv ones, considering the fact that they need to run a 14T to 36T pulley system with approx 70kgs of weight on the board.
Or maybe I could just use the 12v 10Amp 1000RPM DC Motor that I have lying around?
","motor, brushless-motor"
What is a good approach for outlier rejection during real time data filtering?,"I'm trying to finish up a localization pipeline, and the last module I need is a filtering framework for my pose estimates. While a Kalman filter is probably the most popular option, I'm using cameras for my sensing and I wouldn't be able to ensure the kinds of noise profiles KF is good for, I doubt it would work as well with suddenly appearing outliers in my poses: so I am looking for other options which can work with real time data and be immune to outliers.
One thing I came across is a Kalman filter with a threshold based update rejection, something like Mahalanobis distance: but I don't know if this is completely applicable because the localization would be performed in real time, and it's not like I have a large set of 'good poses' to start with. The end result I'm expecting is something like smoothing, but without access to a full set of values. Another option I found is from a paper that proposes a 'robust Bayesian weighted Kalman filter' that's good for real time outlier rejection, but I still need to read through it: and I don't have much experience with filtering/smoothing, so any suggestions would be very helpful, perhaps about a decent go-to mechanism for this?
","localization, kalman-filter, noise"
Why has this configuration chosen for feedback?,"I'm a newbie in electronics and Trying to understand and build this circuit. would somebody tell me the rule of R1 & C2 in this circuit and their magnitude effect on the controlling of process.why didnt feedback start just after the output of opamp?How could i find the transfer function between the voltage output of Hall effect sensor and the voltage output of MOSFET drain?
The schematic is the circuit of a magnetic levitating tool at:
http://www.bis0uhr.de/index.htm?http://www.bis0uhr.de/projekte/schwebekugel/english.php%99https://www.youtube.com/
Thanks so much

",circuit
3D transformation between two coordinate systems,"Given a coordinate system of known unit vectors: System A(x_A,y_A,z_A) defined in original coordinate system O(unit_x, unit_y, unit_z), does library Eigen have a function to directly find the transformation (T) between them. 
Thanks,
",geometry
Correct calibration parameters of stereo system due to vibrations,"In stereo vision, the camera is calibrated using the popular chessboard before use. During the course of use, the camera maybe be subject to environmental factors such as vibrations.This may cause calibration parameters to drift. I hope to correct calibration parameters online without using the chessboard. 
Currently I assume: 

focal length and principal points of two camera are constant,
the distance between them is constant 
only  orientations of two camera changes.

The initial calibration parameters are available. Left/right images are undistorted using each individual camera calibration parameters.
Corresponding points can be established by detecting feature points and matching them from undistorted left/right images
Fundamental matrix can be calculated. I will call it calculated  fundamental matrix.
Fundamental matrix can be derived from calibration parameters of stereo system, and I will call it derived fundamental matrix.
The two fundamental matrix should be same if calibration parameters of stereo system doesn't change.
Otherwise I can get rotation matrix between the equation relating fundamental matrix to ration matrix and translation between two cameras
and each camera intrinsic matrix.
I am new to multiple view geometry. Is this a viable method? Is there any other method for that?
Any link to websites and papers are appreciated.
","cameras, stereo-vision, calibration"
Brushless DC motor problems at high velocities,"For a project we use a brushless DC (BLDC) motor. Everything works fine until we tried to reach high velocities. I will first explain my setup, and than explain our problems using some graphs.
1.0 Setup
The following hardware is used in the setup:

BLDC motor: Tiger motor U8 (135kV)
Motion controller: SOMANET DC 1K
Encoder: RM08 12 bit absolute encoder

An overview of the setup is shown below:

1.1 Requirements and Parameters
We need about 4800 [RPM] from the motor. The Tiger motor has a kv value of 135 [RPM/V], connecting it to a 48 [V] supply means it theoretically should be able to go up to 6500 [RPM]. The specsheat includes a scenario where it reaches 5000 [RPM] while a propeller is connected to it, so 4800 [RPM] with no load should not be a problem.
2.0 Problem
We are not even getting close to the 4800RPM, a plot of the motor velocity vs phase current is shown below. We can identify 2 problems from this plot.

2.1 Inefficient commutation
The first thing which was remarkable from the test is that about 10 [A] was already required to turn at 3200 [RPM] without any load connected. This seems to be caused by inefficient commutation, we figured there are two main possible causes for this.
2.1.1 Phase offset error
Their might be an error in the phase offset used, this will cause an linear increase in required current with velocity. This can be best solved by finetuning the offset at a high velocity. However our curve does not seem linear, thus this does not seem to be the case.
2.1.2 Delay error
There is a certain amount of delay in between requesting the position from the RM08 and applying the new voltage. This delay can cause the current to exponential increase with motor velocity, which is true in our case.

By adding up al delays we found a total delay of ~0.1 ms in the system (see above). Spinning at 3000RPM = 50Hz and using 21 pole pairs means that the electrical turning frequency is 1050Hz, than a delay of 0.1 ms would cause a 37.8 electrical degree error, This likely causes the inefficiency!
2.2 Control going crazy
if we try to go above ~3200 RPM, the motor starts pulling a lot of current and makes a lot of noise. This means the motor is not operational above 3000RPM, this seems to be the most urgent problem at the moment.
Voltage dependency
Normally the motor velocity is limited by back-EMF, if the back EMF would be causing this issue the problem would be voltage dependent. Therefore some measurements where done at different voltage levels, see the two images below:


The moment where the motor stops following the velocity sweep seems to increase linearly with voltage. Another interesting outcome is that at 30V the motor just stops following the velocity sweep, while at the higher voltages (40V and 43V) the motor suddenly dropped to a lower velocity. Note that the 46V test stopped before this moment because to high current peaks were flowing trough the SOMANET (35A).
However it seems unlikely the back-EMF is the problem since Tiger has been able to reach 5000RPM themselves.
Solutions
For the first problem we thought we could use something like: 
Pcorr = Penc + t_delay * Vel. 
With:

Vel: angular velocity 
t_delay: the delay compensation gain  
Penc: the encoder position for the rotor 
Pcor: the delay compensated position

However this didn't save the problem at all. Do you have any other suggestions?
For the second problem we can't think of any cause, can you think of any?
","control, brushless-motor, current"
Do Arduino Boards contain enough processing power for 4 DOF Robotic Arm,"I am starting to work with 4 DOF Robotic Arm project.it has the following specs:
1- the speed of the tip of the end-effector is constant and adjustable.
2- the robot is controlled via joysticks which determine the direction of movement of the end-effector.
3- also the orientation of the end-effector is controlled.
to implement these specs I need a processor that can handle forward kinematics, inverse kinematics, trajectory related calculations in addition to reading from sensors and camera.
can Arduino Handle all that?
what are alternatives available?
","arduino, robotic-arm"
"Running my 3 DOF Inverse Kinematics Code: Works in MATLAB, not in Python","I asked a question similar to this earlier, but I believe I have a new problem. I've been working on figuring out the inverse kinematics given an x,y,z coordinate. I've adopted the Jacobian method, taking the derivative of the forward kinematics equations with respect to their angles and input it into the Jacobian. I then take the inverse of it and multiply it by a step towards the goal distance. For more details, look at http://www.seas.upenn.edu/~meam520/notes02/IntroRobotKinematics5.pdf page 21 onwards. 
For a better picture, below is something:

Below is the code for my MATLAB script, which runs flawlessly and gives a solution in under 2 seconds:
ycurrent = 0; %Not using this  
xcurrent = 0; %Starting position (x)   
zcurrent = 0; %Starting position (y)    
xGoal = .5; %Goal x/z values of (1, 1)   
zGoal = .5;    
theta1 = 0.1; %Angle of first DOF    
theta2 = 0.1; %Angle of second DOF  
theta3 = 0.1; %Angle of third DOF
xchange = xcurrent - xGoal %Current distance from goal
zchange = zcurrent - zGoal
%Length of segment 1: 0.37, segment 2:0.374, segment 3:0.2295 
while ((xchange > .02 || xchange < -.02) || (zchange < -.02 || zchange > .02))    
        in1 = 0.370*cos(theta1); %These equations are stated in the link provided
        in2 = 0.374*cos(theta1+theta2);
        in3 = 0.2295*cos(theta1+theta2+theta3);
        in4 = -0.370*sin(theta1);
        in5 = -0.374*sin(theta1+theta2); 
        in6 = -0.2295*sin(theta1+theta2+theta3); 
        jacob = [in1+in2+in3, in2+in3, in3; in4+in5+in6, in5+in6, in6; 1,1,1];
        invJacob = inv(jacob); 
        xcurrent = .3708 * sin(theta1) + .374 * sin(theta1+theta2) + .229 * sin(theta1+theta2+theta3) 
        zcurrent = .3708 * cos(theta1) + .374 * cos(theta1+theta2) + .229 * cos(theta1+theta2+theta3)        
        xIncrement = (xGoal - xcurrent)/100; 
        zIncrement = (zGoal - zcurrent)/100; 
        increMatrix = [xcurrent; zcurrent; 1]; %dx/dz/phi 
        change = invJacob * increMatrix; %dtheta1/dtheta2/dtheta3  
        theta1 = theta1 + change(1)  
        theta2 = theta2 + change(2)  
        theta3 = theta3 + change(3)
        xcurrent = .3708 * sin(theta1) + .374 * sin(theta1+theta2) + .229 * sin(theta1+theta2+theta3)  
        zcurrent = .3708 * cos(theta1) + .374 * cos(theta1+theta2) + .229 * cos(theta1+theta2+theta3)          
        xchange = xcurrent - xGoal
        zchange = zcurrent - zGoal
end        

Below is my Python code, which goes into an infinite loop and gives no results. I've looked over the differences between it and the MATLAB code, and they look the exact same to me. I have no clue what is wrong. I would be forever grateful if somebody could take a look and point it out.
def sendArm(xGoal, yGoal, zGoal, right, lj):
    ycurrent = xcurrent = zcurrent = 0
    theta1 = 0.1
    theta2 = 0.1
    theta3 = 0.1
    xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
    xchange = xcurrent - xGoal
    zchange = zcurrent - zGoal
    while ((xchange > 0.05 or xchange < -0.05) or (zchange < -0.05 or zchange > 0.05)):
        in1 = 0.370*math.cos(theta1) #Equations in1-6 are in the pdf I linked to you (inv kinematics section)
        in2 = 0.374*math.cos(theta1+theta2)
        in3 = 0.2295*math.cos(theta1+theta2+theta3)
        in4 = -0.370*math.sin(theta1)
        in5 = -0.374*math.sin(theta1+theta2)
        in6 = -0.2295*math.sin(theta1+theta2+theta3)
        jacob = matrix([[in1+in2+in3,in2+in3,in3],[in4+in5+in6,in5+in6,in6], [1,1,1]]) #Jacobian
        invJacob = inv(jacob) #inverse of jacobian
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xIncrement = (xGoal - xcurrent)/100 #dx increment
        zIncrement = (zGoal - zcurrent)/100 #dz increment
        increMatrix = matrix([[xIncrement], [zIncrement], [1]])
        change = invJacob*increMatrix #multiplying both matrixes
        theta1 = theta1 + change.item(0)
        theta2 = theta2 + change.item(1)
        theta3 = theta3 + change.item(2)
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xchange = xcurrent - xGoal
        zchange = zcurrent - zGoal
        print ""Xchange: %f ZChange: %f"" % (xchange, zchange)
    print ""Goals %f %f %f"" % (theta1, theta2, theta3)
    right.set_joint_positions(theta1) #First pitch joint
    right.set_joint_positions(theta2) #Second pitch
    right.set_joint_positions(theta3) #Third Pitch joint


def forwardKinematics(theta1, theta2, theta3):
    xcurrent = .3708 * math.sin(theta1) + .374 * math.sin(theta1+theta2) + .229 * math.sin(theta1+theta2+theta3)
    zcurrent = .3708 * math.cos(theta1) + .374 * math.cos(theta1+theta2) + .229 * math.cos(theta1+theta2+theta3)            
    return xcurrent, zcurrent

","kinematics, inverse-kinematics, matlab, python, jacobian"
How to connect HC-sr04 ultrasonic sensor to APM 2.6?,"I am working on my final project by autonomous Quadcopter. my tasks are to make a quadcopter  which should do object avoidance and it should auto land using ultrasonic sensors.
any possible ans to it, how should i connect HC-sr04 ultrasonic sensor to my APM 2.6 board?
Even APm 2.6 has a port I2C.
",quadcopter
using Li-Ion batteries in parallel,"Looking at the Li-Ion battery packs from Servocity, part #605056, 12V, 6000maH battery pack. Any reason I shouldn't put these in parallel with each other? Any idea on what these might weigh? I've got a robot project going, currently running on a very heavy 12V lead acid RV battery, essentially a small car battery.
",battery
i want to make a system which can take the any preset volume of liquid from the pipeline,"i am trying to design a container which can get filled by a predetermined volume of liquid from the pipeline..for example the container connected to the water line can be automatically filled with the desired volume of liquid and after that it gets autocut..i want to get it designed by simple mechanical methods..ultrasonic sensors are not giving sustained accurate result and high level of accuracy is must for it..hope any one can help.thanks
","control, sensors, design"
Computing Yaw angle from position and velocities,"I have an object to whom I know the x,y,z position, length, width, height, and x,y velocities. Is there a possibility to compute the yaw angle from this information?  
","mobile-robot, dynamics, motion"
Adding a floating joint to a sphere in urdf,"I am trying to create a sphere that I can control through pybullet. I have a basic urdf specification that looks like this:
<?xml version=""0.0"" ?>
<robot name=""urdf_robot"">
<link name=""base_link"">
  <contact>
    <rolling_friction value=""0.005""/>
    <spinning_friction value=""0.005""/>
  </contact>
  <inertial>
    <origin rpy=""0 0 0"" xyz=""0 0 0""/>
    <mass value=""0.17""/>
    <inertia ixx=""1"" ixy=""0"" ixz=""0"" iyy=""1"" iyz=""0"" izz=""1""/>
  </inertial>
  <visual>
    <origin rpy=""0 0 0"" xyz=""0 0 0""/>
    <geometry>
      <mesh filename=""textured_sphere_smooth.obj"" scale=""0.5 0.5 0.5""/>
    </geometry>
    <material name=""white"">
      <color rgba=""1 1 1 1""/>
    </material>
  </visual>
  <collision>
    <origin rpy=""0 0 0"" xyz=""0 0 0""/>
    <geometry>
      <sphere radius=""0.5""/>
    </geometry>
  </collision>
</link>
</robot>

But to control velocity instead of only external force I need add a joint. I tried adding a floating joint: 
<joint name=""control"" type=""fixed"">
  <parent link=""base_link""/>
  <child link=""internal_link""/>
  <origin xyz=""0.0 0.0 0.5""/>
</joint>
<link name=""internal_link"">
  <inertial>
    <mass value=""0.1""/>
    <origin xyz=""0 0 0""/>
    <inertia ixx=""1"" ixy=""0.0"" ixz=""0.0"" iyy=""1"" iyz=""0.0"" izz=""0.01""/>
  </inertial>
</link>

but pybullet crashes when trying to load that without a helpful message. I don't know urdf well. Any ideas? Thanks!
","control, ros, simulation, joint"
Confusion about order of homogeneous transform multiplication,"This is my first post here, so if I unknowingly vioated any rules, mods are welcome to edit my post accordingly.
Ok, so my problem is that following Craig's conventions, I can't seem to find the expected homogeneous transform after a series of transformations.
I have included the image for clarity.
We are given the initial frame {0} as usual and then:
-$\{A\}$ is the frame under rotating {0} $90^\circ$ around $z$ and translating, with $OA = \begin{bmatrix}
1
\\ 1
\\ 1
\end{bmatrix}$
-$\{B\}$ is obtained after translating $A$ by $AB = \begin{bmatrix}
-2
\\ -2
\\ 0
\end{bmatrix}$ 
What I found is:  
$$ {}_A^OT = \left[ {\begin{array}{*{20}{c}}
0&-1&0&1\\
1&0&0&1\\
0&0&1&{1}\\
0&0&0&1
\end{array}} \right], \;\;{}_B^AT=\left[ {\begin{array}{*{20}{c}}
1&0&0&-2\\
0&1&0&-2\\
0&0&1&{0}\\
0&0&0&1
\end{array}} \right],\\ {}_B^OT = {}_A^OT  {}_B^AT=\left[ {\begin{array}{*{20}{c}}
0&-1&0&3\\
1&0&0&-1\\
0&0&1&{1}\\
0&0&0&1
\end{array}} \right] $$
This is wrong, since the last column should obviously have the coordinates $-1,-1,1$, the origin of B
What am I missing? 

","kinematics, frame"
Euler Angles Order For Quadrotor Modelling,"I am modelling a quadrotor and I need to choose an order for the rotations that transfer vectors which are represented in Earth Frame to the Body Frame.   

what is the most logical order for these rotations?
which order is likely used?
does the order have a big effect on the control of the quadrotor?

Thanks in advance for any answers
",quadcopter
How can there be a difference in quaternion values of two points taken from sensors where the orientation is exactly same?,"I have two situations-
A) One, the body with sensor embedded in it kept at rest. 
B) Second, the body is at rest for 10 secs, then undergoes some movement randomly and comes back to the exact orientation as the initial one (at rest) and kept there for rest for 10 secs again.
In the first case, the quaternion values are constant and that is what is expected. But in the second case, these values from the first 10 secs do not match with the last 10 secs. As the orientation is unchanged in both the situations, how can the quaternion values be different? Also, the accelerometer, gyroscope and magnetometer values for corresponding situations is same. 
The sensors which I am using are accelerometer and gyroscope. I dont know the exact way how the quaternion values are getting computed from these sensor values here but I will try to give you a better understanding. So, the quaternion values initially at rest are [1,0,0,0]. If the object is kept at rest, it remains the same (should be like that) but if it moves randomly and then again comes to rest with the exact same orientation as the initial point, the quaternion values are [0.708547,-0.4962,-.4316,-0.2556]. If this is not matching, then what are the absolute quaternion values signifying?
Is there any flaw from my end in understanding the derivation of quaternion values at the conceptual level or I am missing something substantial?
","sensors, accelerometer, gyroscope, robotics-library"
What is the difference between motion planning and trajectory generation?,"What are the major differences between motion planning and trajectory generation in robotics? Can the terms be used interchangeably?
",motion-planning
Connecting two servo motors to drive one shaft,"I need to connect two servo motors (with internal position control) to rotate one shaft (to control its position), is that possible? and how to avoid synchronization problems?
the servo motor
I am making a robotic arm, my problem here is that the servos are internally controlling position, which means that if only one of them arrived to the target and the other one haven't yet, it will still try to get to it (even if it is just 0.5 degree away).. i am afraid that this will make the system vibrate around the required position , or make one motor to continuously drag current, or it might hurt the servo shafts and gears, 
 I am thinking of using pulleys and a synchronous belts to connect the servos to the shaft, this could absorb the bad effects.
so please let me know if you consider this to be safe, or if there is something i can do to improve the performance and the reliability of the system. 
","control, robotic-arm, mechanism, servomotor"
How is a shear pad of remote compliance device formed?,"I am researching on mechanical design of remote compliance devices (RCC)†. In the RCC, manufactured by ATI, they use shear pads for lateral compliance. They mention that shear pads are elastomers bonded with metal shims. I couldn't find how it is being made. Are there layers of elastomers between wedging pieces (shims) or something else? How is adhesion being achieved?

† From wikipedia: ""In robotics, a Remote Center Compliance, Remote Center of Compliance or RCC is a mechanical device that facilitates automated assembly by preventing peg-like objects from jamming when they are inserted into a hole with tight clearance.""
",design
DH parameters of a PUMA-type manipulator,"I'm struggling to find the DH parameters for this PUMA-type manipulator that yield the same results as the author (1):

The way I'm checking if the parameters I have are correct is by comparing the resulting J11, J21 & J22 matrices with the author. These sub-matrices are the constituents of the wrist Jacobian matrix (Jw).
I tried many different combinations of the DH parameters including:
α
=[0,90,0,-90,90,-90]
 θ
=[0,0,0,0,0,0]
a=[0,0,a2,-a3,0,0]
d=[d1,-d2,0,-d4,0,0]
Which result in the same matrices as the author except for some minor differences. The general wrist Jacobian matrix and the sub-matrices obtained by the author are given by:  
 
Whereas the result I got for J11 was:
$$
\left[
\begin{array}{ccc}
 -d_2 c_1-s_1 (a_2 c_2-a_3 c_{23}+d_4 s_{23}) & c_1 (d_4 c_{23}-a_2 s_2+a_3 s_{23}) & c_1 (d_4 c_{23}+a_3 s_{23}) \\
 c_1 (a_2 c_2-a_3 c_{23}+d_4 s_{23})-d_2 s_1 & s_1 (d_4 c_{23}-a_2 s_2+a_3 s_{23}) & s_1 (d_4 c_{23}+a_3 s_{23}) \\
 0 & a_2 c_2-a_3 c_{23}+d_4 s_{23} & d_4 s_{23}-a_3 c_{23} \\
\end{array}\right]
$$
And for the J22 matrix I got:
$$
\left[
\begin{array}{ccc}
 -c_1 s_{23} & c_4 s_1+c_1 c_{23} s_4 & s_1 s_4 s_5-c_1 (c_3 (c_5 s_2+c_2 c_4 s_5)+s_3 (c_2 c_5-c_4 s_2 s_5)) \\
 -s_1 s_{23} & c_{23} s_1 s_4-c_1 c_4 & -c_5 s_1 s_{23}-(c_2 c_3 c_4 s_1-c_4 s_2 s_3 s_1+c_1 s_4) s_5 \\
 c_{23} & s_{23} s_4 & c_{23} c_5-c_4 s_{23} s_5 \\
\end{array}\right]
$$
And the same J12 matrix as the author.
Perhaps the most pronounced difference here is that every Sin
  [
  
   θ2
   +
   θ3
  
  ]
 
 is replaced with  Cos
  [
  
   θ2
   +
   θ3
  
  ]
 
 and vice versa, in addition to some sign differences.  
Where am I going wrong here?
(1) Wenfu Xu, Bin Liang, Yangsheng Xu, ""Practical approaches to handle the singularities of a wrist-partitioned space manipulator"". 
","robotic-arm, manipulator, dh-parameters"
Math behind trajectory planning,"Let's assume the very simple case of a particle and a control system in one dimensional space therefore our particle can move only in a straight line and dynamics of system is described by:
$m\vec{a} = u$.
Now the problem: we would like to make our particle move from point $A$ to point $B$ in time $t$ and constrain our acceleration with some  value $a_{m}$ i.e. $a$ can not exceed $a_{m}$ at any moment.
How would one do this assuming that our control system allows us to control either velocity or acceleration?
The most important things here are names of mathematical methods behind this task and explanation of how to apply them. 
Also consider that
$x(0) = A = 0\\ x(t) = B\\ v(0)=0\\a(0)=0\\ v(t)=0\\ a(t)=0$
","control, motion-planning, movement"
How can i connect two adafruit 16-channel 12-bit PWM/Servo Driver I2C interface (pac9685) to Raspberry or Arduino,"I need to control 24 servos but I do not know how to do with I2C.
I saw some componentes that do it with USB, but I need to do it with I2c.
I am working in a robot with 8 legs and 3 degree of freedom in each leg, so i need to connect and control 24 servos to any arduino board, could be possible with an arduino mega and a sensor shield like that show in https://arduino-info.wikispaces.com/SensorShield but I want to do it using two ""16-channel servo  drive"" like this https://www.adafruit.com/product/1411, each of these can control 16 servos using only 2 pins from the board and it is “Chain-able” design so i could connect 2 of this to an Arduino Uno board or Raspberry but I do not know how to do.
Any one can help me with I2C chains connections.
","arduino, raspberry-pi, i2c"
How to check for a sharp angle with a line follower?,"I have the mBot robot and I want to program it to follow the line. So far it can pass any kind of line that is >90°.
I want it to be able to pass 90°-ish angles as well. Like this one:

The problem is that my mBot robot has only 2 line following sensors (they are 5 mm apart and the line is 2 cm wide) so I can't use just the sensors.

Most of the times it just goes to the line and when it's supposed to turn it just misses the line (goes on the white) and goes back to get back on track. Once it's back on the black line it once again tries to go forward but goes on the white instead of taking a turn. This happens endlessly.
Sometimes it passes the angle by going back and forth and accidentally turning, but that's not even a workaround, let alone a solution.
Here's a test course of the first round of the competition.

My robot can pass this without a problem, but it gets stuck on this (poorly edited, sorry) course:

It can't pass the 20 block if the robot enters it from a 15 or 20 block (so basically it gets stuck if it's coming from an angle and hits a 90 degree turn).
The sensor value could be read as either 0, 1, 2 or 3 depending on what the robot currently sees:

0 - on the line 
1 - on the right of the line 
2 - on the left of the line 
3 - not on the line 
Pseudo code of my current program:
loop forever:
    if (on the right of the line):
        turn_left()
    if (on the left of the line):
        turn_right()
    if (on the line):
        go_forward()
    if (not on the line):
        go_backwards()

So how would I go about taking such sharp turns?
","arduino, motor, line-following"
Is it possible to control joint torque using the position input and torque feedback?,"So I am working with a UR10 manipulator which doesn't have a direct torque interface. However, it provides torque/velocity/position feedback for each joint as well as position/velocity interfaces for joint control.
I have a feeling the answer is ""yes"", but I've been having trouble finding examples and comments on the feasibility of this approach.
Thanks!
","control, robotic-arm, ros, torque, manipulator"
Drones and camera streaming,"How do you stream video feed from a camera on a drone? I would think that at high altitudes Wi-Fi won't work. 
So what would you usually do, and how?
","quadcopter, cameras"
Gyroscope - How can I remove low frequency component with a high pass filter only?,"I'm using Matlab to suppress low frequency components with a high pass filter. 
Objective

Filter angular velocity measurements affected by high frequency noise and bias in order to get the best estimate of the angular position.

The output when the gyroscope is still looks like this.

First Approach
The easiest way to remove baseline is to remove the average and can be achieved with Matlab using one line of code.
yFilt = y - mean(y)

Second Approach
We can design a high pass filter to attenuate low frequency components. If we analyze the frequency components of the signal we will see one peak at low frequency and ""infinite"" small components in all frequencies due to Noise. With a second order ButterWorth filter with normalized cutoff freq Wn = 0.2 we will get what we are looking for.

Filtered data

Tilting the Gyro
When we tilt the gyroscope the situation changes. With a sampling frequency of 300Hz we get the following plot.

The first half of the dft is shown below in a normalized scale.

You can find the sample.mat file here
The first approach works great. I would like to apply the second one to this particular case but here there are other low frequency components that make to job harder.
How can I apply the second approach based on the High Pass filter to remove the bias?
EDIT 1 
You can find more information here
EDIT 2
How can we filter this signal to remove bias while keeping the angular velocity information (from 110-th to 300-th sample) intact?
If gyroscopes have the bias problem only when they are not experiencing any rotation, then the offset is present only in the first ~110 samples.
If the above hypothesis is correct, maybe if we apply high pass filtering only in the first 110 samples and desactivate the filter during rotations of the gyro, the estimated angular position will be more accurate.
","gyroscope, matlab, filter"
Implementing a torque-controlled method on a position-controlled robot,"I am working with a position-controlled manipulator. However, I want to implement a torque-controlled method on this robot. Is there any way to convert a torque command to a position command?
I try to find research papers on this but I have no idea where I should start or which keywords I should use in searching. Do you have any suggestion?
","robotic-arm, industrial-robot, manipulator"
How to compute a path from a Frame A to a Frame B,"the problem I have is the following (see picture below), I am able to compute a starting frame $F_S (X_S, Y_S, Z_S)$, an ending frame $F_E (X_E, Y_E, Z_E)$ and a path from $F_S$ to $F_T$ and what I want to do it to compute a serie of transformations that will transform $F_S$ into $F_E$ along the path.
Naively I computed the Euler angles for $F_E$ end $F_S$, compute the differences and incrementally built the transformations but it does not work. Does somebody can give me some hints or pointers towards existing solutions?
The application is related to the computation of a path for an arm and so the frames are associated with the end effector.
Thank you 

","robotic-arm, path-planning"
What is the cheapest / easiest way of detecting a person?,"I'd like to know if anyone has had success detecting a warm-bodied mammal (ie. Human) using standard off the shelf, inexpensive sensors?  
Ideally, I'd like to use an inexpensive sensor or combination of sensors to detect a person within a room and localize that person. I would like the robot to enter a room, detect if a human(s) is/are present and then move to the detected human. The accuracy does not need to be 100%, as cost is more of a factor. I'd like the computational requirements of such a sensor to be such that it can run on an Arduino, although if it's impossible, I'd be willing to utilize something with more horespower, such as a Raspberry Pi or a BeagleBone Black. I have a few thoughts; however, none of them are ideal:

PIR Sensor - Can detect movement within a large field of vision (ie. usually 120 degrees or more).  Might be the closest thing to a ""human"" detector that I'm aware of; however, it requires movement and localizing/triangulating where a person is would be very difficult (impossible?) with such a large field of vision.
Ultrasound - Can detect objects with good precision.  Has a much narrower field of view; however, is unable to differentiate between a static non-living object and a human.
IR detectors - (ie. Sharp range sensors) Can again detect objects with great precision, very narrow field of view; however, it is again unable to differentiate objects.
Webcam + OpenCV - Possibly use face detection to detect human(s) in a room.  This may be the best option; however, OpenCV is computationally expensive and would require much more than an arduino to run.  Even on a Raspberry Pi, it can be slow.
Kinect - Using the feature detection capabilities of Kinect, it would be relatively easy to identify humans in an area; however, the Kinect is too expensive and I would not consider it a ""cheap"" solution.  

Perhaps someone is aware of a inexpensive ""heat-detector"" tuned to body heat and/or has had success with some combination of (#1-4) above and would like to share their results?
",sensors
Low-cost centimeter accurate satellite positioning (GNSS/GPS),"I am looking for a cheapest possible GPS setup with a centimeter precision without much HW hacking. I am not able to produce my PCB or do any soldering (though I would do that if there is no other way) so a kind of a easy-to-assemble setup would be welcome. I know about the $900 Piksi thing but that is still too expensive for me. It seems like cm precision should be possible for much less - like employing a 50 USD raw GPS sensor with an antenna and ordinary PC with RTKLIB software.
I am not sure if it is better to use two GPS sensor setup for RTK (one base station and one for rover) or whether I can get the corrective DGPS data elsewhere (my region is Czech Republic - there seems to be national grid here allowing to stream correction data for reasonable cost).
My application will be in a passenger car so I will not be limited with power source - no low power needed although that would be nice. I will be using the position readings within OpenCV - so I need to get the data into C/C++ code. The application is data collection so I can use raw GPS post-processing.
",gps
Suggestion for relevant non-complex simulator,"I've recently started working on some localization algorithms like probabilistic road map and SLAM algorithms. What I'm looking for is a software that would help me in simulation for such algorithms. I started with pythons graphics.py and have now started working with Gazebo and ROS, but I've found it complex to use. 
Is there any similar freeware simulation software that is easy to setup and work with, thus allowing me to reduce my time stressing over the simulation part and working more on the algorithms?
","slam, ros, simulation, simulator, gazebo"
"Is it possible to use Matlab ""system"" function to call ROS commands?","Is it possible to use the Matlab's system function to call ROS commands?
For example, using system('rostopic pub /cmd_vel geometry.msgs.Twist {....}
or system('rospack find ipc_bridge).
I'm trying to send some commands to ROS without using something like IPC-Bridge.
PS: I know, however, that I need to use IPC-Bridge to subscribe to topics.
","mobile-robot, software, ros"
How to choose a good IMU for a wheeled robot?,"At our lab, we have a several ""Kurt"" type robots (about the size of a Pioneer, six wheels, differential drive). The built-in gyroscopes are by now really outdated; the main problem is that the gyroscopes have a large drift that increases as the gyro heats up (the error is up to 3°/s). We mainly use the IMU (inertial measurement unit) to get initial pose estimates that are later corrected by some localization algorithm, but even doing that the large initial pose error caused by the IMU is often annoying.
We've temporarily used an Android phone (Galaxy S2) as a replacement IMU, and the results are so much better compared to the old IMUs. However, I don't like depending on a WiFi connection between IMU and the control computer (a laptop running ROS/Ubuntu), so we're looking to buy a new IMU.
What IMU should we choose? What criteria are important to consider for our application?
","ros, imu, odometry, gyroscope, ugv"
How to receive messages from a UR10 URScript?,"I've been looking at existing this UR TCP/IP communication protocol answer, the data linked but I'm still a little confused to how I could retrieve values from calculations, for example get_inverse_kin()
I've tried to figure it out based on the available articles on the Universal Robots support site, but even that has a usage guide: ""How to use this Support site"" :)
I can receive the Realtime data and parse it based on the Client_Interface.xlsx specifications, but that does not include calculations done via the motion module.
The other thing I have in mind is writing a URScript along these lines:

store the result of the get_inverse_kin() in a float[] (e.g. angles = get_inverse_kin(pose_here)
make a string representation of the data (e.g. str = ""{\""angles:\"":[0,1,2,3,4,5]}"")
open a socket to the computer to send the data (e.g. socket_open(""COMPUTER_IP_HERE"", 50000, ""motion_results"")
Send the angles string (e.g. socket_send_line(str, ""motion_results""))

This feels a bit long-winded though. Is how values should be sent ?
What is the most efficient way of receiving URScript motion module results on a computer connected to the Control Box ?
","robotic-arm, communication"
How can I implement tremaux algo in arduino line follower to navigate and create map?,"The Map1

The task is divided into 2 runs, In first run, We have to navigate through whole map and reach the end mark taking all checkpoints.
In second run, using the mapping from first run we have to follow the shortest path.
I am reading Algos and codes from days, I know the steps that should be taken,
Navigate the maze using Tremaux algo and turning right at each possble junction.
then Save that map in memory and use djikstra algo or A* algo to find the shortest path.
But how to implement it practically?
Also since all  the dictances and angles are known and also the wheel dia, motor rpm, bot width then Can I skip wheel encoders and use calculation instead?
","slam, algorithm, mapping, line-following"
System Requirements for Gazebo,"According to this document from Gazebo's (Beginner:Overview) online web site, for the installation of Gazebo version 7.0 on Ubuntu OS, these are the following System Requirements:

A dedicated GPU (Nvidia cards tend to work well in Ubuntu)
A CPU that is at least an Intel I5, or equivalent.
At least 500MB of free disk space,
Ubuntu Trusty or later installed.

Then, I would like the following questions to clarify some doubts that I have:

How much of RAM memory is needed for good performance?
Has anyone already tested Gazebo with Intel HD Graphics?

",gazebo
How to work with arduino softwareserial communication?,"This is my first project using Bluetooth module HC-05. I am using two of these modules. One is connected to an Arduino Nano(slave) and another one is connected to an Arduino Uno(master). I have paired them through AT commands.
For testing, I was giving a pulse in pin 7 on the Nano. If there is a pulse, then slave will send character ""1"". If not, then it will send ""0"".
In the Uno there is an LED connected to pin 13. If the master receives '1', then the LED will on and if receives '0', the LED will remain off.
here is my source code ----------
slave code=>

master code=>

The Arduino IDE isn't showing any bug, but the code is not working at all. Although I am giving a pulse in pin 7 of the Nano, the LED is remaining off in the Uno. I am at a loss now. I have a lot to do after this and my project submission is knocking at the door. Please help me as soon as possible.
","arduino, communication"
Can ROS run on a Raspberry Pi?,"Can ROS run on a Raspberry Pi?
ROS is resigned to run on a network of machines, with different machines, even different cores on the same machine doing different jobs. Can one of those machines be a Raspberry Pi?
I am considering using an R-Pi as the EtherCAT master on a mobile robot, communicating with the main PC over WiFi, using a dongle.

Can an R-Pi even run ROS at all?
Would an R-Pi have enough processing power to do some 1kHz servoing?
Would it be possible to run some servoing on the host through the WiFi connecion?

","ros, raspberry-pi, wifi"
Will this have more or less lifting strength,"Bit of an engineering question. If a snake robot is 100 cm long, and every 10 cm is another motor. Would the robot lift more weight, the longer/more motors attached to it.. or is lifting ability solely based on a single motor lifting.
","motor, power, stepper-motor, torque, force"
DH Parameters of RRP planar manipulator,"I have been learning forward kinematics and having some trouble with coordinate systems and dh parameters with prismatic joints. Trying to work through this question. Trying to work this out I ended up with this system.
 However running through matlab it appears to be wrong. If anyone is able to point out my mistake, or help point me in the right direction, would be appreciated!
","forward-kinematics, dh-parameters"
How can I turn a bowl/glass upside down?,"I want to dispense water/cut-vegetables from a glass/bowl(240 ml) by turning it upside down. The screencast shows the idea:

I can directly mount the clamp on a servo motor, but I think that will be a lot put a lot of downward force on the shaft. What will be a good mechanical arrangement to do this? Thanks!
Ref:

Servo MG996R with Metal Gears
Cup: 


","motor, robotic-arm, actuator"
What type of wheel is preferable for a soccerbot?,"After a few days,there will held a competition in our varsity.in this competition the task is,my robot will have to play soccer.this is a one to one manual game.I am thinking of building a robot consists of 4 wheel and the structure will look like a pyramid.but the difference between the outer surface of the pyramid and my bot is,the outer plane of my bot will be like a concave slope/curve.so that,when the opponent's bot will come to attack my bot,then it's wheel will go over my bot and will loose it's balence.I will take the chance and push that away from my path.
the dimentions of my bot must have to be between-
length=25cm
width=20cm
height=20cm
weight=3kg
but,my main problem is to make the concave slope.if the wheel diameter is large the it is impossible to make a perfect slop on which the opponent will easily ride.but if the diameter is small then it is possible to make a good slope.
now,this is the question.whether the small wheel will have any effect on the other sides of the bot like speed,friction or not ???
please suggest anything...you can..........what should I do? 
","wheeled-robot, wheel, soccer"
Can you detect if encoder wire is connected on a motor?,"I am writing some logic for a PID controlled catapult (In order to improve precision). That is all fine and well. However, if, for some reason, the encoder wire disconnects, the motor spins continuously in the opposite direction, which breaks my catapult. To solve this, I would like to write a function to catch the failure of the wire, and use that to switch the runmode to not using the encoder. I have both control functions working properly. My issue is the transition. How can I detect when an encoder disconnects.
Note:
I have thought about writing a function to checks the return value of the encoder to see if it is disconnected, but I no not know what is returned by the getPosition function when the encoder is disconnected. Is it 0, is it null, or is it something else entirely?
","motor, first-robotics"
Different methods to determine DOF: Chebychev-Kutzbach-Grubler method vs. Screw method,"I'm familiar with Chebychev-Kutzbach-Grubler method to determine degree of freedom of a robot arm. But it seems this method fails to calculate the mobility of some parallel robots, as explained here. 
However I cannot understand screw theory well and I do not know how to apply it to determine DOF.
So I wanna know what is the idea behind screw method to determine DOF?
And could anyone explain with a simple example how Screw method works?

EDIT:
Could you please explain how we can determine total DOF of ,for example, SCARA Arm via screw method?

","mechanism, manipulator"
Determining the speed of each pole for a transfer function,"
I have a transfer function and i want to calculate the speed of each pole and i do not know how to do that
",control
Symbolic Representaion of Links and Joints: How to sketch complex manipulators using simple symbols?,"In some papers and books we can see that authors using symbols to represent robot arms. My question is, is there a convention for such sketches? If so could you provide a reference which shows how these symbols should be used?


",manipulator
Use of universal humanoid robots for lean and agile manufacturing and services?,"How robot industry perceives the idea about the use of universal humanoid robots for the agile, rapidly reconfigurable manufacturing and services? 
Are there examples of such use? E.g. are there examples of use of Nao robots or similar robots in food industriy (where manual work is required) and in hotel services? 
And do the developers of humanoid robots take into account the potential use of their products in the manufacturing and services?
Aparently the manufacturing worflow rapidly evolves and universal, multi-functional robots can be especially suitable for such use.
","industrial-robot, humanoid, manufacturing"
ROS Installation Error : ARCH LINUX,"So I had this old lappy on which I used to play with ROS a lot then it got broken so I bought a new one and installed the same distro as that of old one i.e. arch linux. Now when I am installing the ROS via AUR I get this build error please help me fix this.
Scanning dependencies of target libqt_gui_cpp_sip
[ 85%] Running SIP generator for qt_gui_cpp_sip Python bindings...
Traceback (most recent call last):
File ""/opt/ros/jade/share/python_qt_binding/cmake/sip_configure.py"", line 50, in <module>
    config = Configuration()
 File ""/opt/ros/jade/share/python_qt_binding/cmake/sip_configure.py"", line 19, in __init__
    ['qmake', '-query'], env=env, universal_newlines=True)
  File ""/usr/lib/python2.7/subprocess.py"", line 567, in check_output
    process = Popen(stdout=PIPE, *popenargs, **kwargs)
  File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__
errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1343, in _execute_child
raise child_exception
OSError: [Errno 2] No such file or directory
make[2]: *** [src/qt_gui_cpp_sip/CMakeFiles/libqt_gui_cpp_sip.dir/build.make:90: sip/qt_gui_cpp_sip/Makefile] Error 1
make[1]: *** [CMakeFiles/Makefile2:375: src/qt_gui_cpp_sip/CMakeFiles/libqt_gui_cpp_sip.dir/all] Error 2
make: *** [Makefile:128: all] Error 2
==> ERROR: A failure occurred in build().
Aborting...

","ros, python, linux"
Control method of large linear actuator,"I have a Large linear actuator from Hiwin, the serial number is LAS3-1-1-500-24GE. the data sheet for the LAS3 can be seen on page 18 of the following link: http://www.hiwin.com/pdf/linear_actuators.pdf .
I would like to be able to use this to generate some small sinusoidal motion in the actuator. The speed of this does not matter. 
I am looking to control this using an arduino uno and an H-bridge along with a power supply. For an example of the H-bridge: https://www.amazon.co.uk/gp/product/B00M1JZ7HY/ref=ox_sc_act_title_1?smid=AONS7HEF348I5&psc=1
What would be the most convenient method of generating this sinusoidal motion? is an arduino and the linked H-bridge appropriate?
","arduino, control, actuator"
Real-time TCP position/pose control on a robotic arm such as YuMi: recommendation to calculate IK/Inverse Jacobian?,"I plan to make a mouse or a gesture control robot like this video on YouTube : ABB Externally guided motion. 
For a 6-axes robot, I could implement it by using ABB’s EGM (Externally Guided Motion) option, which allows to send a Cartesian position and pose of TCP, and all the tedious calculation was handled by its controller. 
However, when I started to work on YuMi, I noticed that EGM’s position guide control cannot be applied to the 7-axes robot (For YuMi, a joint control mode in EGM is only available). Are there any recommendations to implement what I described above? 
Also, I'm guessing that I need to implement IK class to get the correct joints' angles from a desirable TCP position. Maybe using OpenRAVE or OMPL? If you have any recommendation to calculate IK / Inverse Jacobian, please let me know too.
","control, robotic-arm, kinematics, inverse-kinematics, motion-planning"
Determining maximum distance measurment error given camera resolution of Optical Flow Sensor,"Good day,
I am currently working on a mobile robot which has an optical flow sensor ADNS3080 used in position tracking. May I ask how can I determine the maximum acceptable distance measurement error if my camera resolution is 5 mm per pixel.
Thank you :)
","mobile-robot, sensors, cameras, differential-drive, sensor-error"
Combining centres of mass and Inertia tensors,"I'm trying to build the dynamic model of a 6DOF robot, and the company that has built it, kindly provides a document having the masses, centres of mass, principal axes of inertia and principal moments of inertia taken at the center of mass, taken at the center of mass and aligned with the output coordinate system, and taken at the output coordinate system (I've come to known that this was obtained from a tool in SolidWorks)
The robot has 6 actuators responsible for the motion of each one of the 6 links available. The problem that I have here is the way I should calculate the inertia matrix $M(q)$. Since the matrix has to have a 6x6 dimension, I know that I have to do some kind of ""combining"" between one link and the correspondent actuator. 
The problem is that I don't really know how can I find the respective centre of mass between the two ""objects"" and the respective moment of inertia of the ""multiobject"". I've seen people saying that it is simply the summation of the respective moments of inertia but they need to be in respect to the same orientation and translation.
Can anyone shed some light into this? Suggestions would be greatly appreciated.
Thanks
","control, robotic-arm, dynamics"
How to create push and pull mechanism using Servo,"How do I create a push and pull mechanism using a standard hobby servo? Eg. SG-5010
Preferably without the need of 3D printing. 
","arduino, robotic-arm, kinematics, servos"
Image Based Visual Servoing algorithm in MATLAB,"I was trying to implement the IBVS algorithm (the one explained in the Introduction here) in MATLAB myself, but I am facing the following problem : The algorithm seems to work only for the cases that the camera does not have to change its orientation in respect to the world frame.For example, if I just try to make one vertex of the initial (almost) square go closer to its opposite vertex, the algorithm does not work, as can be seen in the following image

The red x are the desired projections, the blue circles are the initial ones and the green ones are the ones I get from my algorithm.
Also the errors are not exponentially dereasing as they should.

What am I doing wrong? I am attaching my MATLAB code which is fully runable. If anyone could take a look, I would be really grateful. I took out the code that was performing the plotting. I hope it is more readable now. Visual servoing has to be performed with at least 4 target points, because else the problem has no unique solution. If you are willing to help, I would suggest you take a look at the calc_Rotation_matrix() function to check that the rotation matrix is properly calculated, then verify that the line ds = vc; in euler_ode is correct. The camera orientation is expressed in Euler angles according to this convention. Finally, one could check if the interaction matrix L is properly calculated.
function VisualServo()

    global A3D B3D C3D D3D A B C D Ad Bd Cd Dd

    %coordinates of the 4 points wrt camera frame
    A3D = [-0.2633;0.27547;0.8956];
    B3D = [0.2863;-0.2749;0.8937];
    C3D = [-0.2637;-0.2746;0.8977];
    D3D = [0.2866;0.2751;0.8916];

    %initial projections (computed here only to show their relation with the desired ones) 
    A=A3D(1:2)/A3D(3);
    B=B3D(1:2)/B3D(3);
    C=C3D(1:2)/C3D(3);
    D=D3D(1:2)/D3D(3);

    %initial camera position and orientation
    %orientation is expressed in Euler angles (X-Y-Z around the inertial frame
    %of reference)
    cam=[0;0;0;0;0;0];

    %desired projections
    Ad=A+[0.1;0];
    Bd=B;
    Cd=C+[0.1;0];
    Dd=D;

    t0 = 0;
    tf = 50;

    s0 = cam;

    %time step
    dt=0.01;
    t = euler_ode(t0, tf, dt, s0);

end


function ts = euler_ode(t0,tf,dt,s0)

    global A3D B3D C3D D3D Ad Bd Cd Dd 

    s = s0;
    ts=[];
    for t=t0:dt:tf
        ts(end+1)=t;
        cam = s;

        % rotation matrix R_WCS_CCS
        R = calc_Rotation_matrix(cam(4),cam(5),cam(6));
        r = cam(1:3);

        % 3D coordinates of the 4 points wrt the NEW camera frame
        A3D_cam = R'*(A3D-r);
        B3D_cam = R'*(B3D-r);
        C3D_cam = R'*(C3D-r);
        D3D_cam = R'*(D3D-r);

        % NEW projections
        A=A3D_cam(1:2)/A3D_cam(3);
        B=B3D_cam(1:2)/B3D_cam(3);
        C=C3D_cam(1:2)/C3D_cam(3);
        D=D3D_cam(1:2)/D3D_cam(3);


        % computing the L matrices
        L1 = L_matrix(A(1),A(2),A3D_cam(3));
        L2 = L_matrix(B(1),B(2),B3D_cam(3));
        L3 = L_matrix(C(1),C(2),C3D_cam(3));
        L4 = L_matrix(D(1),D(2),D3D_cam(3));
        L = [L1;L2;L3;L4];


        %updating the projection errors
        e = [A-Ad;B-Bd;C-Cd;D-Dd];

        %compute camera velocity
        vc = -0.5*pinv(L)*e;

        %change of the camera position and orientation
        ds = vc;

        %update camera position and orientation
        s = s + ds*dt;


    end  
    ts(end+1)=tf+dt;
end

function R = calc_Rotation_matrix(theta_x, theta_y, theta_z)

    Rx = [1 0 0; 0 cos(theta_x) -sin(theta_x); 0 sin(theta_x) cos(theta_x)];
    Ry = [cos(theta_y) 0 sin(theta_y); 0 1 0; -sin(theta_y) 0 cos(theta_y)];
    Rz = [cos(theta_z) -sin(theta_z) 0; sin(theta_z) cos(theta_z) 0; 0 0 1];

    R = Rx*Ry*Rz;

end

function L = L_matrix(x,y,z)

    L = [-1/z,0,x/z,x*y,-(1+x^2),y;
       0,-1/z,y/z,1+y^2,-x*y,-x];
end

Cases that work:
A2=2*A;
B2=2*B;
C2=2*C;
D2=2*D;

A2=A+1;
B2=B+1;
C2=C+1;
D2=D+1;

A2=2*A+1;
B2=2*B+1;
C2=2*C+1;
D2=2*D+1;

Cases that do NOT work:
Rotation by 90 degrees and zoom out (zoom out alone works, but I am doing it here for better visualization)
A2=2*D;
B2=2*C;
C2=2*A;
D2=2*B;


","control, algorithm, matlab, visual-servoing"
How to check lane departure in OpenCV?,"I'm using OpenCV 3 in Python 2.7 on a Raspberry Pi 3. My project's aim is to build an autonomous lane departing robot that can detect the two lanes on its sides and continuously correct itself to remain within them. I want to achieve something like this project: https://www.youtube.com/watch?v=R_5XhnmDNz4
So far I've done the line detection part from the live video feed using both HoughLines and HoughLinesP. Here is a screenshot from my video feed and the outputs I'm getting so far:

Till now my logic for detecting if the robot is going left or right is based on the (rho,theta) output of the HoughLines function. What I want to achieve is a more robust way of tracking how the robot is departing from the lanes. Some sort of central line marker that can be used to detect if the robot has moved away from the center. I'm still new to OpenCV and python and the part where I'm stuck at is converting the logic of detecting the lane departure of the robot. 
My understanding is that averaging the lines on the lanes into two lines (left and right lanes) and then working with their slopes should give some result. However, I've not been able to transform this into code. I'd appreciate any suggestions on ways to detect lane departure of the robot. Thanks!! :)
","mobile-robot, computer-vision, raspberry-pi, opencv"
VFH+ (Vector Field Histogram+) : Is it possible to choose a candidate sector without a set goal point?,"Good day
I am currently implementing the VFH algorithm. 

Is it possible to configure the algorithm such that a reactionary motion is generated at the presence of an obstacle? 

I have been able to generate the obstacle map, primary polar histogram and the binary polar histogram.

How does one prioritize a sector to pass through?

I have seen an implementation in labview where in it is possible to implement a simple vector field histogram path planning without any goal points here
","mobile-robot, motion-planning, mapping, c++, vfh"
"Full 3D Pose (Scale, Rotation and Translation) Estimation using Gyro and Acceleromter sensors fusion","I am implementing a 3D pose estimation algoriothm on mobile device (Android) which has Gyro, Accelerometer and Magnetometer sensors. I have already develeoped a Visual SLAM algoirthm to estimate full 3D camera pose. I want to estimate same pose just by using these sensors.
I have seen the code for EKF based sensor fusion techniques, Attitude estiamtor, etc. But none of these give full 3D Pose. Insted these give only orientation (and not scale and translation)
Could any one suggest an open source C++ implementation (Not using ROS) for the problem?
Few links which I have already found:
https://github.com/simondlevy/TinyEKF
https://github.com/AIS-Bonn/attitude_estimator
","odometry, pose"
Does multiple IMU increase accuracy,"I'm just starting up with IMU's and I really want to work on my own flight controller, but a question always hits my mind and I am not able to find answer anywhere, so I'm here.
Will multiple IMUs will help improving stability of a quadcopter? averaging out the values of all the multiple IMUs should reduce the drift, which is a function of time, but I have no experience with IMUs and just cant figure out about the amount of error correction by adding one extra IMU, will it be just additive? or Exponential? 
This question was also posted on the Electrical Engineering Stack Exchange site.
","quadcopter, imu, accelerometer, gyroscope, gps"
How unique are Quarternions and what about the sign?,"I want to track a robot's orientation in space and wanted to choose quaternions for their many advantages.
However I have a few questions that I haven't found to be solved anywhere. The method I use to get quaternions from a rotation matrix is the one by Bar-Itzhack (2000). I want to use the ""version 3"" method always whether or not the rotation matrix is imprecise since the method for precise matrix (version 1) also involves almost the same computational effort (contructing some matrix and getting eigen*)and this way it is more robust if my matrix happens to be imprecise. My questions regarding quaternions are the following:

How unique are they when tracking in 3D space? Can I track the rotations of the tool frame without worrying about going through discontinuities in space? (E.g. like with axis-angle representation when the angle gets close to 0° or 180° and even is undefined for those) And no arbitrary outcomes.
In the method mentioned above a special matrix is constructed from the rotation matrix and then the eigenvector of the highest eigenvalue is used as the resulting quaternion. I wanted to confirm the correctness with the following test. However the resulting fixed-angle is often negative. So I started to just negate the quaternion but I suspect that there may be cases where this is wrong, so what is the method to determine the sign? This is my verification method:

Get rotation matrix of a fixed rotation around an axis (e.g. +42° about x)
From this rotation, apply the linked method above (version 3) to get the quaternion.
Get a rotation matrix from the quaternion back (method used by Craig)
And finally I convert the rotation matrix back into fixed angle representation and see if the angle is the same.


Any help would be much appreciated.
On this project I am not using ROS, everything is self-build.
","robotic-arm, orientation, frame"
Add failsafe to reinforcement learning algorithm,"I'm working on a hexapod that uses A3C to learn how to walk. Ideally I would test it all in a simulator for some structure to the weights/policy but I don't have enough time for that. Obviously there are specific degrees of freedom that would hit each other at certain points, so how could I implement a failsafe that stopped certain movements without messing up the algorithm? If I were to just not allow a movement if I thought it would be dangerous after the algorithm but before the movement, would that disrupt it?
",reinforcement-learning
setup requirement of stereo camera,"In the stereo camera system, two cameras are needed and should be mounted side by side. I see someone just glues two cameras to a wooden board. However one mobile phone manufacture claimed that the two lens of dual camera modules on phones produced by them are parallel within 0.3 degree. Why do two lens on mobile phones need such high precise assembly? Does this will bring any benefit?
","computer-vision, cameras, stereo-vision, calibration"
How can I simulate baxter in MoveIt without a real baxter?,"I follow the tutorials  Rethink Robotics MoveIt tutorial to install my baxter with MoveIt. When I run
$ ./baxter.sh

It shows:

EXITING - Please edit this file, modifying the 'baxter_hostname' variable to reflect Baxter's current hostname.

I don't have a real baxter robot,how can I simulate it with MoveIt ?
",robotic-arm
Transform timestamped messages recorded in a bag file using tf messages,"Note: this question was also asked in the ROS official Q&A website, but even after one week nobody has given me an answer.
I have a .bag which contains recorded messages on topics /topic1 and /topic2. The messages have /world as frame_id, so both of the messages associated with these topics are stamped, i.e. they have a header.
The same .bag file also contains recorded messages on the topic /tf (of type tf2_msgs/TFMessage). These transform messages have the frame_id set to /world and the child_frame_id set to the local frames associated with the IMUs from which the messages are being sent over respectively topic /topic1 and /topic2.
Now, I need the messages sent over the /topic1 and /topic2 to be converted to their corresponding local frame (i.e. the child frame or child_frame_id) from the (fixed) frame /world. Since both the /tf messages and the messages of the topics /topic1 and /topic2 are stamped, I thought we could do this without much trouble, but I'm not sure since I'm very new to ROS.
I've looked around for various solutions, but I didn't find an exact solution for my problem, maybe because I didn't recognize it as such, given my limited knowledge of ROS, as I said.
I would appreciate a step by step description of the approach and, if you don't want to write a full solution with code (preferably in Python), at least point me to the similar examples. Please, do not suggest me to read the tutorials of /tf, I've partially done it, and it didn't help much.
","ros, python"
How to track multiple robots with particle filter,"I am using an IR camera to track N mobile robots driving about on the floor. Each robot has a few IR LEDs on its head in known locations, all at the same height above the floor. Each robot has 5 degrees of freedom, X, Y, theta, rotation rate, and velocity. All the camera sees is a bunch of blobs. I have a working blob detector, and can calculate the coordinates of visible blobs in world space. Now I would like to implement a particle filter.
I have two options:

Implement a single particle filter with a state space of 5xN dimensions.
Implement N particle filters with 5 dimensional state spaces.

My feeling is that 1. is the correct way to approach the problem, because otherwise each particle filter could easily get confused about which particle belongs to which robot. But, on the other hand, it seems like a lot of dimensions, and could be slow.
",particle-filter
"Using accelorometer and gyroscope to get velocity, spin & flightpath of a ball in projectile motion","I'm working on a project to make a SmartBall that can detect the velocity(km/h) , spin(degrees per second) and flightpath(trajectory) of the ball using Intel Edison with the 9DOF block (LSM9DS0 : 3-axis accelerometer, 3-axis gyroscope, and 3-axis magnetometer) & the battery block, I'm reading values from the 9DOF block by RTIMULib(Library for IMU chips). I've been working on integrating the acceleration data from the accelorometer to get the velocity then get the position, I know that this method is not really accurate as the integration error cumulate very fast but I rely on that my calculations will be done in a very short time (about 3 seconds) then i re-calculate again from the beginning after every kick so that error doesn't cumulate hardly, Also i only need an acceptable accuracy not a very high one. I discovered then that i'm dealing with projectile motion(ball kicking), so after considering this & searching in projectile motion equations i found that i must know the initial velocity and the angle of projection(theta) to be able to get my requirements. my problem that I don't know how to get any of these , I tried different approaches like getting the horizontal distance & getting the height to get their resultant(using pythagoras) then get the angle(assuming it's a right angle) in a very small time at the beggining of the projection , but i still couldn't get the height. The gyroscope outputs roll, pitch & yaw angles related to the sensor orientation but i'm still not using this as i'm assuming that the sensor will be fixed inside the ball so it's orientation will not be the same as the projection angle.Now What I really want is any approach/idea on how to get velocity & flightPath of a projectile using accelorometer and gyroscope data. Hope I made it clear , Any help on how to get my requirements is really appreciated, Thanks so much.
",gyroscope
What happens from code to robotic action?,"Starting in the code and through the hardware, what is the ""path"" that explains the robotic movement. Are there electrical signals involved? How they are initiated and formed and/or interpreted then by the ""machine""/robot? Can you explain what happens from code to robotic action? 
",industrial-robot
Programming Inverse Kinematics in C++,"I want to write my own kinematics library for my project in C++. I do understand that there are a handful of libraries like RL (Robotics Library) and ROS with inverse kinematics solvers. But for my dismay, these libraries DO NOT support MacOS platform. I have already written the Forward Kinematics part, which was quite straight forward. But for the Inverse Kinematics part, I am quite skeptical since the solution to a IK problem involves solving sets of non-linear simultaneous equation. I found out the Eigen/Unsupported 3.3 module has a APIs to non-linear equations. But before I begin on this uncertain path, I want to get some insight from you guys on the plausibility and practicality of writing my IK library. My manipulator design is rather simple with 4 DoF and the library will not be used for other designs of manipulators. So what I am trying to achieve is taylor-made IK library for my particular manipulator design than a rather a 'Universal' library.
So,
Am I simply trying to reinvent the wheel here by not exploring the already available libraries? If yes, please suggest examples of IK libraries for MacOS platform.
Has anyone written their own IK library? Is it a practical solution? Or is it rather complex problem which is not worth solving for a particular manipulator design?
Or should I just migrate all my project code (OpenCV) to a Linux environment and develop the code for IK in Linux using existing libraries?
","ros, inverse-kinematics, c++, forward-kinematics, linux"
Testing State of button aruino pyserial,"I am trying to read the state of button connected to arduino & send its value to be written in txt file using pyserial. I have a problem when i try to listen to arduino's port i get an error ""Port busy"" 
python code 
import serial,io
from datetime import datetime
connected=False
outfile='C:\Users\Yassine\Desktop.txt'
port = ""COM6""
baud = 9600
ser = serial.Serial(port, baud, timeout=1)
sio = io.TextIOWrapper(io.BufferedRWPair(ser, ser, 1), encoding='ascii', newline='\r')
with open(outfile,'a') as f:
    while ser.isOpen():
      datastring=sio.readline()
      f.write(datetime.utcnow().isoformat()+ '\t' + datastring + '\n')
      f.flush()
    while not connected:
      serin=ser.read()
      connected=True

ser.close()

","arduino, python"
DC motor - max current,"I'm looking at the data sheet for a DC motor that states:
Current consumption at nominal torque (mA): 380

I have a power supply that can deliver 500 mA. Can I take the above statement to indicate that the motor will never draw more than 380 mA, or does it mean that it usually uses 380 mA, and that I should probably choose a different power supply?
",motor
design PID with euler backward system plant (non-linear ODE),"recently i did a modelling for simple exoskeleton forearm robot using motor for actuator and rotary encoder for the position sensor. i have the total equation, and it's a non linear ODE with input PWM and theta of forearm as the output. so i have tried to solve it using backward euler and simulated it. the response for step input is ramp (of course, the angle always will be increased with constant PWM)
the question is, how can i design a simple PID controller with specific max overshoot and settling time. 
what i have done so far but failed

approx the simulation result with second order linear ODE transfer function/TF using solver in excel, not to sure because the transition is so small so i can't verify the constant for the TF
design it with trial and error. not a good solution, because i need to write a step by step mathematical method (like using root locus analysis)

so i'm stuck in this step, need help
thank you
","control, pid"
How to pick up a card from a pile,"I don't know if this is a purely ""robot"" question or a more DIY/hackish one, but let's give it a try.
I currently have a set of cards that I want to sort based on several criteria.
My setup includes:  

MeArm A 4 DOF robot arm  
Raspberry Pi 3 + Shields for controlling the arm
A mini vacuum pump, hold in place by the gripper

These are the steps:

Move the arm on top of pile of cards
Turn on the vacuum pump
Pick the first card
Move the arm in the right spot
Turn off the pump and let the card fall
Repeat

Everything is working fine, my only main issue occurs when I'm lifting the arm. It seems that between the cards there are some kind of forces and under the first one, several others come up, attached.
I tried to shake the arm and make them fall, but it's not working.
Any suggestions? Maybe I'm missing some simple/obvious solution.
","robotic-arm, raspberry-pi"
(solved) How to glue syringe?,"I foud tons of answer how glue with syringe, but I need to glue syriges together to be airtight and glue some silicon tube there too  and cannot find a hint how to do it. 
Does anybody know what glue does stick with syringe (eventually dissolve it a little)?
","robotic-arm, arm"
I need the scheme of Irobot creator 2,"I've tried to drill a number of holes in Irobot.
I've missed one of them and I need the electronic scheme of Irobot-Creator 2 for trying to restore this input of the data cable.
Can you help me with that, please?
Thanks,
Raz
enter image description here
","irobot-create, mechanism, electronics, electric"
Is it possible to turn an irobot roomba into a create,"As irobot do not sell the create in Australia and will not ship to Australia I am considering buying a second hand roomba to convert to a create.
Is this possible, what models should be avoided and any advise?
Thanks
","irobot-create, roomba"
Difference between Degrees of Freedom (DOF) and Degrees of Motion (DOM),"What is a difference between degrees of freedom (DOF) and degrees of motion (DOM)? I know that DOF is the number of independent movements a manipulation arm can make and robot system can have max 6 independent DOF and unlimited number of DOM but I do not distinguish them from each other.
","manipulator, theory"
Using Standard Digital camera for Computer Vision,"For a University project I have to use computer vision to detect small drones within 40 feet. I know there exists a pixycam for this purpose, but I was not happy with it, when I used it for CV.
I have a normal digital camera which is 16 Megapixels (pic & video), which I don't use anymore. Before I dissect the camera, I was wondering if it is practically possible to train this digital camera for computer vision - detecting small flying drones.  Any thoughts on this - using a digital camera for CV? 
Thanks 
","computer-vision, cameras, stereo-vision, machine-learning"
Arduino original or generic for a beginner?,"I'm new to the robotics and electronics world, but I'm willing to dive into it. I'm a software developer and I want to create a project that uses GPS and Accelerometer data to show as a layer on Google Maps after transferred to PC.
My doubt is about which controller to get. In my country, there are generic controllers based on the Atmega328 that are being sold with a massive difference of price from the original Arduino (talking about the UNO model). 
Should I start with an original model? 
Should I expect to break the controller, fry it, or break any components by connecting them wrong? 
Would the experience with a generic controller be less exciting than with the original Arduino one?
","arduino, beginner"
How does a personal tracking tripod work?,"There are certain tracking devices for cameras on the market these days. Here is one example.
The concept is, you wear a tag or a wristband and the tripod knows to track you while you're out surfing or racing around a track or running back and forth on a soccer field. I always assumed these work via GPS. But there is this other very recent question where it's been implied that tracking technology has been around since the 60s. While military GPS has probably been around that long, it also occurred to me that GPS perhaps doesn't have the high level of accuracy one would need to track precisely.
I'm curious to know what sort of technology these personal tripods use? How does it track its target?
",automation
Is there a better way to deal with untextured regions in stereo matching,"Texture is very helpful for stereo matching. However in real environment, untextured areas always exit. Consistent semi-global matching is proposed to deal with untextured regions. I start to read the paper Stereo Vision in Structured Environments by Consistent Semi-Global Matching. Fixed bandwidth Mean Shift segmentation is used.
If some have some experience with Consistent Semi-Global Matching, I hope to learn whether the algorithm is good for untextured regions. How about the complexity of the algorithm? Is there any better way to deal with untextured regions?
","computer-vision, cameras, stereo-vision"
Arduino robot to move spirally,"I am planning to build robot like irobot Roomba. So cleaning in a spiral pattern is required like the image shown starting from the center:

This code is part of my full code which doesn't give me spiral pattern:
void spiralling() {
  for(int i=0;i<=2;i++) {
    digitalWrite(motor1,HIGH);
    digitalWrite(motor2,LOW);
    digitalWrite(motor3,HIGH);
    digitalWrite(motor4,LOW);
    analogWrite(pwm1,180);
    analogWrite(pwm2,80);
    delay(300);
    p=1;
  }
  analogWrite(pwm2,250); 
  delay(150); 
}

So my question is how can I make my bot trace spiral pattern (algorithm?,Logic? to use), as the only way to change the direction is with two wheels on sides.
Is there any code which constantly increases the radius of bot movement from center to radially outwards?
My robot has arduino uno ,l293d motordriver,two geared motors on either sides as shown in image and castor wheel in front:

","arduino, wheeled-robot"
Effect of camera misalignment errors in dual cameras,"In the camera module assembly process, parameters of camera modules vary due to manufacturing tolerance.
Camera calibration is performed to obtain actual parameters. 
In the paper Effects of camera alignment errors on stereoscopic depth estimates, the 
author analyzes relative sensitivity/importance of camera calibration/alignment parameters 
on the performance of stereoscopic depth reconstruction. 
For dual camera system, five sources of error are listed
Binocular error effects:

depth error due to rotation/roll between two cameras
depth error due to pitch between two cameras
depth error due to yaw between two cameras

Monocular error effects:

depth error due to nonparallel CCD array and lens
depth error due to lens distortion.

In practical applications, camera rectification
will use these parameters to align two images. 
Why do we need to analyze the effect of various
errors?
","cameras, stereo-vision, calibration"
Quadcopter instability with simple takeoff in autonomous mode,"I'm trying to get a quad rotor to fly. The on board controller is an Ardupilot Mega 2.6, being programmed by Arduino 1.0.5.
I'm trying to fly it in simple autonomous mode, no Radio controller involved. I've done a thorough static weight balancing of the assembly (somewhat like this: http://www.youtube.com/watch?v=3nEvTeB2nX4) and the propellers are balanced correctly.
I'm trying to get the quadcopter to lift using this code:
#include <Servo.h>


int maxspeed = 155;
int minspeed = 0;

Servo motor1;
Servo motor2;
Servo motor3;
Servo motor4;

int val = 0;
int throttleCurveInitialGradient = 1;

void setup()
{


val = minspeed;

motor1.attach(7);
motor2.attach(8);
motor3.attach(11);
motor4.attach(12);


}


void loop()
{
setAllMotors(val);
delay(200);
val>maxspeed?true:val+=throttleCurveInitialGradient;
}

void setAllMotors(int val)
  {
    motor1.write(val);
    motor2.write(val);
    motor3.write(val);
    motor4.write(val);
  }

But the issue is, as soon as the quadcopter takes off, it tilts heavily in one direction and topples over. 
It looks like one of the motor/propeller is not generating enough thrust for that arm to take-off. I've even tried offsetting the weight balance against the direction that fails to lift, but it doesn't work (and I snapped a few propellers in the process);

Is there something wrong with the way the ESCs are being fired using
the Servo library?
If everything else fails, am I to assume there is something wrong
with the motors?
Do I need to implement a PID controller for self-balancing the roll
and pitch just to get this quadrotor to take off?

Edit 1:    Thanks for all the replies.
I got the PID in place. Actually, it is still a PD controller with the integral gain set to zero. 
Here's how I'm writing the angles to the servo:
motor1.write((int)(val + (kP * pError1) +(kI * iError1) +(kD * dError1)));  //front left
motor2.write((int)(val + (kP * pError2) +(kI * iError2) +(kD * dError2)));  //rear right
motor3.write((int)(val + (kP * pError3) +(kI * iError3) +(kD * dError3)));  //front right
motor4.write((int)(val + (kP * pError4) +(kI * iError4) +(kD * dError4)));  //rear left 

kI is zero, so I'll ignore that.
With the value of kP set somewhere between 0.00051 to 0.00070, I'm getting an oscillation of steady amplitude around a supposed mean value. But the problem is, the amplitude of oscillation is way too high. It is somewhere around +/- 160 degrees, which looks crazy even on a tightly constrained test rig. 

[  Edit 2: How I calculate the term 'pError' - Simple linear thresholding. 
I've a precomputed data of the average readings (mean and SD) coming out of the gyro when the IMU is steady. Based on the gyro reading, I classify any motion of the setup as left, right, forward or backward. 
For each of these motion, I increase the pError term for two of the motors, i.e, for right tilt, I add pError terms to motors 2 & 3, for left tilt, I add pError term to motors 1 & 4 etc. (check the comment lines in the code snippet given above). 
The magnitude of error I assign to the pError term is = abs(current gyro reading) - abs(mean steady-state gyro reading). This value is always positive, therefore the side that is dipping downwards will always have a positive increment in RPM.  ]

As I crank up the derivative gain to around 0.0010 to 0.0015, the oscillation dampens rapidly and the drone comes to a relatively stable attitude hold, but not on the horizontal plane. The oscillation dies down (considerably, but not completely) only to give me a stable quadrotor tilted at 90 - 100 degrees with horizontal. 
I'm using only the gyros for calculating the error. The gyros were self calibrated, hence I do expect a fair amount of noise and inaccuracy associated with the error values. 

Do you think that is the primary reason for the high amplitude
oscillation?

One other probable reason might be the low update frequency of the errors. I'm updating the errors 6 times a second. 

Could that be a probable reason it is taking longer to stabilise the
error?

And, for the steady state error after the wild oscillations dampen, is it necessary to fine tune the integral gain to get rid of that?
Please help.

Edit 3:  I cranked up the frequency of operation to 150+ Hz and what I get now is a very controlled oscillation (within +/- 10 degrees). 
I'm yet to tune the derivative gain, following which I plan to recompute the errors for the integral gain using a combination of gyro and accelerometer data. 

Edit 4:  I've tuned the P and D gain, resulting in +/- 5 degrees oscillation(approx). I can't get it to any lower than this, no matter how much I try.
There are two challenges about which I'm deeply concerned:
After 5 to 8 seconds of flight, the quadcopter is leaning into one side, albeit slowly. 
A) Can this drift be controlled by tuning the integral gain?
B) Can the drift be controlled by using accelerometer + gyro fused data?
C) Given that my drone still shows +/- 5 degrees oscillation, can I consider this the optimal set point for the proportional and derivative gains? Or do I need to search more? (In which case, I'm really at my wits end here!) 
","arduino, quadcopter, pid, ardupilot"
F3 robot communications protocol?,"I am considering writing a program to communicate with a Thermo Scientific F3 robot arm in order to eliminate their obsolete C500C controller.  Does anyone know the communications protocol to/from the F3?  All I know is that it is RS485.
Thanks
",robotic-arm
Adding magnetic field vector to a Kalman filter,"I currently have an error state Kalman filter with the state vector $(p, v, q, \omega, a, g)$ where $q$ is the quaternion orientation. I would like to add the information coming from a magnetometer to this sensor fusion.
I have calibrated the magnetometer and we can assume that we are getting processed data at the point of input to the filter.

How do I extend my state vector to account for the new input, or since I do not directly care about estimating it, should I not include it?
I think that I can initialize my initial state vector correctly by performing TRIAD using the magnetic field vector, is this the right approach?
How does the magnetic field vector help in stabilizing my quaternion attitude?

I tried to search around but I didn't find many resources on how the math works when I include the magnetometer. Any links would be very helpful as well.
","kalman-filter, magnetometer"
"Seeking dirt cheap, wheeled, programmable robot","I was playing the old ""confuse the cat with a flash-light"" game, when I thought that I might like to program a confuse-a-cat robot.
Something, probably with tracks, which can right itself if he flips it over, and which I can program to move randomly around a room, turning at walls, making an occasional sound or flashing a light.
Since I am on a very tight budget, I wondered if there is some cheap kit which I can program ...
Arduino, Raspberry Pi, any platform, so long as it is programmable.
Thanks in advance for your help
",mobile-robot
Robot detection in motion,"In the environment, I have two robots and a couple of fixed obstacles. In order to detect obstacles I am using ultrasonic sensors. For robots, they need to detect each other and from which side other robot is coming (front, left, right or back), and do this during the motion. For this purpose, I cannot use PIR sensors, because robots are constantly moving. Also, I need to differentiate between moving robots and stationary obstacles, so ultrasonic sensors are also not helpful. 
So I came up with idea to somehow mark the robots with some property unique to environment, so when we detect object with that property, we know that it is a robot, and not another obstacle. One of ideas might be to put lasers on one robot, and on other robot put four laser sensors, one for each side, so we precisely can say from which side the other robot came from. Another option might be to use IR transmitters on one robot and four IR receivers on other robot? What do you suggest, is there any other type of sensors that might help?
","sensors, ultrasonic-sensors"
Strain gauges or flex sensors,"I've been thinking about making myself a fancy data glove. Now that I'm looking into it, I notice a lot of DIY stuff works with these so called flex sensor based on conductive carbon ink. 
I'm not familiar with these sensors but from what I have learned so far, they are more expensive and less accurate compared to a simple strain gauge. Or are they just hard to use because of the length and bend radius of the finger?
So actually I'm just wondering what the pros and cons are of these sensors when faced with data gloves.
",sensors
2.4GHz helicopter hack [Edited],"I bought a little toy helicopter, namely the Revell Control 23982. After flying a few batteries I was wondering whether I could hack it in such a way, that my Arduino Uno can control/manipulate the signals from the transmitter to the receiver on board. However, to me it seems like I have big trouble getting started on the right path.
Can anyone spot out my mistakes?
Hardware hack
My first attempt was to bridge the potentiometers in the transmitter with the Arduino, this did not work at all. I think this is because the Arduino is using PWM and no true DC. Also I do not understand how the potentiometer (shown in the picture) works, where two of the three terminals are connected to each other.

NOTE: The soldered cable is a result of some unsuccessful hardware soldering.
EDIT
Here is further information according to @combos comment.
 
NOTE: The diagram is missing a connection between the line that connects all potentiometers and a 3v pin on the IC. Sorry for that.
This is a simple excerpt of the actual PCB, however, it should be the one which is important for my question. It is true that two terminals of the potentiometers are connected, and all the potentiometers are additionally soldered to ground via their housing. I was not able to find any information about the IC labelled ""???"". It contains the transmitter, that is clear.
A test with my multi-meter showed, that there is a maximum current flow of three volts on the single line of each potentiometer, and the connection between all of them is 3v consistent.
Software hack
My second attempt was trying to reverse engineer the 2.4GHz transmission via an nrf24l01 module, as some other people on the internet have been successful doing this with some other toys. I tried to scan the frequency bands, however with no successful outcome. I have no clue which transmitter module is being used on the board.
TL;/DR;
My questions:

Is it even possible to achieve what I want?
If yes, what do I need to do?
If not, what did others do to achieve this kind of behaviour?
Which circuit do I need to convert digital PWM in analog? 

If you need any additional information please let me know!
","electronics, radio-control"
Testing and Lifespan of Industrial Robots?,"How does one go about testing the robot once it is built? How does one predict the number of hours it can operate? I see most of the industrial robots for instance robotic arms have warranty of 12-18 months, how did they arrive at such an estimate, clearly testing for 12-18 months is not on option, so what is the procedure for determining the lifespan? 
","robotic-arm, industrial-robot"
What is a suitable model for two-wheeled robots?,"What is a suitable model for two-wheeled robots? That is, what equations of motion describe the dynamics of a two-wheeled robot.
Model of varying fidelity are welcome. This includes non-linear models, as well as linearized models.
","mobile-robot, two-wheeled"
ZED Camera Performance,"Anyone ever use multiple GPUs and multiple ZED Cameras with ROS? If so what hardware and how was the CPU utilization. Also wouldn't that be pushing the PCI bus given the amount of data going through it? Thanks for any data you can provide.
","ros, stereo-vision"
Any free or paid software for graphical drawing of Robotics Kinematic Scheme?,"I have been searching for days looking for a dedicated software for easily draw kinematic diagrams of robotics like this:

I came by this software but it is a bit hard to grasp. Is there any GUI based software instead?
",kinematics
Area of Study for Advanced Robotics Work,"I have known for a while now that robotics is something I am very passionate about. Im beginning my studies now in university and am trying to decide between the best major to prepare me for advanced robotics work.
I'm currently in between Math/CS ECE/CS and Stats/CS
I know that Mech E is not for me.
Thoughts on the matter? What is the best for general development? Research/ theoretical? Applied robotics?
Specifically I would love to hear how each major may be more useful to each aspect of robotics development.
Thanks
I understand this post lacks a specific problem, but I figured I may ask any way given the collective knowledge of this community - if there is a more appropriate place to post this please let me know!
",arduino
Transfer Function for Hitec Servo Motor,"I am using Hitec Servo Motor in my 6 Dof Robotic Arm. I am going to run an open loop response and compare it with simulation which using a transfer function in MATLAB. But I could not find any parameters that I need in a transfer function. For example, moment of inertia, damping, electric resistance, electric inductance, and back-emf constant. What I had found on the data sheet are operating speed, output torque,idle current,running current and dead bandwidth. How to relate all these to get the parameters that I need to develop my transfer function ?
",robotic-arm
Second-order dynamical systems,"I have been reading this paper (https://arxiv.org/pdf/1509.06113.pdf), which is about control of a robotic arm. They learn a mapping from robot state to robot control, where the state is the positions and velocities of the arm's joints, and the control is the joint torques.
One passage I am struggling to understand is:

Since we use torque control, the robot and its environment form a second-order dynamical system, and we must include both the joint positions and their velocities.

(I've edited this slightly for readability, but effectively it is the same).
Please can somebody explain what this means? What is a second-order dynamical system? And why does this mean that velocities are required as part of the state?
Thanks!
","control, robotic-arm, dynamics, policy"
Is there a commercially available inertial gyro with electronic control?,"Searching for electronic gyros doesn't turn up what I am after. Rather than an instrument to measure rotation, what I want is a device that I can mount in a flying machine, where I can apply a certain moment in order to have the flying machine react with the opposite moment in order to control attitude.  A classic physical gyroscope behaves like this, but I am unsure if there are any available with electronic controls. 
Is there a name for such devices, that I can Google for?
To clarify, this isn't asking for a product recommendation, just want to know if such a thing exists and what it would be called.
","control, gyroscope"
Denavit-Hartenberg convention,"There are two different conventions that can determine DH parameters.   What is the difference between Craig's [1, Sec 3.4] convention and the Spong [2, Sec. 3.2] convention?
I know that both methods must have the same response.
[1]: Craig, John J. Introduction to robotics: mechanics and control. Addison-Wesley, 1989.
[2]: Spong, Mark W., Seth Hutchinson, and Mathukumalli Vidyasagar. Robot modeling and control. Wiley, 2006.
","forward-kinematics, dh-parameters"
Augmenting Enviroment for Localization,"What is the most robust way to do outdoor localization by augmenting the environment? Originally I was thinking image targets but then I realized they will not work well at night (low light). I could make some kind of RF or IR beacons but they would require power which would require more infrastructure development than I want. I might try using retro reflective image targets but I wanted to see if anyone had experience with them or better suggestions.
","localization, computer-vision"
How to compute the field of view of a robot when obstacles are around?,"Is there a formula to compute the field of view of a robot when it is obstructed by other objects like in this picture
When it is a full circle it should be  FoV = (x-c1)² + (y-c2)² < r² Where (c1,c2) are the coordinates of the robot and r is the range of the sensor.    
Thank you!
","mobile-robot, sensors, kinect"
PID tuning with (Deep) Reinforcement Learning,"I am trying to implement a RL algorithm for an adaptive PID in a robot system. My doubt consists in the creation of the possible states in the problem.
I mean, I understand quite well the problem when the possible states are door numbers, but I don't know know what to do with PID. Do I have to create a finite number of possible PID values in which the algorithm learns?
","pid, machine-learning, reinforcement-learning"
6 axis robot arm with non-perpendicular axes?,"Lately I've been thinking about 6 axis robots and noticed that all examples that I've seen on the internet use the same configuration of axes:

Vertical axis waist (1), horizontal axis shoulder (2) and then the axes are perpendicular (3), parallel (4), perpendicular (5) and parallel (6) with the previous link. 
This means at least three different types of joints, one for waist (1), one perpendicular (2, 3 and 5) and one parallel (4 and 6). 
I was wondering if comparable range of motion can be achieved by having only one type of joint in addition to waist.
I thought that having only axes perpendicular to previous segments (like 2 and 3 in the picture), but instead of consecutive pairs of axes being parallel to each other, have them twisted by some fixed angle (45 degrees?) relative to each other.
Would an idea like this work? Would it have any significant disadvantages? Is there some general method to visualise what positions can be reached with a given configuration of axes?
","robotic-arm, geometry"
Difference between Rao-Blackwellized particle filters and regular ones,"From what I've read so far, it seems that a Rao-Blackwellized particle filter is just a normal particle filter used after marginalizing a variable from:
$$p(r_t,s_t | y^t)$$
I'm not really sure about that conclusion, so I would like to know the precise differences between these two types of filters. Thanks in advance.
","slam, particle-filter"
what is the difference between a voltage source and current source?,"I know,this question is irrelevant.But,while working suddenly this question came through my head..my point is there should not be any difference between a voltage source and a current source.cause,these two are dependent on each other.if there is a  potential difference then,there will be a current flow...similarly,if there is a flow of current then there must be a voltage difference....isn't it?
please,make the topic clear.....
",current
RobotC Code Malfuncion (VEX Robotics Clawbot),"I have a standard VEX Clawbot, which I've been trying to make go straight for some time. 
I've been following this guide:
http://www.education.rec.ri.cmu.edu/products/cortex_video_trainer/lesson/3-5AutomatedStraightening2.html
This is my code:
#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port1,           leftMotor,     tmotorVex393_HBridge, openLoop, driveLeft, encoderPort, I2C_1)
#pragma config(Motor,  port10,          rightMotor,    tmotorVex393_HBridge, openLoop, reversed, driveRight, encoderPort, I2C_2)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

void GOforwards()
{
    nMotorEncoder[rightMotor]=0;
    nMotorEncoder[leftMotor]=0;
    int rightEncoder = abs(nMotorEncoder[rightMotor]);
    int leftEncoder = abs(nMotorEncoder[leftMotor]);

    wait1Msec(2000);
    motor[rightMotor] = 60;
    motor[leftMotor] = 60;

    while (rightEncoder < 2000)
    {
        if (rightEncoder > leftEncoder)
        {
            motor[rightMotor] = 50;
            motor[leftMotor] = 60;
        }
        if (rightEncoder < leftEncoder)
        {           
            motor[rightMotor] = 60;
            motor[leftMotor] = 50;
        }
        if (rightEncoder == leftEncoder)
        {
            motor[rightMotor] = 60;
            motor[leftMotor] = 60;
        }
    }
    motor[rightMotor] = 0;
    motor[leftMotor] = 0;
}


task main()
{
GOforwards();
}

I am using integrated Encoders.
When I run the code my robot runs without stopping and the Encoder values diverge quickly. This is a video of the code running from the debugger windows:
https://www.youtube.com/watch?time_continue=2&v=vs1Cc3xnDtM
I am not sure why the power to the wheels never changes, or why it seems to believe that the Encoder values are equal... much less why it runs off into oblivion when the code should exit the while loop once the right encoder's absolute value exceeds 2000.
Any help would be appreciated.
","robotc, quadrature-encoder, vex"
"What is exact model of haptic interface system in the paper ""Time-Domain Passivity Control of Haptic Interfaces""?","I am trying to simulate the experiments in the paper ""Time-Domain Passivity Control of Haptic Interfaces"", Hanaford and Ryu, 2002, IEEE transactions on robotics and automation vol .18, No1
about the simulation in Fig.8 and its results are fig.9 and fig.10. I don't know an exact model of fig.9 which I can draw again in Matlab/simulink to run. I tried drawing spring and damper as in fig.6 of the same paper, but I don't know which is my input ( force, velocity or position) and also the type of signal of input (sine, step,...) to get a similar result as the paper. 
I added 2 images, one is model which i drew and another is the signal of position.
I am really confused about initial condition of position . should either I choose a initial condition at Discrete-Time Integrator1, or adding a difference of 2 adders into position.
In here: damping=0 ( i want to check the case without Passivity controller)
k=30000 and I added initial condition to Discrete-Time Integrator1 is 50
[![enter image description here][3]][3]
","matlab, algorithm, theory"
Scale problem with monocular visual odometry,"Is monocular visual odometry able to estimate relative scale? Say I have a sequence of 10 images that are taken on a single track each 1 m after the previous. Can some mono odometry method distinguish relative scale when it processes image pairs that are in various distances from each other? I mean like processing 1st vs 10th image and 9th vs 10th image - will the fist give 10x relative scale than the second?
I am examining OpenCV based odometry code (https://github.com/avisingh599/mono-vo) but it only gives something like ""translation vector"" that always has a size of 1 regardless the distance measured. I know mono odometry can not do absolute scale but I thought it can do relative (question is what ""relative"" actually means here). Seems like OpenCV's recoverPose only do a translation vector that has always the same size (I guess the size is 1)?
","cameras, opencv, visual-odometry, scale-model"
Jacobian-based trajectory following,"I would like to control my 7 DOF robot arm to move along a Cartesian trajectory in the world frame.  I can do this just fine for translation, but I am struggling on how to implement something similar for rotation.  So far, all my attempts seem to go unstable.  
The trajectory is described as a translational and rotational velocity, plus a distance and/or timeout stopping criteria.  Basically, I want the end-effector to move a short distance relative to its current location.  Because of numerical errors, controller errors, compliance, etc, the arm won't be exactly where you wanted it from the previous iteration.  So I don't simply do $J^{-1}v_e$.  Instead, I store the pose of the end-effector at the start, then at every iteration I compute where the end-effector should be at the current time, take the difference between that and the current location, then feed that into the Jacobian.
I'll first describe my translation implementation.  Here is some pseudo OpenRave Python:
# velocity_transform specified in m/s as relative motion
def move(velocity_transform):
  t_start = time.time()
  pose_start = effector.GetTransform()
  while True:
    t_now = time.time()
    t_elapsed = t_now - t_start
    pose_current = effector.GetTransform()
    translation_target = pose_start[:3,3] + velocity_transform[:3,3] * t_elapsed
    v_trans = translation_target - pose_current[:3,3]
    vels = J_plus.dot(v_trans) # some math simplified here

The rotation is a little trickier.  To determine the desired rotation at the current time, i use Spherical Linear Interpolation (SLERP).  OpenRave provides a quatSlerp() function which I use.  (It requires conversion into quaternions, but it seems to work).  Then I calculate the relative rotation between the current pose and the target rotation.  Finally, I convert to Euler angles which is what I must pass into my AngularVelocityJacobian.  Here is the pseudo code for it.  These lines are inside the while loop:
rot_t1 = np.dot(pose_start[:3,:3], velocity_transform[:3,:3]) # desired rotation of end-effector 1 second from start
quat_start = quatFromRotationMatrix(pose_start) # start pose as quaternion
quat_t1 = quatFromRotationMatrix(rot_t1) # rot_t1 as quaternion

# use SLERP to compute proper rotation at this time
quat_target = quatSlerp(quat_start, quat_t1, t_elapsed) # world_to_target
rot_target = rotationMatrixFromQuat(quat_target) # world_to_target
v_rot = np.dot(np.linalg.inv(pose_current[:3,:3]), rot_target) # current_to_target
v_euler = eulerFromRotationMatrix(v_rot) # get rotation about world axes

Then v_euler is fed into the Jacobian along with v_trans.  I am pretty sure my Jacobian code is fine.  Because i have given it (constant) rotational velocities ok.  
Note, I am not asking you to debug my code.  I only posted code because I figured it would be more clear than converting this all to math.  I am more interested in why this might go unstable.  Specifically, is the math wrong?  And if this is completely off base, please let me know.  I'm sure people must go about this somehow. 
So far, I have been giving it a slow linear velocity (0.01 m/s), and zero target rotational velocity.  The arm is in a good spot in the workspace and can easily achieve the desired motion.  The code runs at 200Hz, which should be sufficiently fast enough.  
I can hard-code the angular velocity fed into the Jacobian instead of using the computed v_euler and there is no instability.  So there is something wrong in my math.  This works for both zero and non-zero target angular velocities.  Interestingly, when i feed it an angular velocity of 0.01 rad/sec, the end-effector rotates at a rate of 90 deg/sec.
Update: If I put the end-effector at a different place in the workspace so that its axes are aligned with the world axes, then everything seems works fine.  If the end-effector is 45 degrees off from the world axes, then some motions seem to work, while others don't move exactly as they should, although i don't think i've seen it go unstable.  At 90 degrees or more off from world, then it goes unstable. 
","kinematics, jacobian"
(semi)autonomouse robot - where to place main logic unit during developement (sensor-wise),"I want to make autonomous universal inteligent moving robot (with laser gun turret and rockets and warp drive and everything of course). I found, that it is not so easy task, so I choose approach of dividing it to small steps.
My idea is a lot of units (motors, sensors, lasers, communications, ...) all connected to I2C bus, all communicating in logical way rather then just technical values (so ""warp 5 ahead"" rather than ""PWM 70 to pins 4 and 6"", or ""motors stopped due overheating"" rather than ""analogRead(A2)=786"") and ""master unit"" making tactical and strategical decision instead of micromanaging everything.
So far I have 

tested some ultrasound sensors
managed to read IR TV remote
created clock with lap time and many other functions, while driving display 7segments in Timer ISR (so interrupts does not scare me more :)

for the robot itself I already

created library to easily manage arbitrary number of motor shields over I2C
created two such I2C driven shields 
attached them to motors on two tracks undercarriage (with bateries and regulators etc.etc)
created simple ""master unit"" which commands them over I2C (but it have just stupid demo which runs predefined parrent of ""run/rotate this fast for so long"" in cycle)

so it blindly runs on the floor just now.
Next step is to create unit for ultrasound (which answer to ""how far?"" and screams activelly ""we are going to crash soon"") and let ""master unit"" react on it (no move ahead if anything sooner than 5cm).
Now I am hacking gamepad to be ""temporaly"" input device to make easier testing any new function (so many buttons and 2 joysticks inside so it is convenient device to trigger anything) and demonstrate, how far I got. Also to play with it for just fun. The gamepad will have ""arduino"" inside and will communicate over I2C too (over long wire - I tested that it works reliably already).

Now the question:
With sensors and motors on the board (and some bupers too soon and more sensors) on one hand
and so much buttons and analog joysticks in the gamepad on the other hand
what arrangement is more likely to help me continue on the way?

gamepad as pure universal input device - send 17 bits for buttons pressed (packed to 3 bytes send on button change only) and 4 signed bytes for joysticks (on regular interval if joysticks are moving) (both also possible to ask for) - and master unit on board resolving it to any logic

gamepad would not change and would serve like any other sensor
master unit is changing constantly anyway, so lets decode what button triggers here (like testing functions, rotating left or what) 
makes the master code lot longer and gamepad HW dependent
gamepad spams a lot with joystick moves (even if not used)

gamepad as inteligent device sending ""commands"" and reporting ""states""

means to assing all the keys some sense and reprogram that and master any time I want to test new functionality
master simpler, cleaner design
gamepad spams less with joystick moves and unassigned buttons change

gamepad as ovelord, managing its keys and all, asking master for robot state and commanding master what to do, while master would command all units on robot platform

maybe a way to easy switch to some computer (Rapsberry/laptop) on board, running some OS like linux and using big storage for maps/ high level strategy and so
lot of duplicity in master as many commands would be just resend without much modification/added value
more communication over I2C due the duplicity

gamepad as master, no master on board


the robot would not be able to even move without gamepad (at least laying on the board)
master (gamepad) responible to many inputs itself (buttons, joysticks)
lot less communication over I2C, as everything is solved inside the gamepad
less processors, the master code could be later moved to computer on board and leave out the gamepad totally



(uhm lol, took me 5 hours to write all of this and a lot became clearer to me, when I was trying to express it in letters :)
","mobile-robot, control"
Constrained Second Angle Double Pendulum,"If I am trying to model the dynamics of a double-pendulum (on a horizontal plane without the effects of gravity), in which the second angle is constrained to range between values of [-10 deg, 10 deg], how would I derive the equations of motion? I'm having trouble identifying whether I would use some method involving solving the Lagrangian with holonomic or non-holonomic constraints. 
",dynamics
ROS: Best practices?,"I'm going to build a small robot system, and it seems like that ROS serves a nice framework to control and program the system.
However, I am wondering which is the best practice to manage the components of my robot.

Does it make sense to put all the sensors in one node?
Should I only put the sensors of the same type in one node or is it better to have one node for one sensor? 
Is it a good practice to have some kind of handler node, which takes input from sensors and steers the corresponding actuators or should the actuator nodes and sensor nodes communicate directly?



Fused sensor nodes and actuator nodes with handler

Single sensor and actuator nodes with handler

Direct communication


For me, I guess the best is to have some kind of handler, which handles the communication between sensors and actuators and have one node for each element of the robot (like in figure 2), because the system is in this way loosely coupled and can be extended easily, but I want to know what your opinion is.
","control, ros"
Landmark extraction algorithm,"The landmarks are often used in SLAM. What are the algorithms used to extract them, and how can a robot differentiate the landmarks, if they detect one in point A at Xt and another in Xt+1? How can the robot know if it's the same landmark or not?
","slam, ekf, lidar, ransac"
Odometry vs Dead-reckoning,"In terms of robotics, what are the differences between odometry and dead-reckoning? 
I read that odometry uses wheel sensors to estimate position, and dead-reckoning also uses wheel sensors, but ""heading sensors"" as well. Can someone please elaborate on this point for me?
","odometry, deduced-reckoning"
How to control 6v micro solenoid from arduino,"I want to control this solenoid from Arduino, but I am confused which transistors, resistors and diodes to choose. I have seen a lot of tutorials about controlling solenoid from Arduino, but all of them are for 12 volt solenoid or are using some relays which I don't want to use. I will be using 6 of these solenoids for my project.  
","control, motor"
Gazebo: moving joint with model plugin,"This is my first week with Gazebo.  The tutorials are clear (except for my dearth of C++ knowledge) but now that I'm working to move out on my own things are getting cloudy.  I made a model comprising two boxes and a revolute joint.  The file one_r_test.world loads this model.  A plugin is ""loaded"" (?) in model.sdf and that plugin, ModelControl, comes from model_push.cc in the ""Model plugins"" tutorial (http://gazebosim.org/tutorials?tut=plugins_model&cat=write_plugin), which uses SetLinearVel to move a box.  I can get this same behavior out of model_control.cc if I just copy the tutorial code (and change the plugin name as appropriate), but that's not what I want.  I'm seeking to eventually simulate joint control of robotic manipulators and what's not working in this basic simulation is my attempt to move the model joint via the ModelControl plugin.  It moves in the GUI if I set the velocity (or torque) that way.  The model_control.cc code is pasted below in hopes that you can identify a problem.
model_control.cc

#include ""boost/bind.hpp""
#include ""gazebo/gazebo.hh""
#include ""gazebo/physics/physics.hh""
#include ""gazebo/common/common.hh""
#include ""stdio.h""
// In the real file these quotes are greater-than and less-than but I
// don't know how to get that to show up in my question

namespace gazebo
{
  class ModelControl : public ModelPlugin
  {
  public: void Load(physics::ModelPtr _parent, sdf::ElementPtr /*_sdf*/)
    {
      // Store the pointer to the model
      this->model = _parent;

      // Store the pointers to the joints
      this->jointR1_ = this->model->GetJoint(""r1"");

      // Listen to the update event. This event is broadcast every
      // simulation iteration.
      this->updateConnection = event::Events::ConnectWorldUpdateBegin(boost::bind(&ModelControl;::OnUpdate, this, _1));
    }

    // Called by the world update start event
  public: void OnUpdate(const common::UpdateInfo & /*_info*/)
    {
      // Apply a small linear velocity to the model.
      //this->model->SetLinearVel(math::Vector3(.03, 0, 0));

      // Apply angular velocity to joint
      this->jointR1_->SetParam(""vel"", 0, 99);
      this->jointR1_->SetParam(""max_force"", 0, 9999999999);

      double currAngle = this->jointR1_->GetAngle(0).Radian();
      printf(""Current angle is \t %f\n"", currAngle);
    }

    // Maybe I want to keep track of time?
    common::Time last_update_time_;

    // Pointer to the model
  private: physics::ModelPtr model;

    // Pointer to the update event connection
  private: event::ConnectionPtr updateConnection;

    // Pointers to joints
    physics::JointPtr jointR1_;
  };

  // Register this plugin with the simulator
  GZ_REGISTER_MODEL_PLUGIN(ModelControl)
}

edit: If I change
this->jointR1_->SetParam(""vel"", 0, 99);
to
this->jointR1_->SetVelocity(0, 99);
then the joint moves (yes, very, very quickly).  What's wrong with SetParam vs SetVelocity?
","simulator, joint, gazebo"
Is the working principle of all servo motors the same?,"If I can control a small hobby servo motor then can I control high torque servo motor in same way? I want to make a robot which needs a high torque and speed motors. But first before buying some industrial servo motors I want to first test the kinematics of my robot with small cheap servo motors. I will be using ros-orocos toolchain to control the robot and to make an efficient motion planning algorithm. Why I can't go for expensive servo motors right now is that first I want to test the working of the robot arm, whether it moves as expected or not. Although I think it is possible, I want to be sure.
","ros, servomotor"
Are there any comprehensive video courses or lecture notes on Industrial robotics?,"I am really new to the topic. There doesn't seem to be lot of overlap between Industrial robotics and Hobby robotics (atleast in certain areas like control etc).Please correct me if i am wrong. I actually tried going through Fanuc website, and most of the content is restricted. I would like to know if there is any course on how to operate Industrial robots? its PLC programming? or any application specific course etc?
","robotic-arm, industrial-robot"
What is killing the robots exploring Fukushima?,"This article discusses how the robots sent to explore the Fukushima reactors have been damaged by radiation that exceeds 650 sieverts per hour:
http://www.theverge.com/2017/2/17/14652274/fukushima-nuclear-robot-power-plant-radiation-decomission-tepco
This is the seventh such robot that has died probing the reactor.  I assume the robots are well-shielded....
What exactly is damaging the robots (alpha, beta, gamma particles) and how? Is the damage permanent? 
",sensors
Is it possible to use an arduino in hard real time,"I am using the ros_arduino_bridge to control a robot, by connecting the Arduino to a main pc with a usb cable. I was thinking of using the Arduino with main pc with a serial cable and then doing real time control of the Arduino by using the real time clock. 
Is it possible to communicate with the Arduino in hard real time by using the real time clock and serial connections? I want to use Arduino board as a bridge between the main computer and the sensor and motors. And i want to control those sensors and motors in real time. All the high level processing tasks like computer vision and motion planning will be running in the main computer which then sends the commands to motors from the Arduino. So it is just acting like a bridge. 
The reason I want hard real time is so that my robot can control its joints at very high and accurate speed so that the robot can do human level tasks like running, jumping, assembling some parts, balancing its body while moving(walking, running, jumping) at any speed (which requires the joints to be controlled at very high speed and accuracy) etc. I will be using gazebo simulator to test most of the tasks.
","arduino, ros, real-time"
How to create a Industrial Robotic arm controller?,"Can anyone explain me in detail, what a industrial robotic arm controller does? What are its components? Does the industry use the opensource controllers like Arduino? I saw most of industrial controllers look very big whereas the hobby robot arms are having small controllers mostly made from arduino? Also, if one were to reconstruct it what are the topics i need to learn? 
","control, robotic-arm"
Recreating Roomba behavior in a spherical robot,"Disclaimer: I am a beginner, both to this forum and to robotics.
I work in IT, and the guys in my department have decided we would like to build a robot for the office, as a sort of hobby/team-building exercise of sorts.
Our goal is to create a robot that is spherical, like BB8, but randomly navigates the room like a Roomba. I've seen examples of BB8-like robots online before, but all the ones I have found have used a remote to manually control their movements. This seems like a difficult first project, and personally I would like to try something more basic to begin with, but I figured I might as well look at feasibility of the project before I rain on their parade.
The way I see it, there's two possible ways we could go about this:
1. Use an iRobot Create 2 and somehow adapt it to a spherical body
2. Start from scratch on a BB8 robot, and write a program that mimics a Roomba's behavior. (I have seen several examples of this online using Arduino and Raspberry Pi)
My question is: how difficult is it to write a program mimicing the Roomba's behavior? If it is very difficult, then perhaps I should simply buy a Create 2 and go from there.
Sorry if this is a broad question. If it is not appropriate for this forum, please direct me to a more suitable forum for beginners in robotics who have stupid questions like mine :p
","arduino, raspberry-pi, irobot-create, roomba"
What are the properties of projective homography matrix?,"I am trying to calculate the homography matrix for a set of corresponding points of a planar object, knowing that it has a rectangular shape with known dimensions.
I am using the DLT algorithm for this purpose, and I am getting the homography matrix, H, that can be used to warp the image. 
My questions are:

What are the properties that this matrix has (e.g. symmetric, det(H)=1, etc.)? 
What is the role of the camera calibration and undistorting the image before calculating H?
what are the conditions that must be satisfied on the input points and reference points? Do they have to be in the image plane (given in pixels) or the world coordinate (in unit distance) and shall I assume the center of the frame to be (0,0) or (0.5*width,0.5*height)?

An illustrative example will be very much appreciated. Also tips to verify my calculation will be also great.
","computer-vision, robotics-library"
Send numerical values from Matlab to arduino,"I have an array (2x4) in Matlab which may contains integer values as well as values in decimals. For example: [1.1, 23, 1.56, 5.29; 2.14, 2.39, 67, 4.001]. I have to send these values to arduino using matlab. How to do so? I know how to send integer values to arduino from matlab but it is not working with decimal values.
Matlab Code to send integer values is below:
portName = 'COM5';
s = serial(portName,'BaudRate',9600,'Terminator','LF');
s.timeout = 1;
try 
    try
        fopen(s);
    catch
        delete(s);
        fopen(s);
    end
catch
    disp('Unable to open the port ');
end

angle = [1.3,2];
    dataOut = angle;
    dataOut_ = char(dataOut);
    fprintf(s,'%d',dataOut_);

Arduino code is given below:
int d1,d2;
char d[4];

    void setup() {
  // put your setup code here, to run once:
Serial.begin(9600);
pinMode(13, OUTPUT);
}

void loop() {
  // put your main code here, to run repeatedly:
if(Serial.available()>0)
{
  for(int i=0; i<3;i++)
  {
    d[i]= Serial.read();
  }
  d1 = d[0]-'0';
  if (d1 == 1.3)
  {
    digitalWrite(13, HIGH);   // turn the LED on (HIGH is the voltage level)
  delay(2000);              // wait for a second
  digitalWrite(13, LOW);    // turn the LED off by making the voltage LOW
  delay(1000);              // wait for a second
  }
}
}

","arduino, matlab"
Navigation Potential fields algorithm,"I am working with a raspberry pi which has some positional sensors and I manage it from my mobile with an app I am developing. Now I am trying to understand how to implement this algorithm to code, but I don't really know how to start, so I would be really great if someone can help me with some starting code or similar because I cant find this algorithm implementation.
",mobile-robot
Difference between form closure and force closure,"What exactly is the difference between both the above terms? From some of the papers I realize that force closure depends on the frictional forces. Is it correct?
Suppose I  to grasp a cylindrical object with my hand, which closure will it be considered?
","robotic-arm, design"
Interfacing high-resolution image sensors with ARM Board,"I'm working on a project requiring HD (Stereo) Video Processing. Most of High Resolution (5MP+) Sensors use MIPI-CSI interface. 
I managed to get a board with an Exynos5 SoC. The SoC Itself has 2 MIPI-CSI2 interfaces, the problem is that the pins to those interfaces are not exposed and It's (almost) impossible to reach them. So I decided to use the USB3.0 Buses.
The problem is when I get to Significant bandwidth (~5.36 Gibibits/s per sensor), I don't think USB3.0 will work out. Bandwidth = Colordepth*ColorChannels*PixelCount*FPS but this could be solved with a Compressed stream (via a coprocessor)
I was thinking that Cypress' CYUSB306X chip was a good candidate for the job, but one of the problems is that I can't do BGA Soldering by hand nor have been able to find a BGA Soldering Service in Switzerland.
Any Ideas on other interfaces I could implement or other coprocessors with MIPI-CSI2 Interface?
Just a final remark, space and weight are important as this is supposed to be mounted on a drone.
","cameras, usb, stereo-vision"
What does ardupilot do and how does it do it?,"I have been using ardupilot on drones for a while and I don't exactly know what it does. I know it keeps a drone leveled, lets us set way points and automatically fly through them, etc. Is that it? If so why is the pixhawk so expensive? don't you just need a cheap imu and gps with a $5 pi zero. I might be mixing up pixhawk's hardware and ardupilot. But yeah what do they do individually? and how do they do it? It it just hard coded to add more thrust to a few motors if its tilted, just use gps to go to a location, etc or is there more to it.
","imu, ardupilot"
Google's 'Tango' - How it works and what's special about the hardware?,"I'm looking for a good breakdown and explanation of Google's 'Tango' AR platform.  Specifically how the hardware works together to generate depth maps and the SDK's use of it.
I know the hardware composes of a fisheye lens camera and an RGB-I camera.  I am only familiar with stereo vision with identical cameras and disparity maps, I am thinking the different lenses and camera elements make it easier to distinguish variations in the environment but must have some very special (and proprietary) algorithms.  However, there must actually some special hardware and dedicated chipsets for processing the depth map to take the burden off the CPU/GPU?
Also, for the AR software implementation, I assume the SDK has some GPU utilization built into it like OpenCL or CUDA (but specific for the Adreno GPU).  Does it simply use OpenCL (this is supported by the Adreno GPU) or does it have something proprietary from Google similar to CUDA for the nVidia chipsets?
Basis for the question - I work with OpenCV some and am experimenting with stereo vision applications, but would like to move on to developing apps for specialized hardware and this sounds like the right (maybe only?) platform.
","computer-vision, stereo-vision, opencv"
ROS Turtlebot Navigation Stack,"Is there any way to just publish Path messages to a topic on a turtle bot to get it to execute the path or do I have to write my own node to read the path and publish twist messages based on it?
","ros, navigation"
How to decide the battery power for my robot,"I need my motor to be powered with 12V, 5A for 1 hour continuously. How can i decide the Ah rate of the battery. Please suggest some lithium ion battery for the specification
","mobile-robot, motor, power, battery, lithium-polymer"
Fastest maze algorithm for robot,"I'm planning on programming a prebuilt robot to solve a maze as fast as possible.  The robot has forward obstacle sensors (no side sensors) and 3-axis accelerometer.  I'm planning on using the wall following algorithm.  Is this the fastest possible algorithm?  Also, since there are no side sensors, the robot needs to continuously turn to check if there is a wall on its side, so is there a clever way to use the accelerometer and sensors?
",mobile-robot
using range-only sensors for mapping in SLAM,"SLAM noob here but trying to implement an algorithm that fuses odometry data and mapping based on wifi signal strengths for a 2D robot.
1)
After various readings of different resources,
I came across this - http://www.qucosa.de/fileadmin/data/qucosa/documents/8644/Dissertation_Niko_Suenderhauf.pdf
that explained what sensors are used in mapping and how they are categorized.
There are range-bearing sensors (stereo cameras,RGB-d cameras) that provide both distance and angle (range and bearing), from which is easy to locate (x,y) coordinates of landmarks ---> I can develop a map.
But in case I'm using wifi signal strengths (Received signal strengths) etc, in which case it is range-only (meaning, I can only establish from a robot pose(x,y,theta) as to how far this signal is coming from), how am I developing a map at all?
My question is similar to this - What algorithm can I use for constructing a map of an explored area using a number of ultrasound sensors? but not quite same.
Even if I were using IMU/GPS, how am I using GPS to develop a map? What is my state space there? If I am getting GPS signals / wifi signals/ radio signals, am I estimating the transmitter/AP's location as the map? or the walls of a room I'm navigating in, as a map?
A lot of SLAM literature talks about motion model and measurement model, the former gives me the pose of the robot quite easily because of the odometry and imu. 
The latter though is more for development of a map. Am I right in understanding this? If yes, say 
a] I have walls in a room and I'm using Lidar scanner - 
this still gives me the location of the wall using the number of beams that give me bearing, and the average distance from all the beams.
b] Or if I have just a single laser scanner, I can still use a camera (distance) and the heading of the robot to calculate the location of wall (the map). https://shaneormonde.wordpress.com/2014/01/25/webcam-laser-rangefinder/#more-403
But If I have wireless signal strengths, I have a distance (distance of the transmitter from which I'm getting the RSS, not the distance of the wall) as to where they are coming from. But how am I estimating the location of walls here?
2) What does the term ""correspondences"" mean in SLAM literature? 
","localization, slam, artificial-intelligence, mapping, wireless"
Using IR LED and photodiode to estimate range and bearing of other Swarm Robots,"I'm trying to make swarm robots that use 8 IR LEDs and 8 Photodiodes arranged alternately along the circumference of the circular body to determine the range and bearing of other nearby swarm robots (similar ro Rice University's r-one)
Each IR LED and Photodiode is wired as shown below:

The IR LED's and Photo-diodes on one robot are separated by some opaque object. The intention is that when a high analog value is read from certain Photo-diode(s) of the 8 present, another robot's relative range and bearing can be estimated.
The problem is that a high analog value is read even when the robot is near an obstacle because of the reflect infrared light from it's own LED's.
Is there any way for a robot to determine if a high analog value read is because of another robot or because of an obstacle?
Thanks in advance!
","arduino, localization, swarm"
Computation of Coriolis Matrix,"I am controlling a 6-DOF robot. For this, I want to compute the Coriolis Matrix. From my study of examples, I understand there are several ways of going about it. At the moment, I am using the theory based on, ""A Lie group formulation of Robot dynamics"" [p. 615] by Park et al. But this is not computationally efficient as my simulations are very slow.
Based on my study of several projects on Github, I understand many people choose symbolic approach. I was also considering numerical differentiation to get the Christoffel symbols.
I would like seek some guidance regarding the pros and cons of the different methods. I would really appreciate any references that I can use to study.
","control, dynamics"
Roslaunch Gazebo Turtlebot Simulation Not Working,"I have installed and am trying to run a turtlebot package using gazebo and roslaunch.
Installation seems to have gone fine and I am now following the first tutorial, which just explains how to get the simulation started. The tutorial can be found here:
http://wiki.ros.org/turtlebot_gazebo/Tutorials/indigo/Gazebo%20Bringup%20Guide
I entered the command: source /opt/ros/indigo/setup.bash
That seemed to go fine, there were no errors.
Then I entered the command: roslaunch turtlebot_gazebo turtlebot_world.launch, which resulted in the following error log:
    while processing /opt/ros/indigo/share/turtlebot_gazebo/launch/includes/kobuki.launch.xml:
    Invalid  tag: Cannot load command parameter [robot_description]: command [/opt/ros/indigo/share/xacro/xacro.py '/opt/ros/indigo/share/turtlebot_description/robots/kobuki_hexagons_asus_xtion_pro.urdf.xacro'] returned with code [1]. 

    Param xml is (param command=""$(arg urdf_file)"" name=""robot_description""/)
    The traceback for the exception was written to the log file
I tried asking on the ROS Answers website first but got no answer, so I'm hoping the good people of Stack Exchange can help me figure out what is causing this problem.
Additional information: The version of ROS I have installed is Indigo, and I'm on Ubuntu 14.04
","ros, gazebo"
Is it my PID that is unstable or my physical system? (Quadcopter),"I've been working on this Arduino-MPU6050 quadcopter for a while now, and it looks like it's close to being finished. I have programmed it in rate mode, so the PID's control the rotational velocity. Once I get those perfected I will write an outer set of positional PID's to control the rate ones.
But anyway, I'm still having issues getting the drone perfectly stable, and it drifts around more than it should. Below you can see a screenshot of a program I wrote, which shows the angular velocity in blue and the angular position in pink:

As you can see, the quadcopter is wobbling around pretty much randomly, and since there aren't any steady oscillations I'm guessing my PID is okay and that the instability is from something physical like vibration. Is this a reasonable assumption?
I am looking for any suggestions/possible explanations for this instability, and guidance on what I should do next. Invest in ant-vibration foam? Revise my PID?
I should also mention that I have not flown the quadcopter. I have it suspended using ropes attached to the legs of an upside down chair so that I can test one axis at a time.
General List of thing's I've Tried:

Modified the Arduino Servo.h to update the ESC's more frequently.
Changed the precision of my gyro from default 1/16.4 deg to 1/65.5 deg.
Balanced the props with bits of electrical tape.
Adjusted PID gains.
PID sample rate set to 3ms (333hz).

EDIT
Here is a snippet of my PID:
if (millis() - updateTimerPID >= sampleMillisPID) {
    if (thrust <= 1200) { // Don't turn on PID until sufficient throttle is reached.
      NWPower = thrust; //
      NEPower = thrust; // If PID not activated, set motors to base-throttle.
      SWPower = thrust; //
      SEPower = thrust; //

      inAutoRoll = false;  // Roll-Axis PID not activated.
      inAutoPitch = false; // Pitch-Axis PID not activated.
      inAutoYaw = false;   // Yaw-Axis PID not activated.

      I_rollRate = 0;   // Reset roll rate integral term. 
      I_pitchRate = 0;  // Reset pitch rate integral term.
      I_yawRate = 0;    // Reset yaw rate integral term.

    } else { //PID is active - adjust Roll/Pitch/Yaw.

      adjustRollRate();
  //  adjustPitchRate();
  //  adjustYawRate();
    }
    updateTimerPID = millis(); // Reset PID timer.
  } 

void adjustRollRate () {
  float offset = requestedRollRate - rollRate; //How far off from the wanted roll angular velocity.

  I_rollRate += KrI * offset; // Adjust roll rate integral term.

  if(!inAutoRoll) { // Did the PID just turn on?
    lastRollRateOffset = offset; // The previous offset is set to the current one (D-term is now zero for this instance).
    inAutoRoll = true; // The PID is now on.
  }

  float adjust = (KrP * offset) + I_rollRate + (KrD * (offset -lastRollRateOffset)); // Motor power adjust value.

  NWPower += adjust; //
  NEPower -= adjust; // Adjust the motor powers.
  SWPower += adjust; //
  SEPower -= adjust; //

  if (NWPower > maxOutPID)NWPower = maxOutPID;
  else if (NWPower < minOutPID)NWPower = minOutPID;

  if (NEPower > maxOutPID)NEPower = maxOutPID;
  else if (NEPower < minOutPID)NEPower = minOutPID;

  if (SWPower > maxOutPID)SWPower = maxOutPID;
  else if (SWPower < minOutPID)SWPower = minOutPID;

  if (SEPower > maxOutPID)SEPower = maxOutPID;
  else if (SEPower < minOutPID)SEPower = minOutPID;


  lastRollRateOffset = offset;    // Remember the offset for next time.
}

","quadcopter, arduino, pid, imu"
Regarding Pose Graph Slam using Lidar,"So I'm trying to incorporate pose graph optimization into a mapping framework using a Lidar. So I basically have all the relative transformations between the pointclouds and I have pairs of pointclouds which satisfy my place recognition algorithm so I know which poses to complete the loop with, now the question I have is given that I only have these relative transformations (1) how do I calculate the error

where $\hat{z}$ is the ground truth since I only have one set of measurements which are my R,t estimate from consecutive pointclouds?
(2) How do I loop close using g2o?
(3) What will my information matrix be isn't it supposed to be a property of the sensor itself?
Thank you.
","slam, mapping, lidar"
Depth accuracy of the stereo camera,"I am doing a project on calibrating stereo ZED camera and  finding its accuracy and compare with the Manufacturer's accuracy of 1% at 1m depth accuracy.
For this purpose , the formula to calculate the depth accuracy  is
$dz = (z^2 * de) / (f * b)$
but how do we calculate $z$ , $de$ and $f$. Is is taken from matlab stereo-callibration app which gives 'Stereoparameter' ?
$dz$ is the depth error in meters, $z$ is the depth in meters, $de$ is the disparity error in pixels, $f$ is the focal length of the camera in pixels and $b$ is the camera baseline in meters.
","computer-vision, stereo-vision"
Modified DH Parameter for Puma 560,"I'm trying to apply modified DH parameters (from Craig's version) to Puma 560.
As per modified DH says,

And the robot Puma 560 with axes and frame are,

As per above sign convention, the sign of d2 and d3 should be negative. However, for the correct result, it seems that the sign of d2 should be positive.
My question is, should the sign here be positive and if yes, then doesn't it contradict the sign convention for above mentioned modified DH convention?
",dh-parameters
Pose-graph-slam:using only a camera,"I'd like to know how to form my pose graph if the only information I have available is that from a camera,
(1) What are my poses? Are they just the accumulated transformations from pairwise matching?
(2) And then what are my edges?
(3) If I have already detected the loops, how would I loop correct?
Thank you. 
","slam, mapping, visual-odometry"
Confused about time systems in ephemeris computation (from GPS subframes),"I am using the gnss-sdr library to compute ephemeris from the GPS message, and to try to make sense of things, I am reading the well-known IS-GPS-200E specification. To compute ephemeris, the time from ephemeris reference ephoch is defined as (page 101, table 20-IV)
$$t_k=t-t_{oe}.$$
I am unsure about how $t$ is defined, and I find the specification unclear on that. Rummaging in the source code of the aforementioned library, I found out that $t$ seems to be computed as follows:
$t=t_{x}-b$ 
where $b$ is the satellite clock bias, and 
$t_x=R_x- (\text{pseudo-range})/c$
where $c$ is the speed of light. However, I have been unable, so far, to find out what $R_x$ exactly refers to. It seems to correspond to ""time of week at current symbol"", but there is no documentation/precison on that. I suppose that an expert could very simply deduce what $R_x$ is just from the formulas, though. 
So my question is: what is $R_x$? What time system is it expressed in (satellite time? gps time? receiver time?). And if someone could explain to me what those formula are doing or give me pointers, I'd be extremely grateful.
","sensors, gps, gnss, pseudo-ranges, ephemeris"
How do monocular visual odometry algorithms work?,"What is the core principle of a monocular visual odometry algorithm? I mean, after calibrating a single camera (undistortion etc.) images are fed into an algorithm - what exactly does this algorithm do with the images in order to get the translation/rotation between successive frames?
Do various mono algorithms use various techniques or is the core principle same everywhere? I see some libraries use image features (indirect approach) and some use pixel intensity (direct approach) but I am not really able to understand the principles from the papers... I can only see the algorithms use various methods of estimating the translation/rotation matrix (5-point, 8-point algorithms...).
Also, is it true that no mono algorithm is able to get the absolute scale of the scene? How does the relative scale work - is it set randomly?
I found following mono odometry libraries:

indirect methods (using image features)

Avi Singh via OpenCV - uses Nister’s 5-point algorithm
VISO2 - uses 8-point algorithm (paper)
ORB_SLAM / ORB_SLAM2 - indirect approach?

direct approach (using whole edges etc.)

SVO: Fast Semi-Direct Monocular Visual Odometry (paper)
LSD-SLAM: Large-Scale Direct Monocular SLAM (paper) - needs ROS (but only for input/output)
DSO: Direct Sparse Odometry (code)


I understand how stereo visual odometry works - they reconstruct 3D scene in each image frame and then compare (register) the point clouds of successive image frames and get directly the distance traveled like this - pretty simple principle.
","odometry, visual-odometry, monocular"
What mechanical parts can be attached to DYJ48 stepper motor?,"Sorry I am asking a mechanical question here, but, after all, where else people have experience with using motors? If there is a better forum for this, please do guide me.
Everywhere I've seen online, the stepper motor DYJ48 is used in tutorials, to rotate on its own, or, at most, to spin a clothes pin attached to it. I am trying to get Arduino to work for my 10 year old kid. He's got the motor rotating, now what? How does he attach anything to it?
Don't laugh, I made him a wheel out of a raw potato. He is happy with it now. Where can I find any guidance as to what to do next?
","arduino, motor"
How can one determine whether a LiPo battery is going bad?,"In our lab we use LiPo batteries to power our quadrotors. Lately we have been experiencing stability issues when using certain batteries. The batteries seem to charge and balance normally and our battery monitor indicates they are fine even when putting them under load. However when we attempt to fly the quadrotor with one of these batteries, manually or autonomously, it has a severe tendency to pitch and/or roll. My guess is that the battery is not supplying sufficient power to all the motors which brings me to my question. Is this behavior indicative of a LiPo going bad? If so what is the best way to test a battery to confirm my suspicions?
","battery, troubleshooting"
What's the name of a robot manipulator construction with multiple thin arms joined together?,"I am seeing on some videos robots picking items and putting them somewhere in order.
Here are some examples:

https://www.youtube.com/watch?v=wg8YYuLLoM0.
https://youtu.be/ggFdvUlp8YU?t=38

How are these types of manipulators called and where can I find schematic illustration explaining principles of its work?
","robotic-arm, mechanism"
Tuning Line follower PID constants with Q-learning,"I am currently working on a line follower buggy and have managed to tune the PID constants​ manually. The buggy follows the line at a moderate speed.
I will now like to take things further and learn new things as well. I read about Q-learning and will like to ask if what I am about to implement is on the right track.
I have chosen:

Three states: last three positions of line sensors
Three rewards: middle position, end of track and less wobbling (measured with gyroscope).
Four actions: $Kp$, $Ki$, $Kd$, and Max speed.

The computation will be made on a PC as the robot is wirelessly connected.

Am I on the right track?
How do I make the 3 constants have ""states"" because as I understand, the actions have to be non-analog ?


Do I create a range of numbers close to the constants I have now and the Q-learning decides which is best ? (It's inefficient to just try random numbers)


","pid, wheeled-robot, artificial-intelligence, line-following, reinforcement-learning"
Robot Graphical Representation in Real Time,"I'm working with a robot intended to be placed in a tele-echography environment. To control the robot I'm using a 6D space mouse that control each degree of freedom of the robot. However, since the rotation is made in the end effector, the end user would have difficulties in understanding where to move the mouse in order to do the desired motion, since the end effector's reference is constantly changing.
So, I'm thinking of doing a graphical representation of the motion of the robot in real time while the user controls the robot. The robot comes andith many API's to control it and to get sensor data. I'm currently using Qt Creator (C/C++) in order to send the mouse's commands to the robot, so I would like to integrate in my program some kind of simulator. What do you recommend as a C++ package / program in order to accomplish this?
Thanks
","control, robotic-arm, c++, manipulator, c"
Is it possible that a quadcopter hold height for long time stable only using IMU (without GPS) and barometer?,"As we know, we can calculate altitude from barometer readings, and a UAV can hold height by referring to these data. However the real air pressure is varying with many conditions. These variance will cause the UAV's height unstable. Is it possible to avoid height drifting due to pressure changes using sensor fusion without GPS?
","quadcopter, sensors, kalman-filter"
Get the arm joint angles with kinect 1,"I am Carlos Barreiro and I am studying a Robotics Master. Now I am working with my Thesis. The project consists in the teleoperation of the a robot with Kinect (model 1). More specifically, I am working with the humanoid robot Pepper which is developed by Aldebaran (Softbank).  
For the skeleton tracking in real time I am using the Kinect for Windows SDK (v1.8).  Because with the kinect 1 I can’t obtain the Skeleton tracking in Linux :-(. It is not a problem, I think that I can continue with Windows for a few time.  
For this project I use Python because I have more experience with this language and it is easier communicate with Naoqi (the robot middleware). For the communication with Kinect I am using the library PyKinect, it is a wrapper from the C++ Windows SDK.  
My problem is calculating the rotation angle for the arm actuators. The robot needs translate the positions of the points of the skeleton to an angle for each motor (Pitch Roll or Yaw) like the bellow picture.  So I need to get the shoulder pitch, should roll, elbow roll and elbow Yaw.  

The skeleton that I got from Kinet gives me the 3d point and the quaternions of each joint  (shoulder, elbow, …)
I am trying different ways but the result could be improved a lot of.

CASE A:

I am using the joint 3D position for calculate the angle between two points.
For example :
The shoulder robotics has two actuators: the shoulder Roll and the Shoulder Pitch.  
In this function takes I pass two arguments, in this case the 3D position point of the shoulder and the elbow and then I calculate the angle between each axis.
Code:
def angulosXplano (puntoA, puntoB):

def calcularAngulo(uno, dos):
    # Compute the angle
    rads = math.atan2(-dos,uno)
    rads %= 2*math.pi
    degs = math.degrees(rads)
    return degs

dx = puntoB.x - puntoA.x
dy = puntoB.y - puntoA.y
dz = puntoB.z - puntoA.z

yaw  = calcularAngulo(dx, dy)
roll = calcularAngulo(dy, dz)
pitch = calcularAngulo(dx, dz)

For calculating the shoulderPitch angle of the robot  I use the roll angle (that I got from the fuction angulosXplano) and for the shoulderRoll I use the  pitch angle.  
The reason of calculating the angles in this way is because I get better results than the results obtained if I calculate the shoulderPitch with the pitch angle and the shoulderRoll with the roll angle.  
The angular movement for the shoulder roll is good, but the should pitch is moderate and the roll elbow movement is the worst because the shoulder movement affects to the elbow movement.

CASE B:

Also I try to get the shoulder pitch and roll and the elbow yaw and roll with the quaternions of the SDK of the Microsoft.  
In this case I tried the quaternions for the elbow, I obtain the quaternion like:
data.calculate_bone_orientations()[JointId.WristRight].hierarchical_rotation.rotation_quaternion

For the elbow I use the Wrist-Right because I read that the position of the joint depends on the previous joint.  
After obtain this quaternion, I convert it to Euler angles, the next code it’s a method of a class that I had developed. The class has got the qw, qx, qy and qz quaternions params.
def quaternion2euler(self):
    q = self
    qx2 = q.x * q.x
    qy2 = q.y * q.y
    qz2 = q.z * q.z

    test = q.x*q.y + q.z*q.w

    if (test > 0.499):
        roll    = math.radians(360/math.pi*math.atan2(q.x,q.w))
        pitch = math.pi/2
        yaw     = 0
    elif (test < -0.499):
        roll    = math.radians(-360/math.pi*math.atan2(q.x,q.w))
        pitch = -math.pi/2
        yaw     = 0

    else:
        roll = math.atan2(2 * q.y * q.w - 2 * q.x * q.z, 1 - 2 * qy2 - 2 * qz2)
        pitch = math.asin(2*q.x*q.y+2*q.z*q.w)
        yaw = math.atan2(2*q.x*q.w-2*q.y*q.z,1-2*qx2-2*qz2)

    return [roll, pitch, yaw]

If use the yaw for the robot elbow roll, the results for the elbow roll improve a lot compared to the previous method. But I can’t find the angle for the robot elbow yaw.

FUTURE CASE C:

My next step would be try the case A but with vectors instead of 3D points. For example the vector A (middle-Shoulder and right-Shoulder) and the vector B (right-Shoulder and elbow). But I have not developed anything yet.
Any help that can help me to improve the code,  some bibliography or any better idea would be welcome.
","robotic-arm, kinect, forward-kinematics, humanoid"
Robot System Design Feedback and Advice,"I have been working on a robot for a couple months now and would appreciate feedback on my hardware and software design, specifically with optimizing the motion.
Hardware

Chasis: custom printed 3d quadbot using Autocad for Mac
Motors: 12 Standard Servos. (don't have specs, but cost about 5$ each, bought from hobbyking.com)
Power: Power delivered from power supply and regulated by DROK buck converter
Computer: On board Raspberry Pi 2+
Motor Controller: Adafruits Raspberry Pi servo shield

Software

User input: User input received through websocket between client and onboard Flask server. User can use slider or ps3/xbox controller through the Gamepad API (if connected)
Software design: Loosely designed off of a micro-service architecture. User input is fed to the Hypervisor (hypervisor.py), which receives user input data from the Flask server (app.py), and feeds commands to the MotionController (MotionController.py) which are then executed.
Multithreading: The server needs to send incoming data asynchronously (the server will fail if it has to wait for the hypervisor to take data), so the data is enqueued to global queue which is then read by the hypervisor is which manages a thread which checks for changes to the data queue. It is in this fashion that the server and robot software communicate.

My problem

Walking Function Optimization: Currently, I am storing motion offsets in variables, for example:


    stepHeightMid = 60
    stepHeightLeg = 5
    velocity = 0.002

This works fine but is obviously terrible practice. I am wondering:

What is the best way to go about setting the motion variables?
How do I optimize the motion?

I understand applying a neural net to a simulated robot would work but this seems to be outside the scope of this project.

Next Steps: I plan to implement OpenCV using an XBox kintect to enable gesture control and autonomous motion

I have attached an image of the robot for reference.

Additionally, several python files are included for reference
App.py

#!/usr/bin/env python
from flask import Flask,  render_template,  session,  request,  send_from_directory,  send_file
from flask_socketio import SocketIO,  emit,  join_room,  leave_room,  close_room,  rooms,  disconnect
import time
import json
import datetime
import logging
import platform
import os
import sys
from bColors import bcolors
from RobotSystem.Hypervisor import Hypervisor
from RobotSystem.Services.Utilities.RobotUtils import RobotUtils

async_mode = None
app = Flask(__name__,  static_url_path='/static')
app.config['SECRET_KEY'] = 'secret!'
socketio = SocketIO(app,  async_mode=async_mode)
log = logging.getLogger(""werkzeug"")
log.setLevel(logging.ERROR)

thread = None
connections = 0

@app.route('/',  methods=['GET',  'POST'])
def index():
    return render_template('index.html',  async_mode=socketio.async_mode)

def background_thread():
    while True:
        socketio.sleep(1)

@socketio.on('valueUpdate')
def valueUpdateHandler(message):
    RobotUtils.ColorPrinter(""app.py"",'Value update fired ', 'OKGREEN')
    quadbot.inputData(message)
    data = {}
    data['Recieved'] = True
    return json.dumps(data)


@socketio.on('connect')
def test_connect():
    global connections
    connections+=1
    print_str = ""Client connected. ""+ str(connections)+  "" current connections""
    RobotUtils.ColorPrinter(""app.py"",print_str, 'OKGREEN')

    global thread, quadbotThread
    if thread is None:
        print ""init""
        thread = socketio.start_background_task(target=background_thread)

@socketio.on('disconnect')
def test_disconnect():
    global connections
    connections -= 1
    RobotUtils.ColorPrinter(""app.py"",str( 'Client disconnected. ' +str(connections)+ "" current connections"" ), 'OKGREEN')

if __name__ == '__main__':
    global quadbot
    quadbot = Hypervisor()
    try:

        #quadbot.testSuite(""TURN"")
        socketio.run(app,  debug=True,use_reloader=False)

    except KeyboardInterrupt:
        RobotUtils.ColorPrinter(""app.py"", ""Server shutting down"", 'FAIL')
        quadbot.endHypervisor()
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)



Hypervisor.py

#!/usr/bin/python
from Services import *
import time
import math
import json
import sys
import threading
import os
from Queue import Queue,Empty

class Hypervisor():

    def __init__(self):

        if RobotUtils.LIVE_TESTING:
            self.pwm = PWM()
            self.pwm.setPWMFreq(RobotUtils.FREQUENCY)
        else:
            self.pwm = None


        self.agendaThreadAlive = True
        self.inputQueue = Queue()
        self.agendaThread = threading.Thread(group=None,target=self.updateAgendaLoop,name=""agendaThread"")
        self.agendaThread.start()

        self.data_file_name = RobotUtils.DATA_FILE

        self.front_left = None
        self.front_right = None
        self.back_left = None
        self.back_right = None

        self.TURN_LEFT = RobotUtils.TURN_LEFT
        self.TURN_RIGHT = RobotUtils.TURN_RIGHT
        self.FORWARD = RobotUtils.FORWARD
        self.BACKWARD = RobotUtils.BACKWARD
        self.STOP = RobotUtils.STOP
        self.AUTONOMOUS = RobotUtils.AUTONOMOUS
        self.INVALID_DATA_ERROR = RobotUtils.INVALID_DATA_ERROR

        self.horizVidMotor = Motor(50, RobotUtils.HORIZONTAL_VID_PIN, RobotUtils.HORIZONTAL_VID_MIN_VAL, RobotUtils.HORIZONTAL_VID_MAX_VAL, 0, ""horizontal video motor"", self.pwm)
        self.vertVidMotor = Motor( 50, RobotUtils.VERTICAL_VID_PIN, RobotUtils.VERTICAL_VID_MIN_VAL, RobotUtils.VERTICAL_VID_MAX_VAL, 0, ""vertical video motor"", self.pwm)

        self.setup()

        self.motors = [self.front_left, self.front_right,self.back_left,self.back_right, self.horizVidMotor, self.vertVidMotor ]

        self.MotionController = MotionController(self.TURN_LEFT,  self.TURN_RIGHT, self.FORWARD, self.BACKWARD, self.STOP,self.AUTONOMOUS,self.INVALID_DATA_ERROR,
        self.motors, RobotUtils)

        #self.MotionController.stand()
        RobotUtils.ColorPrinter(self.__class__.__name__, '__init__() finished. Robot Created with id ' +str(id(self)), 'OKBLUE')

    def testSuite(self,operation):

        sleep_time_between_same_motions = 1
        sleep_time_between_different_motions = 5

        if operation == ""TURN"":
            RobotUtils.ColorPrinter(self.__class__.__name__, ""In testSuite(). Testing RIGHT turn command"", 'OKBLUE')
            for i in range(9):
                self.MotionController.turn(1)
                time.sleep(sleep_time_between_same_motions)

            time.sleep(sleep_time_between_different_motions)

            RobotUtils.ColorPrinter(self.__class__.__name__, ""In testSuite(). Testing LEFT turn command"", 'OKBLUE')
            for i in range(9):
                self.MotionController.turn(-1)
                time.sleep(sleep_time_between_motions)

        elif operation == ""FORWARD"":
            RobotUtils.ColorPrinter(self.__class__.__name__, ""In testSuite(). Testing RIGHT turn command"", 'OKBLUE')
            for i in range(9):
                self.MotionController.forward()
                time.sleep(sleep_time_between_same_motions)

        else:
            RobotUtils.ColorPrinter(self.__class__.__name__, ""Invalid test suite input"", 'FAIL')


    # loads json data and creates Leg objects with add_leg()
    def setup(self):

        with open(self.data_file_name) as data_file:
            data = json.load(data_file)
            constants = data[""constants""]
            for i in range(len(data[""legs""])):
                self.add_leg(data[""legs""][i],constants)

    # reads dictuanary values from input, creates a Leg object, and adds it to leg variables
    def add_leg(self,legData,constants):

        leg_name = legData[""name""]

        body_pin                = legData[""motors""][""body""][""pinValue""]
        body_offset             = legData[""motors""][""body""][""offset""]
        body_center             = constants[""bodyCenterValue""] + body_offset
        body_min                = constants[""bodyRange""][""min""]
        body_max                = constants[""bodyRange""][""max""]

        mid_horiz_value         = legData[""motors""][""middle""][""horizValue""]
        middle_pin              = legData[""motors""][""middle""][""pinValue""]
        middle_min              = constants[""middleRange""][""min""]
        middle_max              = constants[""middleRange""][""max""]
        middle_offset_to_center = constants[""midOffsetFromHoriz""]

        leg_horiz_value         = legData[""motors""][""leg""][""horizValue""]
        leg_pin                 = legData[""motors""][""leg""][""pinValue""]
        leg_min                 = constants[""legRange""][""min""]
        leg_max                 = constants[""legRange""][""max""]
        leg_offset_to_center    = constants[""legOffsetFromHoriz""]

        leg = Leg( self.pwm, leg_name, body_pin,    body_min,   body_max,   body_center, mid_horiz_value,   middle_pin, middle_min, middle_max, middle_offset_to_center, leg_horiz_value, leg_pin, leg_min, leg_max, leg_offset_to_center)

        if leg_name == ""FR"":
            self.front_right = leg

        elif leg_name == ""FL"":
            self.front_left = leg

        elif leg_name == ""BL"":
            self.back_left = leg

        elif leg_name == ""BR"":
            self.back_right = leg

        else:
            print ""ERROR: LEG CANNOT BE IDENTIFIED""

    # Called by server when a change in user data is detected
    def inputData(self,data):
        self.inputQueue.put(data)

    # Ends agenda loop thread
    def endHypervisor(self):
        RobotUtils.ColorPrinter(self.__class__.__name__,'Ending Agenda Thread', 'FAIL')
        self.agendaThreadAlive = False

    def updateAgendaLoop(self):
        while True:
            if not self.agendaThreadAlive:
                self.agendaThread._Thread_stop()
            try:
                data = self.inputQueue.get_nowait()
                self.updateAgenda(data)
            except Empty:
                pass

            time.sleep(RobotUtils.AGENDA_UPDATE_SPEED)
        print '\033[94m' + ""Robot: QUEUE READING FINISHED"" + '\033[0m'
        sys.exit()

    # acts as central coordinator for the robot - raeads incoming data + state of the bot and calls methods accordingly
    def updateAgenda(self,data):
        self.MotionController.updateCameras(data)
        nextMove = self.MotionController.NextMove(data)
        if nextMove == self.INVALID_DATA_ERROR:
            print ""Fix this""
        else:
            self.MotionController.MakeMove(nextMove)


MotionController.py

#!/usr/bin/python
# -*- coding: utf-8 -*-
import sys
import time

class MotionController:

    def __init__(
        self,
        TURN_LEFT,
        TURN_RIGHT,
        FORWARD,
        BACKWARD,
        STOP,
        AUTONOMOUS,
        INVALID_DATA_ERROR,
        motors,
        RobotUtils,
        ):
        self.TURN_LEFT = TURN_LEFT
        self.TURN_RIGHT = TURN_RIGHT
        self.FORWARD = FORWARD
        self.BACKWARD = BACKWARD
        self.STOP = STOP
        self.AUTONOMOUS = AUTONOMOUS
        self.INVALID_DATA_ERROR = INVALID_DATA_ERROR

        self.front_left = motors[0]
        self.front_right = motors[1]
        self.back_left = motors[2]
        self.back_right = motors[3]
        self.horizVidMotor = motors[4]
        self.vertVidMotor = motors[5]

        self.RobotUtils = RobotUtils
        self.MIN_MOVEMENT_THRESHOLD = \
            self.RobotUtils.MIN_MOVEMENT_THRESHOLD

    def MakeMove(self, move):
        if move == self.TURN_LEFT:
            self.turn(-1)
        elif move == self.TURN_RIGHT:

            self.turn(1)
        elif move == self.FORWARD:

            self.forward()
        elif move == self.BACKWARD:

            self.backward()
        elif move == self.AUTONOMOUS:

            self.autonomous()
        elif move == self.STOP:

            self.stop()
        else:

            print 'Invalid Command Recieved'

    def updateCameras(self, data):
        horizVidValue = float(data['data']['horizontalVideo'])
        vertVidValue = float(data['data']['verticalVideo'])
        self.horizVidMotor.moveTo(horizVidValue)
        self.vertVidMotor.moveTo(vertVidValue)

    def NextMove(self, data):
        xMovement = float(data['data']['xMovement'])
        yMovement = float(data['data']['yMovement'])
        stop = data['data']['stop']
        autonomous = data['data']['autonomous']

        # Stop has higher precedence than any other command

        if stop:
            return self.STOP

        if autonomous:
            return self.AUTONOMOUS

        #                   | y == forwards
        #                   |
        #                   |
        #  -x == left       |                x == right
        #  
        #                   |
        #                   |
        #                   | -y == backwards

        # magnitude is the intensity of the command, i.e. the distance the value is from 50 (baseline)

        xMagnitude = abs(xMovement - 50)
        yMagnitude = abs(yMovement - 50)

        # filter out value fluctuation by ensuring movment commands are past a certain threshold. Movement commands must be greater than 50 +- threshold to perform a command

        if xMagnitude > self.MIN_MOVEMENT_THRESHOLD or yMagnitude \
            > self.MIN_MOVEMENT_THRESHOLD:

            # command to move in the x axis rank higher in importance than command to move in y axis
            if xMagnitude > yMagnitude:
                # if xMovement is greater than 50 than we move left
                if xMovement = 50:
                    return self.TURN_RIGHT
                else:
                    self.RobotUtils.ColorPrinter(self.__class__.__name__,
                            'Invalid Data Recieved from xMagnitude > yMagnitude branch of NextMove()'
                            , 'FAIL')
                return self.INVALID_DATA_ERROR
            elif yMagnitude > xMagnitude:

            # command to move in the y axis rank higher in importance than command to move in x axis

                # move forward
                if yMovement > 50:
                    return self.FORWARD
                elif yMovement  xMagnitude branch of NextMove()'
                            , 'FAIL')
                    return self.INVALID_DATA_ERROR
        else:
            return self.STOP

    # ColorPrinter( caller, message, color):

    def turn(self, direction):
        if direction == 1:
            self.RobotUtils.ColorPrinter(self.__class__.__name__,
                    'Turning Right', 'OKBLUE')
            turnDegree = 20

        elif direction == -1:
            self.RobotUtils.ColorPrinter(self.__class__.__name__,
                    'Turning Left', 'OKBLUE')
            turnDegree = -20

        else:
            self.RobotUtils.ColorPrinter(self.__class__.__name__,
                    'Invalid input to turn command', 'FAIL')
            sys.exit();

        stepHeightMid = 60
        stepHeightLeg = 5
        velocity = 0.002
        time_delay = 0

        self.front_right.standardPivotStep(turnDegree, stepHeightMid, stepHeightLeg, velocity, time_delay)
        time.sleep(time_delay)

        self.back_left.standardPivotStep(turnDegree, stepHeightMid, stepHeightLeg, velocity, time_delay)
        time.sleep(time_delay)

        self.front_left.standardPivotStep(turnDegree, stepHeightMid, stepHeightLeg,velocity,time_delay)
        time.sleep(time_delay)

        self.back_right.standardPivotStep(turnDegree, stepHeightMid, stepHeightLeg, velocity, time_delay)
        time.sleep(time_delay)
        self.reset()

    def lunge(self, FRB, FRM, FRL, FLB, FLM, FLL, BLB, BLM, BLL, BRB, BRM, BRL):
        splitNum = 10
        for x in range(splitNum):
            self.front_right.body.moveOffset(FRB/splitNum)
            self.front_right.middle.moveOffset(FRM/splitNum)
            self.front_right.leg.moveOffset(FRL/splitNum)

            self.front_left.body.moveOffset(FLB/splitNum)
            self.front_left.middle.moveOffset(FLM/splitNum)
            self.front_left.leg.moveOffset(FLL/splitNum)

            self.back_left.body.moveOffset(BLB/splitNum)
            self.back_left.middle.moveOffset(BLM/splitNum)
            self.back_left.leg.moveOffset(BLL/splitNum)

            self.back_right.body.moveOffset(BRB/splitNum)
            self.back_right.middle.moveOffset(BRM/splitNum)
            self.back_right.leg.moveOffset(BRL/splitNum)



    def backward(self):
        self.RobotUtils.ColorPrinter(self.__class__.__name__, 'Backward'
                , 'OKBLUE')

    def stop(self):
        self.RobotUtils.ColorPrinter(self.__class__.__name__, 'Stop'
                , 'OKBLUE')

    def autonomous(self):
        self.RobotUtils.ColorPrinter(self.__class__.__name__, 'Autonomous'
                , 'OKBLUE')

    def reset(self):
        self.front_left.reset()
        self.front_right.reset()
        self.back_left.reset()
        self.back_right.reset()



    def forward(self):

        self.RobotUtils.ColorPrinter(self.__class__.__name__, 'Forward', 'OKBLUE')

        velocity = .01
        time_delay = .025
        std_piv_step_body_delta = -20
        std_piv_step_middle_delta = 50
        std_piv_step_leg_delta = 5

        self.front_left.standardPivotStep(std_piv_step_body_delta, std_piv_step_middle_delta, std_piv_step_leg_delta,velocity,time_delay*.01)
        time.sleep(time_delay)

        self.back_right.standardPivotStep(-std_piv_step_body_delta, std_piv_step_middle_delta, std_piv_step_leg_delta,velocity,time_delay)
        time.sleep(time_delay)

        self.back_right.standardPivotStep(-std_piv_step_body_delta, std_piv_step_middle_delta, std_piv_step_leg_delta,velocity,time_delay)
        time.sleep(time_delay)

        leg_extend_body_delta   = 35
        leg_extend_middle_delta = -5
        leg_extend_leg_delta    = 28

        self.front_right.legExtend( leg_extend_body_delta, leg_extend_middle_delta, leg_extend_leg_delta, velocity, time_delay)
        time.sleep(time_delay)

        splitNum = 10
        leg_condense_FLbody_delta = 40/splitNum
        leg_condense_BRbody_delta = -20/splitNum
        leg_condense_FRmiddle_delta = 20/splitNum
        leg_condense_FRleg_delta = -28/splitNum
        leg_condense_BLbody_delta = 20/splitNum
        leg_condense_BLmiddle_delta = -10/splitNum
        leg_condense_BLleg_delta = 28/splitNum


        # condense forward right
        for x in range(0, splitNum):
            self.front_left.body.moveOffset(leg_condense_FLbody_delta)
            self.back_right.body.moveOffset(leg_condense_BRbody_delta)
            self.front_right.middle.moveOffset(leg_condense_FRmiddle_delta)
            self.front_right.leg.moveOffset(leg_condense_FRleg_delta)
            self.back_left.body.moveOffset(leg_condense_BLbody_delta)
            self.back_left.middle.moveOffset(leg_condense_BLmiddle_delta)
            self.back_left.leg.moveOffset(leg_condense_BLleg_delta)

        leg_step_BLbody_delta = -30
        leg_step_BLmiddle_delta = 30
        leg_step_BLleg_delta = -28
        time.sleep(time_delay)

        # back left standard pivot step with mid offset""
        self.back_left.standardPivotStepWithMidMovement(leg_step_BLbody_delta, leg_step_BLmiddle_delta, leg_step_BLleg_delta,velocity,time_delay)

        leg_step_FRbody_delta = -40
        leg_step_FRmiddle_delta = 5
        leg_step_FRleg_delta = 28

        # front left standard pivot step with mid movement""
        self.front_left.standardPivotStepWithMidMovement(leg_step_FRbody_delta, leg_step_FRmiddle_delta, leg_step_FRleg_delta, velocity,time_delay)
        time.sleep(time_delay)

        frontRightBodySplitDiff = self.front_right.body.center_value - self.front_right.body.value
        frontRightMiddleSplitDiff =self.front_right.middle.value - self.front_right.middle.center_value
        frontRightLegSplitDiff = self.front_right.leg.value - self.front_right.leg.center_value

        frontLeftBodySplitDiff = self.front_left.body.center_value - self.front_left.body.value
        frontLeftMiddleSplitDiff =self.front_left.middle.center_value  - self.front_left.middle.value
        frontLeftLegSplitDiff = self.front_left.leg.center_value - self.front_left.leg.value

        backRightBodySwing = -20/splitNum
        backRightMiddleSwing = -10/splitNum
        backRightLegSwing = 28/splitNum
        backLeftBodySwing = 40/splitNum

        # forward condence
        for x in range(0, splitNum):
            self.front_right.body.moveOffset(frontRightBodySplitDiff/splitNum)
            self.front_right.middle.moveOffset(frontRightMiddleSplitDiff/splitNum)
            self.front_right.leg.moveOffset(frontRightLegSplitDiff/splitNum)

            #self.front_left.body.moveOffset(frontLeftBodySplitDiff/splitNum)
            self.front_left.middle.moveOffset(frontLeftMiddleSplitDiff/splitNum)
            self.front_left.leg.moveOffset(frontLeftLegSplitDiff/splitNum)

            self.back_right.body.moveOffset(backRightBodySwing)
            self.back_right.middle.moveOffset(backRightMiddleSwing)
            self.back_right.leg.moveOffset(backRightLegSwing)
            self.back_left.body.moveOffset(backLeftBodySwing)

        time.sleep(time_delay)

        leg_step_BRbody_delta = 30
        leg_step_BRmiddle_delta = 30
        leg_step_BRleg_delta = -28
        time.sleep(time_delay)

        self.back_right.standardPivotStepWithMidMovement(leg_step_BRbody_delta, leg_step_BRmiddle_delta, leg_step_BRleg_delta,velocity,time_delay)

        leg_extend_body_delta = 35
        leg_extend_middle_delta =-5
        leg_extend_leg_delta = 28

        self.front_right.legExtend( leg_extend_body_delta, leg_extend_middle_delta, leg_extend_leg_delta, velocity, time_delay)
        time.sleep(time_delay)

        RlungeFLbody= 40
        RlungeBRbody= -20
        RlungeFRmiddle = 30
        RlungeFRleg = -28
        RlungeBLmiddle = -10
        RlungeBLleg = 28

        self.lunge(0,RlungeFRmiddle,RlungeFRleg,RlungeFLbody,0,0, 0,RlungeBLmiddle,RlungeBLleg,RlungeBRbody,0,0)

        leg_step_BLbody_delta = -30
        leg_step_BLmiddle_delta = 30
        leg_step_BLleg_delta = -28
        time.sleep(time_delay)

        self.back_left.standardPivotStepWithMidMovement(leg_step_BLbody_delta, leg_step_BLmiddle_delta, leg_step_BLleg_delta,velocity,time_delay)

        self.front_left.legExtend( -leg_extend_body_delta, leg_extend_middle_delta, leg_extend_leg_delta, velocity, time_delay)
        time.sleep(time_delay)

        LlungeFRbody= -40
        LlungeBLbody= 20
        LlungeFLmiddle = 30
        LlungeFLleg = -28
        LlungeBRmiddle = -10
        LlungeBRleg = 28

        self.lunge(LlungeFRbody, 0,0,0,LlungeFLmiddle,LlungeFLleg, LlungeBLbody,0,0 ,0,LlungeBRmiddle, LlungeBRleg)
        self.reset()
        time.sleep(10)
        self.forward()

    # Refer to stand()
    def reset(self):
        self.stand()

    # resets legs to default position
    def stand(self):
        self.front_left.reset()
        self.front_right.reset()
        self.back_left.reset()
        self.back_right.reset()

    def setAllHoriz(self):
        self.front_right.setMidAndLegHoriz()
        self.front_left.setMidAndLegHoriz()
        self.back_right.setMidAndLegHoriz()
        self.back_left.setMidAndLegHoriz()
        time.sleep(5)

    def setMidsToMin(self):
        self.front_right.middle.moveTo(self.front_right.middle.min)
        self.front_left.middle.moveTo(self.front_left.middle.min)
        self.back_left.middle.moveTo(self.back_left.middle.min)
        self.back_right.middle.moveTo(self.back_right.middle.min)
        time.sleep(10)

    def setMidsToMax(self):
        self.front_right.middle.moveTo(self.front_right.middle.max)
        self.front_left.middle.moveTo(self.front_left.middle.max)
        self.back_left.middle.moveTo(self.back_left.middle.max)
        self.back_right.middle.moveTo(self.back_right.middle.max)

","microcontroller, python"
How path following works analytically for way points?,"First of all sorry for the confusing question title. I am also confused about the concept.
I have implemented a quadcopter and its controller. Controller finds the rotor speeds based on the position and yaw angle references.
The thing that I don't understand is, let's say I want the vehicle to climb 5m up and then 5m left. At this point, I think I need to create a vector containing the reference values. By the way, the model is discretized by some deltaT time interval so the reference vector. This does not coincide and well behave according to the dynamics of the vehicle. Let's say, the reference input for altitude is 5m until 5sec and 0 for [5,10]sec. But it is not guaranteed that the vehicle will reach to 5m altitude in 5sec. Thus, my intention is, that reference vector shouldn't rely on the time. Therefore, my perception is to use some if condition to check if the vehicle is reached for the first waypoint and then, register the next one.
Which is a simple if-else statement? This makes me think what is the mathematical or analytical background in this. Is it just the following of the line between two waypoints by geometrical analysis like Line of Sight guidance law things.
Can you give me some insight about the concept which makes me confused?
","mobile-robot, control, path-planning"
Supplying power from a power bank to an arduino and DC Motors,"I have a two port powerbank capable of supplying a maximum of 5V 2.1A
I'm using it to power an Arduino and a L293D IC connected to two DC motors which have the following specifications:

Working voltage : 3V to 9V
No-load current = 60 mA, Stall current = 700 mA

The setup is not working and I have made the following observations
1) The voltage across the output terminals of the powerbank is read as 4.9V (when the arduino is not powered either from the other port of the power bank or another power supply all together)
2) The voltage across the output terminals of the powerbank is read as 4.0V when the Arduino is powered either from the other port of the power bank or my laptop
This voltage is given to the L293D Pin 8 (That is meant to be given 12V). A 12V to 5V buck provides the 5V to the IC itself (this is part of the motor driver board)
3) When only one motor is switched on, the voltage provided to that motor is 2.8V. The motor rotates only when given some manual force on the axle.
4) When both motors are switched on, the voltage provided to each motor is around 0.5V. Both the motors don't rotate at all.
5) When only one motor is switched on, the voltage reads around 0.2V until given manually rotating the axle after which it picks up speed.
I couldn't measure the current (or rather measured current to be 0A) anywhere as the motors don't rotate at all if I connect the ammeter. I understand that the power bank supplies power only when the load connected to it demands the power (due to smart sensing).
What should I do so that the power bank continually provides 5V and this 5V is delivered to each motor?
","arduino, motor, power"
Inverse dynamics for robotic arm using Matlab Simmechanics,"I am learning Simmechanics Matlab to do inverse dynamics for 4 DOF robotic arm. I read many examples to input motion to revolute joints like through PID, slider gain, sine waves, signal Builder etc. But these are not fulfilling my purpose as I have to rotate angles within limits and automatically. For example when I used sine wave signal, it continuously rotate until simulation time is not over. So, basically what I have is angles to rotate (through inverse kinematics) and now I want to find out torque required to reach that pose. How I can do this? How to create signal which fit in this scenario.
Thanks.
","robotic-arm, inverse-kinematics, matlab, dynamics, simulation"
Convert Twist from frame B to frame A,"(full disclosure: this is homework)
I have a twist expressed in frame B:
$\zeta_b = \begin{bmatrix}1\\3\\-2\\0\\-2\\4\end{bmatrix}$
And a general transformation matrix:
$g_{ab} = \begin{bmatrix}-0.4749 & 0.8160 & 0.3294 & -1.5\\-0.2261 & -0.4749 & -0.8505 & -1\\-0.8505 & -0.3294 & 0.4100 & 2\\0 & 0 & 0 & 1\end{bmatrix}$
How would I go about converting my twist into frame A?
I suspect I would break $\zeta_b$ into its component $\omega$ and $v$ vectors using the knowledge that:
$\zeta = \begin{bmatrix}v \\ \omega\end{bmatrix} = \begin{bmatrix}-\omega \times q \\ \omega\end{bmatrix}$ (where $q$ is a point on $\omega$)
But I am unsure.
",rotation
Forward kinematics equations,"I'm stuck at computing forward kinematics equations.
I have configuration of the first two joints like on the following image:

Transformation from the origin to the first joint basis is trivial: just translation by $\vec{OO_{1}}$.
The second transform from joint 1 to joint 2 basis makes me nervous throughout this day. First of all it is a rotation around $Z$ axis. So rotational part will look like this:
$
R_{12}=
\begin{pmatrix}
cos(q_{1}) & -sin(q_{1}) & 0\\
sin(q_{1}) & cos(q_{1}) & 0\\
0 & 0 & 1
\end{pmatrix}
$
Problems are all about the translation part. I see two approaches here.
Since angle between $\vec{O_{1}O_{2}}$ and plane $X_{1}O_{1}Y_{1}$ is constant because rotation is performed around $Z$ axis, length of projection of $\vec{O_{1}O_{2}}$ onto $X_{1}O_{1}Y_{1}$ is constant. Here it is:
$\vec{v} = O_{2} - O_{1} = \begin{pmatrix} v_{x}\\ v_{y}\\ v_{z} \end{pmatrix}
$
Its' projection onto $X_{1}O_{1}Y_{1}$ is $\vec{v_{p}} = \begin{pmatrix} v_{x}\\ v_{y}\\ 0 \end{pmatrix}$ and it's magnitude is $m=\sqrt{v_{x}^2 + v_{y}^2}=const$.
Now let's look at what happens after rotation:

So the translation matrix looks like:
$
S_{12}=
\begin{pmatrix}
m\cdot cos(\alpha+q_{1})\\ m\cdot sin(\alpha+q_{1})\\ v_{z}
\end{pmatrix}
$
And full transformation matrix from joint 1 to joint 2 basis is:
$
T_{12}=
\begin{pmatrix}
R_{12} & S_{12}\\
0 & 1
\end{pmatrix}
$
Unfortunately it gives me wrong results even when $q_{1}=0$. Can not see where my reasoning is wrong.
Second approach is more straightforward. Being able to calculate $\vec{O_{1}O_{2}}$ in initial configuration makes it possible just to rotate this vector by $q_{1}$ around $Z$ axis and this has to be our translation vector. Nevertheless I can't make it work.
$R_{z}=
\begin{pmatrix}
cos(q_{1}) & -sin(q_{1}) & 0\\
sin(q_{1}) & cos(q_{1}) & 0\\
0 & 0 & 1
\end{pmatrix}\\
\vec{v} = O_{2} - O_{1} = \begin{pmatrix} v_{x}\\ v_{y}\\ v_{z} \end{pmatrix}\\
R_{z}\vec{v}= 
\begin{pmatrix}
v_{x}cos(q_{1})-v_{y}sin(q_{1})\\
v_{x}sin(q_{1})+v_{y}cos(q_{1})\\
v_{z}
\end{pmatrix}\\
T_{12}=
\begin{pmatrix}
R_{12} & R_{z}\vec{v}\\
0 & 1
\end{pmatrix}
$
It works until I rotate the first joint(i.e. only when $q_{1}=0$).
Under works I mean ""calculates position of joint 2 origin right"". This is done by multiplying transformation matrix $T_{02} = T_{01}T_{12}$ by $\begin{pmatrix} 0 & 0 & 0 & 1\end{pmatrix}^{T}$
","robotic-arm, kinematics, forward-kinematics, arm"
how to find pwm setup : Max/Min/Center PWM for a ESC of a RC Car,"I am working on a project with this RC car (the version with the brush motor). I have installed a RaspberryPi and a Servo hat to be able to control steering and throttle. I use the python package from Adafruit (Adafruit_PWM_Servo_Driver) to send command to the servo motor and the ESC. The steering seems to work: I can control the servo motor with the RPi and the Servo-Hat. But I can't get the throttle to run with PWM. I connected the ESC cable (back/White/red) to the Servo Hat Pin 0. Here is the code that I use for throttle:
from Adafruit_PWM_Servo_Driver import PWM
pwm = PWM(0x40)

def calibrate_esc(channel, high, low, center):
 pwm.setPWM(channel, 0, int(high) )
 time.sleep(2)
 pwm.setPWM(channel, 0, int(low) )
 time.sleep(2)
 pwm.setPWM(channel, 0, int(center) )
 time.sleep(2)

pwm.setPWFreq(60)
MaxPWM(580)
MinPWM(280)
CenterPWM(430)

calibrate_esc(0, MaxPWM, MinPWM, CenterPWM)
time.sleep(5)

while (True):
 pulse_duration = 480
 pwm.setPWM(0, CenterPWM, pulse_duration)
 time.sleep(2)

When I run the code, the red LED light on the ESC stops blinking and a beep sound is emitted. After the 5 sec wait time, the motor would start at full speed and run constantly at same speed. I tried running the code with different values of the pulse_duration but I don't see any effect on the speed.
1) Do I need to do a manual calibration of the ESC?
2) how can I get the minPWM/MaxPWM/CenterPWM setting values?
The number that I used in the py code is taken from someone else who might likely use a different rc car. 
3) I tried powering the Servo-Hat from the ESC voltage output (6V) or from an external USB battery bank (5V). But I still get the same result: the motor speed cannot be controlled. It only runs full speed.
","raspberry-pi, servos, esc, python"
Detect road surface in a traffic scene point cloud,"I want to analyze a traffic scene. My source data is a point cloud like this one (see images at the bottom of that post). I want to be able to detect objects that are on the road (cars, cyclists etc.). So first of all I need know where the road surface is so that I can remove or ignore these points or simply just run a detection above the surface level.
What are the ways to detect such road surface? The easiest scenario is a straight and flat road - I guess I could try to registrate a simple plane to the approximate position of the surface (I quite surely know it begins just in front of the car) and because the road surface is not a perfect plane I have to allow some tolerance around the plane.
More difficult scenario would be a curvy and wavy (undulated?) road surface that would form some kind of a 3D curve... I will appreciate any inputs.
","mobile-robot, wheeled-robot, computer-vision, algorithm, stereo-vision"
what controller do we use in robots that has forward and inverse kinematics?,"i used ATmega328p chip to make a car, but its the car is always controlled by me.
my question is in robotic arms , the arm moves using the kinematics that i put, what chip and programming language should i use? i know of matlab which works with matrices, but what chip works with it?
","control, microcontroller"
Stereo camera vs. one camera for player detection & ball tracking in basketball scenes?,"There are a number of similar questions such as Monocular vs. stereo computer vision robustness for object detection, but none that address my question specifically.
My weekend project is to build a little robot that can detect and track players and shots made in a basketball game.
Detection and tracking need not be real-time (though it would be ideal).
The goal is to understand if a stereo camera would help improve speed and accuracy of tracking players and shots made, or if a solo camera is sufficient.
Would the depth information of a stereo camera simplify the task? Or would a solo camera, because of assumptions you can make about the basketball scene, be equally as accurate (and therefore preferable since less hardware is required)?
Assume the camera must track activity at both baskets, and is 100 feet from the furthest basket (i.e., at the other end of the court).
Specific Questions:

Could a stereo system let you more quickly detect human bodies and basketballs (i.e., spheres with ~9"" diameter) because you could detect volumetric shapes whereas you can't with one camera?
Could shot detection be more accurate and faster because you can measure depth of the ball (i.e., only trigger analysis when ball is around same depth as hoop)?
Would hoop detection be easier because of depth information?
Obviously, stereo cameras require higher computational load at a nominal level, but could algorithm simplifications (e.g., ignore non-spheres for ball detection) allowed by depth information actually reduce overall computational load?
Argument for solo camera: since the robot only operates against basketball scenes, you can make assumptions like there will be at least one 10-foot basketball hoop. Since you know the height of the hoop, would that allow you to perform depth measurements as if you had a stereo camera? 

The paper ""Real-Time Tracking of Multiple People Using Continuous Detection"" by David Beymer and Kurt Konolige suggests a stereo camera would offer advantages over a solo camera, confirming some of the hypotheses here, but the paper is also very old (1999). Is player & shot tracking better with a stereo camera, or are solo cameras equally as effective?
","computer-vision, cameras, stereo-vision"
Very slow calculations of link orientation for 6DOF IK,"I have written a matlab code to solve IK for 6 DOF robotic arm. I use Newton method to numerically solve IK. Also i use Tikhonov regularization to hand bad conditioned Jacobians. It works fast and reliable when i want just to move the last link in certain position, when i use difference between X,Y,Z coordinates as condition to interrupt loop of Newtoon method. But when i want also to get into the right orientation (use difference between Euler angles as interrupt condition) it takes a very long time 2, 5, 10 minutes even more, regardless i want to get to the right coordinates also or not. So there are questions:

How can i accelerate calculations, or why is it so slow?
Can i use quternions instead of Euler angles? Quternions will increase dimension of Jacobian so it will not be a square matrix anymore and it will not be possible to use Tikhonov regularization that works so good.
How often people use numerical methods to solve such things? I saw may examples of using analitycal solutions but not numerical.
How to get sure that programm will find solution using Newton method and programm will find it in finite number of interations?

UPD: here is my matlab code, that was rewritten using damped least squares method and quaternion. But still i have the same problem.
In this code we move along trajectory, but we can remove it and try to jump directly to the destination point.
%Derivative step for Jacobian composing
step = 0.01;

%Generalized coordinates for start position
q_prev = [34; 89; 1; 1; 89; 0];

%Generalized coordinates for end position. To be sure we can reach it
q_fin = [170; 150; 120; 156; 9; 158];

%get_coordinates() function returns 4x4 matrix of homogeneous transformations. It contains forward kinematics equations
%Coordinates we are at 
A_forward = get_coordinates( q_prev ); 

%Coordinates we need to reach
Dest = get_coordinates( q_fin );

%Getting rotation matrices for start and finish positions
rotmat_curr = A_forward(1:3, 1:3);
rotmat_dest = Dest(1:3, 1:3);

%Matrix_to_quat() - is my analog of rotm2quat() function
%Getting quternions for start and finish positions
quat_curr = matrix_to_quat(rotmat_curr);
quat_dest = matrix_to_quat(rotmat_dest);

%Next steps are not inmportant, but i still comment them

%Here i make a trajectory, and move along it with small steps. It was
%needed for Newton's method but also useful if it is needed to move along a
%real trajectory
                       %X coordinate    Y coordinate    Z coordinate    Quaternion
coordinates_current = [ A_forward(1,4); A_forward(2,4); A_forward(3,4); quat_curr ];
coordinates_destination = [ Dest(1,4); Dest(2,4); Dest(3,4); quat_dest ];

%Coordinate step
step_coord = 5; 

%Create table - trajectory
distance = sqrt( (coordinates_destination(1) - coordinates_current(1)).^2 + (coordinates_destination(2) - coordinates_current(2)).^2 +(coordinates_destination(3) - coordinates_current(3)).^2 );

%Find out the number of trajectory points
num_of_steps = floor(distance / step_coord);

%Initialize trajectory table
table_traj = zeros(7,(5*num_of_steps));

%Calculate steps size for each coordinate
step_x = (coordinates_destination(1) - coordinates_current(1)) / num_of_steps;
step_y = (coordinates_destination(2) - coordinates_current(2)) / num_of_steps;
step_z = (coordinates_destination(3) - coordinates_current(3)) / num_of_steps;
step_qw = (coordinates_destination(4) - coordinates_current(4)) / num_of_steps;
step_qx = (coordinates_destination(5) - coordinates_current(5)) / num_of_steps;
step_qy = (coordinates_destination(6) - coordinates_current(6)) / num_of_steps;
step_qz = (coordinates_destination(7) - coordinates_current(7)) / num_of_steps;

new_coord = coordinates_current;
%Fill trajectory table
for ind = 1:num_of_steps
    new_coord = new_coord + [step_x; step_y; step_z; step_qw; step_qx; step_qy; step_qz];
    table_traj(:,ind) = new_coord;  
end;

%Set lambda size. I found out that algorithm works better when lambda is
%small
lambda = 0.1;

%In next steps i inialize Jacobian, build new destination matrix, calculate
%orientation error at the first step. As orientation error i use max
%element of quaternions difference.
for ind = 1:num_of_steps
    J = zeros(7, 6);

    %quat_to_matrix() - analog of quat2rotm() function
    rot_matr = quat_to_matrix(table_traj(4:7, ind));

    Dest = [ rot_matr, table_traj(1:3, ind);
             0, 0, 0, 1  ]; 

    %mat_to_coord_quat() function takes matrix of homogeneous
    %transformations and returns 7x1 vector of coorditaes
    %X Y Z and quaternion
    differ = mat_to_coord_quat(Dest) - mat_to_coord_quat(A_forward);
    error = max(abs(differ(4:6)));

    %Here is the algorithm. It works until we have coordinates and
    %orientation error less that was set
     while (abs(differ(1)) > 0.05) || (abs(differ(2)) > 0.05) || (abs(differ(3)) > 0.05) || error > 0.01

         %first - calculating Jacobian
         for ind2 = 1:6  %for every coordinate
                %Calculating of partial derivatives:
                q_prev_m1 = q_prev;
                q_prev_m1(ind2) = q_prev_m1(ind2) - step;
                q_prev_p1 = q_prev;
                q_prev_p1(ind2) = q_prev_p1(ind2) + step;
                Fn1 = mat_to_coord_quat(get_coordinates(q_prev_m1)); % in q_prev vector ind1 element is one step smaller than in original q_prev
                Fn2 = mat_to_coord_quat(get_coordinates(q_prev_p1)); % in q_prev vector ind1 element is one step bigger than in original q_prev
                deltaF = Fn2 - Fn1; %delta functions vector
                deltaF = deltaF/(2*step); %devide by step to get partial derivatives for every function
                %composing Jacobian from column of partian derivatives
                J(:,ind2) = deltaF;
        end;
        %Next according to damped least squares method
        %calculate velosities along all coordinates
            A_forward = get_coordinates( q_prev );
            differ = mat_to_coord_quat(Dest) - mat_to_coord_quat(A_forward);

            %calculating generalized coordinates velosities
            dq = (J.'*J + lambda * eye(6))\ J.' * differ;

            %integrate generalized coordinates velosities
            q_prev = q_prev + dq;

            %calculate max orientation error
            error = max(abs(differ(4:7)));   
    end;
end;

","robotic-arm, inverse-kinematics"
Are scissors what I want for moving straight up and out?,"I'm a programmer. I had a tiny amount of experience building robots in college a few years ago, but haven't done anything since.
I'd like to build a robot that can move around my house, pick items up and put them down... IE, a robot that could get items out of the dishwasher and put them away.
I was thinking it would have a square base with a wheel at each corner for moving around the floor, a scissor jack so it can adjust its height (I'm hoping to be able to be able to move between 1 foot tall and 7 feet tall), and then a scissor jack at the top for moving a gripper towards or away from it (between a couple of inches and 3 feet away).
Are two scissor jacks actually what I want? It seems like 90+% of robots with grippers go with arms instead, but it seems to me that those are more complicated and would be less precise. I have no robotic experience beyond little car like things - I've never built one with any sort of gripper or actuator, so advise would be much appreciated. (Huh - there's not even a tag for scissor or jack... what do you guys call them?)
Also... do people normally build these parts themselves, or do they buy them? I've searched around but I can't find any scissor jack kit or anything like that. If I need to build it myself... how would I do that? What would I build it out of?
","robotic-arm, actuator, manipulator, arm"
How to calculate the max load that owi 535 arm can lift?,"I am trying to calculate the max lifting capability of the OWI 535 arm. The robot has 3 DOF and 3 three 3 DC motor with the robot power source delivers an Operating Voltage current of 3 Volts. The motors have a Stall Torque 60g-cm. I would like to know how to do the math to calculate the lifting capacity. 
The Robot has a wrist motion of 120 degrees, an extensive elbow range of 300 degrees, base rotation of 270 degrees, base motion of 180 degrees, vertical reach of 15 inches, horizontal reach of 12.6 inches, and lifting capacity of 100g
Again, I am attempting to use the robot information on the Society of Robots http://www.societyofrobots.com/robot_arm_calculator.shtml
I have middle and high school student that want to be able to calculate this information.
",robotic-arm
Accessibility distribution of three vector fields,"I have to study the controllability of the kinematic model of a Cycab:

$\dot{q}=g_1(q)v+g_2(q)\omega_R+g_3(q)\omega_L$
where 
$\dot{q}=\begin{bmatrix}\dot{x}\\\dot{y}\\\dot{\theta}\\\dot{\gamma}\\\dot{\phi}\end{bmatrix}$ $g_1(q)=\begin{bmatrix}cos(\theta+\gamma)\\sin(\theta+\gamma)\\\frac{sin(\phi-\gamma)}{lcos(\phi)}\\0\\0\end{bmatrix}$  $g_2(q)=\begin{bmatrix}0\\0\\0\\1\\0\end{bmatrix}$  $g_3(q)=\begin{bmatrix}0\\0\\0\\0\\1\end{bmatrix}$
with $x$ and $y$ the Cartesian coordinates of the midpoint of the rear segment joining the two rear wheels and $\theta$ the direction of the midpoints of the two segments joining the wheels centers with respect to the axis $x$. So, I have to study the accessibility distribution $\{g_1,g_2,g_3.[g_1,g_2],[g_2,[g_1,g_2]],...\}$ so I computed
$[g_1,g_2]=\begin{bmatrix}sin(\theta+\gamma)\\-cos(\theta+\gamma)\\\frac{cos(\phi-\gamma)}{lcos(\phi)}\\0\\0\end{bmatrix}$   $[g_2,[g_1,g_2]]=\begin{bmatrix}-cos(\theta+\gamma)\\-sin(\theta+\gamma)\\\frac{-sin(\phi-\gamma)}{lcos(\phi)}\\0\\0\end{bmatrix}$
so the rank of $[g_1,g_2,g_3,[g_1,g_2],[g_2,[g_1,g_2]]]$ is equal to 5 so we can say that the system is controllable.
Now, is it correct to study the accessibility distribution without using the vector field $g_3$, so is it correct to say that the system is controllable without using the vector field $g_3$?
","mobile-robot, control"
Localization on a robot grid,"I have a skid steer drive train with an encoder on each side of the robot along with a gyro to measure the angle of the robot. The width of the robot is 26 inches. Using the encoders I would like to set up an x and y coordinate grid to know the pose of the robot and set up the system to go through waypoints to reach a destination. The robot has a starting reference point and I would like to go to another point in the area. Anybody have an idea of how to approach this?
",localization
"5kHz arm control, grasp end effector","I was having a chat with a robotic expert recently. The guy told me that for an arm with motor drives running at 5kHz your are to set the control so that someone can grasp the end effector and move it around (something like the usual teach mode for an arm).
Anyone knows what kind of control this expert was mentioning? What setup this involves? Any doc on this subject? Any input is welcome...
","control, motor"
Communicating between a BeagleBone Black and a servo controller,"I am a complete newbie and recently joined a robot team at my school in order to gain some experience. I have been assigned a task of driving a servo using a Pololu Mini Maestro USB Servo Controller. I am using the BeagleBone Black (BBB) with the Python adafruit library. How do I make the BBB communicate with the Servo Controller? If you guys could point me in the right direction, I'd really appreciate that. Right now, I don't even know where to start. Not sure if it matters, but this is the servo I am using: https://www.pololu.com/product/1053
","motor, microcontroller"
Long range distance measurements,"We are building a plane which should fly autonomously, to achieve best results in the enviroment we are flying in (many Hills and Mountains) we need some sort of reliable height readings.
We already get some information using a barometer and the google maps elevation API but especially for landing and low alltitude flights we need a precise height.
Most sensors have a bad performance on grass (SRF-08 achieves about 1m; Lidar Lite about 3-4m). Is there some (not to expensive) sensor which can measure distances to at least 50m with a precision of about 20cm? Which method would be suitable for this application?
","ultrasonic-sensors, lidar"
Will Lidar see a fence?,"I am looking at the possibility of using LIDAR to do obstacle avoidance for a robotics project I am working on but the project involves avoiding a chain link fence. Has anyone used LIDAR to detect fences and if so how well did it work? Thanks for your help.
","mobile-robot, mapping, lidar"
How do the joint angles of a 4-legged impact the body's position with respect to the world frame?,"For a four-legged robot (like Big Dog or the one shown here) how are the joint angles and ""feet"" position related to the body's frame in the world/inertial frame? For example, if I know the body's position and orientation in the world frame, and the joint angles, how do I derive the relationship that tells me where the robots ""feet"" are? 
For simplification, if I assume the legs can be represented as a planar 3R manipulator (where the end effector is the foot), it's easy enough to derive the relationship between the end effector and the angles. But the ""base"" is the robot's body, which will change position and orientation when the joint angles change. So do I have to find the matrix which relates the body to the world frame, then find the position of the foot with respect to the world? Or am I thinking of this the wrong way?
","kinematics, dh-parameters, walking-robot"
gzclient: segmentation fault,"I have archlinux indigo ros.
My probelm is that when I type in terminal:
$ gzclient
Segmentation fault (core dumped)

...  
$ roslaunch turtlebot_gazebo turtlebot_world.launch
    ... logging to /home/islam/.ros/log/0f56780c-18b0-11e7-966d-642737d9d3b9/roslaunch-CatchMe-11800.log
    Checking log directory for disk usage. This may take awhile.
    Press Ctrl-C to interrupt
    Done checking log file disk usage. Usage is <1GB.

    started roslaunch server http://localhost:44337/

    SUMMARY
    ========

    PARAMETERS
     * /bumper2pointcloud/pointcloud_radius: 0.24
     * /cmd_vel_mux/yaml_cfg_file: /opt/ros/indigo/s...
     * /depthimage_to_laserscan/output_frame_id: /camera_depth_frame
     * /depthimage_to_laserscan/range_min: 0.45
     * /depthimage_to_laserscan/scan_height: 10
     * /robot_description: <?xml version=""1....
     * /robot_state_publisher/publish_frequency: 30.0
     * /rosdistro: indigo
     * /rosversion: 1.11.20
     * /use_sim_time: True

    NODES
      /
        bumper2pointcloud (nodelet/nodelet)
        cmd_vel_mux (nodelet/nodelet)
        depthimage_to_laserscan (nodelet/nodelet)
        gazebo (gazebo_ros/gzserver)
        gazebo_gui (gazebo_ros/gzclient)
        laserscan_nodelet_manager (nodelet/nodelet)
        mobile_base_nodelet_manager (nodelet/nodelet)
        robot_state_publisher (robot_state_publisher/robot_state_publisher)
        spawn_turtlebot_model (gazebo_ros/spawn_model)

    auto-starting new master
    process[master]: started with pid [11824]
    ROS_MASTER_URI=http://localhost:11311

    setting /run_id to 0f56780c-18b0-11e7-966d-642737d9d3b9
    process[rosout-1]: started with pid [11837]
    started core service [/rosout]
    process[gazebo-2]: started with pid [11852]
    process[gazebo_gui-3]: started with pid [11861]
    process[spawn_turtlebot_model-4]: started with pid [11868]
    process[mobile_base_nodelet_manager-5]: started with pid [11873]
    process[cmd_vel_mux-6]: started with pid [11878]
    process[bumper2pointcloud-7]: started with pid [11880]
    process[robot_state_publisher-8]: started with pid [11882]
    process[laserscan_nodelet_manager-9]: started with pid [11896]
    process[depthimage_to_laserscan-10]: started with pid [11905]
    /opt/ros/indigo/lib/gazebo_ros/gzclient: line 25: 11916 Segmentation fault      (core dumped) GAZEBO_MASTER_URI=""$desired_master_uri"" gzclient $final
    [gazebo_gui-3] process has died [pid 11861, exit code 139, cmd /opt/ros/indigo/lib/gazebo_ros/gzclient __name:=gazebo_gui __log:=/home/islam/.ros/log/0f56780c-18b0-11e7-966d-642737d9d3b9/gazebo_gui-3.log].
    log file: /home/islam/.ros/log/0f56780c-18b0-11e7-966d-642737d9d3b9/gazebo_gui-3*.log
    /opt/ros/indigo/lib/gazebo_ros/gzserver: line 30: 11978 Segmentation fault      (core dumped) GAZEBO_MASTER_URI=""$desired_master_uri"" gzserver $final
    [gazebo-2] process has died [pid 11852, exit code 139, cmd /opt/ros/indigo/lib/gazebo_ros/gzserver -e ode /opt/ros/indigo/share/turtlebot_gazebo/worlds/playground.world __name:=gazebo __log:=/home/islam/.ros/log/0f56780c-18b0-11e7-966d-642737d9d3b9/gazebo-2.log].
    log file: /home/islam/.ros/log/0f56780c-18b0-11e7-966d-642737d9d3b9/gazebo-2*.log

and here is the output of debug:
Reading symbols from gzserver...(no debugging symbols found)...done.
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/usr/lib/libthread_db.so.1"".
[New Thread 0x7fffd4e6d700 (LWP 16809)]
[New Thread 0x7fffd038d700 (LWP 16810)]

Thread 1 ""gzserver"" received signal SIGSEGV, Segmentation fault.
0x00007ffff2838390 in gazebo::event::Connection::Id() const ()
from /usr/lib/libgazebo_common.so.8

My GPU is intel HD3000 , core i5 INSPIRON N5110. I have noted that intel hd3000 works with gazebo
Thanks in advance.
","ros, gazebo"
What is wrong with this Model Predictive Controller with constraints solved with quadratic programming?,"I have a quadrator dynamics equation defined as state-space in matlab. There is a MPC defined and related matrices are generated. I gathered most of the code from1 and implemented in matlab. Final motion of quadrator can be seen as follow,

As you can see here, it starts to ascend slowly then speeds up and misses the reference altitude value set.
My code solves the quadratic programming is as,
%%X = quadprog(H,f,A,b,Aeq,beq,LB,UB,X0) sudo fcn
[DeltaUC, ~, ~]=quadprog(mpc.H,mpc.f,[mpc.M1;mpc.M2],[mpc.N1;mpc.N2],[],[],repmat(mpc.delumin,mpc.Nc,1), repmat(mpc.delumax,mpc.Nc,1));

Matrices written as arguments are not small enough to be written enough but I think they should not be wrong since I got the algorithm and logic from the book that I read.
As I know, a controller should apply greater inputs when the error is larger and lowers the amount of input applied while the error gets smaller. I wonder if quadprog gives inputs as solution to the quadratic programming correct. Does it give greater input when the error is large? Error is implemented in the ""f"" argument where f is;
f=-2*mpc.Phi'*mpc.Q*(mpc.BarRs*ref-mpc.F*mpc.Xf);

Where Phi is square time indepented matrix gatherd by enlarging the state-space matrices over the receeding horizon. Q is the state penalty matrix which is an identity matrix. BarRs is repeated identity matrix which allows ""ref"" to be applied and be constant over the horizon. F is also created by state matrices and finally Xf is augmented state vector which is created by [state output]' style.
","quadcopter, mobile-robot, control"
GPS message: compute ephemeris data and pseudo-ranges from subframes,"I have raw GPS data (see end of the question for details if you think the protocols are relevant), and I need to extract/compute ephemeris and pseudoranges (my goal is to replace the recursive least squares that the receiver solves to compute the position with a home-brewed method based on sensor fusion). Concerning the ephemeris, I have access to the 3 first subframes of each of the 25 frames of the GPS message. Documentations/books/etc that I have found just vaguely mention that the first subframe contains clock data, and the two others contain ephemeris data. However, none of them precisely says what the words in these subframes are, and how I can use them to compute orbital information (I'm assuming that I only want satellite positions?). Can anyone give me pointers to some references on how to do this? Or (even better), is there any open-source code that already implements this?
I really appreciate your help.
Details on the data: They have been transmitted by a Ublox EVK-7p using the UBX RXM-EPH and RXM-RAW protocols.
RXM-EPH: satellite ID (4 byes), HOW word (4 bytes), followed by three 32-byte arrays corresponding to subrames 1 to 3 of the GPS message.
RXM-RAW: time of week, week, num of satellites, reserved (?) , carrier phase, pseudo-range, doppler, and so on....
Update: I found this which seems to answer most of my questions. It is from 1995 though.
","sensors, gps, pseudo-ranges, ephemeris"
Compact 4 Node Raspberry Pi 3 Cluster for Robots?,"Can a Compact 4 Node Raspberry Pi 3 Cluster be enough powerful to elaborate videostreaming input data in real time for a drone?
Thank you for any answer
",raspberry-pi
"Explanation of Quadcopter Dynamics, Components and Control","I am about to build my own quadcopter from scratch. However i am having problem with understanding how it is possible to control the quadcopter without knowing the current rpm of the BLDC motors. According to my understanding the rpm is needed to calculate thrust force etc. in the mathematical model, which will be used for regulation.
The ESCs I have seen have two wires to connect them to the flight controller. The first one is GND and the second is signal wire, which are used for sending PWM signal- no information about the motor speed. There is also the  IMU unit, which provides information about the acceleration of the whole aircraft- but again no information about the motor speed. 
I would be grateful if someone could explain it in details how this is. 
","quadcopter, control, imu, accelerometer, brushless-motor"
holonomic and non holonomic constraints in layman's term,"I'm new to robotics and I've been reading some slides online regarding motion planning. Due to my lack of knowledge in mechanical engineering, I'm having a difficult time understanding what holonomic and non-holonomic constraints mean. 
I saw a post here and it says Holonomic system is when a robot can move in any direction in the configuration space, and Nonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint.
It seems like holonomic system differs from holonomic constraint. What is holonomic constraint and when do we need it? What is non-holonomic constraint and when do we need that?
Thanks in advance.
","dynamics, movement"
CNC Programming - How to mill diagonal edge of a contour (CNC Simulator Pro)?,"Hope this is the right place to ask.. I'm a Mechatronic Engineering student and am having difficulty programming the mill to cut diagonal edges of contours.  I've worked through this tutorial which I understand. The problem I'm having is milling around the outside of the diagonal edge, not on it.  I hope that makes sense. I'm unsure about the tool positioning at the start/end of cut, for example. Can someone explain how to do it? Thanks.
",cnc
Displacement with accelerometer,"I want to use a sensor to find displacement with accelerometer.
How can I use accelerometer to find displacement? I want to use this for a quadcopter.
","quadcopter, imu, accelerometer, uav"
Can i use a predictive kalman filter to 'increase' my sample rate?,"I have a slam algorithm that outputs at around 30Hz, an implementation of ORBSLAM2.
https://github.com/raulmur/ORB_SLAM2
I am reading this into a renderer that expects 60+ Hz.
Because my sample speed is low, I am getting 'shuddering' in the display, as the renderer adds linear 'steps' between the samples. 
For example, I am seeing a result like:
time   sample    result

1         20          20
2         n/a         20
3         n/a         20
4         22          22
5         n/a         22
6         n/a         22
7         24          24
8         n/a         24
9         n/a         24

What i need to do, is predict the next sample, and fill in the gaps, so to speak, so that I end up with something like:
time   sample    result

1         20          20
2         n/a         20.66
3         n/a         21.33
4         22          22
5         n/a         22.66
6         n/a         23.66
7         24          24
8         n/a         24.33
9         n/a         25.66

I need to predict 6DOF, for which i have translation xyz, and a quaternion xyzw.
But if I can find a way to predict even one axis, for a start, that would be great.
I have the data outputting as xyz and xyzw, at around 30Hz. I also have an xsens IMU, which i am using to pass in an initial rotation value.
Can i use a predictive filter for this purpose? Is a kalman suitable? 
I am looking at:
https://github.com/simondlevy/TinyEKF
and a Bayes filter:
http://bayesclasses.sourceforge.net/Bayes++.html
But am a little out of my depth.
Thank you, please ask if I have not made sense!
","slam, kalman-filter, c++"
Getting velocity from IMU acceleration,"I am doing to research about why you cannot use IMU acceleration the integrate to get velocity.  Everyone says you cannot do that due to there being error but what error is this exactly and where does it come from?
",imu
Inverse kinematics solutions pros and cons,"I know at least 3 different solutions to inverse kinematics problem. They are pseudo inverse jacobian, cyclic coordinate descent and ANFIS networks. I would like to know their advantages and disadvantages comparing to each other.
",inverse-kinematics
sendTransform() takes exactly 6 arguments (2 given),"I was trying to run this tutorial in python code, but i got above error when i try to run it. My python converted program of given tutorial is:
#!/usr/bin/env python

import rospy
from sensor_msgs.msg import JointState
from std_msgs.msg import Header
import tf
import geometry_msgs.msg
import math

def talker():
    pub = rospy.Publisher('joint_states', JointState, queue_size=1)
    rospy.init_node('state_publisher')

broadcaster = tf.TransformBroadcaster()   
rate = rospy.Rate(30) # 10hz 
M_PI = 3.145
degree = M_PI/180;

# robot state
tilt = 0 
tinc = degree 
swivel=0 
angle=0 
height=0 
hinc=0.005

# message declarations
t = geometry_msgs.msg.TransformStamped()
hello_str = JointState()
t.header.frame_id = ""odom""
t.child_frame_id = ""axis""

while not rospy.is_shutdown():
    # update joint_state
    hello_str.header.stamp = rospy.Time.now()
    hello_str.name = ['swivel','tilt','periscope']
    hello_str.position = swivel
    hello_str.velocity = tilt
    hello_str.effort = height

    t.header.stamp = rospy.Time.now()
    t.transform.translation.x = math.cos(angle)*2
    t.transform.translation.y = math.sin(angle)*2
    t.transform.translation.z = .7
    #t.transform.rotation = tf.createQuaternionMsgFromYaw(angle+M_PI/2)
    t.transform.rotation = tf.transformations.quaternion_from_euler(0, 0, angle)
    # send the joint state and transform
    pub.publish(hello_str)
    broadcaster.sendTransform(t)

    # Create new robot state
    tilt += tinc
    if (tilt<-.5 or tilt>0):
       tinc *= -1
    height += hinc
    if (height>.2 or height<0):
      hinc *= -1
    swivel += degree
    angle += degree/4

    rate.sleep()

if __name__ == '__main__':
    try:
        talker()
    except rospy.ROSInterruptException:
        pass

How can i remove this, i am using ros indigo. Even if i change that line with :
broadcaster.sendTransform((0.5,1.0,0),tf.transformations.quaternion_from_euler(0, 0, angle+M_PI/2),rospy.Time.now(),""odom"",""axis"") 
Its not working. Than it shows error:
 field position must be a list or tuple type
[state_pub-2] process has died [pid 11654, exit co

",ros
Jacobian Matrix and joint dependecies,"I'm working with a robotic arm and needed to compute the jacobian matrix of it in order to send torque commands. THe arm has 6 joints all revolute. After calculating the jacobian matrix from the DH parameters provided in the datasheet, I noticed that the jacobian has dependency only from the first five joints.
The sixth joint corresponds to the hand which only rotates over itself. My question here is: Can the jacobian have no dependency from a joint? On which cases can this happen?
Thanks
",jacobian
Baby Child Following Robot,"I am Planning to make a crawling bot which will follow a baby,having a pillow on the board.The Problem with the project is:
Suppose a case when the baby falls, the bot should come exactly at the place where the baby's head will hit the ground?
Any idea of how i can make the bot move to that point before the baby's head hit the ground ?
",arduino
motor to turn a screw to move a lever,"Where I can buy an actuator that consists of a motor that turns a screw that moves a rod? 

",actuator
Is there any C++ library I could use to program a robotic manipulator involving forward and inverse kinematics?,"I came across robotics library (RL), but quite unclear about its real purpose. Is it a FK/IK solver library or simply an graphical simulator?. RL has poor documentation, so its not clear how to use it. Im looking for some C++ library where there APIs to solve FK/IF analytically. Thank you.
","inverse-kinematics, c++, forward-kinematics, robotics-library"
Humanoid robot arm - inverse kinematics - choose joint solutions,"I have developed a 7DOF arm for a humanoid robot (see pic below for more details) 
I have implemented the IK using a closed form solution and of course I come up with eight solutions - each one is actually positioning the end effector at the right position and orientation (I implemented the method described in the paper: ""Kinematics and Inverse Kinematics for the Humanoid Robot HUBO2+"").
The question now is how to choose the right one, knowing the fact that the end effector will follow a trajectory. The idea is to compute iteratively the $[N, S, A, P]$ matrix that will be provided to the IK module. 
One solution I am thinking off is to choose the joint solution in the decision table that will minimize the given metrics:
$$\sum_i (\theta_i^{current} - \theta_i^{next})^2$$
where $\theta_i^{current}$ represents the the current value of the $i^{th}$ joint and $\theta_i^{next}$ is the computed value in the decision table.
Do you think it is the right approach or there are other methods out there to find the best joint solution from the decision table. 

","robotic-arm, inverse-kinematics, humanoid"
What is the direction of current flowing through the freewheeling diodes in H-bridge if I use BJT?,"I am new to electronics.the question is buzzing in my head since i first saw the H bridge configuration.please answer the question briefly
",h-bridge
"Error while doing inverse kinematics in Matlab Simmechanics ""To be legal in kinematics mode, all independent degrees of freedom must be""","I am doing inverse kinematics of 4dof robotic arm in Simmechanics Matlab. Details about my model are in the question Error in simmechanics matlab while doing inverse dynamics using custom joint.
The error says:
Mechanical model motion is not completely specified when running in kinematics mode. To be legal in kinematics mode, all independent degrees of freedom must be kinematically actuated. Check joints, constraints, drivers, actuators, and DoFs.
What does this error mean and how do I correct it?
","robotic-arm, inverse-kinematics, matlab, simulation, simulator"
How to use RL (Robotics Library) with Xcode?,"I came across RL to program robotic manipulators. But according to the website, currently supported websites are Ubuntu and Windows only. Since Ubuntu and Mac OS are based on UNIX kernel, is it possible to deploy RL on MacOS Xcode?
Link : http://www.roboticslibrary.org
Thank you.
","inverse-kinematics, forward-kinematics, manipulator, robotics-library"
Simple inertial 2d path planning library for arduino,"I have a pretty simple 2d manipulator which uses an arduino to control a payload weighing about 2kg. I want to implement a simple 2d path planner which takes as input:

Current position, velocity, acceleration as 2d vectors
Target position as 2d vector
Bounding box as min/max x/y
Maximum acceleration
Maximum Jerk

... and outputs a path (function from time to (x,y)) which leads me to the target point as quickly as possible without violating the constraints.
I want to specify the initial velocity and acceleration (not just position) because a movement instruction might interrupt a movement already underway.
I want to specify the bounding box so that my payload doesn't hit any walls.
I want to specify a maximum acceleration so that the inertia of my payload doesn't overwhelm my control authority.
I want to specify a maximum jerk so that the springiness of my manipulator doesn't absorb some acceleration and whip it back at the end of travel. (I'm not sure whether I really care about maximum jerk except at the end of travel.)
Now I don't think the math here is very hard. I don't want to reinvent the wheel, but neither do I want to spend a week learning how to use a complicated and overpowered general kinematics library. Is there a very simple library that I can plug into my arduino IDE that could accomplish this for me?
","arduino, path-planning"
Can pose accuracy actually justify map accuracy in SLAM?,"Many papers evaluate SLAM systems by the accuracy of the resulting pose, but never (to the best of my knowledge) the accuracy of the resulting map. 
I have read somewhere that the pose accuracy justifies the accuracy of the map (I can't find the source), but shouldn't the accuracy of the pose only justify the belief of the accuracy of the map?
","slam, mapping, pose"
How to Visualize a real robot's movements on a map?,"I am working on obstacle avoidance and path planning in robotics using X80SV robot. The obstacle avoidance module of the robot works well. The programming language I have used in this work is C#. Next, I wanted to visualize the real-time motion of the robot. What should be done? The robot is equipped with ultrasonic sensors, infrared sensors, camera, human sensors etc. 
","mobile-robot, path-planning, visualization"
What is the scope of Electronics Engineering in Robotics/Automation?,"I am beginner in Robotics .I have taken admission for Electronics engineering one year back as we don't have specifically Robotics Engineering Branch in my Country.Now I am suffering from questions like what is the scope of Electronics( not Electrical) Engineering in Robotics/ Automation?I am unable to distinguish between the role of Electronics engineer and Computer Engineer in Robotics as in both cases programming is required
Also,if I don't like to do programming(coding),are there any other options to stick to Robotics / Automation field as per my branch(Electronics Engineering ) is concerned?. 
","control, electronics, embedded-systems"
Self-Motion Manifolds,"In this paper by J. W. Burdick, the main result basically says that
for a redundant manipulator, infinity solutions corresponding to one end-effector pose can be grouped into a finite set of smooth manifolds.
But later in the paper, the author said only revolute jointed manipulators would be considered in the paper. 
Does this result (grouping of solutions into a finite set of manifolds) hold for redundant robots with prismatic joint(s) as well? Is there any significant difference in analysis and result when prismatic joints are included? So far I couldn't find anyone explicitly address the case of robots with prismatic joints yet.
(I am not sure if this site or math.stackexchange.com would be the more appropriate place to post this question, though.)
","inverse-kinematics, redundant-robots"
Using 3D graphics animate the robot in Matlab GUI,"I have seen this example ( http://in.mathworks.com/matlabcentral/fileexchange/14932-3d-puma-robot-demo/content/puma3d.m) in file exchange and want to do similar thing with 4 dof rootic arm. I follow below steps. 1. Create a very simple 4 dof roots links using Solid Works and convert it into .stl file (ASCII) by using cad2matdemo.m file and store all data manually. 2. Alter the code according to my requirements. But I'm unable to create 3d model in Matlab GUI. My code is given below.
function rob3d
    loaddata
    InitHome
        function InitHome
            % Use forward kinematics to place the robot in a specified
            % configuration.
            % Figure setup data, create a new figure for the GUI
            set(0,'Units','pixels')
            dim = get(0,'ScreenSize');
            % fig_1 = figure('doublebuffer','on','Position',[0,35,dim(3)-200,dim(4)-110],...
            % 'MenuBar','none','Name',' 3D Puma Robot Graphical Demo',...
            % 'NumberTitle','off','CloseRequestFcn',@del_app);
            fig_1 = figure('doublebuffer','on','Position',[0,35,dim(3)-200,dim(4)-110],...
                'MenuBar','figure','Name',' 3D Puma Robot Graphical Demo',...
                'NumberTitle','off');
            hold on;
            %light('Position',[-1 0 0]);
            light                               % add a default light
            daspect([1 1 1])                    % Setting the aspect ratio
            view(135,25)
            xlabel('X'),ylabel('Y'),zlabel('Z');
            title('Robot');
            axis([-1000 1000 -1000 1000 -1000 1000]);
            plot3([-1500,1500],[-1500,-1500],[-1120,-1120],'k')
            plot3([-1500,-1500],[-1500,1500],[-1120,-1120],'k')
            plot3([-1500,-1500],[-1500,-1500],[-1120,1500],'k')
            plot3([-1500,-1500],[1500,1500],[-1120,1500],'k')
            plot3([-1500,1500],[-1500,-1500],[1500,1500],'k')
            plot3([-1500,-1500],[-1500,1500],[1500,1500],'k')
            s1 = getappdata(0,'Link1_data');
            s2 = getappdata(0,'Link2_data');
            s3 = getappdata(0,'Link3_data');
            s4 = getappdata(0,'Link4_data');
            s5 = getappdata(0,'Link5_data');
            a2 = 300;
            a3 = 300;
            a4 = 300;
            d1 = 300;
            d2 = 50;
            d3 = 50;
            d4 = 50;
            %The 'home' position, for init.
            t1 = 0;
            t2 = 0;
            t3 = 0;
            t4 = 0;
            % Forward Kinematics
            % tmat(alpha, a, d, theta)
            T_01 = tmat(90, 0, d1, t1);
            T_12 = tmat(0, a2, d2, t2);
            T_23 = tmat(0, a3, d3, t3);
            T_34 = tmat(0, a4, d4, t4);
            % Each link fram to base frame transformation
            T_02 = T_01*T_12;
            T_03 = T_02*T_23;
            T_04 = T_03*T_34;
            % Actual vertex data of robot links
            Link1 = s1.V1;
            Link2 = (T_01*s2.V2')';
            Link3 = (T_02*s3.V3')';
            Link4 = (T_03*s4.V4')';
            Link5 = (T_04*s5.V5')';
            % points are no fun to watch, make it look 3d.
            L1 = patch('faces', s1.F1, 'vertices' ,Link1(:,1:3));
            L2 = patch('faces', s2.F2, 'vertices' ,Link2(:,1:3));
            L3 = patch('faces', s3.F3, 'vertices' ,Link3(:,1:3));
            L4 = patch('faces', s4.F4, 'vertices' ,Link4(:,1:3));
            L5 = patch('faces', s5.F5, 'vertices' ,Link5(:,1:3));
            Tr = plot3(0,0,0,'b.'); % holder for trail paths
            setappdata(0,'patch_h',[L1,L2,L3,L4,L5,Tr]);
            %
            set(L1, 'facec', [0.717,0.116,0.123]);
            set(L1, 'EdgeColor','none');
            set(L2, 'facec', [0.216,1,.583]);
            set(L2, 'EdgeColor','none');
            set(L3, 'facec', [0.306,0.733,1]);
            set(L3, 'EdgeColor','none');
            set(L4, 'facec', [1,0.542,0.493]);
            set(L4, 'EdgeColor','none');
            set(L5, 'facec', [0.216,1,.583]);
            set(L5, 'EdgeColor','none');
            %
            setappdata(0,'ThetaOld',[0,0,0,0]);
            %
        end
        function T = tmat(alpha, a, d, theta)
            % tmat(alpha, a, d, theta) (T-Matrix used in Robotics)
            % The homogeneous transformation called the ""T-MATRIX""
            % as used in the Kinematic Equations for robotic type
            % systems (or equivalent).
            %
            % This is equation 3.6 in Craig's ""Introduction to Robotics.""
            % alpha, a, d, theta are the Denavit-Hartenberg parameters.
            %
            % (NOTE: ALL ANGLES MUST BE IN DEGREES.)
            %
            alpha = alpha*pi/180;    %Note: alpha is in radians.
            theta = theta*pi/180;    %Note: theta is in radians.
            c = cos(theta);
            s = sin(theta);
            ca = cos(alpha);
            sa = sin(alpha);
            T = [c -s*ca s*sa a*c; s c*ca -c*sa a*s; 0 sa ca d; 0 0 0 1];
        end
        function del_app(varargin)
            delete(fig_1);
        end
        function loaddata
            % Loads all the link data from file linksdata.mat.
            % This data comes from a Pro/E 3D CAD model and was made with cad2matdemo.m
            % from the file exchange.  All link data manually stored in linksdata.mat
            [linkdata1]=load('linksdata.mat','s1','s2','s3','s4','s5');
            %Place the robot link 'data' in a storage area
            setappdata(0,'Link1_data',linkdata1.s1);
            setappdata(0,'Link2_data',linkdata1.s2);
            setappdata(0,'Link3_data',linkdata1.s3);
            setappdata(0,'Link4_data',linkdata1.s4);
            setappdata(0,'Link5_data',linkdata1.s5);
        end
    end

Below figure shows desired and actual comes out model.

All others thing, which may be useful (like linksdata file, sw model etc.) I shared on dropbox. Anybody can accesss from there. Dropox link: 
   https://www.dropbox.com/sh/llwa0chsjuc1iju/AACrOTqCRBmDShGgJKpEVAlOa?dl=0
I want to know to connect two components in 3d model in Matla gui. Any study about this will be very helpful. Thanks.
","robotic-arm, matlab, simulation"
How big is the set of kinematics singularities?,"Suppose we have an $n$-DOF robot manipulator and let $q \in \mathbf{R}^n$ denotes a robot joint configuration. Then a singular configuration $q'$ is a configuration at which the Jacobian $J(q')$ does not have maximum rank. Let $S$ be the set of all singular configurations of a given robot
Is there any (general) result regarding characterization of the set $S$? Any work discussing or answering questions such as ""Is $S$ a manifold?"", ""Does $S$ contain only isolated points?"", etc.?
So far I have found quite a few work talking about classification of singular configurations. I think they still do not really answer my questions. Can anyone point me to some related stuff?
","kinematics, singularity"
Single Board Computer/Microcontroller Recommendations for controlling large DC motors?,"Hey looking for recommendations. Right now I am using a sabertooth 2x25 motor controller for my drive train, receiving signals from a raspberry pi. From what I am seeing online - there is a lot of mixed reviews regarding this sort of setup. Everything seems to be running fine on my end, but I am curious to see what is the best optimal way to interface and program a large DC motor. Reasons why I decided to use a pi was because I needed ROS to be set up with my robot to perform autonomous tasks.
I understand that the pi's clock is not powerful enough to give precise PWM output signals, while its running computations at the same time. Is there a board out there that could possibly handle both?
Edit: Around 6A continuous load for the motors, 12 V
","motor, microcontroller"
Mounting gear on a servo,"The outputs of most hobby servos is a spline. To mount a custom gear on the spline so that the servo turns the gear, how should it be best done? I see that some people just screw the gear into the threaded hole in the spline, but wouldn't this be inadequate?

It'd just be the bottom of the screw touching the gear.
","servomotor, servos, gearing"
"I want to code with arduino, but do not have a good computer","Before I buy any arduino products, I want to make sure that I can use this website:
https://create.arduino.cc
to create the code for the arduino. The thing is, I do not know if I can use this to export the code to the arduino. Can someone please tell me if I can use it to program an actual arduino instead of simulating one?
By the way, the I do not have an actual computer, just a chromebook thinkpad (lenovo EDU series), so I can not use windows or apple software, it must be usable on the chrome web browser.
Thanks.
",arduino
How to make a particle filter evaluation function with LIDAR sensing?,"I am currently trying to implement a particle filter an a robot in a view to localize it on a 2D plane (i.e. to determine x, y and its orientation theta ). I am using a LIDAR which give me (alpha, d) with alpha the angle of measurement and d the distance measured at this angle. For now, I can compute the theoretical measures for each of my particle. But I am struggling with the evaluation function (the function that will give me the probability (or weight) of a particle considering the real measures).
Suppose my LIDAR give me 5 values per rotation (0°, 72°, 144°, 216°, 288°), thus I store one rotation in an array (5000mm is my maximum value) :

Real LIDAR value : [5000, 5000, 350, 5000, 5000]
Particle 1 : [5000, 5000, 5000, 350, 5000]
Particle 2 : [5000, 5000, 5000, 5000, 350]

In this example, I want the function to return a higher probability (or weight) for Particle 1 than for Particle 2 (72° error vs 144°). 
For now I am just doing the invert of the sum of the absolute difference between the two value at the same place in the array (e.g. for Particle 1 : 1 / (5000-5000 + 5000-5000 + 5000-350 + 5000-350 + 5000-5000)). The problem with this method is that, in this example, Particle 1 and 2 have the same probability.
So, what kind of function should I use to have the probability of a particle to be the right one with those kind of measurements ?
PS : I am trying to adapt what is in this course : https://classroom.udacity.com/courses/cs373/lessons/48704330/concepts/487500080923# to my problem.
","localization, particle-filter, lidar"
How do you make a device that can pick up nerve signals?,"I am trying to make a robotic arm that mimics the movement of the user's arm, They way I need it to work is to detect nerve signals and send them to an arduino. The arduino would then have a servo motor mimic the movement of the user's arm when the arduino tells it how quickly to rotate and to what point to rotate, based on the user's input.
Any ideas on how this can be done?
","arduino, control, robotic-arm, servomotor"
What method is commonly used in industral manipulators,"Position control versus  torque control. What method is commonly used in industral manipulators
",torque
PID Controller for line following robot,"I had this idea of using the PID Controller as the algorithm of the line following mechanism for my Robot. The problem (which is a nature behavior of the PID controller ) on the line gaps ( where there are no line e.g. 10 cm) the robot doesn't go straight forward but turns right . 
I thought about it a lot , and i couldn't find any better idea for modifying this algorithm to work in this situation but, adding two more sensors and specify the (white area on the 3 sensors  ) as a special situation where the robot should go straight forward .
Now my question is is there any better idea , that i can use ? 
","pid, not-exactly-c"
Arduino + SIM900: Identify if call is received?,"I try to find command to identify if call is received and when user press any button of number on their phone. I use arduino + sim900A + Ethernet. I try alot but still no see command to do that. Are there any way exit ? If you know, please help me.
",arduino
Inverse kinematic solution not giving the same forward kinematics answer,"I'm working on an assignment where I need to derive IK for 5 DOF Youbot kuka robotic arm manipulatorofficial website. I'm using inverse kinematics decoupling and following a geometrical approach using simulink and matlab. The answer of the IK problem is 5 angles but when I apply those angles to the forward kinematics I'm receiving a different coordinates. Is that normal or am I supposed to get the exact coordinates?
I'm using the following Matlab code:
 function [angles, gripperOut, solution] = IK(pos,toolangle,gripperIn)
l=[0.147 0.155 0.135 0.081 0.137];
x = pos(1);
y= pos(2);
z = pos(3);

surfangle = toolangle(2);

 theta1= atan2(y,x); % atan2(y/x) for theta1
 s=z-l(2); %  S = Zc - L2
 r=sqrt(x.^2+y.^2)-l(1);
 D = (r^2 + s^2 -l(3)^2 -l(4)^2)/2*l(3)*l(4);
 %D = (pow2(r)+pow2(s) -pow2(l(3)) -pow2(l(4)))/2*l(3)*l(4);
 D2 = sqrt(1-D.^2); 
if D2<0
    theta3 = 0;
    theta2 = 0;
    solution = 0;
else
 theta3=atan2(D2,D);
 theta2=atan2(r,s)-atan2(l(4)*sin(theta3),l(3)+l(4)*cos(theta3));
 solution = 1;
end

%  R35 = subs(R35,[theta(1) theta(2) theta(3)],[theta1 theta2 theta3]);
%  theta4=atan2(R35(1,3),R35(3,3));
%  theta5=atan2(R35(2,1),R35(2,2));

theta4 = surfangle-theta2-theta3; 
theta4 = atan2(sin(theta4),cos(theta4));
theta5 = toolangle(1);

 angles=[theta1 theta2 theta3 theta4 theta5];
 if(solution == 0)
    angles = [0 0 0 0 0];
end

 gripperOut = gripperIn;

end

as shown I'm kind of fixing the last two angles for the tool so I can avoid using the subs command which is not supported for code generation.
Any help would be very appreciated. 
","inverse-kinematics, matlab, kuka"
Does anyone know of any tutorials on how to control servo with nerve signals?,"I am wanting to build a robot that mimics human movement. But to do so, does anyone have any tutorials on using Muscle Sensors / Nerve signals to control a servo?
","arduino, control, robotic-arm, servomotor, movement"
Optimal trajectory for manipulators using optimal control,"I'm trying to implement direct-multiple shooting method to my problem.
Objective function: tf
constraints       : q<q_max
                    v<v_max (v=dq/dt)
                    a<a_max
                    tau<tau_max (tau=M(q)a+B(q,v)+G(q))
                    C(q)=r_0-|P-P_0|  (obstacle avoidance)

Initial condition q(0)=q_0 (q_0 is given) 
                  q(t_f)=q_f (q_f is given) and 
                  v(0)= 0
                  v(t_f)=0

As I understand from the theory, I have to divide the variables as state variables and control variables.
State variables are: q and v 
Control variable is: tau
In each time interval I'll generate cubic splines which are q(t)=a_0+a_1*t+a_2*t^2+a_3*t^3

Could you help me how I will implement it? I don't understand what is the ODE here and how I should construct the algorithm?
Are there any example about it?
edit to make the equations clear I'll rewrite them here again:
based on the link
state variables:
x1(t) = (q1(t) , ··· ,qn(t))^T and x2(t) = (q˙1(t) , ··· ,q˙n(t))^T.  and derivatives of the state variables are equal to x˙(t) = f(x(t) ,u(t)) where f is
f(x(t), u(t)) = ((q˙1(t), . . . , q˙5(t))^T;
                  M(x(t))−1· (u(t) − N(x(t))) 
I don't know how to insert cubic polynomials in that equation system and how to solve ODE Will it be like [T,X]=ode45('f', [0 t_f], [q_0 q_f]) 
",manipulator
Can an arduino control a large servo motor?,"A I wanting to build a robotic hand, a big one at that. Similar to this one but in a much larger scale:
http://www.instructables.com/id/DIY-Robotic-Hand-Controlled-by-a-Glove-and-Arduino/
Does it really matter how large the servo is, just as long as it has a power source? Or are there other things I need to know?
","arduino, robotic-arm, servomotor"
Add hardware reset button for Create2,"Is there any way to add a reset button to the Create2 that would be the equivalent  of temporarily disconnecting the battery? 
",irobot-create
Do I need gyro sensor to make sure my automower turn 180deg if I already used 9DOF IMU?,"I am doing a project to build an autonomous lawn mower and I need to decide several type of sensor to complete the features. one of my features is the vehicles need to stop running when someone held it on the air. so i decide to use 9 DOF IMU for this features. 
as i know that 9 DOF IMU already have 3 axes magnetometer that can read the angular yaw position. so i just confused that do I still need to add another gyro sensor to make sure that my lawn mower do a turning 180deg? 
Thanks for any words on this.  
","imu, gyroscope"
About official robotics system toolbox Matlab and Peter Corke Robotics toolbox matlab.,"I came to know that matlab released robotics system toolbox in 2016 version but I'm using Matlab 2014b. At that time I installed Peter Corke Robotics toolbox matlab and start working on it. I develop few GUI and wrote others code too using the same Peter Corke Robotics toolbox. But now I want to install new official version of robotics toolbox. But my doubt is if I install new version then what happen to older one. Will I  able to run my old codes (which used old toolbox) on matlab after installing new official version or it may vanish older one. Will matlab shows error in my codes or GUI after installing new version. I want to work with both toolbox.
Thanks.
","robotic-arm, matlab"
Help with fusing multiple measurements with known covariances,"I am working on an estimation application for multiple robots, each of which uses measurements from various sources to calculate position and orientation data. For now, I am looking at about three sources: 

Orientation from an IMU
Both position and orientation from a camera
Position and orientation from relative measurements w.r.t another robot.

Naturally, the most widely used technique that combines a model with measurements is something like an EKF. But in my case, the orientation data (IMU) comes from an autopilot, which already has an EKF onboard, and hence provides covariance estimates as well. The vision based estimation (both individual and relative) are computed through a few iterations of bundle adjustment, and the bundle adjustment solver also provides covariance of its resultant estimate. Finally, I am not really interested in utilizing a complex, non linear model for the robot, but mostly in just fusing the measurements to provide one final pose estimate. 
I have read about the concept of 'covariance intersection' in the context of Kalman filtering, which has been implemented in cooperative pose estimation using multiple data sources. I was wondering if I could get some advice as to whether Kalman filters etc. would still be applicable in my case, and if so, how to adapt them?
","localization, kalman-filter, pose"
How to get Max Torque on Robot arm 's Joints (RRR),"Suppose I have a Robot Arm with 3 joint(revolute)( 3 DOF) as shown below , and I'm given 
-The ranges of each joint q1,q2,q3
-The lengths of each link L1,L2,L3
-Load on the end effector [Fx Fy M]
how can I calculate the max torque at each joint of the robot ? 
*image below describes the robot configuration.
If I missed any details , mention it in a comment and I will add it .
Thanks in advance

","robotic-arm, matlab, torque, jacobian"
how to read and write data with pyserial at same time,"How can i make pyserial read and write at same time or at same program? Right now i only know how to either write or to read but not both simultaneiously. So how can i do it? I tried this simple code:
int incomingByte = 0;

void setup() {

Serial.begin(9600); // opens serial port, sets data rate to 9600 bps

}

void loop() {

incomingByte = Serial.read(); // read the incoming byte:

Serial.print("" I received:"");

Serial.println(incomingByte);

}

and python program is:
import serial
ser = serial.Serial('/dev/ttyACM0',9600)
ser.write(""333"")
ser.close()
ser.open()
data = ser.readline()
print data
ser.close()

","arduino, serial"
Why to combine a Raspberry Pi (or similar board computer) and an Arduino for controlling motors?,"I am planning to build a omnidirectional holonomic robot, and checking what I should use for the hardware, I saw many people using a Raspberry to compute most stuff, which in turn calls an Arduino to control the motors. But since the Arduino will have to drive some H-bridges, and the board computer is probably a lot more powerful, it seems to me that those drivers for the motors could be controlled by the Raspberry-ish board.
What am I missing?
Why so many people use both at the same time?
","arduino, control, motor, raspberry-pi"
Gazebo joint->SetForce() call applies forces to the whole model. How to circumvent that?,"The following code applies a force to the joint in an update method call. The problem is that the force seems to dissipate / is applied consecutively to other parts of the model, specifically the chassis, which holds the rotating laser. How can I circumvent that? 
The chassis shold be a moving vehicle so I can't just fix it to the ground plane, using a fixed joint.
This is my onUpdate() from within the gazebo plugin. Essentially its making the joint rotate back and forth on a specified axis.
public: void OnUpdate(const common::UpdateInfo & /*_info*/){

    rotation = this->joint->GetAngle(0);
    this->joint->SetStopDissipation(0,0);
    double degree = rotation.Degree();
    if (degree <= -90) {
            this->joint->SetForce(rotationAxis,effort*2);
    }
    else if (degree >= 90) {
            this->joint->SetForce(rotationAxis,-effort*2);
    }
    std::cerr << degree << ""\n""; }

The definition from the model.sdf is this:
    <joint name=""back_and_forth_joint"" type=""revolute"">
        <child>laser</child>
        <parent>chassis</parent>
        <axis>
            <xyz>1 0 0</xyz>
            <limit>
                <lower>-1.57079633</lower>
                <upper>1.57079633</upper>
            </limit>
        </axis>
    </joint>

Thanks.
Update: 
One of the possibilities is to simply add mass to the chassis like so:
        <link name='chassis'>
        <pose>0 0 .1 0 0 0</pose>
        <inertial>
          <mass>10</mass>
        </inertial>

","ros, gazebo, joint"
1 motor and 180 degree servo or just 2 motors?,"I am planning to use this motor: http://www.adrirobot.it/robot_kit/feetech_FT-DC-002/TGP01D-A130_TT-Motor.pdf Model: TGP01DA130 12215-48.
But I have some questions: How much a robot will be slower if it has 1 motor and one 180 degree servo instead of just 2 motors? And which one is the best option?
","mobile-robot, raspberry-pi, servomotor"
Is pyserial real time,"I am using arduino with FreeRTOS and a computer with patched xenomai linux. I am using python library called pyserial to communicate with arduino. Right now I am using simple servo motors. I want to be sure that whether the communication between the arduino and my main computer is real time or not. 
How can I check this, properly. I want hard real time communication between arduino and computer for balancing robot.
","arduino, ros, python, real-time"
Stereo camera Vs Kinect,"Any one can advice me on the ideal perception sensors for pick and place application using a robotic manipulator with ROS support. I have generally looked at things like Kinect and stereo cameras (Bumblebee2) which provide depth that can be used with PCL for object recognition and gripper positioning. Are there any other sensors would be preferred for such application and if not what are the drawbacks of stereo cameras in comparison to Kinect or other sensing capability. 
Thanks,
Alan
","sensors, robotic-arm, kinect, stereo-vision"
Biped Robot not getting on one foot,"I am creating a biped robot with an arduino and I have the lower part of the body complete, the legs and hip joints. Before I am going to 3D print the top part of the body I wanted to make sure that it would walk. So when I started the walking tests I could not get the robot to go onto one foot no matter what I tried. The foot would just raise up the body and not lift the other foot here is a picture of what is happening:
and this is what I want to happen:

Does it just need more weight on the top or is there a specific sequence of movements that I can do? I have two hip servo that moves left and right, a knee servo and a foot servo that move side to side.
","mobile-robot, arduino, servomotor, servos, walking-robot"
Why does one IMU axis influence another?,"For a home robotics project I just bought a BerryIMU to connect it to my Raspberry Pi. After hooking it up I ran the provided Python code to read out some values while moving it around.
If I keep the IMU (more or less) straight and in Northern direction I get the following line of output:
ACCX Angle  0.60 
ACCY Angle  4.58       
GRYX Angle -125.14  
GYRY Angle 114.15  
GYRZ Angle 93.74     
CFangleX Angle -0.26   
CFangleY Angle  4.45  
HEADING   1.02  
tiltCompensatedHeading  3.73  
kalmanX  0.48   
kalmanY  4.43

I am most interested in the compass (in 360 degrees), and how much it tilts right/left and front/back.
As far as I understand, the tiltCompensatedHeading tells me that it points 3.73 degrees right of the magnetic north. And I think kalmanX and kalmanY should give me the tilting of the IMU to the left/right (X) and to the front/back (Y) (compensated by a Kalman filter for smoothening).
So I played around with it and saw what the numbers did. In the images below I look slightly down on it. I hope the description on it explains how you see it.

From what you see here the X and Y degrees independently behave as I would expect them to. But what I don't understand is why ""the other one"" is always between 90 and 130. So if I tilt it 90 degrees forward I would expect 
X ≈ 0
Y ≈ 90

similarly, if I tilt it 90 degrees backward, I would expect 
X ≈ 0
Y ≈ -90

Instead X is around 100 for both of them and I really don't understand why it's not around 0.
Does anybody see the logic in this? What am I missing?
","sensors, kalman-filter, imu, gyroscope, noise"
Arduino uno A0 pin not working properly,"void setup() {
  pinMode(A0, INPUT);
  Serial.begin(9600);
}
void loop() {
  if (digitalRead(A0)== HIGH) {
    Serial.println(""YES"");
  }
}
Not giving any input when i am supplying input via push button. I have 5 v supply through 10 kohm resistor to push button and then have other side to A0 and a led to ground. It takes input when I take out wire connected to A0 and just leave it unconnected.
",arduino
Matlab Inverse Kinematics 6 DOF,"I'm trying to write an inverse kinematics Matlab code for a 6 DOF robotic arm that has the following link parameters:
Twist angle (alpha): [-90, 0, 90, -90, 90, 0]
Link length (a): [0, 0.5, 0, 0, 0, 0]
Offset distance (d): [0, 0.25, 0, 1, 0, 0.5]
and Px, Py, Pz are the following [1,1,0]
I'm using the following equations for theta 1,2 and 3 values (closed form solution):

As seen in the equations theta 1 and 2 have 2 two roots (2 possible solutions) thus, the robot has eight groups of inverse kinematics solutions. How do I modify my code to select the ideal solution for theta ? 
%Theta 1
     theta1 = (atan2(real(py),real(px)))-atan2(real(d2),real(sign1*sqrt(px^2+py^2-d2^2)));

 c1 = cos(theta1);
 s1 = sin(theta1);

 %Theta 2
 A = (c1*px)+(s1*py);
 B = (A^2+pz^2+a2^2-d4^2)/(2*a2);

 theta2 = (atan2(real(A),real(pz)))-atan2(real(B),real(sign2*sqrt(A^2+pz^2-B^2)));

 c2 = cos(theta2);
 s2 = sin(theta2);

 %Theta 3
 A1 = (c2*px)+(s2*py);
 theta3 = (atan2(real(A1-(a2*c2)),real(pz+(a2*s2)))) - theta2;

 c3 = cos(theta3);
 s3 = sin(theta3);

","robotic-arm, kinematics, inverse-kinematics, algorithm, matlab"
How can I compensate for pendulum and cart motion when using an accelerometer to detect the tilt angle?,"I have an accelerometer mounted to an inverted pendulum (i.e. a cart-pole robot) which I'm using to measure the tilt angle from the vertical upright position (+y direction).  If the inverted pendulum is held motionless at a fixed angle, the accelerometer essentially detects the gravity direction as a vector $(g_x,g_y)$ and the tilt angle $\theta$ can be determined by
$$\theta=tan^{-1}(\frac{g_x}{\sqrt{g_x^2+g_y^2}}).$$
However, if the inverted pendulum is in motion (e.g. if I'm dynamically trying to balance it like a cart-pole system), the pendulum itself is accelerating the accelerometer in a direction which is not necessarily the gravity direction.  Acceleration from the cart may also distort the measurement of the gravity direction by the accelerometer as well.  In such a case, I don't think the formula above is necessarily appropriate.  
Of course, I'm not 100% sure that the swinging motion of the pendulum and the forward-backward motion of the cart are significant enough to distort the angle measurement on the accelerometer.  I presume that if I start with initial conditions close to $\theta=0$ on the pendulum, it shouldn't be that significant.  Nonetheless, if the pendulum is significantly perturbed by a disturbance force, I think the accelerometer's measurements must be compensated in some way.
How can I compensate for pendulum and cart accelerations when using an accelerometer to detect the tilt angle?  
","sensors, accelerometer, balance"
Is there a way to determine which degrees of freedom are lost in a robot at a singularity position by looking at the jacobian?,"For a 6DoF robot with all revolute joints the Jacobian is given by:
$$
\mathbf{J} = 
\begin{bmatrix}
\hat{z_0} \times (\vec{o_6}-\vec{o_0}) & \ldots & \hat{z_5} \times (\vec{o_6}-\vec{o_5})\\
\hat{z_0} & \ldots & \hat{z_5}
\end{bmatrix}
$$
where $z_i$ is the unit z axis of joint $i+1$(using DH params), $o_i$ is the origin of the coordinate frame connected to joint $i+1$, and $o_6$ is the origin of the end effector.  The jacobian matrix is the relationship between the Cartesian velocity vector and the joint velocity vector:
$$
\dot{\mathbf{X}}=
\begin{bmatrix}
\dot{x}\\
\dot{y}\\
\dot{z}\\
\dot{r_x}\\
\dot{r_y}\\
\dot{r_z}
\end{bmatrix}
=
\mathbf{J}
\begin{bmatrix}
\dot{\theta_1}\\
\dot{\theta_2}\\
\dot{\theta_3}\\
\dot{\theta_4}\\
\dot{\theta_5}\\
\dot{\theta_6}\\
\end{bmatrix}
=
\mathbf{J}\dot{\mathbf{\Theta}}
$$
Here is a singularity position of a Staubli TX90XL 6DoF robot:

$$
\mathbf{J} = 
\begin{bmatrix}
          -50     &    -425     &    -750      &      0     &    -100      &      0\\
       612.92     &       0     &       0      &      0     &       0      &      0\\
            0     & -562.92     &       0      &      0     &       0      &      0\\
            0     &       0     &       0      &      0     &       0      &      0\\
            0     &       1     &       1      &      0     &       1      &      0\\
            1     &       0     &       0      &     -1     &       0      &     -1
\end{bmatrix}
$$
You can easily see that the 4th row corresponding to $\dot{r_x}$ is all zeros, which is exactly the lost degree of freedom in this position.
However, other cases are not so straightforward.

$$
\mathbf{J} = 
\begin{bmatrix}
          -50   &   -324.52  &    -649.52   &         0   &   -86.603   &         0\\
       987.92   &         0  &          0   &         0   &         0   &         0\\
            0   &   -937.92  &       -375   &         0   &       -50   &         0\\
            0   &         0  &          0   &       0.5   &         0   &       0.5\\
            0   &         1  &          1   &         0   &         1   &         0\\
            1   &         0  &          0   &    -0.866   &         0   &    -0.866
\end{bmatrix}
$$
Here you can clearly see that joint 4 and joint 6 are aligned because the 4th and 6th columns are the same.  But it's not clear which Cartesian degree of freedom is lost (it should be a rotation about the end effector's x axis in red).
Even less straightforward are singularities at workspace limits.

$$
\mathbf{J} = 
\begin{bmatrix}
          -50     &     650   &       325  &          0    &        0     &       0\\
       1275.8     &       0   &         0  &         50    &        0     &       0\\
            0     & -1225.8   &   -662.92  &          0    &     -100     &       0\\
            0     &       0   &         0  &    0.86603    &        0     &       1\\
            0     &       1   &         1  &          0    &        1     &       0\\
            1     &       0   &         0  &        0.5    &        0     &       0
\end{bmatrix}
$$
In this case, the robot is able to rotate $\dot{-r_y}$ but not $\dot{+r_y}$.  There are no rows full of zeros, or equal columns, or any clear linearly dependent columns/rows.  
Is there a way to determine which degrees of freedom are lost by looking at the jacobian?
","robotic-arm, inverse-kinematics, industrial-robot"
cameracalibrator.py doesn't draw gui,"I'm trying to calibrate a single camera using cameracalibrator, as explained in http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration
Despite being able to see the camera using rosrun image_view image_view image:=/camera/image_raw, I cannot start cameracalibrator. 
After issuing rosrun camera_calibration cameracalibrator.py --size 9x6 --square 0.05   images:=/camera/image_raw camera:=/camera, I get the following output:
('Waiting for service', '/camera/set_camera_info', '...')
OK
init done

The window appears, but contents are not draw. There's no video feed nor slider controls when I use that command. How do I solve that? This is using Kinetic on Ubuntu 16.04. All the questions/answers I've found so far are related to the service not existing.
If it helps:
$ rostopic list
/camera/camera_info
/camera/image_raw
/camera/image_raw/compressed
/camera/image_raw/compressed/parameter_descriptions
/camera/image_raw/compressed/parameter_updates
/camera/image_raw/compressedDepth
/camera/image_raw/compressedDepth/parameter_descriptions
/camera/image_raw/compressedDepth/parameter_updates
/camera/image_raw/theora
/camera/image_raw/theora/parameter_descriptions
/camera/image_raw/theora/parameter_updates
/camera/mycam/parameter_descriptions
/camera/mycam/parameter_updates
/rosout
/rosout_agg

",ros
Camera selection for long range stereo vision system (up to 100 meters),"I want to implement a real-time stereo vision system for long range (up to 100m) depth estimation. I know that there are some general considerations as described in this SOV post. I have seen some typical cameras such as zed stereo cameras which has limited range (max. 20m).
Maximum allowable baseline distance between cameras is 0.5m and about field of view, i think that a camera with 8mm (or 12 or 16) focal length can provide reasonable FOV. I need depth resolustions at 100m to be 1% or maybe lower. 
Are ip cameras proper for such applications? If no, Can anyone please suggest proper cameras for application in long range stereo vision system?
I will be grateful for any information you can provide.
","cameras, stereo-vision, real-time, errors, rangefinder"
Covariance analysis,"I am a SLAM newbie here. 
The thing i want to do is to perform a covaraince analysis, to ensure whether the estimation of the covariance is done correctly or not.
The localization output creates a log file which includes localization and mapping information also the covariance of the localization. 
I extracted the covariance matrices, acquired the diagonal elements and tried to plot the elements: Vslam estimation, GPS(ground truth), Vslam+σ and Vslam-σ. 
What i was expecting was that the ground truth should lie within the Vslam+σ and Vslam-σ, but the result i am getting is really weird. 
According to me the uncertainity should grow over time.  
I am providing a link to the folder which contains the generated output files, the expected result visualization and the actual plot that i am getting. 
DATA
The covariance is calculated from intersecting the object covariances i.e the covariance intersection method.
","slam, uav"
iRobot Create2+Arduino: Make the robot stop when facing an obstacle,"I have been working on writing a code to control the iRobot Create movements (forward, backward, right, left and stop) from serial monitor and finally I got the correct code. I was trying to understand how to make stop moving when it face an obstacle but I couldn't get it. Also, I didn't know how to make move for a specific distance. Could anyone help me with this?
Here is my code:
#include ""Arduino.h""
#include ""Morse.h""
#include <SoftwareSerial.h>
#define rxPin 10
#define txPin 11
SoftwareSerial softSerial = SoftwareSerial(rxPin, txPin);
char inByte = 0; // incoming serial byte

irobot::irobot(int pin)
{
  pinMode(pin, OUTPUT);
  _pin = pin;
}


void irobot::Begin()
{
   delay(2000); // Needed to let the robot initialize
// define pin modes for software tx, rx pins:
 pinMode(rxPin, INPUT);
 pinMode(txPin, OUTPUT);
// start the the SoftwareSerial port 19200 bps (robotís default)
 softSerial.begin(19200);
// start hardware serial port
Serial.begin(19200);
softSerial.write(128); // This command starts the OI.
softSerial.write(131); // set mode to safe (see p.7 of OI manual)
delay(2000);
}

void irobot::runIt()
{
    softSerial.write(142); // requests the OI to send a packet of
    // sensor data bytes
    softSerial.write(9); // request cliff sensor value specifically
    delay(250); // poll sensor 4 times a second
    if (Serial.available() > 0) {
        inByte = Serial.read();
        if(inByte == '8')
        {
            goForward();
        }
        if(inByte == '2'){
            goBackward();
        }
        if(inByte == '4')
            goLeft();
        if(inByte == '6')
            goRight();
        if(inByte == '5')
            stop();
    }
    Serial.println(inByte);
}


void irobot::goForward()
{
   // Drive op code
    softSerial.write(137);
    // Velocity (-500 ñ 500 mm/s)
    softSerial.write((byte)0);
    softSerial.write((byte)200);
    //Radius (-2000 - 2000 mm)
    softSerial.write((byte)128);
    softSerial.write((byte)0);
}



void irobot::goBackward()
{
    // Drive op code
    softSerial.write(137);
    // Velocity (-500 ñ 500 mm/s)
    softSerial.write(255);
    softSerial.write(56);
    //Radius (-2000 - 2000 mm)
    softSerial.write((byte)128);
    softSerial.write((byte)0);
}



void irobot::goLeft()
{
    // Drive op code
    softSerial.write(137);
    // Velocity (-500 ñ 500 mm/s)
    softSerial.write((byte)0);
    softSerial.write((byte)200);
    //Radius (-2000 - 2000 mm)
    softSerial.write((byte)0);
    softSerial.write((byte)1);
}


void irobot::goRight()
{
    // Drive op code
    softSerial.write(137);
    // Velocity (-500 ñ 500 mm/s)
    softSerial.write((byte)0);
    softSerial.write((byte)200);
    //Radius (-2000 - 2000 mm)
    softSerial.write((byte)255);
    softSerial.write((byte)255);
}

void irobot::stop()
{
    softSerial.write(137);
    softSerial.write((byte)0);
    softSerial.write((byte)0);
    //Radius (-2000 - 2000 mm)
    softSerial.write((byte)0);
    softSerial.write((byte)0);
}

","arduino, irobot-create"
Motion input in Simmechanics Matlab.,"I have this figure in which motion is given to revolute joint in Simmechanics.  
In this, constant 2 is used and then integration is used. I want to  know what will be effect of 2 here and how integration is happening. What is the actual value of input here. Is it 2 degree or 2 degree/sec or something else. Is 2 is the upper limit of motion or lower limit? What will happen if replace 2 with 5.
Thanks. 
","matlab, simulation, simulator"
rosserial_arduino tutorial servo controller not working properly,"I am following this tutorial, but after I publish the /servo topic to move the motor by giving command rostopic pub servo std_msgs/UInt16 90, the motor moves but the node shuts down showing this error:
[INFO] [WallTime: 1489221090.819442] ROS Serial Python Node

[INFO] [WallTime: 1489221090.823630] Connecting to /dev/ttyACM0 at 57600 baud
[INFO] [WallTime: 1489221092.949300] Note: subscribe buffer size is 280 bytes
[INFO] [WallTime: 1489221092.949652] Setup subscriber on servo [std_msgs/UInt16]
Traceback (most recent call last):
  File ""/home/robot/ros_arduino_ws/src/rosserial/rosserial_python/nodes/serial_node.py"", line 85, in <module>
    client.run()
  File ""/home/robot/ros_arduino_ws/src/rosserial/rosserial_python/src/rosserial_python/SerialClient.py"", line 503, in run
    self.requestTopics()
  File ""/home/robot/ros_arduino_ws/src/rosserial/rosserial_python/src/rosserial_python/SerialClient.py"", line 389, in requestTopics
    self.port.flushInput()
  File ""/usr/lib/python2.7/dist-packages/serial/serialposix.py"", line 500, in flushInput
    termios.tcflush(self.fd, TERMIOS.TCIFLUSH)
termios.error: (5, 'Input/output error')

Here after the message Setup subscriber on servo, I published the topic /servo with value 1-180, than the error came. After this error comes when I try to again launch the same rosserial_python node it gives error saying:
[ERROR] [WallTime: 1489221031.517359] Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino

After I reinsert the arduino port, this error goes, but the first error comes again. Since all other tutorials are running properly I think the problem is not with rosserial package. Because I have tried them from both debian and source as I found in this answer, but the same error comes. So how can I remove this error?
","arduino, ros"
Inverse kinematics for a four-legged robot,"I want to make a 4 legged robot like spider or dog, but I don't know how I can use kinematics to make it walk and run. I didn't find any resource to know how these types of robots walk and run by balancing their center of gravity balanced in each step and how they move their legs to move any direction. Right now I am trying to make a robot in gazebo simulation and test its working, only then I'll go for real hardware.
",inverse-kinematics
Question about EKF covariance equations,"I am trying to perform uncertainty aware planning, where my planner tries to connect start and goal in such a way that the resultant path provides the least covariance at the end. This is inspired by techniques such as LQG. 
The way I 'estimate' what covariance would result from a certain path is by using the EKF equations while assuming maximum likelihood observations, and I am trying to test what's called the 'light-dark' scenario that was used in many papers: where a 2D robot is traversing the environment, and there's a specific region where it would receive measurements that would reduce the covariance greatly. Hence, the uncertainty aware planner tries to take the robot to this 'light' area, receive good measurements, and then proceed to the goal. As seen in this picture from [1], the final covariance drops significantly by using this modified path, than the shortest path from start to goal (ignore the red line).
https://i.stack.imgur.com/dPYDC.jpg
I am trying to replicate similar behavior with my planner, which does result in covariance reduction compared to some other path that doesn't visit the good area, but the reduction isn't significant at all. In my sample environment, which is a 20x20 grid, the X coordinate of 17 represents the 'light' area, so I express the environmental noise as a matrix which is written as 
$\begin{bmatrix} x-17+0.01 && 0 \\ 0 && x-17+0.01 \end{bmatrix}$,
, hence would get a (0.01,0.01) matrix whenever I'm precisely at the x=17 column in the grid. Problem is, my result looks something like this, with the covariance ellipses plotted in red (the span of which I obtain from the Eigen values of the matrix). 

Although the robot does visit the good area thanks to the planner, my covariance still increases rapidly once I leave: so I'm guessing I am making a mistake in my EKF equations. This is how I am 'simulating' the covariance at coordinates x2, when stepping from x1 to x2 with P1 being the covariance at x1 (adapted from equations in some open source code).
function P2 = predictCovariance(P1, x1, x2)
H = eye(2);
u = x2-x1; 

G = [u(1) 0 ; 0 u(2)];
Q = eye(2);
R = eye(2);
M = [(x2(1)-17 + 0.01) 0 ; 0 (x2(1)-17 + 0.01)];
P = P1 + G*Q*G';
S = H*P*H' + M*R*M';
K = (P*H')/S;
P2 = (eye(2)-K*H)*P;
end

[1] Van Den Berg, Jur, Sachin Patil, and Ron Alterovitz. ""Motion planning under uncertainty using iterative local optimization in belief space."" The International Journal of Robotics Research 31.11 (2012): 1263-1278.
",kalman-filter
Help with setting up Inverse Kinematics,"I'm working through the Inverse Kinematic example for the Unimation PUMA 560  from Introduction to Robotics by Craig. In it he specifies the IK equations like so:

In my software program I have three sliders on the screen that will give me the rotation of the end point in x, y, z like so (this is in Unity):

Each one of these sliders will control a float variable in the code (C#) and I can read these into my script (Using Unity 5). I am trying to replicate the inverse kinematic solution for this PUMA robot inside Unity, so that for a given position and rotation of the end effector the link rotations will update accordingly. I have already written out the IK equations that Craig specified in the example to calculate theta(i), but how do I ""read"" the slider values and ""input"" them to these equations? If I am not making any sense I apologize, I have been chipping away at this for some time and hit a mental blank wall. Any advice appreciated.
Edit: So in my near-delirious state I have not posited my question properly. So far, these are the equations I have written so far in code:
public class PUMA_IK : MonoBehaviour {
GameObject J1, J2, J3, J4, J5, J6;

public Vector3 J2J3_diff, J3J4_diff;

public Slider px_Slider;    
public Slider py_Slider;    
public Slider pz_Slider;    
public Slider rx_Slider;    
public Slider ry_Slider;    
public Slider rz_Slider;    

public float Posx, Posy, Posz, Rotx, Roty, Rotz;

float a1, a2, a3, a4, a5, a6;   //Joint twist
float r1, r2, r3, r4, r5, r6;   //Mutual perpendicular length
float d1, d2, d3, d4, d5, d6;   //Link offset
public float t1, t2, t23, t3, t4, t5, t6;   //Joint angle of rotation

public float J1Rot, J2Rot, J3Rot, J4Rot, J5Rot, J6Rot;

float r11, r21, r31, r12, r22, r32, r13, r23, r33, c23, s23, Px, Py, Pz, phi, rho, K;

int pose;   //1 - left hand, 2 = right hand

// Use this for initialization
void Start ()
{
    pose = 1;

    J1 = GameObject.FindGameObjectWithTag(""J1"");
    J2 = GameObject.FindGameObjectWithTag(""J2"");
    J3 = GameObject.FindGameObjectWithTag(""J3"");
    J4 = GameObject.FindGameObjectWithTag(""J4"");
    J5 = GameObject.FindGameObjectWithTag(""J5"");
    J6 = GameObject.FindGameObjectWithTag(""J6"");

    J2J3_diff = J3.transform.position - J2.transform.position;
    J3J4_diff = J4.transform.position - J3.transform.position;



    //Init modified DH parameters

    //Joint twist

    a1 = 0;
    a2 = -90;
    a3 = 0;
    a4 = -90;
    a5 = 90;
    a6 = -90;

    //Link length

    r1 = 0;
    r2 = Mathf.Abs(J2J3_diff.x);
    r3 = Mathf.Abs(J3J4_diff.x);
    r4 = 0;
    r5 = 0;
    r6 = 0;

    //Link offset

    d1 = 0;
    d2 = 0;
    d3 = Mathf.Abs(J2J3_diff.z);
    d4 = Vector3.Distance(J4.transform.position, J3.transform.position);
    d5 = 0;
    d6 = 0;

}

void Update ()
{
    Posx = px_Slider.value;
    Posy = py_Slider.value;
    Posz = pz_Slider.value;

    Rotx = rx_Slider.value;
    Roty = ry_Slider.value;
    Rotz = rz_Slider.value;

    Px = Posx;
    Py = Posy;
    Pz = Posz;

    c23 = ((cos(t2)*cos(t3)) - (sin(t2)*sin(t3)));
    s23 = ((cos(t2)*sin(t3)) + (sin(t2)*cos(t3)));

    rho = Mathf.Sqrt(Mathf.Pow(Px, 2) + Mathf.Pow(Py, 2));
    phi = Mathf.Atan2(Py, Px);

    if (pose == 1)
    {
        t1 = Mathf.Atan2(Py, Px) - Mathf.Atan2(d3, Mathf.Sqrt(Mathf.Pow(Px, 2) + Mathf.Pow(Py, 2) - Mathf.Pow(d3, 2)));
    }

    if (pose == 2)
    {
        t1 = Mathf.Atan2(Py, Px) - Mathf.Atan2(d3, -Mathf.Sqrt(Mathf.Pow(Px, 2) + Mathf.Pow(Py, 2) - Mathf.Pow(d3, 2)));
    }

    K = (Mathf.Pow(Px, 2)+ Mathf.Pow(Py, 2) + Mathf.Pow(Px, 2) - Mathf.Pow(a2, 2) - Mathf.Pow(a3, 2) - Mathf.Pow(d3, 2) - Mathf.Pow(d4, 2)) / (2 * a2);

    if (pose == 1)
    {
        t3 = Mathf.Atan2(a3, d4) - Mathf.Atan2(K, Mathf.Sqrt(Mathf.Pow(a2, 2) + Mathf.Pow(d4, 2) - Mathf.Pow(K, 2)));
    }

    if (pose == 2)
    {
        t3 = Mathf.Atan2(a3, d4) - Mathf.Atan2(K, -Mathf.Sqrt(Mathf.Pow(a2, 2) + Mathf.Pow(d4, 2) - Mathf.Pow(K, 2)));
    }

    t23 = Mathf.Atan2(((-a3 - (a2 * cos(t3))) * Pz) - ((cos(t1) * Px) + (sin(t1) * Py)) * (d4 - (a2 * sin(t3))), ((((a2 * sin(t3)) - a4) * Pz) - ((a3 + (a2 * cos(t3))) * ((cos(t1) * Px) + (sin(t1) * Py)))));

    t2 = t23 - t3;

    if (sin(t5) != 0)   //Joint 5 is at zero i.e. pointing straight out
    {
        t4 = Mathf.Atan2((-r13 * sin(t1)) + (r23 * cos(t1)), (-r13 * cos(t1) * c23) + (r33 * s23));
    }

    float t4_detection_window = 0.00001f;

    if ((((-a3 - (a2 * cos(t3))) * Pz) - ((cos(t1) * Px) + (sin(t1) * Py)) < t4_detection_window) && (((-r13 * cos(t1) * c23) + (r33 * s23)) < t4_detection_window))
    {
        t4 = J4Rot;
    }

    float t5_s5, t5_c5; //Eqn 4.79

    t5_s5 = -((r13 * ((cos(t1) * c23 * cos(t4)) + (sin(t1) * sin(t4)))) + (r23 * ((sin(t1) * c23 * cos(t4)) - (cos(t1) * sin(t4)))) - (r33 * (s23 * cos(t4))));

    t5_c5 = (r13 * (-cos(t1) * s23)) + (r23 * (-sin(t1) * s23)) + (r33 * -c23);

    t5 = Mathf.Atan2(t5_s5, t5_c5);

    float t5_s6, t5_c6; //Eqn 4.82

    t5_s6 = ((-r11 * ((cos(t1) * c23 * sin(t4)) - (sin(t1) * cos(t4)))) - (r21 * ((sin(t1) * c23 * sin(t4)) + (cos(t1) * cos(t4)))) + (r31 * (s23 * sin(t4))));

    t5_c6 = (r11 * ((((cos(t1) * c23 * cos(t4)) + (sin(t1) * sin(t4))) * cos(t5)) - (cos(t1) * s23 * sin(t5)))) + (r21 * ((((sin(t1) * c23 * cos(t4)) + (cos(t1) * sin(t4))) * cos(t5)) - (sin(t1) * s23 * sin(t5)))) - (r31 * ((s23 * cos(t4) * cos(t5)) + (c23 * sin(t5))));

    t6 = Mathf.Atan2(t5_s6, t5_c6);

    //Update current joint angle for display

    J1Rot = J1.transform.localRotation.eulerAngles.z;
    J2Rot = J2.transform.localRotation.eulerAngles.y;
    J3Rot = J3.transform.localRotation.eulerAngles.y;
    J4Rot = J4.transform.localRotation.eulerAngles.z;
    J5Rot = J5.transform.localRotation.eulerAngles.y;
    J6Rot = J6.transform.localRotation.eulerAngles.z;

}

void p(object o)
{
    Debug.Log(o);
}

float sin(float angle)
{
    return Mathf.Rad2Deg * Mathf.Sin(angle);
}
float cos(float angle)
{
    return Mathf.Rad2Deg * Mathf.Cos(angle);
}

}
The issue is not with the mathematics of what is going on per se, I am just confused at how I interface the three values of the X, Y, and Z rotation for the sliders (which represent the desired orientation) with these equations. For the translation component it is easy, the slider values are simply equal to Px, Py and Pz in the IK equation set. But in his equations he references r11, r23, etc, which are the orientation components. I am unsure how to replace these values (r11, r12, etc) with the slider values.
Any ideas?
Edit 2: I should also say that these sliders would be for positioning the tool center point. The XYZ sliders will give the translation and the others would give the orientation, relative to the base frame. I hope this all makes sense. The goal is to be able to use these sliders in a similar fashion to how one would jog a real robot in world mode (as opposed to joint mode). I then pass these calculated angle values to the transform.rotation component of each joint in Unity.
So what I am really asking is given the three numbers that the rotation sliders produce (XRot, YRot and ZRot), how do I plug those three numbers into the IK equations?
",inverse-kinematics
How to choose the right propeller/motor combination for a quadcopter?,"There are many sites which explain briefly this problem and even propose combinations. I however would like a much more detailed explanation. What is going to give my quad the most agility? Do I need bigger motors/props in a heavy quad to achieve the same level of agility than in a lighter quad?
EDIT:
Here is what I have understood on the subject:

A quadcopter doesn't need high revving motors as there are 4 propellers providing thrust and high revving motors require more battery power.
Larger propellers give more thrust per revolution from the motor.

The question is focused more on the general characteristics of various combinations but some specific questions do spring to mind: 

For a given combination what would be the effect of upgrading propeller size in comparison to installing higher revving motors?
What changes would need to be made to lift a heavier quad?
How can I achieve more agility in my quad?

",quadcopter
"The 3 key components of a robot are controller, servo, and reducer. Can someone give us an ""official"" explanation of what they do respectively?","I've googled a lot but wasn't able to find official definitions of these 3 parts. Maybe the explanations of servo and controller are good enough, but I'm still trying to look for a more ""official"" one.
Any ideas?
","microcontroller, rcservo"
Steering using different speeds in DC motors or using a servo?,"I am trying to assess the pros and cons of steering a robot car using different speeds of 2 or more DC motors versus using a servo and a steering mechanism? From your experience which is better in terms of:

Steering accuracy (e.g. prompt responsiveness or skidding while on higher speeds)
Efficiency in electrical power consumption
Durability and maintenance
Control complexity (coding and electronics)

I researched and understood how both approaches work, but I need some practical insight to select the most suitable approach. Any hint or research direction is appreciated.
","motor, servomotor, steering"
Which kind of motors and how powerfull should they be for a robotic arm,"I am building a robotic arm with these specifications:

1 meter long
approx. 1kg weight
it is made of 4 motors. (2 at the base. One for rotating the whole arm left and right and another one for rotating up down. 1 motor for rotating the second half of the arm, only up and down. 1 for the claw used for grabbing)
it must be able to lift at least 4kg + 1kg (it's own weight), and have a speed of 180 degrees in 2 seconds = 360 degrees in 1 second resulting 60rpm.

Which kind of motor would be best for the project (servo or stepper) and how much torque will it need? (Please also give me an explanation of how I can calculate the torque needed). Could you also give me an example or two of the motors I would need and/or a gearbox for them (models or links).
","motor, robotic-arm, servomotor, stepper-motor, torque"
Question regarding the orientation of Coxa servos in a HEXAPOD,"I'm making a Hexapod robot (this is my first robotics project ever) and I'm having a hard time deciding the orientation of the servo motors on the COXA section. The hexapod I'm making is based on this with MG996R servo motors
The problem is that all the Hexapod assembly guides I've read (including the one above which I'm using) say that the servos on the coxa link have to be installed in a way that the Zero angles (the angle the servos move to automatically when they're plugged in) of the servos should point to look like this from above: 
(the red circles are the servos in question)

I understand that the reason for setting up the zero angles of the coxa servos this way is so that the Hexapod can directly stand up when it is powered up (since the motors go to their zero angles on powering up). 
Now here's the problem: the servos can only move forward from the zero angles and then back to the zero angles in COUNTER-CLOCKWISE direction and do not go backwards from the zero angles i.e not move in the opposite direction. Knowing this, I don't understand how I'm supposed to install the servos like in the picture since doing that would mean the motors on the LEFT side would not be able to turn in the clockwise direction for the tripod gait. Can anyone please explain me how I'm supposed to get the motors moving in clockwise direction for the tripod gait while keeping the zero angles set in the directions as shown in the picture?
","rcservo, legged, hexapod"
Adafruit PCA9685 servo driver connected to Beaglebone Green Wireless generating erratic movement when driving multiple servos,"I have a hexapod that a friend and I built this summer, but there is a big problem whenever we try to move multiple servos. When using the Adafruit_Python_PCA9685 library, I am able to move the servos perfectly fine for a short period, but then they will breakdown and start erratically twitching.
To illustrate the problem, I just modified a few lines from Adafruit's simpletest.py program.
Here is the code:
# Simple demo of of the PCA9685 PWM servo/LED controller library.
# This will move channel 0 from min to max position repeatedly.
# Author: Tony DiCola
# License: Public Domain
from __future__ import division
import time

# Import the PCA9685 module.
import Adafruit_PCA9685


# Uncomment to enable debug output.
import logging
logging.basicConfig(level=logging.DEBUG)

# Initialise the PCA9685 using the default address (0x40).
# pwm = Adafruit_PCA9685.PCA9685()

# Alternatively specify a different address and/or bus:
pwm = Adafruit_PCA9685.PCA9685(address=0x40, busnum=2)

# Configure min and max servo pulse lengths
servo_min = 300  # Min pulse length out of 4096
servo_max = 400  # Max pulse length out of 4096

# Helper function to make setting a servo pulse width simpler.
def set_servo_pulse(channel, pulse):
pulse_length = 1000000    # 1,000,000 us per second
pulse_length //= 60       # 60 Hz
print('{0}us per period'.format(pulse_length))
pulse_length //= 4096     # 12 bits of resolution
print('{0}us per bit'.format(pulse_length))
pulse *= 1000
pulse //= pulse_length
pwm.set_pwm(channel, 0, pulse)

# Set frequency to 60hz, good for servos.
pwm.set_pwm_freq(60)

print('Moving servo on channel 0, press Ctrl-C to quit...')
while True:
# Move servo on channel O between extremes.
for i in range(0, 3):
    for j in range(0,3):
    k = (4*i)+j
    pwm.set_pwm(k, 0, servo_min)
    time.sleep(1)
    pwm.set_pwm(k, 0, servo_max)
    time.sleep(1)

And here is a video of the ""erratic movement"" (the first 8 seconds are normal movement)
I am running the code on a Beagle Bone Green Wireless with ubuntu on it and I am using turnigy TGY-S091D servos.
Here is a photo of the wiring

I don't have enough reputation to post more detailed pictures, but hopefully this is enough.
Please help.
","rcservo, python, beagle-bone"
Choosing servo motors for quadropod spider,"We are working on an amateur quadropod spider robot which can wander around earthquake debris and thought that it should be a tiny robot at all or a robot with a tiny with relatively long legs in such a situations.
For a tiny robot it is important to choose strong but tiny servos. The robot should climb a slope with 80% ratio.
How can I calculate required torque? Is it enough that adding up weight and dynamic force to climb and multiply it by the distance between two joints?
Which factors should I consider?
Thanks in advance.
","motor, torque, rcservo, legged"
Robotc color sensor error,"I am trying to write a simple program where the robot(Lego NXT2) will follow a blue line.
#pragma config(Sensor, S1,     ,               sensorCOLORBLUE)
//!!Code automatically generated by 'ROBOTC' configuration wizard               !!//

task main()
{
    while(true)
    {
        if(SensorValue[S1] == sensorCOLORBLUE)
        {
            motor[motorB] = 0;
            motor[motorC] = -50;
        }
        else
        {
            motor[motorB] = -50;
            motor[motorC] = 0;
        }
    }
    wait1Msec(1);
}

I am using an nxt color sensor and the problem is that only 1 motor is moving. I know that none of the motors are broken either because I tested them out.
Can somebody help me diagnose my problem?
","sensors, robotc"
Granite devices Ionicube,"Is it possible to enable/disable the 4 Ioni drives mounted in Ionicube individually?  It appears with IoniCube that enable/disable applies to all Ioni drives simultaneously.  However, Ionicube 1X does appear to have enable/disable for individual Ioni drives.  Can you confirm?
I wish to control the enable/disable state of each Ioni drive separately.  And if possible to do this on IoniCube.
Most grateful for your help!
Best wishes,
Mark
",motor
Mathematical modelling of system dynamic on matlab,"I want to make a matlab (simulink) control model for the system in the image below.

The original pdf is only accessible if logged in Carleton's Learning Management System.
How do I get the dynamics of the system with given details in the image?
","matlab, dynamics"
A sensor that can see glass/transparent objects and surfaces,"I am currently thinking about making a robot that will autonomously drive around the place. The place I want this robot to drive in however contains quite a few glass walls. When mapping the area I would need to be able to see the glass. For this reason I am in need of a sensor that can see the glass, and not see through it. What kind of sensor would be the best for me? I need it to have a maximum range of about 2-10 meters and a minimum range of about 0.25 meters (preferably as small as possible). I was thinking about maybe using ultrasonic, but I was told that a laser-based sensor would probably be best. I could however only find industrial grade laser sensors that could see glass/transparent objects.
","sensors, slam, mapping, ultrasonic-sensors, laser"
Motor Selection for PV powered solar tracking system,"I am busy designing an automatic single axis solar tracking system for a small parabolic trough that I am building.
I have come accross the idea a few times on youtube and on other websites that use two solar panels, at different angles, to power a motor. 
The Panels are arranges as is shown below, such that if one side receives more power than the other, the motor is turned to centre the system. And the as the sun moves, the system slowly tracks its movement as well.

My parabolic trough will be 1.2M wide and 0.4M deep, and roughly 10 kg max, with the centre of gravity just below the focal point and axis of rotation.
I've been having some trouble deciding on which motor I should use for this?
I know that people have used DC motors in the past, but as I understand, DC is meant for continuous rotation, and with the motor needing only small movements would this not be damaging for the motor?
I am trying to make this as simple, cheap and effective as possible, and would ideally have it able to run on its own without other power sources.
","motor, servomotor, stepper-motor"
how to program encoder in c++,"I am reading the encoder pins by using the ros arduino bridge, where i am subscribing to the encoder pins inside my ros node i.e a c++ program. Here every time a new pin value arrives in program all its variables are reset. But as i saw the encoder programs one variable has to store the privious encoder position, so how can it be done in c++. The arduino program to read encoder is:
/* Rotary encoder read example */
#define ENC_A 14
#define ENC_B 15
#define ENC_PORT PINC

void setup()
{
  /* Setup encoder pins as inputs */
  pinMode(ENC_A, INPUT);
  digitalWrite(ENC_A, HIGH);
  pinMode(ENC_B, INPUT);
  digitalWrite(ENC_B, HIGH);
  Serial.begin (115200);
  Serial.println(""Start"");
}

void loop()
{
 static uint8_t counter = 0;      //this variable will be changed by encoder input
 int8_t tmpdata;
 /**/
  tmpdata = read_encoder();
  if( tmpdata ) {
    Serial.print(""Counter value: "");
    Serial.print(counter, DEC);
    counter += tmpdata;
  }
}

/* returns change in encoder state (-1,0,1) */
int8_t read_encoder()
{
  static int8_t enc_states[] = {0,-1,1,0,1,0,0,-1,-1,0,0,1,0,1,-1,0};
  static uint8_t old_AB = 0;
  /**/
  old_AB <<= 2;                   //remember previous state
  old_AB |= ( ENC_PORT & 0x03 );  //add current state
  return ( enc_states[( old_AB & 0x0f )]);
}

Currentely my roscpp program is: 
#include <message_filters/subscriber.h>
#include <message_filters/time_synchronizer.h>
#include <sensor_msgs/Image.h>
#include <sensor_msgs/CameraInfo.h>
#include <iostream>
#include <ros_arduino_msgs/Digital.h>
using namespace ros_arduino_msgs;
using namespace message_filters;

void callback(const DigitalConstPtr& e1, const DigitalConstPtr& e2)
{ 
  std::cout<<""callback""<<std::endl;
  std::cout<<e1<<std::endl;
}

int main(int argc, char** argv)
{
  ros::init(argc, argv, ""vision_node"");
  std::cout<<""main""<<std::endl;
  ros::NodeHandle nh;
  message_filters::Subscriber<Digital> e1(nh, ""/arduino/sensor/left_encoder_A"", 10);
  message_filters::Subscriber<Digital> e2(nh, ""/arduino/sensor/left_encoder_B"", 10);
  TimeSynchronizer<Digital, Digital> sync(e1, e2, 10);
  sync.registerCallback(boost::bind(&callback, _1, _2));

  ros::spin();

  return 0;
}

Now in the callback function how do i program the encoder? Here since every time the ros node subscribes to a new topic all the parameters inside the program is also reset, i am getting the problem. 
","arduino, ros"
serial monitor not showing proper output,"I was reading the encoder value in arduino uno. But the output is not comming properly. It is coming like this:

I am using this arduino code to read encoder:
/* Rotary encoder read example */
#define ENC_A 14
#define ENC_B 15
#define ENC_PORT PINC

void setup()
{
  /* Setup encoder pins as inputs */
  pinMode(ENC_A, INPUT);
  digitalWrite(ENC_A, HIGH);
  pinMode(ENC_B, INPUT);
  digitalWrite(ENC_B, HIGH);
  Serial.begin (115200);
  Serial.println(""Start"");
}

void loop()
{
 static uint8_t counter = 0;      //this variable will be changed by encoder input
 int8_t tmpdata;
 /**/
  tmpdata = read_encoder();
  if( tmpdata ) {
    Serial.print(""Counter value: "");
    Serial.print(counter, DEC);
    counter += tmpdata;
  }
}

/* returns change in encoder state (-1,0,1) */
int8_t read_encoder()
{
  static int8_t enc_states[] = {0,-1,1,0,1,0,0,-1,-1,0,0,1,0,1,-1,0};
  static uint8_t old_AB = 0;
  /**/
  old_AB <<= 2;                   //remember previous state
  old_AB |= ( ENC_PORT & 0x03 );  //add current state
  return ( enc_states[( old_AB & 0x0f )]);
}

",arduino
How to measure pull force on a (kite) rope?,"I'm building a kite flying robot, in which I've already got some sensors built in. I now also want to measure the pulling force on the rope which is attached to the kite. So I thought of attaching a hanging digital scale to the rope which can then measure the pulling force in kilograms. I of course need to read out the data from the scale using GPIO pins or USB. 
Furthermore, I also have a raspberry pi installed within the kite and I want to measure some pull on ropes in the air. So if possible, I also need very small/light scales from which I can read out the data. It would be great if the scales can measure up to about 50kg, but with some pulleys about 20kg would be fine as well.
Unfortunately I can't find any digital hanging scale with a possibility to read out the data and which is reasonably light. There are some simple USB powered hanging scales, but I think they just use USB for charging, not for reading out data. And I also found this one, but that's a bit overkill (too heavy and too expensive).
So question #1: Does anybody know where I can get a simple existing hanging scale from which I can read out data?
If needed I can also build something, but I just wouldn't know where to start. I did find this page on Alibaba, where they show the contents of the scale they offer:

So as far as I understand I need the component which I highlighted. But I have no idea how that component is called (what do I search for?) and whether it is actually doable to read it out from a Raspberry Pi.
Question #2: does anybody know what the highlighted component is called, where I could possibly get it and if I can read it out from a Raspberry Pi?
In conclusion; can anybody point me in the right direction?
","mobile-robot, sensors, raspberry-pi, uav"
Can I model a 1D segway as a cart-pole system?,"The equations of motion for a cart-pole (inverted pendulum) system are given as  
$$(I+ml^2)\ddot{\theta}+mglsin(\theta)+ml\ddot{x}cos(\theta)=0$$  
$$(M+m)\ddot{x}+ml\ddot{\theta}cos(\theta)-ml\dot{\theta}^2sin(\theta)=F$$  
However, I want to model a two wheeled segway-like robot with motion constrained to only forward and backward actions (effectively restricting motion in 1D).  I initially thought that I could model such a constrained segway robot by modeling a cart-pole system with a massless cart (M=0).  Is this the right approach to model the dynamics of a 1D segway robot?  Or is there a better model for the dynamics of such a robot in 1D?  
","control, balance"
DX100 controller communication protocol,"I have a motoman robot for use in a pick and place application. It has a DX100 controller which has an ethernet interface which could be used to control a slave device using the Modbus TCP protocol. 
The DX100 controller also supports Ethernet/IP and DeviceNet.
I know Modbus can be quite complex for first timers and I have little experience when it comes to programming these devices. 
I would like to know, if someone here has ever worked with this controller, which communication protocol they used and why.  
","microcontroller, serial, communication"
Optimal location of the center of mass for an inverted pendulum,"I'm building an inverted pendulum to be controlled by DC motors, but I've run across a conundrum.  Personal life experience tells me that it's better to have a lower center of mass to maintain balance.  On the other hand, the greater the moment of inertia (e.g. the higher the center of mass), the easier it is to maintain balance as well.
These two views both seem plausible, and yet also seem contradictory.   For an inverted pendulum, is there an optimal balance between the two perspectives?  Or is one absolutely right while the other absolutely wrong?  If one is wrong, then where is the error in my thinking?
","dynamics, balance"
How to mock a device during development?,"I am coding a little robot controlled by a raspberry pi zero. 
Disclaimer: This is a more general development question, and only indirectly related to raspberry pi. 
Background: I don't want to test everything on the robot directly, this is too time consuming. Therefore I am trying to implement mock interfaces. For example, I'd am using a set of fake sensor readings and feed them to the class responsible for sensing and driving. Running this code in command line should for example print out ""driving left"" instead of starting the right motor and so on.
Question: How to mock a device so that I can use the main code without modification on my robot.
Code examples
I have currently the following file structure:

main.py: running the loop so the robot is doing something and       currently holding mock sensor data.  
acc.py: AutonomousCruiseControl class: taking care of the process of       measuring and steering  
hcsr04.py: a class mocking the sonar    distance reader device  
servo.py: currently doing nothing: Should    contain a mock device writing to console what motor is turned  into    what direction

In the following code examples, you can see e.g. in the drive() method, that I have code purely for testing mixed with the code destined for production.
main.py
#!/usr/bin/env python3
# coding=UTF-8

from navigation import AutonomousCruiseControl


def main():
    #self.objectColisionRange = [10,15,20,15,10]
    readings = [
        [11, 16, 21, 16, 11],       # path free
        [100, 100, 19, 100, 100],   # front blocked
        [9, 100,20, 100, 100],      # center & left blocked
        [100, 100, 10, 100, 10],    # center & right blocked
        [10, 100, 10, 100, 10],     # center, left & right blocked
        [100, 10, 100, 100, 100],   # center free, left front object to avoid
        [100, 100, 100, 10, 100],   # center free, right front object to avoid
        [100, 10, 100, 10, 100]     # center free, but left and right (45°) objects too narrow
    ]

    ACC = AutonomousCruiseControl()
    # range second parameter is numbers to generate not the right boundary
    for i in range(0,8):
        ACC.front_sonar.readings = readings[i]
        print(""{}. reading: {}"".format(i, ACC.read_front_sonar()))
        print(""{}. FOS: {}"".format(i, ACC.get_front_object_status()))
        ACC.drive()

if __name__ == '__main__':
    main()

acc.py
#!/usr/bin/env python3
# coding=UTF-8

from navigation.hcsr04 import Hcsr04
from navigation.servo import Servo
from random import randint


class AutonomousCruiseControl:
    """"""helping to steer a robot trough obstacles""""""

    def __init__(self, object_colision_range=None, front_sonar_angles=None):
        # initiate sensors
        self.front_sonar = Hcsr04()
        self.front_sonar_servo = Servo()
        self.collision_distance = 20

        if object_colision_range is None:
            self.object_colision_range = [10, 15, 20, 15, 10]
        else:
            self.object_colision_range = object_colision_range

        if front_sonar_angles is None:
            self.front_sonar_angles = [0, 45, 90, 135, 180]
        else:
            self.front_sonar_angles = front_sonar_angles

        self.front_sonar_distances = [200, 200, 200, 200, 200]
        self.read_front_sonar()
        print(self.front_sonar_distances)

        self.front_object_status = [0, 0, 0, 0, 0]

    def read_front_sonar(self):
        """""" Setting the angle and calling the read_distance method. """"""
        # TODO implement reading from right to left and left to right, so both servo swings can be used
        # go through all angles and read distance put it into front_sonar_distances
        # initiate only once for test cases, to not mix values
        for i in range(0, len(self.front_sonar_angles)):
            # set servo to angle
            angle = self.front_sonar_angles[i]
            # read distance
            distance = self.read_distance(self.front_sonar, angle)
            # print(""distance is {}\n"".format(distance))
            self.front_sonar_distances[i] = distance
        return self.front_sonar_distances

    def get_front_object_status(self):
        """""" Creating a simplified form of the distance measures. Currently if a reading is equal or below the
            reading angle specific collision range, the status is set to 1. Zero means, no obstacle. """"""
        for i in range(0, len(self.front_sonar_distances)):
            if self.front_sonar_distances[i] <= self.object_colision_range[i]:
                self.front_object_status[i] = 1
            else:
                self.front_object_status[i] = 0
        return self.front_object_status

    def read_distance(self, sensor, angle):
        """""" Remove angle later from this method and from hcsr04. """"""
        if sensor is self.front_sonar:
            # TODO: implement servo, set angle
            # print(""Set servo to angle {} °"".format(angle))
            # angle/index won't be necessary after the real servo routine is working
            # this just helps the dry run tests
            index = self.front_sonar_angles.index(angle)
            return self.front_sonar.get_distance(index)

    def drive(self):
        """""" Contolling the motors with obstacle avoidance switch.
            Light search is not yet implemented. The robot should aimlessly
            cruise around without hitting detectable obstacles. """"""

        stat = self.front_object_status
        dist = self.front_sonar_distances

        if stat[1] + stat[2] + stat[3] is 0:
            # road is free
            print(""Road free: L(eft) & R(ight) forward"")
        elif stat[1] is 1:
            # left blocked
            print(""Obstacle left: L(eft) faster  & R(ight) slower forward"")
        elif stat[3] is 1:
            # right blocked
            print(""Obstacle right: L(eft) slower  & R(ight) faster forward"")
        elif stat[2] is 1 or (stat[1] + stat[3] is 2):
            # front blocked
            print(""obstacle front: stop all"")
            dist_sum_left = dist[0] + dist[1]
            dist_sum_right = dist[3] + dist[4]

            if dist_sum_left == dist_sum_right:
                # both sides are free and distance the same
                # choose randomly left and right 0 is left and 1 is right
                rand = randint(0, 1)
                if rand is 0:   # left
                    print(""Random turn left, L backw, R forw"")
                else:           # right
                    print(""Random turn right, R backw, L forw"")
            elif dist_sum_left > dist_sum_right:
                # left has more leeway
                print(""Left leeway, turn left"")
            elif dist_sum_left < dist_sum_right:
                # right has more leeway
                print(""Right leeway, turn right"")

hcsr04.py
#!/usr/bin/env python3
# coding=UTF-8

# Test method

class Hcsr04:
    'this just fakes the reading from sonar it needs the servo angle where the reading should come from'
    def __init__(self):
        self.readings = [88, 88, 88, 88, 88]

    def get_distance(self, index):
        return self.readings[index]

",raspberry-pi
"What's the mathematical definition of quaternion 3x3 covariance matrices, and how does it relate to euler angles?","I'm adapting a KF orientation filter that represents the orientation as quaternion, and uses a 3x3 covariance matrix. Would someone know what 3x3 covariance represents in the case of a quaternion, and how that representation would relate to euler angles ?
",kalman-filter
Camera Pose Estimation Cost Function,"What I actually have is more a terminology question in regards to some visual odometry equations.
From Hartley and Zisserman's Multiple View Geometry we are given multiple different cost functions for least squares method. Among them is the transfer error:
$\sum d(x_{t}^{'},Tx_{t-1}^{'})^{2}$
x' is a measured point and T is rigid body transform. The only parameter estimated is T.
and the re-projection error
$\sum d(x_{t-1}^{'},x_{t-1})^{2} + d(x_{t}^{'},Tx_{t-1})$
where x' is the measured point, x is the estimated point and T is again the rigid body transform. In this equation both T and x are the estimated parameters.
I know I am missing the camera calibration parameters in these equations.
Now in almost all the papers I have found on visual odometry they mention that they minimize the reprojection error.(E.g SPTAM ,SVO,VO_Paper3 and whole bunch of others)
However, in almost all of them they only minimize the transform, rather than both the transform and the point position which to me signifies that they are minimizing the transfer error, and not the re-projection error. So I am just wondering if I am misunderstanding the differences between the cost functions as given by Hartley and Zisserman, or if just everyone in the visual odometry community is using the term reprojection error in a different manner.
A couple of bonus questions if you have time.
1) When calculating the residuals is it better to have the residual be 1 number or a vector? So for the transfer error is it better to have just one residual with the distance between the points, or to split it up into two residuals with one having distance in the x direction, and the other the distance in the y.
2) Why could there be a case where powells dog leg method is able to solve a problem, but levenberg marquadt can not?
",visual-odometry
Robot never goes straight,"I am using 2 identical DC motors and a castor wheel. The motors are connected to L293D motor driver and are controlled by RPi. 
The robot is not going straight. It veers off to the right. I am running both the motors at 100% PWM.
What I tried to correct the error:

I adjusted the PWM of the wheel going faster to 99%, but the robot just turns to the other side; 
I adjusted the weight on the robot and the problem still persists.

I once tried to run the motor without any load. Is that the cause of this, as I was later told that, running a DC motor without any load damages them?
If that is not the cause, then please tell me how to solve this problem without using any sensors for controlling it. 
","control, motor, wheeled-robot, raspberry-pi"
Why do I need an additional moment of inertia term in the cart-pole dynamics equation?,"I'm trying to understand the dynamic equations for the cart pole system according to this control tutorial from the University of Michigan, where the angular acceleration equation can be written as
$(I+ml^2)\ddot{\theta}+mglsin(\theta)+ml\ddot{x}cos(\theta)=0$
I'm having trouble understanding what forces are represented separately by the terms  $ml^2\ddot{\theta}$ and $I\ddot{\theta}$.  Both seem to represent a moment force exerted by the mass of the pole, so they almost seem to represent exactly the same thing.  My intuition tells me that I really only need the term $ml^2\ddot{\theta}$, but the fact that there is an additional $I\ddot{\theta}$ term suggests that my intuition is not accounting for some other force present in the system.  
Any help to clarify and distinguish the meaning behind these two terms would be greately appreciated! 
","dynamics, balance"
Why doesn't our ultrasonic rangefinder work?,"so for some reason me and my friends are unable to find how to make this rangefinder work. We're a group of four and the furthest we got was pretty close, but for some reason it won't react to getting close. Here's our code so far:
#pragma config(Sensor, dgtl1,  button,         sensorTouch)
#pragma config(Sensor, dgtl11, input,          sensorSONAR_inch)
#pragma config(Motor,  port1,           leftM,         tmotorVex393_HBridge, openLoop)
#pragma config(Motor,  port10,          rightM,        tmotorVex393_HBridge, openLoop)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//
task main() {
  robotType(recbot);
  while (SensorValue(button) == 0) {
    motor[leftM] = 0;
    motor[rightM] = 0;
  }

  while (SensorValue(button) == 1) //button is pushed
  {
    motor[leftM] = 127; //both motors turn on
    motor[rightM] = 127;
    sleep(10);

    untilSonarLessThan(148, dgtl11); //wait while it goes to wall

    motor[leftM] = -127; //both motors go reversed
    motor[rightM] = -127;
    sleep(10);

    untilSonarGreaterThan(226, dgtl11);

    motor[leftM] = 0; //both motors stop
    motor[rightM] = 0;
    SensorValue(button) = 0; //button switch
  }
}

Please help us out! We feel like we've tried everything. And yes, the output is in digital 12. 
","c, ultrasonic-sensors, vex"
"Dynamics of a 3R planar robot manipulator,matrices M(q),C(q,q') and G(q)","The dynamics equation is T=M(q)*q''+C(q,q')*q'+G(q).
Can somebody provide me with the M(q), C(q,q') and G(q) matrices of a 3R manipulator with link mass,length and rotational inertia m_i, l_i and Inertia_i respectively?

","dynamics, manipulator"
Arc welder for 3d printing,"Has anybody experimented with GMAW for additive manufacturing? The thing is, welding wire is so much cheaper than ABS or PLA filaments and, well, it is steel you are printing in, not some flimsy plastic! I imagine the arc deposition printhead would be constructed similarly to one used in plastic filament printers, except there is no need for the heating element (so, even simpler). Welding often requires fast Z speed (to finely control the arc) so i think Delta (DeltaMaker) chassis would work best. GMAW calls for some sort of inert gas to insulate heated metal from oxygen. It would make sense to seal off most of the interior of the printer and fill it with heavier than air inert gas during printing. 
I would highly appreciate any pointers on existing 3d printer designs employing this deposition method as well as flaws in design i outlined here.
",3d-printing
real-time stereo vision system for long range (up to 100m) depth estimation,"I want to implement a FPGA-based real-time stereo vision system for long range (up to 100m) depth estimation. Also, I want to use two ip cameras in the system. I have calculated depth error using the equation below with these parameteres:
baseline = 1m (at the expense of increased frontal blind zone area), z = 100m, f = 4mm, pixel_size = 4um, disparity_accuracy = 0.25px,
depth_error = dz = ((z^2)/b.f)*dp = 2.5m
My questions are: Are these calculations and equation above valid for long range stereo vision?
Are there any other considerations for designing this long range stereo vision system which may not be important in typical (short range) stereo vision systems?
I will be grateful for any information you can provide.
","cameras, stereo-vision, real-time, errors, rangefinder"
the order of camera calibration and stereo calibration,"To perform stereo calibration, I have two options.
Option 1: I calibrate each cemara first by calling cvCalibrateCamera2 and do stereo calibration by calling cvStereoCalibrate with the flags equal to CV_CALIB_FIX_INTRINSIC.
Option 2: I do stereo calibration by calling cvStereoCalibrate with the flags equal to 0.
In my view, two options should produce similar results. If the flags is equal to 0, cvCalibrateCamera2 is called for each camera to obtain intrinsic parameters in function cvStereoCalibrate. I am told that two options produce different results. Is this possible?
","cameras, stereo-vision, calibration, 3d-reconstruction"
Robot arm matlab Simulink simulation error,"I am doing robotic arm simulation in Matlab Simulink but get the error. How I can resolve this.

My model given below.

Other details:
1. image of coordinates with world coordinates.
2. 6 DOF joint setting


","robotic-arm, matlab, simulation"
"2DOF arm with quick movement: Stepper, servo, or DC motor?","To make a two degree of freedom arm capable of speeds needed to play air hockey, how should the arm be controlled? I'm wondering about speed vs accuracy.
The following image is from a project involving a 2DOF drawing arm:

For this air hockey project, the general mechanics of that arm seem appropriate. it can move the ""hand"" around a 2D plane. I also like how both motors are at the base of the arm, instead of having the 2nd motor at the elbow (which adds weight to the arm and slows it).
However, the drawing robot is slow. 

Steppers:
I understand that in general, steppers are 'slow'.
But, steppers have accurate positioning, which is very important in a game of air hockey. I also know that the steppers can be geared-up, but then they lose torque. The steppers would need to handle the inertia of quick, back-and-forth arm movement.
Hobby servos:
I've used small servos for other projects, but the accuracy was poor. Most don't have a way to read the angle externally, and those that do, have noisy signals. I'm not sure how strong the hobby servos are, and if they can be accurate enough for this project. I understand that using digital servos improve dead band issues.
DC motors with external feedback:
The only other method I can think of for controlling the arm would be to use DC motors with sensors like as rotary encoders. I think I would need an absolute rotary encoder.  But they seem to be around \$50-$1000, which is a bit much for a solution I'm not sure will even work out. Perhaps there are cheaper solutions to motor angle measurement. I could just use a potentiometer, but then I'm worried about noise again.

It's worth noting that I don't know of any easy or affordable way to design my own drivetrain. All I have is a drill, and I don't know how I would mount shafts/bearings and such, even if the gears themselves were affordable.
This means that if I need to gear-up or down, I don't think I can unless it's cheap and involves simple tools.
So for the arm: DC motors with external feedback, servos, steppers, something else?...
Which method would be the best in terms of speed, and which for accuracy? I'm wondering which would cost less as well, but I understand that is a grey area.
I'm leaning towards servos out of simplicity. I'm may try digital servos with ball-bearings, in the hope they that will move quick enough, but be strong enough to handle the inertia of the arm.
(Note that a 2DOF arm is desired, and not something else like a 3D-printer x-y belt system.)
","motor, robotic-arm, kinematics, stepper-motor, servos"
Hexapod walking algorithm,"I am making a hexapod project with my friend. There is a issue about its walking style. Generally tripod gait is formed as two steps: first move legs 1,3,5, then 2,4,6 which corresponds to moving one leg from one side and two legs from other side but the legs are located circular instead side by side. So, this situation makes us wondering if it would rotate or not instead of moving forward when the 
tripod gait is implemented.
Our robot's legs are like this one: https://www.youtube.com/watch?v=iPdRUbJcNzM
What kind of walking algorithm is suitable for that kind of robot? 
Thanks in advance.
","hexapod, walk"
Which motors are used in boston spot robot,"I want to know which motors or smart servos are used in boston dynamics spot robot. Where can i read its spicification and is it possible to buy the motor? 
","motor, servomotor, actuator"
how to make a frame with actobotics,"What is the bet way to make a but connection with acrobotics channels? I am hoping to make a few 10' columns and beams connected together.
Actobotics
servocity
",stability
What are the recommended interconnects for wiring a battery to a motor via its driver?,"I am currently working with a combination of Tamiya and T-connectors. I have not previously had issues with either, but the T-connectors I'm using currently do not seem to be maintaining a connection. If I torque the T-connectors in a particular way they will start working, but if I leave them ""floating"" they disengage. My first thought was that the problem was I had 2 different brands of T connectors, but it turns out, after switching all to one brand, that some of them do not work properly.
My first question is if there are known problems with T connectors or if I probably just got a bad batch?
My second question is if there are better interconnects out there than either Tamiya or T? (specifically for wiring a motor to its driver and the battery to the driver)
Let me be clear, I am NOT looking for opinions. I obviously realize this could be based on personal preference, but I am specifically asking if there are engineering principles at play in the choice being made. If it really just comes down to preference, then I am only interested in the first question. I also realize the possibility of avoiding interconnects, but let's assume that's not an option.
","wiring, connector"
Why is it so hard to walk?,"At least, on two legs. Asimo, one of the best known humanoid robots, is already capable to walk, although it doesn't seem to do this very stable. And it is a recent result.
As far I know, the legs are essentially many-dimensional non-linear systems, the theory of their control is somewhere in the border of the ""very hard"" and the ""impossible"". 
But, for example, the airplanes are similarly many-dimensional and non-linear, despite that, autopilots are controlling them enough well some decades ago. They are enough trusted to confide the lives of hundreds of living humans to them.
What is the essential difference, what makes the walking so hard, while the airplane controlling so easy?
","control, stability, walking-robot"
Scaling monocular SLAM with another source?,"I have implemented a stable monocular slam tracking system, based on ORBSLAM2. I am trying to find a way to add real-world distance/scale to this.
At the same time, I am running a (less stable) stereo slam system.
What i would like to do is:
Using the correct scale of the stereo data, figure out the scale factor that is need to correct the mono to real-world scale, and multiply the output by this factor.
I do not want to adjust the slam algorithm, I need to take the two output streams (currently an Eigen::Vector3f for each stream) and find an algorithm that I can run for 30 seconds or so to find the scale difference, and therefore the factor that I need to multiply the mono by.
I am currently:
//Start system.

//Store both values as Eigen::Vector3f;

Eigen::Vector3f initialWrong;
Eigen::Vector3f initialTruth;

//sleep the thread for a few seconds, and move the sensors.


// get the distances
float stopWrong = CurrentWrong - initialWrong;
float stopTruth = CurrentTruth - initialTruth;

scaleFactor = stopTruth / stopWrong ;

This works, within a margin of error, but is very manual. I am looking for a more automatic / iterative way to do this that will minimize the error as much as possible.
How can I use a ground truth to scale monocular slam in real time?
Any thought or tips here would be greatly appreciated.
Thanks.
","slam, stereo-vision, c++, monocular"
Creating MAVLink commands from python,"I am struggling to find any clear documentation on how to create MAVlink commands using python.
I am looking to create an autonomous glider and require some of the basic functions

Retrieve GPS Data and store into a file
Import GPS data in Python function
Send waypoint lat, long, alt to the autopilot (APM)

I am currently using MAVProxy as my GCS and the ErleBrain 3 as my autopilot hardware but the aim is to not require a GCS and just have a python script to automatically add and remove waypoints based on the received GPS data
","python, ardupilot, mavlink"
Robot clutch to disengage wheels,"I have a robot that moves around autonomously. Very often I want to push the robot several feet to start a test over again, or sometimes I want to wheel the robot outside to my car or to a nearby field.  
Pushing my robot is pretty tough. Its 60lbs (27kg) and when I push it with the motors engaged its very difficult. I want a way to decouple the 2 back wheels so I can haul it around like a suit case. I've seen similar posts to this one where people suggested just leaving them coupled and recharging the battery. I don't really care to exert that much of my own energy just to recharge the battery. I just want to make transporting easier.
I also Want to be able to engage and disengage without the robot being on. This means that electromagnet clutches are out. Id like the solution to be under $300 and to be fairly easy to machine. I have access to a lathe, mill, welder... etc and can machine some complex stuff, but I don't want it to be like making a custom gear box. 
Does anyone have any suggestions? I'm using standard dolly wheels from Northern tool. (http://www.northerntool.com/images/product/2000x2000/425/42570_2000x2000.jpg) I've considered a quick release pin but given the geometry of the dolly wheel that would be tough. The pin would hit the rim as you are pulling it out.
Thanks in advance for any suggestions.
","motor, gearing"
how to calculate the angular velocity of end effector of two link robot arm?,"I am trying to calculate the angular velocity of end effector of two-link robot arm. Can anyone help me to find it?
If $q_1$, $q_2$  are joint angular position and $\dot{q_1}$ and $\dot{q_2}$ are joint angular velocities, and $\omega$ is angular velocity of end effector, then I use $\omega=\dot{q_1}+\dot{q_2}$.
Is that correct?
",robotic-arm
LQR for Inverted pendulumn,"I'm studying an optimal control for the inverted pendulum in the following figure

the state and the output of the system are defined as
$$x=\begin{bmatrix}r & \theta & \dot{r} & \dot{\theta} \end{bmatrix}^T, \quad y=\begin{bmatrix}r & \theta-\alpha & \dot{r} \end{bmatrix}^T$$
so the continuos state space model at the upright equilibrium is
\begin{cases}
\dot{x}(t)&=Ax(t)+B_u u(t)+B_\alpha \alpha(t)+B_\tau \tau(t)+B_{F_s}F_s\text{sign}(\dot{r}(t)) \\
y(t)&=Cx(t)+D_\alpha\alpha(t)
\end{cases}
where the disturbance inclination $\alpha$ is suppose costant, $\tau$ is a torque disturbance,$B_{F_s}F_s\text{sign}(\dot{r}(t))$ is a Coulomb's friction term and $u$ is the input force applied at the cart. The numerical values for the transfer matrices are
$$A=\begin{bmatrix}0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 2.8040 & -5.2658 & 0 \\ 0 & 18.5885 & -19.6959 & 0 \end{bmatrix}, \quad B_u=\begin{bmatrix}0 \\ 0 \\ 3.7629 \\ 5.5257 \end{bmatrix}, \quad B_\alpha=\begin{bmatrix}0 \\ 0 \\ -12.6447 \\ 18.5885 \end{bmatrix}, \quad B_\tau=\begin{bmatrix}0 \\ 0 \\ 0.5650 \\ 3.7629 \end{bmatrix}, \quad B_{F_s}=\begin{bmatrix}0 \\ 0 \\ -1.1187 \\ -0.5650\end{bmatrix}  $$
$$C=\begin{bmatrix} I_3 & 0_{3\times1}\end{bmatrix} ,\qquad D_\alpha=\begin{bmatrix}0 & -1 & 0\end{bmatrix}^T$$
where $I_n$ is the identity matrix $n\times n$ and $0_{m\times n}$ is the null matrix $m\times n$.
For a digital implementation is required a suitable sampling of the previus system, so the discrete relative state-space model is in the form
\begin{cases}
x_{k+1}&=\Phi x_k+\Gamma_u u_k+\Gamma_\alpha \alpha_k+\Gamma_\tau \tau_k+N_k(\dot{r}_k) \\
y_k&=Cx_k+D_\alpha\alpha_k
\end{cases}
Now there is my problem. To counteract the effects of constant rail inclinations a discrete-time integrator is appended to the model $(\Phi,\Gamma_u)$. Is taked in the simple form
\begin{equation}\tag{1} w_{k+1}=w_k+r_k\end{equation}
so the extended state of the system become
$$x^{\text{e}}=\begin{bmatrix}x & w\end{bmatrix}^T$$
and the control state feedback is designed by minimization the cost function
$$J(u)=\sum_{k=0}^\infty (x_k^\text{e})^TQx_k^\text{e}+Ru_k^2$$
where the cost's matrices $Q$ must be semi-defined positive and the scalar cost $R$ must be strictly positive.
I can't understand the function of the integrator $(1)$. The rail inclination affect both input and output of the system, so the integral action counteracts only the effect on output. Moreover, the state matrix $A$ is singular, so the system at least has one integral action by itself, and no more integrator has to appended to the system.
Maybe the integrator $(1)$ is considere for drive the position of the cart to the start of the rail, i.e. $r=0$.
Thanks in advance for any suggestion.
",control
Low power computer for stereo vision,"I would like to build a motorized robot with stereo vision that sends visual input to a PC over wifi for processing. A Raspberry Pi would have been perfect for the task if it would be possible to connect 2 cameras. The Pi 2 would be powerful enough for two USB webcams (with minimum 8fps) I guess, but the shared USB seems to be a bottleneck (2 cameras + wifi dongle).
What other options are there to send input from 2 cameras and control a motor (or an arduino)?
","computer-vision, cameras"
Are there any open source path planning libraries that implement Voronoi diagram techniques?,"I've been looking into path planning for a non-holonomic robot with 3 DOF in a 2D plane and recently learned about Voronoi diagrams but I cannot find any open source planning libraries that use this technique. Are there any open source implementations of using Voronoi diagrams (preferably in C++)? If yes, then where? If not, then why? 
","mobile-robot, motion-planning, navigation, path-planning"
How to relate forward kinematics into PWM/speed?,"I am having a hard time understanding how to relate forward kinematics to things one can control in a robot like PWM and reading encoder values.
For example, how to relate the encoder values from motors and RPM to PWM values of the motors to make a robot follow a curved path?
","motion, forward-kinematics"
detect post-it from an image of a visual management Board,"'m trying to detect all the post-it in an image and get them into an arrayList. I tried many alternatives (removing backgroud -> detecting contours, haar Cascade
classifier, detecting rectangular objects...) but none of them gave me good results. 
Any Idea how to proceed? Any help will be appreciated.
","computer-vision, opencv"
Equilibrium points and complex numbers in nonlinear systems,"I'm reading this interesting book Nonlinear Control Systems: Analysis and Design. In the equilibrium points section, the author states the following

Definition 1.1 A point $x = x_e$ in the state space is said to be an equilibrium point of the autonomous system
  $$\dot{x} = f(x)$$
  if it has the property that whenever the state of the system starts at $x_e$, it remains at $x_e$ for
  all future time.

The definition means that the derivative is zero or no motion. This is why the object stays or remains at its position for all future time. The author provides the following example to illustrate this simple phenomena. 

Example 1.3 Consider the following first-order system $$ \dot{x} = r + x^2 $$   where $r$ is a parameter.

To solve the preceding example, basically we set the derivative to zero, hence
$$
r + x^2 = 0
$$ 
When $r$ is greater than zero, the equation has no real solution as stated in the book, therefore, the author stated that the system has no equilibrium point. 
When I came across this example, I've asked my self why the notion of equilibrium points can't be extended to the complex plane in this context? I would like to hear your thoughts regarding this issue. 
","control, theory"
Matlab Simulink showing following errror: kinematic constraints cannot be maintained,"I'm using simulink to get joints torque by giving motion input. I create very simple CAD model in solidworks to learn simulink. Simulink model is showing in below figure.

During update it shows no error but when I Run it shows error as

3D-CAD model is shown below figure.

Can anybody help me to solve this problem.
Thanks.
","robotic-arm, matlab, simulation"
How to reduce the DC motor starting current?,"I am making a bot with 4 motors with a stall current rating of 11.7A. I was using 4 separate motor controllers which has a peak current rating of 20A to control each motor. But when I gave power to the motors in my bot, the motor drivers blew off within a second. 
Was it due to the starting peak current of the motors? If yes, then how can I reduce the starting current? If no, then what could be other possible problems and their corresponding problems?
","motor, driver"
What exactly does a motor driver do ? Why do we need additional power supply for motors,"What exactly does a  motor driver do ? Why do we need additional power supply for motors. I'm a hobbyist. making a line follower from scratch.  
","arduino, motor, servomotor"
High speed usb webcam,"I am looking for a high speed usb webcam.
The plan is to rotate the camera at about 20-30 revolutions per second, the problem is this requires me to have a framerate of about 120fps (depending on the lense...) and secondary the exposure time needs to be really short for the image not to be blurred (and still have a decent quality).
At the moment i am using a 120fps USB-Webcam by ELP but the results are not satisfiying (the image is used by a computer-vision algorithm which needs a more or less sharp image).
Is there any camera available which can achieve the desired results (and is quite small and lightweight). Money is not our primary concern.
",cameras
Need help modelling and coding a feedback scenario,"
Above illustration is the basic setup for the question. The scenario will be the following:

A potentiometer(providing 0 to 5V) is an analog input to A0. This
  potentiometer voltage is converted to PWM(pin9) which controls a fan
  M1. Which means the human controls directly the M1 fan speed.
This directly speed-controlled M1 fan creates air pressure on a sensor
  Sen1.
On the other hand I want M2 fan to reach to a speed(via pin10 PWM)
  such that Sen2 should output the same voltage as Sen1. Which means
  eventually voltage at A1 analog input should be equal to A2. So one
  can achieve same air pressures at Sen1 and Sen2.

I'm not experienced in PID or other type of controls. I would be glad if someone can help me to relate parameters and code this scenario. I was suggested to ask here instead of Arduino stack-exchange.
edit: Do you think I can use the following idea.  If I make changes as following: PWM to LED in that example will be PWM to M2 in mycase; poti input in the example will be Sen1 input in mycase. And LDR input in the example will be Sen2 input in mycase. Do you think it is worth to try?
","arduino, pid"
Robotc graphical lift arms while reversing,"I am trying to help my son program his robot using RobitC graphical. He has it doing most of what he wants it to do but the code is stuck and won't continue past the arm raise/ hold. What it is supposed to be doing at the point where it gets stuck is raise the arms (to lift an object) Hold the arms up while reversing Turn 90 degrees Reverse (This is as far as he's created as he realized it isn't working) Turn 90 degrees Reverse to the basket and raise the arms again
His code is trying to say to hold the arms up until he creates a new set point
He has tried both of these codes
I know it says (left drive) instead of (arm) on one bit that was corrected and still doesn't work correctly 
","robotic-arm, wheeled-robot"
Recording GPS position,"I build 3d models using Agisoft and need to record the position of the Ground Control Points as accurately as possible, latitude , longitude and altitude. What is the best way of doing this?
","quadcopter, gps"
Denavit Hartenberg parameters - 3DOF articulated manipulator,"I am trying to solve a forward kynematics problem for a 3DOF manipulator.
I am working with the Robotics Toolbox for MatLab created by Peter Corke and after calculte the DH parameters and introduce them into MatLab to compute the fordward kynematics the plotted robot is not what it should be.
I guess I made some mistakes calculating the DH parameters.
Attached is the file where you can see what are the DH frames calculated for each joint and the DH parameters for each frame.
Anyone could give me a clue whether this is the correct answer?
Here is the image with the frames calculated by me.

And here the robot I get from Matlab (using the Robotics Toolbox by P.Corke)

","robotic-arm, inverse-kinematics, forward-kinematics"
Is reprojection error enough in stereo calibration?,"How can I evaluate the quality of calibration procedure when I calibrate one stereo camera? OpenCV uses reprojection error to quantify calibration quality. 
Many reason may cause the error andI learned by google search that the possible reasons are followed
lens distortion / yaw error / sensor tilt / pitch error / roll error / baseline error / focal length error.
During the stereo camera manufacturing process, calibration is performed.
The initial calibration produces the intrinsic and extrinsic cameras parameters which are used to remove lens distortion and
provide image rectification. 
Is reprojection error enough in stereo calibration? My goal is to calculate the depth map.
","cameras, stereo-vision, calibration, 3d-reconstruction"
Model Predictive Control (MPC) horizon and constraints,"I am new to the MPC idea and I am trying to understand the key concepts but there are two things which I found confusing and I didn't find answers regarding to them.
The first one is about the optimized control signal sequence which can be computed from the cost function. If we want to predict, say five steps further, then we will have 5 control signals. After the calculation is done, we will apply only the first signal from the sequence to our system, and them remaining four will be ""wasted"" (I read that it can be used as an initial guess for the next optimization but thats not my point here). My first question is why don't we just predict one step further, and instead of optimizing five signals, we restrict our optimization to just one, which makes the computation faster?
The second question is about constraints. Lets say that I have some restriction on my input signal, say $0 < u < 5$. With some math, we can include these constraints to the optimization task but it takes more time to solve. Why don't we just do an unconstrainted optimization and after our input signals are ready we apply our contraints on them? Obviously this can not be done for state contraints, but i am interested about input constraints.
Thanks for your answers in advance.
",control
How to rotate covariance?,"I am working on an EKF and have a question regarding coordinate frame conversion for covariance matrices. Let's say I get some measurement $(x, y, z, roll, pitch, yaw)$ with corresponding 6x6 covariance matrix $C$. This measurement and $C$ are given in some coordinate frame $G_1$. I need to transform the measurement to another coordinate frame, $G_2$. Transforming the measurement itself is trivial, but I would also need to transform its covariance, correct? The translation between $G_1$ and $G_2$ should be irrelevant, but I would still need to rotate it. If I am correct, how would I do this? For the covariances between $x$, $y$, and $z$, my first thought was to simply apply a 3D rotation matrix, but that only works for a 3x3 submatrix within the full 6x6 covariance matrix. Do I need to apply the same rotation to all four blocks? 
",kalman-filter
Markov Localization using control as an input,"When using Hidden Markov Models in Global Localization problems on the prediction step there is a need to calculate the probability of robot's pose given the actions (control u, odometry):
p(Xt|Xt−1, Ut−1)

where xt-1 and ut-1 are the robot's previous pose and control.
There are different tutorials (this one for instance) and articles on the web with the examples, but most of these examples are for the 1D localization problem where action simply equals to 1 if the odometry is perfect.
What if I am considering 2D space?
For example in 1D example for time step T=1 I would compute like:
p(Xt = 2 | Xt-1 = 1, Ut = 1) = 1

How should I do the computations for 2D case?
","localization, odometry, probability"
Why are two cameras mounted in paralle in stereo application?,"For stereo cameras on the market, two cameras are always mounted side by side and with a displacement that is perpendicular to the cameras’ optical axes. I take this setup for granted. One idea came to my mind whether this is necessary? If two cameras are not parallel and have different focal length, camera calibration can correct the difference. Why are two cameras mounted in parallel? My guess is that two cameras can have a large overlapping region. Am I correct?
","cameras, stereo-vision, calibration, 3d-reconstruction"
Why ODE for optimal control theory,"I am trying to understand optimal control theory which forms the base for reinforcement learning techniques in AI. Whenever I open a lecture or a book or any online notes, everything starts with an ode and then derivation goes the payoff function which is straight forward. 
I am trying hard to comprehend why an ODE models any system ? Many say it easy to begin with but why this model ?
$dx/dt = f(x(t))$
I could not find the reason and decided to ask for help.
","control, theory"
How to implement PD controller to this 2D planar quadcopter dynamics?,"I want to code the dynamics of 2D planar quadrotor and than control it to drive it from one state to another.
Dynamics that I use is taken from the online course fiven by Vijay Kumar in Coursera as follows,
$
\begin{bmatrix}
    \ddot{y}\\
    \ddot{z}\\
    \ddot{\phi}
\end{bmatrix} =
\begin{bmatrix}
    0\\
    -g\\
    0
  \end{bmatrix} +
\begin{bmatrix}
   -\frac{1}{m}sin\phi & 0\\
    \frac{1}{m}cos\phi & 0\\
    0 & -\frac{1}{I_{xx}}
\end{bmatrix}\begin{bmatrix}
   u_1\\
    u_2
\end{bmatrix}
$
it has some linearizations also as $sin\phi->\phi$ & $cos\phi -> const.$
And u1, u2 is defined by;
$u_1=m\{g+\ddot{z}_T(t)+k_{v,z}*(\dot{z}_T(t)-\dot{z})+k_{p,z}*(z_{T}(t)-z)\}$
$u_2=I_{xx}(\ddot{\phi}+k_{v,\phi}*(\dot{\phi}_T(t)-\dot{\phi})+k_{p,\phi}*(\phi_{T}(t)-\phi))$
$\phi_c=-\frac{1}{g}(\ddot{y}_T(t)+k_{v,y}*(\dot{y}_T(t)-\dot{y})+k_{p,y}*(y_{T}(t)-y))$
it is assumed to be the vehicle is near hover condition and commanded roll angle $\phi_c$ is calculated based on desired y-component and is used to calculate u2 which is net moment acting on CoG.
The thing that I dont understand is, don't I need any saturation on actuators? Do I need to implement some limiting part on my code to limit the control signals.
The other thing is, I don't have any desired acceleration. There is those terms in control signal equations. Can I remove them?
The last thing is, my control signals creates some signals causes to vehicle to have order of 10^5 in roll angle by integrating the high angular rates caused by high u2 moment signal I guess. Since the linearization works on small angle approximation, those high angles and rates are problematic. Thus how can I handle it?
","quadcopter, pid, matlab"
How to calculate quadcopter lift capabilities?,"I'm looking for an equation (or set of equations) that would allow me to predict (with fair accuracy) how heavy a payload a quadcopter is capable of lifting.
I assume the main variables would be the weight of the copter as well as the size + power of the 4 rotors. What general approach can one use to make such a determination?
",quadcopter
Advise on ROS move_base costmap footprint not connected error,"I'm using ROS indigo in my project and all the nodes and visualisation (Rviz) seems to be functional when I launch the program using roslaunch. Here are the sensors used in the scooter:

Hokuyo Lidar 
Phidgets Encoder

However, when I start moving the scooter (the robot) using the joystick, the scooter will move physically, but in Rviz, the object detected by the lidar moves to the scooter, instead of the scooter moving in the map. Furthermore, the costmap obstacles will get smeared as I continue to move the scooter forward. Where could I have missed out?
Here are the node connections:

A screenshot of the tf tree:

EDIT:
When I did roswtf, it shows that the robot footprint is disconnected, though the rqt graph shows otherwise. Here is the log of it:
Beginning test of your ROS graph. These may take a while...
analyzing graph...
... done analyzing graph
running graph rules...
... done running graph rules
running tf checks, this will take a second...
... tf checks complete

Online checks summary:

Found 1 warning(s).
Warnings are things that may be just fone, but are sometimes at fault

WARNING The following node subscriptions are unconnected:
* /rqt_gui_py_node_646:
  * /statistics
  * /tf_static
* /amcl:
  * /tf_static
* /rviz_1476945491120458509:
  * /tf_static
  * /map_updates
* /move_base
  * /tf_static
  * /move_base/TebLocalPlannerROS/obstacles
  * /move_base/cancel
* /robot_pose_publisher
  * /tf_static


Found 1 error(s).

ERROR The following nodes should be connected but aren't:
 * /move_base->/move_base (/move_base/global_costmap/footprint)
 * /move_base->/move_base (/move_base/local_costmap/footprint)

Please advise on the possible ways that I can undertake to debug this issue.
","ros, motion-planning, navigation"
Inverse kinematics for a robot lamp,"I've been working on a robot lamp.
Looking something like this:


So i had a few questions. I've decided to simplify the movements by giving the head a fixed orientation like you see in the picture. The lamp can move around it's own center, but those movements are directly relatable to the X Y Z position of the end effector. 
If I have understood all I've read correctly, I'll be able to calculate the other angles, which are now all in the same 2d plane, just by using some geometry right? I've worked out these formula's already (haven't tested anything yet). 
If i do want the head to also move around the same plane, the calculations would get A LOT harder and more complicated and there would be a lot of possible solutions, is that right? Or is there an easy step to get to this point when i already did it with the fixed orientation? And even if it would be manageable, would it be a good addition? Because I want the lamp to look at you, and the generated angles could make it look in a weird direction.
Is writing an algorithm the way to go, or would i be better of just hardcoding movements? What would be your tips if you were to make something like this, regarding movement?
Thank for reading and i hope someone can tell me if i'm on the right track here, thanks.
","robotic-arm, kinematics, inverse-kinematics"
census transform in stereo matching,"The paper Evaluation of Stereo Matching Costs on Images with Radiometric Differences reads that Census gives the best and most robust overall performance on all test sets with all stereo algorithms. I hope census transform can compensate for auto-exposure difference for input left/right images.
I write one Matlab program to apply census transform to left/right images. The resultant images of census transform are more noisy compared to input images. Is there something wrong with my Matlab program.
The input image is Teddy from Middlebury stereo matching website.

The images after census transform 

Image1Name = 'teddy2.png';
Image2Name = 'teddy6.png';
Image1 = imread(Image1Name);
Image2 = imread(Image2Name);
Image1 = rgb2gray(Image1);
Image2 = rgb2gray(Image2);

[DistanceImage, censusImage1, censusImage2] = CensusTransform(Image1, Image2, 5);
figure; imshow(censusImage1, [0, 32]);
figure; imshow(censusImage2, [0, 32]);
figure; imshow(DistanceImage, [0, 32]);


function [DistanceImage, censusImage1, censusImage2] = CensusTransform(image1, image2, window)
[height, width] = size(image1);
radius = floor(window / 2);
image1_padding = padarray(image1, [radius, radius], 'replicate');
image2_padding = padarray(image2, [radius, radius], 'replicate');
censusImage1 = zeros(height, width, 'uint8');
censusImage2 = zeros(height, width, 'uint8');
DistanceImage = zeros(height, width, 'uint8');
for i = 1:window
 for j = 1:window
   image1_shifted = image1_padding(i: i + height - 1, j: j + width - 1);
   cmp1_image = image1_shifted > image1;
   censusImage1 = censusImage1 + uint8(cmp1_image);
   image2_shifted = image2_padding(i: i + height - 1, j: j + width - 1);
   cmp2_image = image2_shifted > image2;
   censusImage2 = censusImage2 + uint8(cmp2_image);    
   xor_image = xor(cmp1_image, cmp2_image);
   DistanceImage = uint8(xor_image) + DistanceImage;
end
end
end

","computer-vision, cameras, matlab, stereo-vision, 3d-reconstruction"
Polling or Timer interrupt?,"We hope to build a simple line follower robot and we got a problem when we were discussing about PIC programming.
We planed to write a endless loop, check the sensor panel reading and do the relevant stuff for that reading.
But one of our friends told us to use a timer interrupt to generate interrupts in definite time periods and in each interrupt check the sensor panel reading and do the relevant stuff for that reading.
But we can't figure out which is best: the endless loop in main method OR timer interrupt method.
What is the best way, and why?
","sensors, microcontroller, interrupts"
is designing a pcb with multiple controllers a good schematic design?,"I am trying to design a bot, it has to measure the temp I am using an 8051 so the adc is taking up a lot of i/o pins so I am thinking of switching it out with a thermostat or put another controller and interface them (which also has the advantage of letting me add more features) But I wanted to make sure in general is it a good design to add multiple micro-controllers on the same pcb, will it cause hardware problems?
","microcontroller, design, circuit, hardware"
issues with LSA-08 line sensor for line tracing,"I am using LSA-08 self calibrating sensor for line following purpose. But the problem with my sensor is it can't distinguish between faint blue color and white color (having RGB values 185, 217, 235). Is there any technique for distinguishing them ?
",line-following
Self learning robotics algoritm: feedback needed,"I'm working on a robotics project and I've got this idea for a self learning algorithm. I'm looking for some feedback on this, and specifically on whether this is a more common way of doing things. 
I simply want to store a lot of numerical log data of previous actions the robot took and the results it got. I then want to let it search through the DB multiple times per second so that the robot can make decisions based on that and thus learn from it's actions (like humans do).
So I constantly log a lot of data. One simple log record could be for example:

speed: 5.43
altitude: 35.23
wind_speed: 6.19
direction: 27
current position: [12, -20]
desired position: [18, -25]
steering decision: 23
success: 12

The desired_position are coordinates on a 2 dimensional matrix and the success is how close it came to the desired position within 10 seconds (so the lower the better).
I then have a certain situation for which I want to find a comparable experience in the database. So let's say my current situation is this:

speed: 5.13 
altitude: 35.98 
wind_speed: 7.54 
direction: 24 
current position: [14, -22]
desired_position: [17, -22]

As you can see it doesn't have a steering decision or a success yet, because it still has to make a steering decision and only after taking that action and seeing the result the success rate can be calculated.
So I want to search for the record ""closest"" to my current situation within certain boundaries, and which has the best (lowest) success rate. So for example, the boundaries could be that the direction cannot be more than 10% difference, and the heading cannot be more than 15% difference. So I first make a selection based on that. I'm left with A LOT of records. I then calculate for every field in every record the percentage difference, and accumulate those per record so that I get some sort of “closeness factor”. Once that's done I combine the closeness with the success, order by that number, and take the top record to base my decision on for the action to be taken. I take the steering decision of that record and randomly change it to something which is within a certain change percentage range. I do this because people also experiment, you try something new every time. And as the system becomes better at steering the robot (success factors get better) I also reduce the randomization change percentage. This is because as people get better, they also know that they are closer to the objective and they don't need to experiment as much any more.
So to steer my robot I will run the following process as often as possible (I will try to run it 20 times per second) and use this system to steer my robot.
So my questions are; 

Do you think that this can work? 
Is this kind of thing used more often? 
Does anybody know a database which would make it possible to query based on a dynamically calculated factor (the closeness factor)?

","algorithm, steering, self-driving"
Path planning for visual servoing,"I am at moment trying to implement a visual servoing application. 
the robot I am using is a UR5, and TCP has a stereo camera mounted on to it. The idea is to move the end effector according to the object being tracked. 
The path-planning algorithm for this system should comply with some rules. 

The path which it creates should be collision free, and always keep the object being tracked at sight at all time. 

Having a path that keeps the object in sight has been bit of problem.  Sometime will the end effector rotate around itself, messing up  measurements taken and thus the tracking itself. 

It should be able to maneuver away from static obstacles. 

A Possible solution?
I thought of a possible solution. Since my current state and desired state is defined by two different sphere, A possible solution would be to create a straight line between each center of each sphere, and between the current position and desired position, such that a straight path in between could be computed easily.  which always keeps itself oriented toward the object. Problems is that I am not sure how I should handle collision here.

Update
  Or use it as a heuristic for a heuristic based path planning? 
","robotic-arm, motion-planning, algorithm"
"One motor, two wheels (motor direction rotates one wheel forward, reverse for other wheel)?","Rewriting this whole question as I've learned a lot more since I first tried to ask.
I'm building a tiny bot and looking to use two motorized wheels for movement, but only one motor. I'll have a 3rd wheel (caster) for balance.
My goal is to have the wheels move opposite to each other when the motor turns and each wheel reverse direction when the motor reverses direction.
The tricky part is I want the ratio to be uneven between the wheels. By this, I mean when one wheel rotates clockwise, it should rotate faster than the other wheel rotating counter-clockwise. This needs to hold true regardless of which wheel is rotating which way.
The end result should be that I could run the motor in one direction continuously to have my bot drive in a small circle. If I switch motor directions, it would drive in the same size circle the opposite direction. If I alternate motor directions frequently enough, the bot should move in a fairly straight line (or if I alternate the motor too infrequently, an S like pattern).
The closest I can envision so far (thanks to those that have responded here) is to use bevel/miter gears to have my wheels rotate opposite to each other and then to use two differently sized ratcheting mechanisms per wheel working in opposite directions. This would allow each ratcheting mechanism to trigger only in one direction and the RPM per direction would be related to the size of that direction's ratcheting mechanism.
Is this the best way to achieve my goal? Is there a name for this concept or is it so uncommon I'll have to build/fabricate it all myself? As far as a ratcheting mechanism, I believe I'd be looking to use a freewheel clutch or I'd need an oscillating lever carrying a driving pawl over top a ratchet gear. My biggest struggle in finding affordable parts is the ratcheting mechanism.
","motor, gearing"
How to Calculate the Translational and Rotational Velocity of a 4 Wheeled Robot?,"I have a robot with a 4 wheel drive using mecanum wheels to allow for more mobility. For the engineering documentation I am looking to find both the translational velocity and rotational velocity of the robot. To begin I started with the (incorrect) assumption that the tangential velocity of the wheels was the overall linear velocity, but that yielded unreasonably high values. What is the correct way to mathematically evaluate the translational and rotational velocity of the robot?
","wheeled-robot, kinematics"
Strange EKF localization covariance behavior during prediction only,"I've implemented a simple robot simulation based on the equations from EKF localization with known correspondence found in Probabilistic Robotics by Thrun et. al. Everything seems to be working but I noticed the covariance matrix has an odd behavior when doing prediction only. 
When I move the robot with a forward velocity and some angular velocity and no correction the covariance grows (size of eigenvalues), as expected. But when I move the robot backwards with some angular velocity the covariance shrinks a bit before growing again. I expect the covariance to always increase if there is no correction.
I checked my implementation many times for errors but I now suspect this issue may be attributed to the fact that the Jacobians V and M uses signed values of velocity and angular velocity, instead of absolute?
Here is a video showing the covariance shrinking and growing. In this video the true pose of the robot is in green with an imaginary depth sensor shown as a green cone. There are no landmarks, so no correction step. The gray is the estimated pose plus covariance ellipse at 95% confidence interval.
https://www.youtube.com/watch?v=RcHkCijyG7c&feature=youtu.be
UPDATE:
I've attached a graph that illustrates this issue better. Two graphs are shown below. The graph in red is running the EKF prediction on noise free input and plotting the area of the ellipsoid of the covariance. The area (uncertainty) monotonically increases as expected. I repeat the same thing but at t=10 I inverted the velocity, resulting in the blue graph. There is an oscillation in the area for some reason yet to be determined.

The Octave script I wrote can be found here http://pastebin.com/rQyczVbm
","localization, ekf"
Alternatives to Kalmam Filter,"I am currently working on a self balancing robot project. I am going to use a MPU6050 to get data from both the accelerometer and the gyroscope. Since I need to get accurate data in a very small amount of data I need to filter the raw data I get. So many people have suggested me to use the Kalman Filter but I could not comprehend it (the maths behind it). Are there any other types of filters I can use in my project?
Thanks in advance.
","arduino, kalman-filter, balance, filter"
How to make sense of IMU (BNO055) data,"I have a project that I'm working on that needs data about which directions something has moved and how quickly from a given point (accelerometer and magnetometer?). I have the working python code for the BNO055, I'm just having trouble interpreting it. I guess these are my specific questions:

Is the data from a starting point (doesn't seem likely) or a set time period that continues? If so, what is that time interval? Do I define it by how often I ask for the data?
Is magnetometer and accelerometer data what I should be using for my task?
What is a rough pseudocode outline for how I could convert this data into something easier to understand (i.e. an updating distance or x,y,[z] coord system)

","imu, python"
Problem with inverse kinematics in robotic arm using Matlab,"I define my robotic arm with following code
% Link('d', 0.15005, 'a', 0.0203, 'alpha', -pi/2)
L(1) = Link([0  0.15   0    pi/2     0], 'standard');
L(2) = Link([0  0     0.15   0       0], 'standard');
L(3) = Link([0  0     0.15   0       0], 'standard');
L(4) = Link([0  0     0.15   0       0], 'standard');
% set limits for joints
 L(1).qlim=[deg2rad(-45) deg2rad(45)];
 L(2).qlim=[deg2rad(-45) deg2rad(45)];
 L(3).qlim=[deg2rad(-60) deg2rad(60)];
 L(4).qlim=[deg2rad(-50) deg2rad(50)];
 %build the robot model
rob = SerialLink(L, 'name','rob');
qready = [0 0 0 0]; % initial position of robot

And I solve inverse kinematic and plot robotic arm with code
 Td = transl([0.05 0 -0.20]);
q = rob.ikine(Td, qready,[1 1 1 0 0 0]);
plot(rob,q,'noname');

Its results are 0  -139.0348635    82.65184319 -1.286384217 which is four angles named theta1, theta2, theta3 and theta4 respectively. Now the point is I gave joint limit for theta2 as -45 to 45 degree but result output is -139 degree. Same case with theta3. Why it is so? Another thing is when I plot these angles robotic arm cross each other as shown in figure. I want to know what is wrong with code or I'm missing any thing. 

","robotic-arm, inverse-kinematics, matlab"
Nao doesn't talk in ROS,"I started to use ROS and NAO. I want to make NAO say ""hello world"" using the command:
rostopic pub ""/speech"" std_msgs/String ""Hello World"".

and I get this: 
publishing and latching message. Press ctrl-C to terminate 

But nao doesn't speek (it blocks at that output and I have to press ctrl-C to stop it).
Can anyone help me please? What could be the problem?
Further information: I find the topic /speech and with the command rostopic echo /speech, I get: 
data: Walker online
---
data: Hello World
---

",ros
Possible stabilize Crazyflie 2.0 drone autonomous flight without additional sensors?,"I just bought the Crazyflie 2.0 drone. This is my first drone, and it is my first time programming a drone.
My first goal is simple: Make the drone hover in place stably for 10 seconds.
I found a simple example script that turns up the thrust and then lands the drone. I modified this to 1) extend the time to 10 seconds, 2) reverse thrust if the drone starts tipping, and 3) constantly display the roll, pitch, and yaw in the console.
When I run this, the drone often flies randomly around the room and runs into things; it does not just lift up and hover stably. Why is this? How can I improve things so that it's much more stable? Do I need more sensors, or can I pull this off with just programming?
""""""
Simple example that connects to the first Crazyflie found, hovers, and disconnects.
""""""

import time
import sys
from threading import Thread
import logging

import cflib  # noqa
from cfclient.utils.logconfigreader import LogConfig  # noqa
from cflib.crazyflie import Crazyflie  # noqa

logging.basicConfig(level=logging.ERROR)


class HoverTest:
    """"""Example that connects to the first Crazyflie found, hovers, and disconnects.""""""

    def __init__(self, link_uri):
        """""" Initialize and run the example with the specified link_uri """"""

        self._cf = Crazyflie()

        self._cf.connected.add_callback(self._connected)
        self._cf.disconnected.add_callback(self._disconnected)
        self._cf.connection_failed.add_callback(self._connection_failed)
        self._cf.connection_lost.add_callback(self._connection_lost)

        self._cf.open_link(link_uri)

        self._status = {}
        self._status['gyro.x'] = 0
        self._status['gyro.y'] = 0
        self._status['gyro.z'] = 0

        print(""Connecting to %s"" % link_uri)

    def _connected(self, link_uri):
        """""" This callback is called form the Crazyflie API when a Crazyflie
        has been connected and the TOCs have been downloaded.""""""
        print(""Connected to %s"" % link_uri)

        # The definition of the logconfig can be made before connecting
        self._lg_gryo = LogConfig(name=""gyro"", period_in_ms=10)
        self._lg_gryo.add_variable(""gyro.x"", ""float"")
        self._lg_gryo.add_variable(""gyro.y"", ""float"")
        self._lg_gryo.add_variable(""gyro.z"", ""float"")

        # Adding the configuration cannot be done until a Crazyflie is
        # connected, since we need to check that the variables we
        # would like to log are in the TOC.
        try:
            self._cf.log.add_config(self._lg_gryo)
            # This callback will receive the data
            self._lg_gryo.data_received_cb.add_callback(self._gryo_log_data)
            # This callback will be called on errors
            self._lg_gryo.error_cb.add_callback(self._gryo_log_error)
            # Start the logging
            self._lg_gryo.start()
        except KeyError as e:
            print(""Could not start log configuration,""
                  ""{} not found in TOC"".format(str(e)))
        except AttributeError:
            print(""Could not add gyro log config, bad configuration."")

        # Start a separate thread to do the motor test.
        # Do not hijack the calling thread!
        Thread(target=self._hover).start()

    def _connection_failed(self, link_uri, msg):
        """"""Callback when connection initial connection fails (i.e no Crazyflie
        at the specified address)""""""
        print(""Connection to %s failed: %s"" % (link_uri, msg))

    def _connection_lost(self, link_uri, msg):
        """"""Callback when disconnected after a connection has been made (i.e
        Crazyflie moves out of range)""""""
        print(""Connection to %s lost: %s"" % (link_uri, msg))

    def _disconnected(self, link_uri):
        """"""Callback when the Crazyflie is disconnected (called in all cases)""""""
        print(""Disconnected from %s"" % link_uri)

    def _gryo_log_error(self, logconf, msg):
        """"""Callback from the log API when an error occurs""""""
        print(""Error when logging %s: %s"" % (logconf.name, msg))

    def _gryo_log_data(self, timestamp, data, logconf):
        """"""Callback froma the log API when data arrives""""""
        # print(""[%d][%s]: %s"" % (timestamp, logconf.name, data))

        self._status['gyro.x'] = data['gyro.x']
        self._status['gyro.y'] = data['gyro.y']
        self._status['gyro.z'] = data['gyro.z']

    def _hover(self):
        start_time = time.time()
        run_time = 7

        thrust_multiplier = 1
        thrust_step = 500
        thrust = 20000
        max_thrust = 39000
        roll = -1.00
        pitch = -2.00
        yaw = 0

        # Unlock startup thrust protection.
        self._cf.commander.send_setpoint(0, 0, 0, 0)

        # Turn on altitude hold.
        self._cf.param.set_value(""flightmode.althold"",""True"")

        while thrust >= 20000:
            # Update the position.
            self._cf.commander.send_setpoint(roll, pitch, yaw, thrust)

            time.sleep(0.01)

            if thrust >= max_thrust and time.time() - start_time >= run_time:
                # Reverse thrust
                thrust_multiplier = -1

            if thrust <= max_thrust or thrust_multiplier == -1:
                thrust += thrust_step * thrust_multiplier

            # Reverse thrust if the drone tips over.
            if abs(self._status['gyro.x']) >= 75 or abs(self._status['gyro.y']) >= 75:
                print('Aborting')
                thrust_multiplier = -2

            print(self._status['gyro.x'], self._status['gyro.y'], self._status['gyro.z'])

        self._cf.commander.send_setpoint(0, 0, 0, 0)

        # Make sure that the last packet leaves before the link is closed
        # since the message queue is not flushed before closing
        time.sleep(0.1)
        self._cf.close_link()


if __name__ == '__main__':
    # Initialize the low-level drivers (don't list the debug drivers)
    cflib.crtp.init_drivers(enable_debug_driver=False)
    # Scan for Crazyflies and use the first one found
    print(""Scanning interfaces for Crazyflies..."")
    available = cflib.crtp.scan_interfaces()
    print(""Crazyflies found:"")
    for i in available:
        print(i[0])

    if len(available) > 0:
        le = HoverTest(available[0][0])
    else:
        print(""No Crazyflies found, cannot run example"")

As you can see, I tried simply adjusting the roll and pitch (to -1.00 and -2.00), but that did not help much. When I use the GUI and a joystick to control the drone and I adjust the roll and pitch trim values to -1.00 and -2.00, this definitely helps stabilize the drone.
Any ideas are welcome. Thank you!
","quadcopter, uav"
How does one implement a third order complementary filter for estimating altitude using data from an accelerometer and a barometer?,"I am working with the CJMCU build of cleanflight on a small drone. As of now, the algorithm for altitude hold uses a first order complementary filter to combine data from the barometer and the accelerometer (after integrating the accelerations twice). However, I have noticed a considerable lag in the altitude readings and this seems to be hampering the control algorithm's performance.
The filter in question has been implemented in https://github.com/diydrones/ardupilot/blob/db8a2f7e8bb2183e6d281e7a348d455d855cf5e1/libraries/AP_TECS/AP_TECS.cpp
However, I'm unable to understand how this works.
Pardon me for any errors I may have committed.
","quadcopter, sensors, sensor-fusion"
Create 2 kill switch?,"Any ideas on ways to install a kill switch on the Create 2?  I saw how the battery attaches to the main board with springs, so that blocks me from using a plain old battery switch.  Are there any test points that can be grounded to shut down when in full mode?
",irobot-create
Need clarification on potential fields for robotics,"I am taking a course on AI robotics from a computer science department but my background is in mechanical engineering. I am having some difficulty with ambiguous terminology in virtual potential fields. All of the sources I have seen to define the virtual potential fields with a physics basis:
$$F(q) = \nabla U(q) $$
or that the force imposed by the potential field is the gradient of the potential function.
Then, the CS sources I have seen will later set the velocity set point of the robot controller equal to:
$$\mathbf{q}_s = \nabla U(q)$$
essentially using the imposed force as a velocity setpoint. But none of the sources mention this swap. So am I missing something, or is the term virtual potential field sort of a misnomer? Maybe it should be virtual velocity potential field?
Here are some CS course slides I am looking at:
http://cs.gmu.edu/~kosecka/cs685/cs685-potential-fields.pdf
http://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf
Thank you!
","mobile-robot, artificial-intelligence, path-planning"
Motion planning with sequential convex optimization,"I recently read a paper titled Finding Locally Optimal, Collision-Free
Trajectories with Sequential Convex Optimization by John Schulman, Jonathan Ho, Alex Lee, Ibrahim Awwal, Henry Bradlow and Pieter Abbeel.
The authors mention that the end-effector final pose constraint can be readily incorporated in the planning scheme which is based on solving an unconstrained optimization with the equality and inequality constraints added in penalty function.
Let $F_{targ} \in $ SE(3) be the desired pose and $F_{cur}(\theta)$ be the current pose, then the pose error is given as $F^{-1}_{targ}F_{cur}(\theta)$. However, I am wondering that if we plan motion in the joint space, then how can this error be incorporated in the objective function as a penalty term?,since $F_{cur}(\theta)$ is a highly nonlinear, $nonconvex$ forward kinematics map, are we linearizing the forward kinematics map to make it convex and add it in the penalty formulation?
","motion-planning, humanoid"
SLAM: Why use two cameras (stereo) if SLAM can be done using single camera (monocular)?,"I am aware that algorithm for monocular SLAM is complex compared to stereo SLAM. But my question is if by any means it is possible to do SLAM using one camera why one should use two cameras to do the same thing?
","slam, computer-vision, navigation, visual-odometry, monocular"
How does a measurement model for camera based PNP work?,"I am trying to write an EKF that can estimate the covariance of a pose estimate, where the estimation is being done by a PNP algorithm and 3D-2D correspondences in images. Although EKF based camera SLAM is pretty common, I've noticed that usually those techniques tend to integrate IMU data, as well as try to refine the map points along with the pose of the robot itself, thus considering both the map and localization somewhat unknown. But if I want to consider the map points as predefined and stable, and just want to compute the pose of a camera in 6 DOF, a relationship that is expressed simply between 3D and 2D points as:  
$ \begin{bmatrix}
             x \\ y \\ 1 
    \end{bmatrix} 
    = 
    K*\begin{bmatrix} R && t \end{bmatrix} * \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} $
where $R$ and $t$ are the unknowns. This result can further be optimized by trying to minimize the reprojection error, but I am wondering how this can be reconstructed as an EKF measurement equation, and thereby estimate the state covariance, with my state containing 3D coordinates and the Euler angles of the camera pose as $[x, y, z, r, p, y]$.
","localization, kalman-filter, cameras"
Line follower robot with 2 or 4 motors?,"I am developing a line follower robot and I am not sure about how many motors should I use: two or four. I'm thinking of using four, but I do not know if it's worth it (it will let the car be heavier, consume more power...). Does anyone have an idea? I'm planning in use something like this design here, of Aniki Hirai: http://anikinonikki.cocolog-nifty.com/.shared/image.html?/photos/uncategorized/2014/11/19/cartsix04.jpg.
The engine I'll use is a micro-metal motor, from Pololu, just like in the link:
https://www.pololu.com/product/3048.
I know the question is a little bit vague, but I don't know another way to ask this.
","motor, design, line-following"
Visual SLAM for wheeled robot,"I would appreciate your help for choosing the appropriate real-time SLAM algorithm.
I am interested in having autonomous robot, thus a SLAM component is needed for autonomous navigation.
The platform is Kobuki robot (Turtlebot2 without Kinect) which has wheel odometry.
To get a better localization, I want to use camera (Fisheye) - optionally looking upwards.
There is a lot of algorithms available online, most of them output 6DoF (do I need 6DoF for a wheeled robot? (x,y,theta) is enough, right?). I tried using several algorithms, but still didn't find the optimal one.
I believe that Loop-Closure is needed as well, because the final application would be to have a robot that coverage the area (visit each point once).
ROS is used to implement this robot, with small dev board (Intel Atom x5 processor).
Thanks in Advance
","slam, wheeled-robot, odometry, visual-odometry"
How do I hack RC transmitter of my drone by realised communication with my PC?,"I have a Chinese drone (cost less than $100), that works with a 2.4Ghz transmitter of 2.4. I am trying to build an application in C++ to control the drone with PC. I am thinking of using an ARDUINO and a NRF24L01 module.  But I have not been able to establish communication with the drone. I don`t understanding the form of communication very well, is there a post, information, book, blog or idea that can help me?
","quadcopter, arduino, control, radio-control"
Geometrical solution to an IK problem for a humanoid robot leg,"On the attached figure, I show a graphical representation of the problem I am facing. I have developed a humanoid robot with a thigh which makes an angle with respect to the leg. It means that there is always a constant distance (R) – whatever the rotation of the thigh is - between the Pelvic and the leg. Besides, the foot is articulated with a forefoot and a midfoot.

If I want to compute the orientation of the leg and thigh with respect to a given position and orientation of the Pelvic (represented by the point $C$) and the Foot (represented by $A$ the position of the ankle and $\vec{U}$ the orientation of the foot.
I come up with the geometrical problem of computing a plane $P$, passing through $C$ with one orientation given by $\vec{U}$, which is tangent to a sphere whose center is the extremity of the Pelvic (point $C$).
Knowing the point of tangency $T$ I can compute the position of the knee $K$ and then the angular values for the leg and the thigh.
But I cannot find the equations to solve it so far... or may be there is another geometrical solution I did not think of?
I am trying to find a geometrical answer before going through a DH description + finding the values via decoupling and so on...(""classical"" Ik resolution).
","inverse-kinematics, humanoid"
Standard Deviation from one axis to another axis,"I have radar mounted on a car.  For each detection, the radar returns these variables.

relative distance between the object/host vehicle in forward direction
standard deviation of the relative forward distance
relative distance between the object/host vehicle in left/right direction
standard deviation of the relative left/right distance

I'm trying to do coordinate transform of the above data, so that I get relative distance/standard deviation in global coordinates (North, South, East, West)
Distance is easy since it only requires to rotate the axis by the amount of angle between the vehicle body-fixed axis and the global axis.
How about standard deviation?  How do I transform the standard deviation from the vehicle body-fixed axis to global axis?
","localization, statistics"
How can I calculate Center of mass for each link of Robot?,"I want to calculate 'r'

I know , and the Position of(X, Z)
But I don't know how to apply it in this robot.
","mobile-robot, robotc, humanoid"
Inverse kinematics Calculation for 3dof robotic arm,"I am doing a project, to draw images provided using robotic arm. Initially edge detection is done on image to be drawn and coordinate value is obtained as (x,y).
How to calculate 3 joint angles inverse kinematics from this value.
",inverse-kinematics
SLAM without data association?,"I would like to build 2D EKF-SLAM in openGL. I've implemented the entire virtual environment in which there is a robot that moves in 2D and there are some landmarks(feature-based map). I have the motion and observation models. Also, I've implemented the sensors with Gaussian noise. Now, I would like to use MRPT to build SLAM. At this point, I don't want to use data association that is the robot moves and detects its pose and landmarks and discard the previous data which means I only concern with the current state vector. My question is Is it possible to build SLAM without data association? Please suggest me some articles so that I can enrich my background about only this issue. 
","slam, ekf"
which is best IMU for Quad-Copter,"Actually i am trying to write my own Flight Controller for Quad-Copter that is controlled by Remote over radio signals.So for a flight controller i have to buy a inertial measurement unit(IMU).So problem is that i visit a two different sites both selling a MPU6050 Triple Axis Accelerometer and Gyro.But both sites name it same but it's look different IMU unit that one is buying from hallroad.org.and that one is .is that same IMU or different if they are different what is difference b/w them?Which is best for Quad-Copter Flight-Controller?
","quadcopter, imu, accelerometer, gyroscope"
Utilizing the inertial sensors in an AR Drone,"I'm trying to figure out how to use the inertial sensors in an AR Drone to perform a rough version of dead reckoning. I want to move the drone around a room (without flying it) and using the velocity and orientation data from the drone to plot the trajectory that I have moved it so far.
I know that inertial odometry data is prone to heavy drift with continuous use, but for a short term exploration, I am ok with that.
I'm running the ardrone_autonomy package on ROS and I am using the odometry data from the drone to plot my trajectory on Rviz. However, Rviz only shows me the orientation of my drone and does not update movement whatsoever.
This is how my Rviz looks: 

As you can see, even if I move the drone around the entire room, the position on the map does not get updated, but the orientation does.
Can anyone tell me what I am doing wrong here?
","ros, accelerometer, gyroscope, uav"
Is it better to use software or hardware for object sorting?,"For a sorting machine where the objects to be sorted have various sizes, color, shapes, and patterns, is it more optimal (in terms of minimal time of the overall process and maximal precision and accuracy) to use a sorting algorithm or to use different dimensions in the physical design to do the sorting?
","sensors, design, automation"
Autonomous boat obstacle avoidance,"I am currently considering joining the MicroTransat challenge and develop an autonomous boat able to survive the harsh Ocean environment.
However, as for security and to ensure the boat doesn't sink due to collisions, I am looking for possible ways to implement a basic obstacle avoidance system. The prototype will harvest power from solar panels (and maybe a Stirling engine) and will run a Raspberry Pi as route planner.
Up to now I have come up with the following:

AIS receiver: can be useful to track ships above 300t and simple receivers don't use too much power;
Hydrophones array: may sense boat engines and the sound origin, very unreliable as a system;
Lidar/ultrasonic rangefinder: not suitable for marine environment;
Computer vision: difficult to implement but highly reliable, I guess sea waves will be a issue.

Ideally, the boat should keep away from other boats, floating objects and rocks, yet just ship avoidance would be very helpful.
Which one(s) do you believe can be included to offer a basic obstacle avoidance ability?
","control, navigation, research"
NAO applications using ROS,"I want to develop an application for NAO robot using ROS (I already started to use ROS with NAO). But I don't know how to choose a scenario for my robot depending on ROS packages (SLAM, objects recognition...).
My question is: is there any ROS packages for NAO applications that i can started with (turtlebot robot for example has the navigation stacks -SLAM- in ROS).
Can anyone help me please??
",ros
Forward and Revers Kinematics For Modified Hanging Plotter,"I'm building a modified version of the standard hanging plotter (v-plotter). The basic idea is that you have two cables hanging from stepper motors which form a triangle supporting the pen at the tip. 
My design the strings anchored at points $C$ and $D$ which causes the behavior to be somewhat different that the normal plotter, especially when operating close to the motors. 

I was able to work out the forward kinematics fairly easily, but the inverse kinematics are turning out to be a real headache. You can see my attempt at a standard geometry solution on math.stackexchange here. 
Is there anything specific to calculating kinematics for robotics which could help me?
I'm not interested in modifying the hardware to make the math easier. I'm also not interested in discussing the center of gravity of the pen, cable weight...etc. I want to calculate the kinematics for this as an ideal system.
Any and all advice is appreciated.
Thank you.
","control, kinematics, inverse-kinematics"
Save Depth Map Video,"I have recently purchased an Orbbec Astra camera, which uses the same technology and produces the same style depth map as a Microsoft Kinect.
What would be the correct file format to save the depth map frames, How would I go about saving the videos recorded?
I have been able to load a stream but am not sure what format the frames should be saved in so that i can load them for testing at a later stage and still have all the same information.
I am using OpenNI2, OpenCV3.1.0 and C++.
","c++, opencv, openni"
How can I apply different control approach without having a state space model?,"Recently i am working on a hydraulic control system of a vehicle. But in this system I don't have any mathematical modeling of the system so i am not able to get any state space model for the given system. I have to control the length of Piston in Hydraulic Cylinder by giving input of desired piston position. Modeling of this system is established in Matlab/Simulink using physical modeling tools SimScape Toolbox and simFluids. As one approach i had already try to apply a feedback with P-control as show in the figure Matlab System  and also the gain of P control is derived by trial and error.
Now if i want to apply state obeserver or optimal Control than how can i apply on this given system.
And is necessary to derive mathematical model of this system to desing controller?
Thank You.
","control, matlab"
Cheapest GPS that is accurate to ca. 30cm in woodland,"As part of my PhD field work it would be useful to have latitude/longitude measurements for the locations of ant nests that I am working on. These ant nests can be as close as 50cm together so the accuracy of the system would (I think) need to be higher than is available from a phone or basic GPS system. Does anyone know what system would be best for getting this sort of accuracy? My budget is probably only around £200. If it isn't possible in that price range it would be good to know what system I would need to use so that I can see if I can just borrow such a system.
Thanks a lot! 
",gps
Arduino output SPWM using Matlab,"I have Arduino and Matlab which has hardware support package for Arduino.
I want to create SPWM signal (sinusoidal pulse width modulation) to be the output of the Arduino board.
I could generate the signal required in Matlab using this code
function spwm = SinWave(frequency)
nsamples = 1250 * frequency;
t = linspace(0,1,nsamples);
sn = sin(2*pi*frequency*t);
st = sawtooth(2*pi*frequency*10*t);
spwm = abs(sn) > abs(st);
plot(t, sn);
hold on;
plot(t, st);
plot(t,spwm);
axis([0,1,-1.2,1.2]);

Now SPWM has the samples for the signal, I tried sending it on pin 13 using the following function
function writeSPWM(arduino, spwm)
for k=1:length(spwm)
    writeDigitalPin(arduino, 'D13' ,spwm(k))
end

Then I used the following two lines in command window
a = arduino()
writeSPWM(a, SinWave(5))

I getting my signal shape with very low frequency(it is really much bigger)
Is there a better way to achieve my goal? using Matlab is necessary but I have no problem if I have to combine coding throw Matlab and Arduino C.
","arduino, matlab, pwm"
Prediction of Ready to Use Robotic Control Cards Codes,"Suppose you have a control card. We do not have access to the control card's code and we do not know which control algorithm is used (PID, PD, PI, Fuzzy, ...). We can only measure the control inputs we apply and the output signals the card generates. For examples a quadrotor card, inputs are yaw, pitch roller and throttle and the outputs are propeller pwm signals. Can we predict the algorithm and coefficients/properties used? Is there such a method or study? There are many ready to use control cards, we can imitate or improve for robot control. I think it is important to develop a method for extraction/prediction of algorithm on control board. Thank you for your help and your views.
","mobile-robot, control, pid, algorithm"
SLAM for Autonomous car,"I am working on SLAM for autonomous car like vehicles with 2D lasers and IMU (deriving odometry).
I would like to know how efficient is using the existing SLAM algorithms (for example: gmapping in ROS based on rao blackwellized particle filter).
till now i find MAP are high in volume and speed of vehicle is high and most importantly computational time compared to Mobile robots.
Are there  any other important factors to consider for car like vehicles in using SLAM algorithm.
","mobile-robot, localization, slam"
How to calculate what wind speed is required to lift 1kg?,"as the title says, i want to calculate how much km/h is required to lift 1kg of weight.
Please explain this to me as simple as you can since i'm really noob in this.
Thanks.
",motor
Proportional controller with state feedback controller,"I have a transfer function G(s)=1/s(s+10) with sampling and holding, it is easy to relocate poles at 0.5 and -0.5 using a state feedback controller. But if the proportional controller is added to the transfer function, as in G(s)=K/s(s+10), how can the state feedback controller be designed?
",control
What is the effect that the size of the correlation window (in pixels) has on the matching performance of stereo algorithms?,"While stereo algorithms perform matching, they usually compare pixels within a region around the feature. I would like to know what effect the size of that search window has on the matching performance. 
And what sort of features don't get detected if you go on increasing the size?
Thank you.  
",stereo-vision
Going around Voltage Limitation on Battery for Steppers?,"Am building a device that will use two stepper motors and a servo. One of the main requirements, is that the on-board battery cannot exceed 9.6 volts.
The stepper motors have not been picked yet, and thus I have the option of choosing one with a lower voltage.
Which method would be the best:

Use two 12v stepper motors with a 9.6 volt battery, and simply have them run at lower voltage.
Or, Use two 12v steppers, and use a DC to DC voltage booster to increase voltage to 12v. ( I don't know how well this would work, regarding constant current and whatnot.)
Or, Use two 6v steppers, and use a ~6v battery. (Not fond of this idea, as 6v steppers seem to have much different specs than normal Nema17 12v ones. However, I would still use them if needed.)

","battery, stepper-motor"
Parallel connection of batteries with equal voltage but different Ah?,"I bought some batteries for a project with the recommended voltage but too weak Ah.
The batteries are to be connected in parallel. I can't currently afford to upgrade both batteries, but I was curious if I can try upgrading one of them and connecting it in parallel to one of the ones I have now? My robot is going to weigh less than the one on which it is based, so perhaps just upgrading one is sufficient. 
Or is that unsafe?
",battery
How to use registration inputs on a Kinetix servo?,"I am working with a Kinetix 5500 servo drive with a rotational actuator to locate a keyway on a shaft. The shaft will rotate, and a laser sensor will detect the height difference when the keyway passes in front of the sensor. I need to record the position of the keyway so I can orient it to a known position.
The laser sensor will be wired to Registration Input 1 (see pages 62 and 65 of user manual above), to capture the position of the servo more accurately than using a sensor wired in to the PLC's I/O.
I have been able to find little information about how to actually use a registration input in my PLC program. I know there is some sequence that is required to arm the registration input to look for a transition, and then grab the saved servo position from the drive. 
How does this process work? Is there documentation from Rockwell that I'm not finding? (I found a programming manual for PowerFlex, but not Kinetix)
Additional information: the PLC is an AB ControlLogix, and the drive is connected via Ethernet. I am familiar with setting up, tuning, and issuing Motion Direct commands for Kinetix drives.
Related nonexistent tags: Allen-Bradley, Kinetix, ControlLogix
","sensors, servomotor, plc"
Simulating Dynamixel motors in Gazebo,"I'm trying to simulate a humanoid robot using Gazebo with plugins. Since our actual model uses Dynamixel motors, I'd like to know how exactly they work to make the simulation as realistic as possible.
Gazebo offers two options to control joints. One is a PID controller, provided by the JointController class. The other way is to directly set a torque to the joint. (The PID method too is ultimately implemented using torques).
Currently, I'm trying the PID-based implementation. I've used a P-only controller with damping on all joints (I've had to guess both values). However, there is a large amount of noise, and the difference between actual and desired position is at times as much as 10-12 degrees (especially when the foot of the robot hits the ground).
Does the actual motor use a PID controller as well? I can't seem to find the details here, Dynamixel EX-106 User's guide, but this link, Dynamixel EX-106+ Robot Actuator mentions ""Compliance/PID : Yes"".
If the motor does use a PID controller, then what are the parameters? And how does it allow us to set moving speed then?
If the motor doesn't use a PID controller, then what is the pattern of torque provided? In the manual (first link), I found this

From the current position 200 to 491 ( 512-16-5=491 ), movement  is 
  made  with appropriate  torque  to  reach  the  set  speed; from  491 
  to  507  ( 512-5=507 ), torque is continuously reduced to the Punch 
  value; from  507  through 517 (  512+5=517  ), no torque is generated.

This is rather vague though, and no further details are provided.
Also, I'm aware that extremely high damping and extremely high P-values might do the trick. But I want to simulate what actually happens on the motors, and that is probably not the way to go. 
I'd appreciate it if anyone has any idea of what Dynamixel servos do, or examples of simulated Dynamixel motors anywhere else.
","pid, simulation, gazebo, humanoid, dynamixel"
"Proper cable to connect 12 V, DC 6 Ah batteries to Sabertooth Motor Controller for Lawnbot?","I am building a (fork of) the Lawnbot-400 robot found on Instructables.com and in the book Arduino Robotics.
The author, while brilliant, tends to gloss over and omit details, one of those being how he actually connected the 2 recommended 12 V DC batteries to anything (I realize for the more experienced builder this is probably obvious though).
Could you please provide guidance on the types of cables to use to do this? 
It seems like I have seen some which use little clamps on one end and / or which have a plastic terminal in the middle of the cable that can be disconnected while the other 2 ends are connected to the battery and motor controller. Do I need those features?
The batteries I am using are Mighty Max 12 V 6 Ah high-rate gel series.
","motor, battery"
How to find state-space representation of quadcopter?,"I want to design a linear controller for a quadcopter which is 6dof nonlinear.
I have non-linear equations. But in order to design a linear controller, I need to find a linear state-space model of the vehicle.
I skimmed bunch of articles and thesis without any result. Most of them have some approximations for separate parts of the model to linearize.
I need the state-space as a form like below,
$$\dot{x}=Ax(t)+Bu(t)$$
I couldn't find the $A$ and $B$ matrices. I made the small angle and hover condition approximations so that equations become simpler yet they are still non-linear.
Non-simplified equations are as follow,

",quadcopter
Draw circle with 4-axis robotic arm,"I am new in robotics. I am playing with a 4 axis robotic arm called uArm and I was wondering how to draw a circle with it. 
For this I mean the math of describing a circle in task space for the robot joints to achieve with the implementation of the algorithm in code.
What will be the best approach? Any sources where to research? I tried to research around the web but I did not find anything useful.
Thank you in advance
","robotic-arm, geometry"
Cloth (non-rigid body) manipulation with computer vision?,"I am currently working on a prototype device to automate cloth ironing task. So basically the mechanical design involves 4 manipulators and a separate manipulator arm with a hot pad (iron)  to manipulate a given cloth to perform the ironing task. I understand its a very complicated task and I have come up with the following steps to potentially go about solving this problem. I would appreciate any kind input on the plausibility of this idea (mechanical and software challenges).
For my first EB (Engineering build), I'm solely focusing on solving this problem for a T-shirt, probably extend this solution to other cloth types with modifications. 

From a pile of clothes, choose a certain cloth for ironing using background subtraction and colour based image segmentation (already programmed this part and its working!)
Classify or recognise the chosen cloth into one of any known classes using CNN (Convoluted neural networks). I personally think this part is quite achievable given that I have enough training data to train my network. 
This is where the problem begins!. After classifying the given cloth, find coordinates of points of interest to manipulate the cloth in a certain 'predetermined' mechanical movement to facilitate the manipulation of cloth. What I mean by points of interest here is, for example, in the case of a T-shirt (see image below); the algorithm should localise the locations the following parts
1) Midpoint of T-shirt hem (see image for details)
2) Corner point of T-shirt hem (see image for details)
2) Start of yoke
3) End of the yoke

I believe (through my mental visualisation) that these are the key points to place the T-shirt flat on the surface (a known STATE). When the T-shirt is flat on the surface, it could be stretched just enough to make the cloth wrinkle free to facilitate ironing through some kind of force sensing mechanism (Haven't really though about it, but I presume its not huge challenge).

When the cloth is in a known STATE, proceed to ironing using path planning algorithms. 


I do  understand that this is a very complex problem, just want to get some inputs to see if I am on the right path. I am really not sure if STEP 4 is mechanically achievable since non-rigid body manipulation is a huge problem for robots. I am quite confident on the software aspect of the project, I believe a large enough data set and well tested algorithm will help me identify the afore-mentioned points of interest in a test cloth. But I would like hear the mechanical challenges and plausibility of this project from design engineers standpoint. Hope I'm clear about my query.
Any input is appreciated.
Thank you so much and cheers :)
","robotic-arm, computer-vision, mechanism, manipulator"
Quantitative Information (survey results or studies) about most used Robotics Frameworks and Programming Languages,"I know this question is prompt to be perceived as vague and receive generic answers, that is why I am going to specify it as much as possible. 
Basically, I am looking for a source of information comparable to Stack Overflow surveys (https://stackoverflow.com/research/developer-survey-2016) or similar sudies (e.g. https://arc.applause.com/2016/07/28/c-programming-language-popularity/) oriented to Robotics frameworks popularity. 
If the frameworks are too diversified, the same information but referred to the languages supporting the frameworks, instead of the particular framework itself, would also work (in the case of RobotC it would be C, in ROS it would be whether C, Ptyhon, or LISP,...).
I am NOT asking about personal preferences, or opinioned articles and lighthearted chats, but quantitative information.
Thanks 
",programming-languages
Why there is so much research going on in humanoid robots?,"It seems like humanoid robots is the hottest field of research in robotics. Government agencies are giving huge sums of money to private firms and labs to develop them (For example, Boston Dynamics has developed some really amazing humanoid robots, and some of them look scary!).
My question is: The human body is highly inefficient. We can't run for very long times, have to learn for several months before we start walking, have only two hands, and are slow. Then why spend so much money and effort emulating such an inefficient thing? May be it is time that we took a step away from getting ""inspired"" by Nature, and build a man-made, highly efficient body. 
An example: Balancing a robot on two legs in very difficult. My question is, then why use some other method for locomotion, that is easier and more effective. A robot on two legs can run only so much faster. Why not come-up with some optimal shape, and then model your robots on it?
","mobile-robot, humanoid"
How does Amazon ensure its drones are not shot down or captured when in service,"If Amazon plans to deliver packages using drones, how does it ensure the drones reach their destination safely and come back? Won't thieves be able to easily steal packages and drones by shooting them down? What about threat from birds like falcons?
",uav
What is the difference between a Robot and a Machine?,"What is the difference between a Robot and a Machine? At what point does a machine begin to be called a robot?
Is it at a certain level of complexity? Is it when it has software etc?.
For instance: A desktop printer has mechanics, electronics and firmware but it is not considered a robot (or is it). A Roomba has the same stuff but we call it a robot. So what is the difference.
I have always believed that a robot is a robot when it takes input from it's environment and uses it to make decisions on how to affect it's environment; i.e. a robot has a feedback loop.
",industrial-robot
Wake up iRobot Create 2 while on dock,"I have a 7 pin cable connected to the OI port of my Create 2. While the Create is on the floor away from the dock, if I touch the BRC pin (7) to the ground pin (5), the Create wakes up, makes a beep, and the green Clean button light turns on.
However, if I put the Create on the dock and let it sit for several minutes and then touch the BRC pin to the ground pin, nothing happens.
Is there a method for waking up/starting the Create while it is on the dock/charging?
(Note: I have seen this answer and it does not solve the issue IRobot Create 2: Powering Up after Sleep)
If I send a command or try to fetch sensor data after the Create has been sitting on the dock for several minutes, I get no response and the Create doesn't do anything (no lights, sounds, movements, etc).
While the Create is already awake, the response from a restart command (0x7) looks like this
bl-start
STR730
bootloader id: #x4718535E 7DDBCFFF
bootloader info rev: #xF000
bootloader rev: #x0001
2007-05-14-1715-L

EDIT: simplified the scope of the question.
EDIT 2: added info from Jonathan's questions.
",irobot-create
"Robot won't work properly, claw will only go one way and will only open or close once per cycle","We're students trying to make a clawbot for a Science Seminar class. However, for some reason, whenever we try to move the arm or the claw in a certain way, it will lock up and only move that direction. Code attached. Please help.
#pragma config(Motor, port1, frWheel, tmotornormal, openLoop, reversed) //Setting up the motors
#pragma config(Motor, port5, brWheel, tmotornormal, openLoop, reversed)
#pragma config(Motor, port3, flWheel, tmotornormal, openLoop)
#pragma config(Motor, port4, blWheel, tmotornormal, openLoop)
#pragma config(Motor, port10, Arm, tmotornormal, openLoop)
#pragma config(Motor, port6, Claw, tmotornormal, openLoop)

task main() 
{
    int a = 0; //Arm integer
    int c = 0; //Claw integer
    while(true) 
    {
        motor[frWheel] = vexRT(Ch2); //Wheels
        motor[brWheel] = vexRT(Ch2);
        motor[flWheel] = vexRT(Ch3);
        motor[blWheel] = vexRT(Ch3);
        if(a >= -30 && a <= 30)  {
            if(vexRT[Btn8D] == 1) //If arm down button pressed...
            {
                motor[Arm] = --a; //then arm will go down.
            }
            else if(vexRT[Btn8U] == 1)
            {
                motor[Arm] = ++a;
            }
            else(vexRT[Btn8U] == 0 && vexRT[Btn8D] == 0);
            {
                motor[Arm] = a;
            }
        }
        else
        {

        }

        if(c <= 30 && c >= -30)
        {
            if(vexRT[Btn7U] == 1) //If claw up button pressed...
            {
                motor[Claw] = ++c; //Claw will open.
            }
            else if(vexRT[Btn7D] == 1)
            {
                motor[Claw] = --c;
            }
            else(vexRT[Btn7D] == 0 && vexRT[Btn7U] == 0);
            {
                motor[Claw] = c;
            }
        }
        else
        {

        }
    }
}

","robotic-arm, wheeled-robot, c, vex"
How to remotely send command to a robot connected to a separate machine through serial port?,"I work with a XL20 robotic tube sorter(lets say it's called X) which is connected to lets say computer(Y - IP: 10.216.1.222 - Running win7) through serial(com port 8 at 9600 baud and odd parity). If I open putty at computer Y and send my commands to the Xl 20, it works just fine.
Now I have a computer(Z- ip: 10.216.1.223 - Running win7) which is in the same network and I want to connect and send instruction to the Xl20 from this computer via putty or some other means.
Basically I'm trying to remotely communicate to the XL20 which is connected to a computer through serial port, so can anyone point me to any useful guide, documentation, clue or suggestion about how can I do this?
Thanks.
",serial
PID for variable Setpoint,"Can a PID with variable setpoint work?
I have already done tests, but these are not conclusive. On the other hand I am not certain of this algorithm.
Background
This project involves improving the tracking speed of a telescope.
An Arduino card (UNO32) that I program gives the instructions in ""Pulse and Dir"" mode to a driver. This driver controls a two-phase step motor (200 steps) in micro step mode (128 micro steps per step). This stepper motor rotates a worm screw against a wheel that has 360 teeth. And finally, this wheel rotates the axis of the telescope to make one revolution per day.
To reduce the speed error caused by imprecision of the wheel and screw, a high precision encoder (1800 000 PPR) is placed at the end of the axis of rotation of the telescope. My goal is to work in closed loop with the stepper motor and the encoder feedback.
I need your advice for the PID algorithm. Here is my idea but I am not sure that this is valid:
1)   SP (Setpoint) :  the desired position of the encoder. The encoder is in  4x reading mode. 
At time t, this gives :  SP(t)= ( t sec) * (4* 1800000 PPR)/(86164 sec/day)
I can convert into micro-step :
(360 turns_motor)(200 steps)(128 micro-steps) = 4*1800 000 encoder pulses
2)   PV : the process variable. This is the measured position of the encoder.
3)   E :  the error , E= SP-PV
4)   Command :  I act on the frequency of the pulses to vary the speed of the motor according to this error
",algorithm
Which is best way to calculate inverse kinematics for scara robot?,"I am working on scara robot project and I have one big confusion.  I am using simple trigonometric way(tan inverse traingl formula) to calculate inverse kinematics . But lot of people suggested me to use DH algorithm to calculate inverse kinematics .
Which one is better and faster algorithm for scara robot .
","robotic-arm, inverse-kinematics, software, dh-parameters"
How can I move manually move an arm of a simulated baxter robot on gazebo to perform inverse kinematics?,"I am looking for a method to manually move each arm of a baxter robot which has been simulated using gazebo to collect joint angles for each degree of freedom using inverse kinematics. Can someone please help?
","robotic-arm, inverse-kinematics, gazebo"
How to tune the PID parameters using Fuzzy Logic?,"I previously used the Ziegler method to tune the parameters of my PID controller to control my robot's position. I then implemented fuzzy logic for self-tuning the parameters. 
I have two inputs to the fuzzy logic controller; one is the position error and the error rate.
I know that my problem might be due to not understanding the effect of each parameter very well.
The problem is that I am confused in setting up the fuzzy rules. When do I need to use high and low values for Kp, Kd and Ki to achieve the best tuning? Is it that Kp must be very low when the error is almost zero (hence, the robot is at the desired position)? The same question applies for all of the three parameters.
","control, pid, tuning"
Stereo camera pair focal lengths,"I came across the standard representation of stereo cameras where they are side to side and the epipolar lines are the same image scan line and the cameras have the same focal length. 

Source: http://vision.deis.unibo.it/~smatt
Now, do all stereo camera pairs need to have the same focal length? And if not then how does that change the stereo depth calculation? 
Thank you. 
",stereo-vision
Can high voltage power lines provide a super highway for drones?,"Is there a way to convert the EM field that surrounds high voltage power lines for direct usable electricity for drones to travel along without stopping?
Added: Yes could the EM field be used for a guide as well?
https://electronics.stackexchange.com/questions/280558/can-high-voltage-power-lines-provide-a-super-highway-for-drones
","quadcopter, arduino, sensors, power"
Tuning PD for line follower,"I am trying to make line following robot. I am using atmega328p mcu, pololu 10:1 motors, pololu qtr6-rc sensor, 2s li-po. Here is my code:
/*
* LineFollower.c
*
* Created: 30.04.2015 16:00:05
*  Author: Mikk
*/
#define F_CPU 20000000         //we're running on 20mHz clock

#define numberOfButtons 1

#define READPORT    PORTC
#define READDDR     DDRC
#define READPIN     PINC        // lines connected to PC0 - PC5

#define MAXTICKS    2500
#define QTRCNT      6

#include <avr/io.h>
#include <util/delay.h>
#include <avr/interrupt.h>
#include <Mikk/Button.h>
#include <Mikk/QTRRCSensors.h>

int baseSpeed = 70;
int maxSpeed = 140;
const float Kp = 8.1;
const float Kd = 400;

uint8_t mode = 0;                //indicates in which mode program is 

uint8_t RmotorSpeed = 0;         //
uint8_t LmotorSpeed = 0;         //motors

void button(void);

void setMotors(int ml, int mr)
{
    if(ml > maxSpeed)             //make sure that speed is not out of range for left motor
        ml = maxSpeed;
    if(ml < -maxSpeed)
        ml = -maxSpeed;

    if(mr > maxSpeed)             //make sure that speed is not out of range for right motor
        mr = maxSpeed;
    if(mr < -maxSpeed)
        mr = maxSpeed;

    if(ml > 0)                    //if left motor speed is positive then drive motor forwards
        LmotorSpeed = ml;
    if(ml == 0)                   //if left motor speed is 0 then stop motor
        LmotorSpeed = 0;

    if(mr > 0)                    //if right motor speed is positive then drive motor forwards
        RmotorSpeed = mr;
    if(mr == 0)                   //if right motor speed is 0 then stop motor
        RmotorSpeed = 0;
}

void emittersOn(void)            //function for turning emitters on
{
    PORTD |= (1 << PIND0);
}

void emittersOff(void)           //function for turning emitters off
{
    PORTD &= ~(1 << PIND0);
}

void LedOn(void)                 //function for turning led on
{
    PORTB |= (1 << PINB5);
}

void LedOff(void)               //function for turning led off
{
    PORTB &= ~(1 << PINB5);
}

void stop(void)                 //stop everything
{
    LedOff();
    setMotors(0, 0);
    emittersOff();
}

void calibration(void)          //calibration takes about 5 seconds
{
    //turn led on
    LedOn();
    //turn emitters on
    emittersOn();
    // reset minimums and maximums
    for (int i = 0; i < QTRCNT; i++)
    {
        QTRmax[i] = 0;
        QTRmin[i] = MAXTICKS;
    }
    //calibrate sensors
    for(int i=0; i<250; i++)
    {
        calibrateQTRs();
        _delay_ms(5);
    }
    //turn emitters off
    emittersOff();
    //turn led off
    LedOff();
}

void start(void)
{
    //turn led on
    LedOn();

    //create all necessary variables
    int power_difference = 0;
    float error = 0;
    float lastError = 0;
    float derivative = 0;
    int position = 0;

    //turn emitters on
    emittersOn();
    _delay_ms(500);               //wait so you can pull your hand away
    while(mode == 2)
    {
        //check for mode change
        button();

        //read position
        position = readLine();
        //make calculations
        error = position - 2500;
        derivative = error - lastError;

        //remember last error
        lastError = error;

        //calculate power_difference of motors
        power_difference = error/(Kp/100) + derivative*(Kd/100);

        //make sure that power difference is in correct range
        if(power_difference > baseSpeed)
            power_difference = baseSpeed;
        if(power_difference < -baseSpeed)
            power_difference = -baseSpeed;

        //drive motors
        if(power_difference > 0)
            setMotors(baseSpeed+power_difference, baseSpeed-power_difference/2);
        else if(power_difference < 0)
            setMotors(baseSpeed+power_difference/2, baseSpeed-power_difference);
        else if(power_difference == 0)
            setMotors(maxSpeed, maxSpeed);
    }
}

void button(void)
{
    char buttonState = 0;
    //check for current button status
    buttonState = ButtonReleased(0, PINB, 1, 200);
    //check if button is pressed
    if(buttonState) //pin change from low to high
    {
        mode++;
        if(mode == 1) calibration();
    }
}

void pwmInit(void)
{
    //set fast-PWM mode, inverting mode for timer0
    TCCR0A |= (1 << COM0A1) | (1 << COM0A0) | (1 << WGM00) | (1 << WGM01) | (1 << COM0B1) | (1 << COM0B0);
    //set fast-PWM mode, inverting mode for timer2
    TCCR2A |= (1 << COM2A1) | (1 << COM2A0) | (1 << WGM20) | (1 << WGM21) | (1 << COM2B1) | (1 << COM2B0);
    //set timer0 overflow interrupt
    TIMSK0 |= (1 << TOIE0);
    //set timer2 overflow interrupt
    TIMSK2 |= (1 << TOIE2);

    //enable global interrupts
    sei();
    //set timer0 prescaling to 8
    TCCR0B |= (1 << CS01);
    //set timer2 prescaling to 8
    TCCR2B |= (1 << CS21);
}

int main(void)
{   
    DDRB |= 0x2A;                 //0b00101010
    DDRD |= 0x69;                 //0b01101001
    DDRC |= 0x00;                 //0b00000000

    //clear port d
    PORTD |= 0x00;

    //enable pull-up resistor
    PORTB |= (1 << PINB1);

    initQTRs();
    pwmInit();

    //blink 2 times indicate that we are ready
    for(int i=0; i<4; i++)
    {
        PORTB ^= (1 << PINB5);
        _delay_ms(500);
    }

    while(1)
    {
        button();
        if(mode == 0) stop();
        if(mode == 2) start();
        if(mode >= 3) mode = 0;
    }
}

//update OCRnx values
ISR(TIMER0_OVF_vect)
{
    OCR0A = RmotorSpeed;
}
ISR(TIMER2_OVF_vect)
{
    OCR2A = LmotorSpeed;
}

And here is my qtr library:
    #ifndef QTRRCSensors
#define QTRRCSensors

#define SLOW        1
#define FAST        0

static inline void initQTRs(void) 
{
    TCCR1B = (1 << CS11);
}

uint16_t QTRtime[QTRCNT], QTRmax[QTRCNT], QTRmin[QTRCNT];

static inline void readQTRs(uint8_t forceSlow) {
    uint8_t lastPin, i, done = 0;

    for (i = 0; i < QTRCNT; i++)                    // clear out previous times
        QTRtime[i] = 0;

    READDDR |= 0b00111111;                          // set pins to output
    READPORT |= 0b00111111;                         // drive them high

    _delay_us(10);                                  // wait 10us to charge capacitors

    READDDR &= 0b11000000;                          // set pins to input
    READPORT &= 0b11000000;                         // turn off pull-up registers

    TCNT1 = 0;                                      // start 16bit timer at 0
    lastPin = READPIN;

    while ((TCNT1 < MAXTICKS) && ((done < QTRCNT) || forceSlow))     // if forceSlow, always take MAXTICKS time
    {
        if (lastPin != READPIN)                     // if any of the pins changed
        {
            lastPin = READPIN;
            for (i = 0; i < QTRCNT; i++)
            {
                if ((QTRtime[i] == 0) && (!(lastPin & (1<<i))))    // did pin go low for the first time
                {
                    QTRtime[i] = TCNT1;
                    done++;
                }
            }
        }
    }
    if (done < QTRCNT)                              // if we timed out, set any pins that didn't go low to max
        for (i = 0; i < QTRCNT; i++)
            if (QTRtime[i] == 0)
                QTRtime[i] = MAXTICKS;
}

void calibrateQTRs(void) {
    uint8_t i, j;

    for (j = 0; j < 10; j++) {                      // take 10 readings and find min and max values
        readQTRs(SLOW);
        for (i = 0; i < QTRCNT; i++) {
            if (QTRtime[i] > QTRmax[i])
                QTRmax[i] = QTRtime[i];
            if (QTRtime[i] < QTRmin[i])
                QTRmin[i] = QTRtime[i];
        }
    }
}

void readCalibrated(void) {
    uint8_t i;
    uint16_t range;

    readQTRs(FAST);

    for (i = 0; i < QTRCNT; i++) {                  // normalize readings 0-1000 relative to min & max
        if (QTRtime[i] < QTRmin[i])                 // check if reading is within calibrated reading
            QTRtime[i] = 0;
        else if (QTRtime[i] > QTRmax[i])
            QTRtime[i] = 1000;
        else {
            range = QTRmax[i] - QTRmin[i];
            if (!range)                             // avoid div by zero if min & max are equal (broken sensor)
                QTRtime[i] = 0;
            else
                QTRtime[i] = ((int32_t)(QTRtime[i]) - QTRmin[i]) * 1000 / range;
        }
    }
}

uint16_t readLine(void) {
    uint8_t i, onLine = 0;
    uint32_t avg;                                   // weighted total, long before division
    uint16_t sum;                                   // total values (used for division)
    static uint16_t lastValue = 0;                  // assume line is initially all the way left (arbitrary)

    readCalibrated();

    avg = 0;
    sum = 0;

    for (i = 0; i < QTRCNT; i++) {                  // if following white line, set QTRtime[i] = 1000 - QTRtime[i]
        if (QTRtime[i] > 50) {                      // only average in values that are above a noise threshold
            avg += (uint32_t)(QTRtime[i]) * (i * 1000);
            sum += QTRtime[i];
            if (QTRtime[i] > 200)                   // see if we're above the line
                onLine = 1;
        }
    }

    if (!onLine)
    {
        // If it last read to the left of center, return 0.
        if(lastValue < (QTRCNT-1)*1000/2)
            return 0;

        // If it last read to the right of center, return the max.
        else
            return (QTRCNT-1)*1000;

    }

    lastValue = avg/sum;                            // no chance of div by zero since onLine was true

    return lastValue;
}

#endif

I am trying to find Kp constant but when it's 7 then my robot just turns off the line always on the same spot. When Kp is 8 then it follows staright line but wobbles a lot and can't take corners. I also tried to increase Kd 10 to 20 times when my Kp was 8 but it didn't change much. How can I get it working?
Here is my robot and the track I want to follow.


","pid, line-following, avr, tuning"
Negative torque and negative velocity during reverse dynamics in Matlab Robotics Toolox.,"I'm using Robotics Toolox Matlab and I did Reverse dynamics using code below.
[qreach,err] =  rob.ikcon(T1, qready);
[q,qd,qdd]=jtraj(qready,qreach,t);
    %compute inverse dynamics using recursive Newton-Euler algorithm
tauf = rne(rob, q, qd, qdd);

And when I plotted angular velocity and torque it shows some negative values as shown in Fig. 
I want to know why so and what is the physical significance of negative veloity and torque in robotics.
Thanks.
","robotic-arm, inverse-kinematics, matlab"
how to implement and code inner and outer PD controllers for quadrotor for position tracking,"The quadrotor system is  multi-ODEs equations. The linearized model is usually used especially for position tracking, therefore one can determine the desired x-y positions based on the roll and pitch angles. As a result,  one nested loop which has inner and outer controllers is needed for controlling the quadrotor. For implementation, do I have to put while-loop inside ode45 for the inner attitude controller? I'm asking this because I've read in a paper that the inner attitude controller must run faster (i.e. 1kHz) than the position controller (i.e. 100-200 Hz). In my code, both loops run at 1kHz, therefore inside ode45 there is no while-loop. Is this correct for position tracking? If not, do I have to insert while-loop inside ode45 for running the inner loop? Could you please suggest me a pseudocode for position tracking?
To be more thorough, the dynamics equations of the nonlinear model of the quadrotor is provided  here, if we assume the small angles, the model is reduced to the following equations 
$$
\begin{align}
\ddot{x}      &= \frac{U_{1}}{m} ( \theta \cos\psi + \phi \sin\psi) \\
\ddot{y}      &= \frac{U_{1}}{m} ( \theta \sin\psi - \phi \cos\psi) \\
\ddot{z}      &= \frac{U_{1}}{m} - g   \\
\ddot{\phi}   &= \frac{l}{I_{x}} U_{2} \\
\ddot{\theta} &= \frac{l}{I_{y}} U_{3} \\
\ddot{\psi}   &= \frac{1}{I_{z}} U_{4} \\
\end{align}
$$
The aforementioned equations are linear. For position tracking, we need to control $x,y,$ and $z$, therefore we choose the desired roll and pitch (i.e. $\phi^{d} \ \text{and} \ \theta^{d}$)
$$
\begin{align}
\ddot{x}^{d} &= \frac{U_{1}}{m} ( \theta^{d} \cos\psi + \phi^{d} \sin\psi) \\
\ddot{y}^{d} &= \frac{U_{1}}{m} ( \theta^{d} \sin\psi - \phi^{d} \cos\psi) \\
\end{align}
$$
Therefore, the closed form for the desired angles can be obtained as follows
$$
\begin{bmatrix}
\phi_{d} \\
\theta_{d} 
\end{bmatrix}
=
\begin{bmatrix}
\sin\psi & \cos\psi \\
-\cos\psi & \sin\psi 
\end{bmatrix}^{-1}
\left( \frac{m}{U_{1}}\right) 
\begin{bmatrix}
\ddot{x}^{d} \\
\ddot{y}^{d}
\end{bmatrix}
$$
My desired trajectory is shown below

The results are 

And the actual trajectory vs the desired one is 

My code for this experiment is 
%%
%######################( Position Controller )%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all;
clc;

dt = 0.001;
 t = 0;

% initial values of the system
 x = 0;
dx = 0;
 y = 0;
dy = 0;
 z = 0;
dz = 0;

   Phi = 0;
  dPhi = 0;
 Theta = 0;
dTheta = 0;
   Psi = pi/3;
  dPsi = 0;


%System Parameters:
m  = 0.75;      % mass (Kg)
L  = 0.25;      % arm length (m)
Jx = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jy = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jz = 0.039380; % inertia seen at the rotation axis. (Kg.m^2)
g  = 9.81;      % acceleration due to gravity m/s^2

errorSumX = 0;
errorSumY = 0;
errorSumZ = 0;

errorSumPhi   = 0;
errorSumTheta = 0;

pose = load('xyTrajectory.txt');

DesiredX = pose(:,1);
DesiredY = pose(:,2);
DesiredZ = pose(:,3);

dDesiredX = 0;
dDesiredY = 0;
dDesiredZ = 0;

DesiredXpre = 0;
DesiredYpre = 0;
DesiredZpre = 0;

dDesiredPhi = 0;
dDesiredTheta = 0;
DesiredPhipre = 0;
DesiredThetapre = 0;



for i = 1:6000

   % torque input
   %&&&&&&&&&&&&( Ux )&&&&&&&&&&&&&&&&&&
   Kpx = 50; Kdx = 8; Kix = 0; 
   Ux = Kpx*( DesiredX(i) - x  ) + Kdx*( dDesiredX - dx ) + Kix*errorSumX;

   errorSumX = errorSumX + ( DesiredX(i) - x );

     dDesiredX = ( DesiredX(i) - DesiredXpre ) / dt;
   DesiredXpre = DesiredX(i);

   %&&&&&&&&&&&&( Uy )&&&&&&&&&&&&&&&&&&
   Kpy = 100; Kdy = 10; Kiy = 0; 
   Uy = Kpy*( DesiredY(i) - y  ) + Kdy*( dDesiredY - dy ) + Kiy*errorSumY;


   errorSumY = errorSumY + ( DesiredY(i) - y );

     dDesiredY = ( DesiredY(i) - DesiredYpre ) / dt;
   DesiredYpre = DesiredY(i);



   %&&&&&&&&&&&&( U1 )&&&&&&&&&&&&&&&&&&
   Kpz = 100; Kdz = 20; Kiz = 0; 
   U1 = Kpz*( DesiredZ(i) - z ) + Kdz*( dDesiredZ - dz ) + Kiz*errorSumZ; 

   errorSumZ = errorSumZ + ( DesiredZ(i) - z );

      dDesiredZ = ( DesiredZ(i) - DesiredZpre ) / dt;
   DesiredZpre = DesiredZ(i);

   %#######################################################################
   %#######################################################################
   %#######################################################################
   % Desired Phi and Theta
   R = [  sin(Psi),cos(Psi); 
         -cos(Psi),sin(Psi)];


   DAngles = R\( (m/U1)*[Ux; Uy]);


   %Wrap angles
   DesiredPhi   = wrapToPi( DAngles(1) ) /2;
   DesiredTheta = wrapToPi( DAngles(2) );


   %&&&&&&&&&&&&( U2 )&&&&&&&&&&&&&&&&&&
   KpP = 100; KdP = 10; KiP = 0;
   U2 = KpP*( DesiredPhi - Phi ) + KdP*( dDesiredPhi - dPhi )  + KiP*errorSumPhi;

   errorSumPhi = errorSumPhi + ( DesiredPhi - Phi );

     dDesiredPhi = ( DesiredPhi - DesiredPhipre ) / dt;
   DesiredPhipre = DesiredPhi;


   %--------------------------------------
   %&&&&&&&&&&&&( U3 )&&&&&&&&&&&&&&&&&&

   KpT = 100; KdT = 10; KiT = 0;
   U3 = KpT*( DesiredTheta - Theta ) + KdP*( dDesiredTheta - dTheta ) + KiT*errorSumTheta;

    errorSumTheta = errorSumTheta + ( DesiredTheta - Theta );

     dDesiredTheta = ( DesiredTheta - DesiredThetapre ) / dt;
   DesiredThetapre = DesiredTheta;



   %--------------------------------------
   %&&&&&&&&&&&&( U4 )&&&&&&&&&&&&&&&&&&
   KpS = 80; KdS = 20.0; KiS = 0.08;
   U4 = KpS*( 0 - Psi ) + KdS*( 0 - dPsi );


   %###################( ODE Equations of Quadrotor )###################
   %===================( X )=====================
   ddx = (U1/m)*( Theta*cos(Psi) + Phi*sin(Psi) );

    dx = dx + ddx*dt;
     x =  x +  dx*dt;
   %===================( Y )=====================
   ddy = (U1/m)*( Theta*sin(Psi) - Phi*cos(Psi) );

    dy = dy + ddy*dt;
     y =  y +  dy*dt;

   %===================( Z )=====================
   ddz = (U1/m) - g;

    dz = dz + ddz*dt;
     z =  z +  dz*dt;

   %===================( Phi )=====================
   ddPhi = ( L/Jx )*U2;

    dPhi = dPhi + ddPhi*dt;
     Phi =  Phi +  dPhi*dt;

   %===================( Theta )=====================
   ddTheta =  ( L/Jy )*U3;

    dTheta =  dTheta + ddTheta*dt;
     Theta =   Theta +  dTheta*dt;

   %===================( Psi )=====================
   ddPsi =  (1/Jz)*U4;

    dPsi = dPsi + ddPsi*dt;
     Psi =  Psi +  dPsi*dt;

   %store the erro
     ErrorX(i)   = ( x - DesiredX(i) );
     ErrorY(i)   = ( y - DesiredY(i) );
     ErrorZ(i)   = ( z - DesiredZ(i) );
%    ErrorPhi(i)   = ( Phi - pi/4 );
%    ErrorTheta(i) = ( Theta - pi/4 );
     ErrorPsi(i)   = ( Psi - 0 );


   X(i) = x;
   Y(i) = y;
   Z(i) = z;

   T(i) = t;

%    drawnow 
%    plot3(DesiredX, DesiredY, DesiredZ, 'r')
%    hold on
%    plot3(X, Y, Z, 'b')
   t = t + dt; 


end


Figure1 = figure(1);
set(Figure1,'defaulttextinterpreter','latex');
%set(Figure1,'units','normalized','outerposition',[0 0 1 1]);

subplot(2,2,1)
plot(T, ErrorX, 'LineWidth', 2)
title('Error in $x$-axis Position (m)')
xlabel('time (sec)')
ylabel('$x_{d}(t) - x(t)$', 'LineWidth', 2)

subplot(2,2,2)
plot(T, ErrorY, 'LineWidth', 2)
title('Error in $y$-axis Position (m)')
xlabel('time (sec)')
ylabel('$y_{d}(t) - y(t)$', 'LineWidth', 2)

subplot(2,2,3)
plot(T, ErrorZ, 'LineWidth', 2)
title('Error in $z$-axis Position (m)')
xlabel('time (sec)')
ylabel('$z_{d} - z(t)$', 'LineWidth', 2)


subplot(2,2,4)
plot(T, ErrorPsi, 'LineWidth', 2)
title('Error in $\psi$ (m)')
xlabel('time (sec)')
ylabel('$\psi_{d} - \psi(t)$','FontSize',12);
grid on 


Figure2 = figure(2);
set(Figure2,'units','normalized','outerposition',[0 0 1 1]);

figure(2)
plot3(X,Y,Z, 'b')
grid on

hold on 
plot3(DesiredX, DesiredY, DesiredZ, 'r')

pos = get(Figure2,'Position');
set(Figure2,'PaperPositionMode','Auto','PaperUnits','Inches','PaperSize',[pos(3),pos(4)]);
print(Figure2,'output2','-dpdf','-r0');

legend('actual', 'desired')

The code of the desired trajectory is 
clear all; clc;

fileID = fopen('xyTrajectory.txt','w');

 angle = -pi; radius = 5; z = 0; t = 0;

for i = 1:6000
    if ( z < 2 ) 
        z = z + 0.1;
        x = 0; 
        y = 0;
    end
    if  ( z >= 2 )
        angle = angle + 0.1;
        angle = wrapToPi(angle);
        x = radius * cos(angle);
        y = radius * sin(angle);
        z = 2;
    end

    X(i) = x;
    Y(i) = y;
    Z(i) = z;

    fprintf(fileID,'%f \t %f \t %f\n',x, y, z); end

fclose(fileID); plot3(X,Y,Z) grid on

","quadcopter, matlab, microcontroller"
Road Detection When the Robot is Off-Road,"I'm trying to make a lightweight method of outdoor road following for a small ground robot. In nearly all road detection work that I've seen, they all assume that the robot is already on the road, which allows for techniques like finding the vanishing point or sampling pixels near the bottom of the camera frame. However in my application, the robot can be a few meters away from the road and needs to first find the road. As the robot computation runs on an Android phone, I'm hoping to avoid heavy computer vision techniques, but also be robust to variable outdoor lighting conditions. Obviously there is a trade-off, but I'm willing to sacrifice some accuracy for speed and ease of computation. Any ideas on how to achieve this?
","wheeled-robot, computer-vision"
How to decide links length of manipulator?,"I'm working on pick and place robotic arm with 4 dof. I'm using MATLAB for inverse kinematics. But, I want to know how to decide links length. Say, I have four point in space upto where my robotic arm should reach i.e at upper most point, lower most point, extreme right point and extreme left point.I want theory or any approach so that I can calculate link length using these points.
Thanks.
Edit: Add picture of robotic arm. 
","robotic-arm, manipulator"
Best UGV platform?,"My lab is interested in a good all-terrain UGV that can also be used indoors. We are particularly interested in the Clearpath Husky, Clearpath Jackal, and the Robotnik Summit XL (or XL HL), though we would welcome any other suggestions. Does anyone happen to have experience with more than one of these, and can speak to their pros and cons?
","ugv, platform"
How to consider gravity in the design of a compensator?,"I have to solve an exercise for the Digital Control System course (using MATLAB software) which stands:

""A ball is suspended inside a vertical tube by airflow 'u' and connected via a spring of stiffness K to the bottom of the tube. The ball is subjected to gravity and a viscous friction with coefficient 'B'. The force 'F' exerted on the ball by the airflow is proportional to the airflow 'u' via the constant G; airflow can only be positive (entering the tube).""
I have also all the data needed to solve the problem numerically, but this is not important for the question. What I need to do is: ""Write the system equations in state space form with airflow as input and the ball vertical position 'z' as output.Then, select a sampling time and design a digital control system that regulates the ball position by acting on the airflow to the following specifications:

Zero steady-state error (in response to the desired altitude step input).
Max overshoot: 30%;
Settling time at 5% less than 8 seconds.""

After this, we have to compute the transfer function of the plant and put it in unitary feedback with the compensator.
I usually write first the system dynamics equations, then from these I choose a suitable set of state variables and I write the matrices A, B, C and D according to the state variables (I use the 'ss' function). The problem is that I don't know how to consider the gravity in this case because it comes in the system dynamics as a constant term (-g*m). For example by considering the state variables as [z' z] I obtained the following matrices:
A = [-B/m -K/m; 1 0];
B = [G/m; 0];
C = [0 1];
D = 0;

I tried to design the compensator (a simple PID) without considering the gravity and by adding it later in the Simulink model used to test the system (after designing the compensator we have to build a Simulink model in which the discrete time compensator is tested with the transfer function of the continuous time system) but of course the system output is no more able to meet the requirements.
For the gravity transfer function I considered to have the mass as input and the position as output
Am I wrong in not considering the gravity when designing the compensator? Or perhaps, if correctly implemented the gravity should not affect the system output?
","control, pid, matlab"
"In HRI, how is the ""uncanny valley"" experienced by people on the autism spectrum?","I'm familiar with the idea of the uncanny valley theory in human-robot interaction, where robots with almost human appearance are perceived as creepy. I also know that there have been research studies done to support this theory using MRI scans. 
The effect is an important consideration when designing robotic systems that can successfully interact with people. In order to avoid the uncanny valley, designers often create robots that are very far from humanlike. For example, many therapeutic robots (Paro, Keepon) are designed to look like animals or be ""cute"" and non-threatening.
Other therapeutic robots, like Kaspar, look very humanlike. Kaspar is an excellent example of the uncanny valley, since when I look at Kaspar it creeps me out. However, people on the autism spectrum may not experience Kaspar the same way that I do. And according to Shahbaz's comment, children with autism have responded well to Kaspar.
In the application of therapeutic robots for people on the autism spectrum, some of the basic principles of human-robot interaction (like the uncanny valley) may not be valid. I can find some anecdotal evidence (with Google) that people on the autism spectrum don't experience the uncanny valley, but so far I haven't seen any real studies in that area.
Does anyone know of active research in human-robot interaction for people on the autism spectrum?  In particular, how does the uncanny valley apply (or doesn't it apply) when people on the autism spectrum interact with a humanlike robot?
","research, hri, uncanny-valley"
What are the prerequisites for learning ROS?,"It is helpful in robotics to first learn about ""Linux kernel development"" or ""device driver development in Linux"" before I start learning ROS? I know C and JAVA! In brief, I want to know any prerequisites which are essential to understand ROS better.
","ros, linux"
remote controlled quadcopter : Why is it not possible to send all the commands over one channel?,"First of all, this might be a stupid or bad question but i am quite new to this topic.
I want to build a rc transmitter (with some joystick buttons, an arduino and a hc-12 transceiver). I've searched a lot about this topic. But i still have a question that rests unanswered. 
Why is it necessary to use multiple channnels to control for example pitch,jaw,throttle of a quadcopter. transmitters in shops have 4 or 6 channels but i don't understand why these different channels are necessary. These transmitter send the information of each button over a different channel, why is this necassary?
Is it not possible to send the commands over one channel (all at the same frequency)? For example send p30 for a pitch of 30 degrees and j30 for a jaw of 30 degrees? Than, the receiver can interpret this as well?
I guess it is to send al the commands on the same time?
Thanks in advance 
",quadcopter
Is it possible to merge two or more Occipital Structure sensors data without IR interference?,"As this sensor use structured light process with IR, I wonder if more than two sensors can generate ""too much"" IR in the scene and corrupt data acquisition. 
Has anyone already tested this kind of configuration with Occipital Structure sensors ?
Thanks for your answers !
Arthur 
","sensors, cameras, sensor-fusion"
Reactive obstacle avoidance using 2D LiDAR,"I'm working now on project with quadcopter. What I want to achieve is to have LiDAR sensor mounted on top of this quadcopter to be able to detect and avoid obstacles along the path in outdoor environments (avoid trees, people etc).
I have a global planner which is planning on map that I'm creating along the way. But now I want also some local planner which will take care of dynamic obstacles in the environment that are not in the map yet. Therefore rewrite the path given by global planner to be able to avoid those obstacles and continue on it's journey.
For the obstacle detection I wanna use LiDAR, but it's not the requirement (I also saw some monocular cameras for this, but they can detect only obstacles in front of the quadcopter). Now I need to find some algorithm that will take care of obstacle detection and avoid those obstacles. Either by giving me new waypoints to follow or at least yaw and speed of quadcopter.
I was googling for a while but I can't really find some simple approaches. I've also tried SNDN (smooth nearness diagram navigation) but it's probably better suited for ground robots in indoor environments.
Thank you for any advice. 
","quadcopter, motion-planning"
Are propellers dangerous?,"Aren't all propellers super dangerous? How are these startups like Hex and Pocket Drone selling drones as 'kid-friendly' to consumers? What happens if a kid puts his finger in a propeller's movement space while its flying? 
",quadcopter
Difference between motion model and sample motion model,"I'm studying probabilistic book by sebastian thrun and 
for motion models it talks about odometry motion model and velocity motion model. 
For each motion model, it provides two different algorithms: 

motion_model_odometry($x_t$, $u_t$, $x_{t-1}$)
sample_motion_model_odometry($u_t$, $x_{t-1}$)

I'm not sure when to use what. Do we first sample the motion model, get our prediction and plug it into motion model to get a distribution of our final position?
","slam, path-planning"
Roslaunch include file remotely,"I am trying to launch a file from remote computer but I could not success. Actually I can connect to remote computer but I think the problem is with including a file from remote computer. In other words, I am looking for a machine tag for include. Here is the my code:
<launch>  

    <group >
      <machine name=""marvin-1"" address=""tek-marvin-1"" user=""blabla"" password=""blabla"" env-loader=""/home/blabla/.rosLaunchScript.sh""/>  

      <include file=""$(find openni_launch_marvin)/launch/kinect_left.launch""/>
    </group>     

</launch>

",ros
is my lipo faulty or charger,"i have 3s lipo - 11.1v 2200mah ,charging with imax R3 pro balanced charger...
the voltage gaps between cells are too much..
1 cell - 3.70
2nd cell - 3.74
3rd cell - 3.81
i am unable to  diagnose if my charger is faulty or lipo 
procedure of charging - connect the battery , turn on the charger , led still red , turn off the charger , remove the battery , check voltage through buzzer
","battery, lithium-polymer"
Mobile robot path following using Model Predictive Control (MPC),"I'am trying to implement a path following algorithm based on MPC (Model Predictive Control), found in this paper : Path Following Mobile Robot in the Presence of Velocity Constraints 
Principle: Using the robot model and the path, the algorithm predict the behavior of the robot over N future steps to compute a sequence of commands $(v,\omega)$ to allow the robot to follow the path without overshooting the trajectory, allowing to slow down before a sharp turn, etc.
$v:$ Linear velocity
$\omega:$ Angular velocity
The robot: I have a non-holonomic robot like this one (Image extracted from the paper above) :

Here is my problem: Before implementing on the mobile robot, I'am trying to compute the needed matrices (using Matlab) to test the efficiency of this algorithm. At the end of the matrices computation some of them have dimension mismatch
What I did:
For those interested, this calculation is from §4 (4.1, 4.2, 4.3, 4.4) p6-7 of the paper.  

4.1 Model
$z_{k+1} =  Az_k + B_\phi\phi_k + B_rr_k$ (18) 
  with:
  $A = \begin{bmatrix} 1 & Tv \\ 0 & 1 \end{bmatrix}$
  $B_\phi = \begin{bmatrix} {T^2\over2}v^2\\ Tv  \end{bmatrix}$
  $B_r = \begin{bmatrix} 0 & -Tv \\ 0 & 0 \end{bmatrix}$
  $T$: sampling period
  $v$: linear velocity
  $k$: sampling index (i.e. $t= kT$)
  $z_k:$ the state vector $z_k = (d_k, \theta_k)^T$ position and angle difference to the reference path
  $r_k:$ the reference vector $r_k = (0, \psi_k)^T$ with $\psi_k$ is the reference angle of the path at step k 
4.2 Criterion
The predictive receding horizon controller is based on a minimization of the criterion
  $J= \Sigma^N_{n=0} (\hat{z}_{k+n} - r_{k+n})^T Q(\hat{z}_{k+n} - r_{k+n}) + \lambda\phi^2_{k+n}$, (20)
  Subject to the inequality constraint
  $ P\begin{bmatrix} v_n \\ v_n\phi_n \end{bmatrix} \leq q,$
  $n=0,..., N,$
  where $\hat{z}$ is the predicted output, $Q$ is a weight matric, $\lambda$ is a scalar weight, and $N$ is prediction horizon.
4.3 Predictor
An n-step predictor $\hat{z}_{k+n|k}$ is easily found from iterating (18). Stacking the predictions $\hat{z}_{k+n|k},n = n,...,N$ in the vector $\hat{Z}$ yields
  $\hat{Z} = \begin{bmatrix} \hat{z}_{k|k} \\ \vdots \\ \hat{z}_{k+N|k}\end{bmatrix} = Fz_k + G_\phi\Phi_k + G_rR_k$  (22)
  with
  $\Phi_k = \begin{bmatrix} \phi_k, \ldots, \phi_{k+N}\end{bmatrix}^T$,
  $R_k = \begin{bmatrix} r_k, \ldots, r_{k+N}\end{bmatrix}^T$,
  and
  $F = \begin{bmatrix}I & A & \ldots & A^N \end{bmatrix}^T$
  $G_i = \begin{bmatrix} 0 & 0 & \ldots & 0 & 0 \\ B_i & 0 & \ldots & 0 & 0 \\ AB_i & B_i & \ddots & \vdots & \vdots \\ \vdots & \ddots & \ddots & 0 & 0 \\ A^{N-1}B_i & \ldots & AB_i & B_i & 0 \end{bmatrix}$  

where index $i$ should be substituted with either $\phi$ or $r$

4.4 Controller
Using the N-step predictor (22) simplifies the criterion (20) to
  $J_k = (\hat{Z}_k - R_k)^T I_q (\hat{Z}_k - R_k) + \lambda\Phi^T_k\Phi_k$, (23)
  where $I_q$ is a diagonal matrix of appropriate dimension with instances of Q in the diagonal. The unconstrained controller is found by minimizing (23) with respect to $\Phi$:
  $\Phi_k = -L_zz_k - L_rR_k$, (24)
  with
  $L_z = (lambda + G^T_wI_qG_w)^{-1}G^T_wI_qF$
  $L_r = (lambda + G^T_wI_qG_w)^{-1}G^T_wI_q(Gr - I)$

I'am trying to compute $\Phi_k = -L_zz_k - L_rR_k$ but the dimension of $L_r$ and $R_k$ does not match for matrix multiplication.
Parameters are :   

$T=0.1s$
$N=10$
$\lambda=0.0001$
$Q=\begin{bmatrix} 1 & 0 \\ 0 & \delta \end{bmatrix}$ with $\delta=0.02$

I get :
$R_k$ a (11x2) matrix (N+1 elements of size 2x1, transposed)
$G_w$ a (22x11) matrix
$G^T_w$ a (11x22) matrix
$I_q$ a (22x22) matrix
$F$ a (22x2) matrix
$G_r$ a (22x22) matrix    
so Lz computation gives (according to the matrix sizes)
$L_z=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)(22x22)$
a (11x2) matrix.
as $z_k$ is (2x1) matrix, doing $L_zz_k$ from (24) is fine.  
and Lr computation gives (according to the matrix sizes)
$L_r=(scalar + (11x22)(22x22)(22x11))^{-1} (11x22)(22x22)((22x22) - (22x22))$
a (11x22) matrix.
as $R_k$ is (11x2) matrix, doing $L_rR_k$ from (24) is not possible.
I have a (11x22) matrix multiplicated by a (11x2) matrix.
I'm sure I'm missing something big here but unable to see what exactly.
Any help appreciated.
Thanks
","mobile-robot, navigation"
Balance Bot PID tuning,"so I am working on a Self Balancing Bot with Arduino Mega. I'm using 12V 200 RPM motors with built in 840 PPR quadrature encoders. The torque rating is as follows:
Rated Torque: 2.4 Kg-cm
Stall Torque: 6 Kg-cm
As of now I've implemented a simple PID controller (based on Brett Beauregard's PID library) to minimize only the tilt angle error. Still haven't implemented PID using the encoder for the position error.
I've tried a lot to tune the PID values. The robot is quite stable when it is standing its own. However, here's the issue, when the robot tilts by an angle greater than 7-8 degrees (or when it is pushed slightly) from its stable position, the motors start rotating at the maximum speed (PWM: 255) and the robot still doesn't seem to recover back and just keeps running and then finally falls down in the direction in which it was moving. It doesn't seem to recover at all.
Is it just a problem with the PID tuning? As I said earlier, it's stable for the range -5 to +5 degrees (0 degrees is the upright position) without oscillating much. However it is not able to recover for errors greater than 7 degrees.
Or could this be an issue with the robot being too heavy and insufficient counter motor torque? Total weight of the robot is around 1 kg (including the motors). I'm using 2200 mAh LiPo battery and have placed it at the highest level so as to decrease the angular acceleration.
Would implementing the PID feedback for encoders as well solve this issue? Also do suggest some links on how to implement dual PID using an IMU and encoders to correct tilt angle and position. I'm planning to use cascaded PID controllers in which the encoder PID measures the error in position and its output will be the set-point for the angle PID loop which in turn will control the PWM of the motors. So if there's an offset in position the angle set-point will change to, let say -2 degrees (from upright 0 degrees). So now the angle PID will make the robot move in order to correct the angle offset and therefore the position error also gets corrected. Is this a good method. Any other suggestions or different approaches to this?
","arduino, pid, balance"
"Locate a quadcopter in x,y space","I have a quadcopter using image processing to detect shapes on the ground to the determine x,y position. It works ok. My problem is that if the illumination isnt perfect.
I would like to know if there's a way to fuse the image processing data with another kind of way to find the position x,y of the quadcopter.
My teacher told me about Bluetooth Low Energy and some beacons but I dont think it will work.
",quadcopter
How do I determine the required encoder resolution for a control system?,"I'm designing a joint which will have to move with a velocity of ~60RPM and I have to come up with resolution requirement for the encoders within this joint. I notice however that this is easier said than done.
A 1m beam will be connected to the joint, I figured I would need to know the position of the tip of the beam with a resolution of approximately 1mm. with some simple calculations I found that for this a 12-bit encoder will be sufficient.
However, I was wondering whether this would be sufficient to actually control the joint in a smooth manner. I found some information about how the resolution influences the joint behavior but didn't find anything about how to use this to find resolution requirements. For example I found:

""When you tune the constants right, you should be able to run your arm
  at a constant speed. However, this is dependent that you have a fairly
  high-resolution encoder,""

I have no idea what a ""fairly high-resolution encoder"" is. I was wondering if any of you have any experience with this or know any methods to determine the required resolution.
","control, sensors"
How to make RRT to work for dynamic systems?,"I want to make path planning algorithm for a quadrotor with RRT in my thesis. I have searched lots of articles and come up with the concept of ""dynamic RTT"" and one of the articles has a title ""kinodynamic RRT*"". I have emailed the author of the article with no response. 
The main point that I couldn't understand is, we need to sample random state for dynamic RRT like 2 position and 2 velocity values for planar vehicle or an angle and its rate in case of 2D-quadrotor.
How should the samples be so that speeds and positions does not confused and when should I consider the saturation limits of the actuators or vehicle acceleration limits.
I can't understand how to handle what if two consecutive samples for positions are A(0,0) and B(10,10) this needs positive velocity at the point B but sampling can cause negative velocity. Am I wrong?
Other issue is, how should the control signal be determined so that it can be applied for duration of delta t to move as close as possible to the sampled point. I am not sure how to determine the input or move the vehicle. 
Do I need optimizations so that it can reach to the sampled point in shortest time possible?
Please let me know if there is a missing part to be understood.
Thanks in advance.
Wish a hopeful new year.
","quadcopter, control, motion-planning, rrt"
Public dataset for monocular visual odometry,"I am planning to develop a monocular visual odometry system. Is there any indoor dataset available to the public, along with the ground truth, on which I can test my approach?
Note: I am already aware of the KITTI Vision Benchmark Suite which provides stereo data for outdoor vehicles.
If anyone has access to the datasets used in the following paper [SVO14], it would be even more great: http://rpg.ifi.uzh.ch/docs/ICRA14_Forster.pdf
","computer-vision, odometry"
Are the any self reproducing robots?,"Is there a possibility of making robots which utilise  raw materials to create more versions of themselves? Clarifying the point what I would like to ask is that whether there are are any projects or research on robots that can reproduce!!!
",manufacturing
Finding the position of a servo,"I am building a collision avoidance system for my robot. As a part of this system I am using a pan and tilt kit 
(http://www.robotshop.com/en/lynxmotion-pan-and-tilt-kit-aluminium2.html)
My aim is to pan the sensor attached to this kit, and thus plan the route the robot should take.
In order to pan this sensor, I need to know the angle the kit has panned, and need to be able to call upon that angle at point in time. 
Basically the sensor keeps panning, and at a point in time when certain conditions are met, it stops panning. When those conditions are met, I need to be able to extract the angle the sensor is panned at.
The servo being used on is: http://www.robotshop.com/en/hitec-hs422-servo-motor.html
If someone could help me find a way to extract this information that would be helpful. I did read somewhere that the servo could be hacked and changed into a closed loop system where the effective angle would be shown, but that option is not viable.
Thanks
","mobile-robot, sensors, rcservo"
How to use SLAM with simple sensors,"What 2D SLAM implementations (preferably included in ROS) can be used with simple distance sensors like IR or ultrasonic rangefinders?
I have a small mobile platform equipped with three forward facing ultrasonic sensors (positioned at 45 degrees, straight ahead, and -45 degrees), as well as a 6-DOF accel/gryo and wheel encoders, and I'd like to use this to play around with a ""toy"" SLAM implementation. I don't want to waste money on a Kinect, much less a commercial laser rangefinder, so methods that require high-density laser measurements aren't applicable.
","slam, ros"
Cannot launch iRobot Create. Powers down upon minimal launch?,"I just got an iRobot iCreate base and I've followed the instructions given in  ROS Tutorials to setup the turtlebot pc and the workstation. I could successfully ssh into username@turtlebot through workstation so I'm assuming that is all good. I had an issue with create not able to detect the usb cable which I solved using the detailed answer given for question here. This solved the problem of ""Failed to open port /dev/ttyUSB0"" that I was facing before. 
Now the next step would be to ssh into the turtlebot (which I've done) and use roslaunch turtlebot_bringup minimal.launch to do whatever the command does (I've no idea what to expect upon launch). But apparently something's amiss since the create base chirps and then powers down after showing [kinect_breaker_enabler-5] process has finished cleanly as output and the log file location (see output below), but I dont see a prompt. I checked the battery and that's charged so that's not the problem. Following is the terminal output.
anshul@AnshulsPC:~$ roslaunch turtlebot_bringup minimal.launch
... logging to /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/roslaunch-AnshulsPC-5038.log
Checking log directory for disk usage. This may take awhile.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.

started roslaunch server http://128.110.74.233:48495/

SUMMARY
========

PARAMETERS
 * /cmd_vel_mux/yaml_cfg_file
 * /diagnostic_aggregator/analyzers/digital_io/path
 * /diagnostic_aggregator/analyzers/digital_io/startswith
 * /diagnostic_aggregator/analyzers/digital_io/timeout
 * /diagnostic_aggregator/analyzers/digital_io/type
 * /diagnostic_aggregator/analyzers/mode/path
 * /diagnostic_aggregator/analyzers/mode/startswith
 * /diagnostic_aggregator/analyzers/mode/timeout
 * /diagnostic_aggregator/analyzers/mode/type
 * /diagnostic_aggregator/analyzers/nodes/contains
 * /diagnostic_aggregator/analyzers/nodes/path
 * /diagnostic_aggregator/analyzers/nodes/timeout
 * /diagnostic_aggregator/analyzers/nodes/type
 * /diagnostic_aggregator/analyzers/power/path
 * /diagnostic_aggregator/analyzers/power/startswith
 * /diagnostic_aggregator/analyzers/power/timeout
 * /diagnostic_aggregator/analyzers/power/type
 * /diagnostic_aggregator/analyzers/sensors/path
 * /diagnostic_aggregator/analyzers/sensors/startswith
 * /diagnostic_aggregator/analyzers/sensors/timeout
 * /diagnostic_aggregator/analyzers/sensors/type
 * /diagnostic_aggregator/base_path
 * /diagnostic_aggregator/pub_rate
 * /robot/name
 * /robot/type
 * /robot_description
 * /robot_pose_ekf/freq
 * /robot_pose_ekf/imu_used
 * /robot_pose_ekf/odom_used
 * /robot_pose_ekf/output_frame
 * /robot_pose_ekf/publish_tf
 * /robot_pose_ekf/sensor_timeout
 * /robot_pose_ekf/vo_used
 * /robot_state_publisher/publish_frequency
 * /rosdistro
 * /rosversion
 * /turtlebot_laptop_battery/acpi_path
 * /turtlebot_node/bonus
 * /turtlebot_node/port
 * /turtlebot_node/update_rate
 * /use_sim_time

NODES
  /
    cmd_vel_mux (nodelet/nodelet)
    diagnostic_aggregator (diagnostic_aggregator/aggregator_node)
    kinect_breaker_enabler (create_node/kinect_breaker_enabler.py)
    mobile_base_nodelet_manager (nodelet/nodelet)
    robot_pose_ekf (robot_pose_ekf/robot_pose_ekf)
    robot_state_publisher (robot_state_publisher/robot_state_publisher)
    turtlebot_laptop_battery (linux_hardware/laptop_battery.py)
    turtlebot_node (create_node/turtlebot_node.py)

auto-starting new master
process[master]: started with pid [5055]
ROS_MASTER_URI=http://128.110.74.233:11311

setting /run_id to 9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9
process[rosout-1]: started with pid [5068]
started core service [/rosout]
process[robot_state_publisher-2]: started with pid [5081]
process[diagnostic_aggregator-3]: started with pid [5102]
process[turtlebot_node-4]: started with pid [5117]
process[kinect_breaker_enabler-5]: started with pid [5122]
process[robot_pose_ekf-6]: started with pid [5181]
process[mobile_base_nodelet_manager-7]: started with pid [5226]
process[cmd_vel_mux-8]: started with pid [5245]
process[turtlebot_laptop_battery-9]: started with pid [5262]
[WARN] [WallTime: 1403641073.765412] Create : robot not connected yet, sci not available
[WARN] [WallTime: 1403641076.772764] Create : robot not connected yet, sci not available
[kinect_breaker_enabler-5] process has finished cleanly
log file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log

Following is the log file: /home/anshul/.ros/log/9d936a6a-fbdc-11e3-ba6b-00265e5f3bb9/kinect_breaker_enabler-5*.log output:
[rospy.client][INFO] 2014-06-24 14:20:12,442: init_node, name[/kinect_breaker_enabler], pid[5538]
[xmlrpc][INFO] 2014-06-24 14:20:12,442: XML-RPC server binding to 0.0.0.0:0
[rospy.init][INFO] 2014-06-24 14:20:12,443: ROS Slave URI: [http://128.110.74.233:51362/]
[xmlrpc][INFO] 2014-06-24 14:20:12,443: Started XML-RPC server [http://128.110.74.233:51362/]
[rospy.impl.masterslave][INFO] 2014-06-24 14:20:12,443: _ready: http://128.110.74.233:51362/
[xmlrpc][INFO] 2014-06-24 14:20:12,444: xml rpc node: starting XML-RPC server
[rospy.registration][INFO] 2014-06-24 14:20:12,445: Registering with master node http://128.110.74.233:11311
[rospy.init][INFO] 2014-06-24 14:20:12,543: registered with master
[rospy.rosout][INFO] 2014-06-24 14:20:12,544: initializing /rosout core topic
[rospy.rosout][INFO] 2014-06-24 14:20:12,546: connected to core topic /rosout
[rospy.simtime][INFO] 2014-06-24 14:20:12,547: /use_sim_time is not set, will not subscribe to simulated time [/clock] topic
[rospy.internal][INFO] 2014-06-24 14:20:12,820: topic[/rosout] adding connection to [/rosout], count 0
[rospy.core][INFO] 2014-06-24 14:20:20,182: signal_shutdown [atexit]
[rospy.internal][INFO] 2014-06-24 14:20:20,187: topic[/rosout] removing connection to /rosout
[rospy.impl.masterslave][INFO] 2014-06-24 14:20:20,188: atexit

From the logs, I could tell something told the create to power down. And since the log is named with 'kinect', I tried minimal.launch w/ and w/o kinect attached to the turtlebot pc. It doesn't make any difference. 
Any clue what I might be missing? Or is this the way bringup works (I guess not)?
","ros, irobot-create"
Does iRobot 681 have a serial interface?,"I've read that recent Roomba 6XX from iRobot got a serial connector, but I removed the cover on my Roomba 681, and I haven't found any. From what I've found, 681 hasn't the same cover than other 6XX robots. Does this model have an accesible serial port?
 
","irobot-create, serial"
Lego Mindsotrms RCX 1.0 CPU,"I am working on a project and I need to know what is the exact model number of the CPU that is in the Lego Mindstorms RCX 1.0 . It would be nice if you listed the specs of the CPU, beacause I am looking for its MIPS performance.
","microcontroller, mindstorms"
Handling GYROSCOPE BIAS DRIFT,"I am working on a MEMS based project which requires me to calculate the orientation(Euler Angles) of an object using only GYROSCOPES.
The GYROSCOPE BIAS is calculated in the beginning for 2 seconds keeping it stationary. 
Right now GYROSCOPES give me an accuracy of 2 degrees for a period of 3-4 mins of continuous movement.
Now if there is continuous movement beyond 3-4 mins, the GYROSCOPE BIAS would have changed and hence the errors increases rapidly.
My Question is:
1). If the bias drift changes randomly(as read),the angles start increasing in one direction, then why cant we track it for first ten seconds and then keep subtracting the present angles from  initially calculated angles,for every ten seconds during movement.
I tried this out, but it does not work as expected.
Can GYROSCOPE BIAS be tracked by any way?
Thanks in advance.
","gyroscope, orientation"
Self balancing robot control system strategies,"I’m in the early stages of designing a self-balancing robot as a way to refresh my knowledge on control theory, which has been gradually slipping away since graduating about a year and a half ago. I'm wondering if anyone has any input on how best to approach this problem.
My plan is to use a 6 DOF IMU as an angle sensor, and to control the pitch by accelerating and decelerating the cart. I'm looking for robust response to disturbance, and to add in RC differential drive capabilities later on. This block diagram is a pretty close match for what I was planning to do,  (source: Sebastian Nilsson's Blog):

Would this be a good approach? Any recommended alternatives? Thanks.
","control, wheeled-robot, imu, two-wheeled"
"""Time-varying"" and ""nonautonomous"" dynamical systems and their Lyapunov analysis","It is possible to distinguish the properties ""time-varying"" and ""nonautonomous"" in dynamical systems regarding Lyapunov stability analysis?
Does it make a difference if the system depends explicitly on $t$ or indirectly on $t$ due to a time-varying parameter?
I want to explain the problem in detail:
Let a dynamical system denoted by $\dot x = f$, with state $x$.
We say that a dynamical system is nonautonomous if the dynamics $f$ depend on time $t$, i.e. $$\dot x = f(t,x).$$
For instance the systems $$ \dot x = - t x^2 $$ and $$ \dot x = -a(t)x,$$ are nonautonomous. Let $a(t)$ be a bounded time-varying parameter, i.e. $||a(t)||<a^+$ and strictly positive, i.e. $a(t) > 0$. 
Particularly, the second example is more likely denoted as a time-varying linear system, but of course it is nonautonomous. 
In Lyapunov stability analysis autonomous and nonautonomous systems must be strongly distinguished to make assertions about stability of the system, and the Lyapunov analysis for nonautonomuos systems is much more difficult.
And here for me some questions arise. When i want to analize stability of the second example must i really use the Lyapunov theory for nonautonomous systems? 
It follows for the candidate $V = 1/2 x^2$
$$\dot V = -a(t)x^2,$$
which is negative definite. Is the origin really asymptotically stable, as i suppose, or must i take the nonautomous characteristic into account in this case?
I would suppose it makes a difference if a system depends explicitly on $t$ as in the first example or just indirect due to a time-varying parameter, since $t$ approaches infinity, but a parameter does not.
","control, dynamics"
How to calculate battery size for servo motors?,"I am making a robotic device with the following servo motors.
How do I go about calculating the battery capacity?
It is supposed to run for 10 minutes.
I am bit confused as to use the no-load current or load current or the stall current to add up the amps.
Thanks
Ro


HS-422  
HS-485HB  
HS-645MG 
HS-755HB
HS-805BB
HS-85BB
HS-785HB

","battery, servomotor"
What is the learning (control) algorithm inside Cubli?,"As in this video: https://www.youtube.com/watch?v=qce5Vguj5Jg
In this new version (did not see the learning part in the past versions), with three to four trials, Cubli can learn to balance on a new surface.
",control
Problem with angle calculation,"I'm using MPU6050(accelerometer+gyro) connected to ATmega328P microcontroller, but probably it isn't even important in my case.
In my project, I need an angle around the X axis. And it's calculated like this:
angle = -(atan2(acc.XAxis, sqrt(acc.YAxis*acc.YAxis + acc.ZAxis*acc.ZAxis))*180.0)/M_PI;

where acc is vector of accelerations in all directions.
The problem is, that it gives me credible value only when angle between Z axis and ground is right (so it's not rotated around Y axis). When I start to rotate it around Y axis it also changes value of X axis rotation.

I know, that it's due to YAxis acceleration in my algorithm, but I have no idea how to get rid of it.
How can I solve this problem?
","accelerometer, gyroscope, rotation"
Powering a 6v servo and 12v servo,"In my project, there are two types of servos ( 6v and 12v supply needs ).
I need to power with the same battery. 
How should I go about it?
Use a 6v battery with a step-up voltage regulator? 
How does that impact the battery power calculations?
If voltage regulator should be used please recommend one.
Thanks
Ro
",battery
Using gmapping without ROS?,"Is gmapping on OpenSLAM.org still maintained? Or is the maintenance entirely over in ROS ( https://github.com/ros-perception/slam_gmapping/tree/hydro-devel/gmapping ). When trying to compile gmapping without ROS I noticed that it still has Qt3 as a dependency which made me think no one uses or maintains OpenSLAM's gmapping anymore. Is this accurate? How does OpenSLAM's gmapping vs ROS's gmapping compare in terms of performance and accuracy?
Thanks!
","localization, slam, ros, mapping"
Resources for object detection with 2D Laser Scanner (planar only),"Would you happen to know some good books, tutorials or articles on how to detect objects and their poses, using 2D laser scanners?
My goal is to equip a mobile robot with a laser scanner for object detection in a industrial like environment. I would like to detect legs, some pallets and some trolleys, and measure their poses as well.
My first intuition is extracting lines from the 2D readings. But then I'm somewhat lost in the next steps.
","laser, lidar, pose"
Learning micro-controller programming,"Is learning microcontroller programming useful for one who want to specialize in robotics I don't mean platforms like Arduino , Teensy , I mean mcu itself like Arm cortex etc.
",microcontroller
"Building a quadcopter, what motors, props and what are the calculations?","Before I start, I am a 13 year old, I would like to apologise because I am a beginner to all this and I wouldn't really understand any professional talk, I am only doing this for a hobby.
I am building a quadcopter,

Flight controller: KK 1.2.5
ESC: Q Brain 25amp
Frame: KK 260 / FPV 260
Frame Addon: KK/FPV 250 Long Frame Upgrade Kit
Tx & Rx: HobbyKing 6ch tx rx (Mode 2)
Battery: Turnigy Nano-Tech 2200 mAh 3S

I am not sure about what motor and propellers I should use.
All I know is: for the frame the motor mounts are: 16mm to 19mm with M3 screws
I am not sure what 1806 and 2208 means.
Here are my questions:

What calculations should I do to find out how much thrust the quad needs to produce / any other useful calculations
Using the calculations what would be the best and CHEAPEST motors I could have
And finally, what propeller would be best suited for the motor.

p.s: I am looking for a durable and really cheap motors also, I live in London, so shipping might be a problem if there is an immense bill.
Thanks a lot for your time,
Sid
","quadcopter, motor"
Direct vs semi-direct methods for visual inertial odometry,"I was reading these papers on visual inertial odometry from IROS 15:
Semi-Direct EKF-based Monocular Visual-Inertial Odometry
Robust Visual Inertial Odometry Using a Direct EKF-Based Approach

I would appreciate if someone could explain how semi-direct and direct methods vary exactly? As far as I understand, direct methods use pixel intensities in their framework. However, both these papers listed above use photometric intensities/pixel intensity values and yet one is semi direct and the other's direct. 
",ekf
Having a hard time understanding this equation in monocular EKF SLAM,"Reading this paper on visual odometry, where they have used a bearing vector to parameterize the features. I am having a hard time understanding what the state propagation equation for the bearing vector term means :

The vector N is not mentioned in the equations, so its not very clear what it does. Would really appreciate if someone would help me understand it :)
","localization, slam, navigation, ekf, mapping"
Could anyone complete my challenge?,"Earlier while writing by hand, I was wondering if anyone could build a pen holder, that could write for me? Maybe you could make some good business from them. Thanks for your time.
","wheeled-robot, line-following, wifi"
Reliability of Raspberry Pi in Wild Life Guardian Drone project,"I'm new here, my name is Mark, I'm 24 years old and I'm a Linux lover, by 2020 I intend to build up a wildlife guardian drone based on the Raspberry pi which will be connected to the internet but that will operate by its own using artificial intelligence.
Talking about this to a technician at my school in Italy, I have been told that the Raspberry is not suitable for an important project but just for little experiments.
The way he said that seemed to me just a bit too dismissive but he made me become doubtful about the Raspberry's quality so I hope you guys can help me understand if the Raspberry can suite my ambitious plan to build a guardian drone to protect the wildlife of a very little Papua New Guinea island or if it will break the first time it encounters trouble.
Is the Raspberry pi 3 only for little experiments or will it also work for very important stable projects like building robots?
",raspberry-pi
What types of motor should I use for a particular application?,"I want to create an amateur wire looping machine with Arduino, that has similar functionality than this machine. I don't need the automatic wire feeding as for my purposes this part can be done manually. I just want to automate the wire loop creation process, assuming that I already have straight wires.
I'm new to the world of motors, robotics, etc., so please be as descriptive as possible :)
From the video, I can tell that there are two motors:

Makes the initial wire bending
Spins to create the loop

The wire that I will be working with is galvanized steel of 11 gauge (2.0 - 2.5 mm diameter).
So what type of motors would be recommended for this application, taking into account:

They need to be accurate in their positioning for repeat ability
They need to have enough torque (specially the one that creates the loop itself) to work with this type of material
They're as fast as (or close to) the ones in the video
This is not going to be an industrial grade machine that will be running all the time
Ideally, they need to be not that expensive.. I don't want to be bankrupt by the end of this project :)
If links can be included for recommended products, it would be great.

Thanks!
",motor
Do I need extra software on the EV3 to use Robotary?,"Do we need to install extra software on the EV3 or does Robotary (https://robotaryapp.com/) install programs just like the default LEGO Software.
Thanks, Josh
",mindstorms
Tuning PID controller for sharp turns in line follower robot,"I'm working on a TWDD line-following robot using qtr-8a reflectance sensor array.
The sensors are connected to BeagleBone Black, and BBB is sending the speed serially to an arduino Due.
My approach is using a PID controller for the sensor so the error equals to zero when the robot is centred on the line and a positive/negative error depending on the robot's position. Applying Trial and error method I finally reached a Kp value that tracks straight lines perfectly. However, I'm still unable to turn and stay on the line even on a similarly smooth turns. I guess this is related to the Kd value. I'm not using the integral part Ki since the error is keep increasing. I tried to set conditions when the robots is drifting away from the line but it's not working probably (even without the conditions it is somehow turning smoother but then losing the line)
I'm using the following draft code:
from bbio import *
import time


integral = 0
last_prop = 0
Kp = 20
Ki= 0
Kd = 150
amax = list(0 for i in range(0,8))
amin = list(1024 for i in range(0,8))
timeout = time.time() + 10

# Read ADC data from MCP3008
# ch: 0-7, ADC channel
# cs: 0-1, SPI chip select
# See MCP3008 datasheet p.21
def adc_read(cs, ch):
    spidata = SPI1.transfer(cs, [1,(8+ch)<<4, 0])
    data = ((spidata[1] & 3) << 8) + spidata[2]

    return data

def setup():
# SPI1 setup
    Serial2.begin(9600)
    Serial5.begin(9600)
    pinMode(GPIO1_7, OUTPUT)
    digitalWrite(GPIO1_7, LOW)
    SPI1.begin()
    SPI1.setMaxFrequency(0,50000)  # => ~47kHz, higher gives occasional false readings
  #  SPI1.setMaxFrequency(1,50000)
    calibrate()

# reading the IR sensor data    
def read_sensors():
    sensors = []
    for i in range(8):
        sensors.append(adc_read(0,i))

    return sensors
# calculating the error from PID controller
def calc_pid(x,sp):
    global  integral, last_prop , Kp, Ki ,Kd
    set_point = sp
    pos = sensor_average(x)/sensor_sum(x)
    prop = pos - set_point
    integral = integral + prop
    deriv = prop - last_prop
    last_prop = prop
    error = (prop*Kp + integral*Ki + deriv*Kd)/100

    return error

def get_position(s):
    return sensor_average(s)/sensor_sum(s)        

def sensor_average(x):
    avg = 0
    for i in range(8):
        avg += x[i]*i*100

    return avg

def sensor_sum(x):
    sum = 0  
    for i in range(8):
        sum += x[i] 
    return sum


def get_sensor(x):
    j= read_sensors()[x]
    return j

def calc_setpoint(x,y):
    avg = 0
    sum = 0
    for i in range(8):
        avg += (x[i]-y[i])*i*100
        sum += x[i]-y[i]
    return avg/sum
# calibrate the sensors reading to fit any given line
def calibrate():
    global amin
    global amax

    while(time.time() < timeout):
        for i in range(0,8):
            amin[i] = min(amin[i],read_sensors()[i])
            amax[i] = max(amax[i],read_sensors()[i])
            digitalWrite(GPIO1_7, HIGH)
    digitalWrite(GPIO1_7,LOW)


# calculating the correspondent speed
def calc_speed(error):
    avg_speed = 150
    min = 100
    pos = get_position(read_sensors())
    speed = []
    if(error < -35):
        right = avg_speed -(2*error)
        left = avg_speed + (2*error)
    elif(error > 35):
        right =avg_speed - (2*error)
        left = avg_speed + (2*error)
    else:      
        right = avg_speed - (error)
        left = avg_speed + (error)
    speed.append(right)
    speed.append(left)
    return speed


def loop():

    s = read_sensors()
    setpoint = calc_setpoint(amax,amin)
    position = get_position(s)
    err = calc_pid(s,setpoint)

    print err

    #print ""divided by 100:""
    speeds = calc_speed(err) 
    print speeds
    right_motor = speeds[0]
    left_motor = speeds[1]


    Serial2.write(right_motor)

    Serial5.write(left_motor)
    delay(10)

run(setup,loop)        

PS: the sent speed over serial is limited to 255 and I'm multiplying it by a factor from the Arduino side.
","pid, python, line-following, beagle-bone"
How to transform Accelerometer data from the robots frame to the world frame,"I'm current workin on an EKF for an carlike robot and I'm not sure how to transform the IMU accelerometer data to offset the roll, pitch and yaw and get the acceleration with respect to the ground.
I'm only computing the x,y position of the robot and the yaw of this vehicle.
","accelerometer, ekf"
Differential Drive Odometry Model,"%PREDICT Apply odometry model for differential drive robot.
%   [R,FXR,PATH] = PREDICT(R,ENC,PARAMS) 
%   calculates the final pose and final pose covariance matrix given
%   a start pose, a start pose covariance matrix and the angular wheel
%   displacements in ENC. PARAMS contains the robot model and the
%   error growth coefficients.
%
%   Input arguments:
%      R   : differential drive robot object with start pose R.X, R.C
%      ENC : structure with fields
%            ENC.PARAMS.KL: error growth coefficient of left wheel
%            ENC.PARAMS.KR: error growth coefficient of right wheel
%               with unit in [1/m]. 
%            ENC.STEPS(i).DATA1: angular displacements of left wheel
%            ENC.STEPS(i).DATA2: angular displacements of right wheel
%               in [rad] and monotonically increasing
%      PARAMS.B : wheelbase in [m]. Distance between the two wheel
%                 contact points
%      PARAMS.RL: radius of left wheel in [m]
%      PARAMS.RR: radius of right wheel in [m]
%
%   Output arguments:
%      R    : differential drive robot object with final pose R.X, R.C
%      FXR  : 3x3 process model Jacobian matrix linearized with
%             respect to XROUT
%      PATH : array of structure with fields PATH(i).X (3x1) and 
%             PATH(i).C (3x3) which holds the poses and the pose
%             covariance matrices over the path
%
%   The function implements an error model for differential drive
%   robots which models non-systematic odometry errors in the wheel 
%   space and propagates them through the robot kinematics onto the
%   x,y,theta-pose level.
%
%   Reference:
%      K.S. Chong, L. Kleeman, ""Accurate Odometry and Error Modelling
%      for a Mobile Robot,"" IEEE International Conference on Robotics
%      and Automation, Albuquerque, USA, 1997.
%
%   See also SLAM.

% v.1.1, ~2000, Kai Arras, ASL-EPFL, Felix Wullschleger, IfR-ETHZ
% v.1.2, 29.11.2003, Kai Arras, CAS-KTH: toolbox version

function [r,Fxr,path] = predict(r,enc)

When I try to run this complete code, it asks me to input values for R and Enc. I am not sure whether they are just one digit values or a matrix. Can anyone help me by looking at the description in the code, what kind of input is required. 
","mobile-robot, odometry, forward-kinematics, differential-drive"
How to tilt a camera 180 using mirrors,"How would you vertically tilt a camera 180 degrees using mirrors?
I'm trying to add a pan/tilt mechanism to a Raspberry Pi's camera. The camera uses one of those flat cables with unstranded wires, and even with a strain gauge, I don't trust it to handle repeated bending, so I'm trying to design a tilt mechanism that allows the camera to be rigidly mounted so no wires move. The tilting also has to happen very quickly, so I'm trying to minimize the amount of mass I need to move.
Then I saw the Oculus kit that actuates a mirror to effectively tilt a laptop's fixed webcam. I'm trying to extend this idea, but I having trouble working out the mechanics that would allow the tilt to extend to 180 degrees. The layout in the Oculus's mechanism only supports a tilt angle of about 90 degrees, and the mirrors are relatively large. Is it possible to modify this to support 180 degrees?
Are there other ways to ""bend"" the view of a camera without having to move the actual camera?
","mechanism, cameras"
How to seal a threaded rod so I won't lose oil?,"I have an M5 threaded rod that penetrates a box. Outside it is driven by a motor and it rotates a part inside the box. The box is sealed and I would like to seal the area around the rod as well. I have seen double lip oil seals but I don't think they will work.
The oil is very runny, like water and the reason I am not using a smooth rod (which would be simpler to use in this application) is mainly in order to save on tooling costs as I can get the rods already threaded for very cheap.
Any help would be appreciated!
",mechanism
How to get live audio from robot?,"I am building a robot and I want to be able to hear sounds from it's environment (ideally from my laptop). What is the best way to get live audio from my robot's microphone to my computer?
I have looked into a few solutions for hosting live audio streams using packages such as darkice and icecast. I'm just wondering about better solutions for robotic applications.
Additional details:
- I have access to hardware such as Raspberry Pi, Arduino, etc.
","real-time, digital-audio"
KUKA arm's damping ratio,"Does anyone know how one can calculate the exact damping of KUKA arm during impedance control mode. According to their manual one can control only the Lehr's ratio. However, I want to know the exact value,say in Ns/m.
",kuka
How many solutions are there to this,"I'm studying for an exam and I came across these questions where you have to determine the number of possible inverse kinematic equations like  (qs from Craig Introduction To Robotics, 3rd Edition)
How exactly do you find the number of solutions? I know how to find the DH paramaters and then the inverse kinematics but is there some way to calculate the number of solutions quickly like this question suggests? Like, I can visualize it (4 for the second one) but is there a proper way?
Thanks.
",robotic-arm
RVIZ Transform error Base_link and Camera_link,"I am working on a differential drive robot with two motor wheels with encoders and caster wheels. The robot also has an Intel RealSense depth camera.
When I launch RVIZ: The Global option > Fixed frame is set to Base_link and shows all the transforms for the differential driver nodes. But an error appears for the Depth camera nodes with message saying:
No transform from Camera_depth_frame to baselink
No transform from Camera_depth_optical_frame to baselink
No transform from Camera_link to baselink
No transform from Camera_rgb_frame to baselink

If I change the Global option > fixed frame to Camera_link, I can see all the transforms for the depth camera but now the differential drive transforms are not available
Hope you can help.
",ros
"Plug & play motors, actuators and drivers system","I was wondering if anyone knew of any easy-to-use plug and play systems, that allow you to build and programme a number of actuators and motors quickly and easily?
I'm thinking something like Lego Mindstorms, but for adults. I.e. with more powerful motors. I need to be able to generate a torque of around 10 Nm, at a speed of around 10 rpm, and I need to control the rotation angles. Could be either stepper motors, or normal DC motors with hall sensors to measure number of rotations. I'm at the beginning of my journey into robotics, so don't mind paying a bit more for the parts in favour of speeding up my learning curve!
If it's not clear already, I'm looking for an ""off-the-shelf"" product, with a range of products I can browse, and accumulate an increasing number of components over the years.
Many thanks in advance for any suggestions!
","motor, actuator"
Covariance Check?,"I have a localization data estimated and GPS_truth and generated [3x3] covariance matrix along with them. 
What i would like to do is to see if the covariance is correct or not? 
Can we check it by plotting the covariance? 
","localization, slam, visualization"
Converting a differential equation into its Laplace transform,"Given the differential equation of a current controlled electric actuator, how would I convert the differential equation into its Laplace transform?
$$ J \frac{d^2 \phi(t)}{dt^2} + D \frac{d \phi(t)}{dt} + H \phi(t) = K i(t) = 0 $$
I know the reason you're meant to convert this to laplace is because it's easier to work with but I'm finding it difficult to understand why you do certain things in certain parts.
Any concrete method on how to convert this would be appreciated.
",actuator
Parallel axis theorem to determine the moment of inertia,"I need to use the parallel axis theorem to determine the moment of inertia of each robotic arm link referred to the joints of the 3 DOF manipulator. The moments of each link's centre of mass are:

The link joints are located according to the following coordinates from each link's centre of mass and the link masses are specified as:

I know that solve Izz, Iyy and Ixx to use the following equations:
Ixx = Ixx + m(ry^2 + rz^2)
Izz = Izz + m(rx^2 + ry^2)
Iyy = Iyy + m(rx^2 + rz^2)
But I'm not sure how to solve this particular problem, I was thinking since y1 and z1 both equal 0, to solve the question for that link I need to solve I1yy and I1zz is that right?
","robotic-arm, dynamics, joint"
Denavit-Hartenberg parameters for SCARA manipulator,"I'm going through the textbook Robot Modeling and Control, learning about the DH convention and working through some examples. 
I have an issue with the following example. Given below is an image of the problem, and the link parameter table which I tried filling out myself. I got the same answers, except I believe there should be a parameter d1 representing the link offset between frames 1 and 2. This would be analogous to the d4 parameter. 

If anyone could explain why I might be wrong, or confirm that I have it right, that would be great. I hate it when it's me against the textbook lol. 
Cheers.
","forward-kinematics, dh-parameters"
"Sensors, Methods for Finding Other Robots","If I can constrain two robots to a hypothetical box, what sensors or common formal methods exist that would enable the two robots to meet? I am specifically interested in the communication of relative position between robots.
","communication, swarm"
Reading multiple signed digits from serial port,"I'm using simulink support package for arduino to read serial data from port2 in Arduino due. My plan is to read signed integers (-415 for example) representing motor speed and feed it to the pid controllers as in the image.
from the far end i'm sending delimited data in the following shape . The matlab function in sumlink is supposed to read the received ASCII characters and add them to a variable until it reaches the end character '>'. I'm using the following simple code just to give the output on both the Right and Left to check if I'm receiving the correct data, However I'm not. 
    function [Right ,Left] = fcn(data,status)


SON = '<';
EON = '>';

 persistent receivedNumber;
 receivedNumber = 0;

 persistent negative;
 negative = false;


 if(status ==1)

    switch(data)

    case EON
        if (negative)
            receivedNumber = -1*receivedNumber;


        else
            receivedNumber = 1*receivedNumber;
        end


    case SON
        receivedNumber = 0;
        negative = false;


    case {'0','1','2','3','4','5','6','7','8','9'}
        receivedNumber = 10*receivedNumber;
        receivedNumber = receivedNumber + double((data - 48));


    case '-'
        negative = true;  



    end       

 end

    Right = receivedNumber;
    Left = receivedNumber;



end

Can anybody tells if there are any other approaches to read multiple signed digits in simulink? Taking into consideration that I have to use the support package for Arduino since my pid controllers are already configured in Simulink and interfaced with port2 in Arduino (which will be connected to BeagleBone black)    
","arduino, matlab, serial"
How to get data from ArduPilot through Serial Port,"I'm doing a project related to telemetry, and I want to make ArduPilot (programmed with ArduPlane 2.73) send through a serial port the sensors informations such as height, GPS position, etc.. I have tried to use ArduStation, but I could not change its firmware to do what I want. 
The idea would be to read the Ardupilot's serial port using an Arduino Uno, and then saving it in a SD card in real-time. So ArduPilot needs to send data without any user input or something like that. I've already tried to manipulate ArduPlane source code to do that, but I couldn't either.
Has someone here did something like that before? I need some advice!
","arduino, ardupilot"
Is ROS (Robot Operating System) mandatory?,"Do we have to build ROS for robotic research/application? What is the main advantage? When or in which situations ROS is mandatory?
","ros, simulator"
Integrating Forward Kinematics Map,"Let the forward kinematics map be denoted by $\mathcal{F}$ such that
$\mathcal{F}: \theta \in \mathbb{R}^{n} \rightarrow g \in SE3$
Let the minimal representation of $g$ be given by $x \in \mathbb{R}^{6}$ using axis-angle or other forms of attitude parametrization. If we differentiate the forward kinematics map, we get
$\dot{x} = J_{a}\dot{\theta}$, where $J_{a}$ is the analytical Jacobian. This equation is commonly used in numerical inverse kinematics. However, can we do the reverse?
$x(t_{f})-x(t_0) = \int^{t_{f}}_{t_{0}}J_{a}\dot{\theta}dt$
","robotic-arm, kinematics, forward-kinematics, jacobian, manipulator"
Velocity Relation for Parallel Robots,"In my course of ""Advanced Robotics"" with ""Fundamental of Robotic Mechanical Systems"" as the reference book I saw the following equation as the velocity relation for parallel manipulators such as the one depicted on the pic. 
$$\mathbf{J}\mathbf{\dot{\theta}}=\mathbf{K}\mathbf{t}$$
$\mathbf{\dot{\theta}}$ is the joint rate vector and $\mathbf{t}$ is the twist.

where $\theta_{J1}$ for $1\leq J\leq 3$ is the actuator.
It was indicated there that usually the matrix $\mathbf{J}$ is a diagonal matrix referring to the borders of the work space. But the proof for this velocity relation was just given case by case. In another word there was no general proof of this equation. 
So here are my questions

Is there a way to derive a general velocity formula for parallel manipulators ? (a formula that would show the relation between joint rates and twists). 
Are there cases in which $\mathbf{J}$ is not a diagonal matrix ?

","kinematics, inverse-kinematics, jacobian"
Is this large bipedal robot real or realizable?,"I saw this link today. This robot seems real, but redditors on a reddit thread argued that it might be CGI. It doesn't seem unfeasible to make such as robot IMO. There isn't anything out of the norm from the video except the size. Plus, there exist Kurata and Megabot, though they don't make bipedal robot, the scale is similar. From roboticist's point view, is this robot real? What are the technological limitation preventing such a robot being developed if it's fake? 
",design
Cascaded PID in Quadcopter control,"I'm working on diy flight controller for quadcopter. I have a question for which I can't find a good answer. So perhaps you could help me.
I'm using a cascaded PID controller for Pitch and Roll regulation. 
First there is a stabilize PID and rate PID. For the first (stab.) you input desired angle from transmitter and actual angle from IMU. then this output is feed into rate controler. From there it goes to the motors. 
In code I'm pooling a function with ""data is ready to read from IMU"" which happens every 1ms. In this function I'm calculating one regulator and writing to motors. Loop time when this condition is not true is way lower then that. So one regulator should be inside this slow loop and one outside. So which one should be fast and which one slow?
In my understanding, stabilize PID should be the fastest? Is that correct?
Also should both regulators be PID? 
Thanks for your help!
","quadcopter, pid"
How to interpret and use values of an accelerometer,"Currently, I'm building a quadcopter using Arduino. To make the copter able to stabilize I use an mpu6050 accelerometer + gyroscope.
I understand that I can get the angle of rotation by integrating the values of the gyro. I understand too that I can calculate the angle using the amount of G working on the different axes of the accelerometer.
But from the accelerometer I get values from around 1000 to 14000.

What are these values?
How can I get the angle from these values?
How can I turn these values into motor rotation?

",accelerometer
How can i get the Laplace Transfer function for control process for quadcopter?,"I'm a beginner for quadcopter construction with PID controller and PIC 16F877 microcontroller. Now, I'm troubling with how do make the control process with mathematical model for my quadcopter :( and how to draw flow chart for this?
","quadcopter, control"
Comparison of Balance Bot design,"I'm planning to build a Balance Bot and I would like to know which design should I go with. There's a vertical design such as the ArduRoller and on the other hand, there's the typical stacked type of deign. I have attached images below for both of them.
Although both of these are based on the principle of Inverted Pendulum, how do these differ from each other in terms of stability and response (assuming both the designs are of the same size)? Is the mass distribution in the vertical design better? Also, is there a difference in programming them?
 
","design, stability"
Reading Intention for exoskeletons motion estimation,"I'm interested in exoskeletons and wearable rehabilitation robotics. I wonder how we can estimate/predict the intention of human body/part motion. I want to prevent the exoskeleton from interfering with human movements.  Intention reading is the process of predicting how the movement will take place and how it will happen at the beginning of any movement.
There is an exoskeleton example (https://www.youtube.com/watch?v=BdoblvmTixA) which detects muscle activation with EMG and generates artificial muscle attraction. But this is only a open and close action. And it will begins after the movement. Also EMG system has a lot of disadvantage like sliding probes, affecting from other/crossing muscles. I want to estimate every motion like turn,twist, amount of contraction. I'm open to suggestions or issues (The troubles you have experienced.)
This matlab webinar which is about ""Signal Processing and Machine Learning Techniques for Sensor Data Analytics"" shows how to classify different actions. But this example predicts kind of motion after the motion completed. I need to know motion information at very first.
I want know how can I estimate different motions at the beginning of limb action. Which system (EMG,EEG,IMU,etc.) and processing technique will be better or which combination should I use.
","sensors, robotic-arm"
What is the bicycle model for a dynamic robot?,"I was asked this in a phone interview for a robotics job. Googling has not really helped. I assume it is some sort of state prediction model that can be used in a Kalman filter.
Can anyone give me a formal description? A link to a reference would also be nice.
EDIT to clarify, the interview was for a self-driving car company and before the question we had been discussing Kalman filters, Particle filters, and path planning algorithms (A*).
","mobile-robot, kalman-filter, dynamics"
Angles from gyroscope readings using quaternions,"I am using gyroscope only to get real time angles as I move the IMU using a micro controller.
I am able to get angles at a pretty decent accuracy(2 to 3 degree error). I am using quaternions for obtaining angles.
The angles are with respect to the initial position of the IMU.
I rotate the IMU around individual axis i.e around any one Axis at a time, I get good accuracy. But when I rotate around all axis at once the problem starts as stated below.
Problem:
The problem here is when I give a pitch and then yaw all seems fine. Now at this position of pitch and yaw, I give a roll, the other two angles also change. 
The gyroscope gives raw angular rate in DPS, which is converted to radian/sec for processing. The quaternions are calculated from raw angular rate.
The code to convert quaternions to euler angles is as follows,
//Local variables for clarity
fqw = gsangleparam.fquaternion[0];
fqx = gsangleparam.fquaternion[1];
fqy = gsangleparam.fquaternion[2];
fqz = gsangleparam.fquaternion[3];


//Calculate Angles*/
gsangles.yaw = atan2(2 * (fqx*fqy + fqw*fqz), (fqw*fqw + fqx*fqx - fqy*fqy -    fqz*fqz)) * 180/PI;

gsangles.pitch = asin(-2 * (fqx*fqz - fqw*fqy)) * 180/PI;

gsangles.roll = atan2(2 * (fqy*fqz + fqw*fqx), (fqw*fqw - fqx*fqx - fqy*fqy  + fqz*fqz)) * 180/PI;

The Problem of roll is present only when initial position(and also 180 deg counterpart) is as follows:
X - Facing left
Y - Facing towards us 
Z - Facing UP
The problem is, When I give a pitch(around X here),yaw(around Z) and then roll(around Y here) or any sequence of roll pitch and yaw, then yaw changes.
In fact whenever I rotate about Y axis(roll here), Yaw(around Z) changes. 
There is no problem when the initial position is(and also 180 deg counterpart) as follows, 
X - Facing away from us 
Y - Facing towards us
Z - Facing UP
There is no problem. i.e When I give a pitch(around Y here),yaw(around Z) and then roll(around X here) or any sequence of roll pitch and yaw, there is no problem.
Why is this happening?? can anyone please explain. 
Thanks in advance.
","accelerometer, gyroscope"
"showing error while using inverse kinematics ""ikine"" for 4 dof robotic arm","I want to do forward dynamics but before that I got struck in inverse kinematics for 4 dof. My code is given below:
preach = [0.2, 0.2, 0.3];
% create links using D-H parameters
L(1) = Link([ 0 0 0 pi/2 0], 'standard');
L(2) = Link([ 0 .15005 .4318 0 0], 'standard');
L(3) = Link([0 .0203 0 -pi/2 0], 'standard');
L(4) = Link([0 .4318 0 pi/2 0], 'standard');
%define link mass
L(1).m = 4.43;
L(2).m = 10.2;
L(3).m = 4.8;
L(4).m = 1.18;
%define center of gravity
L(1).r = [ 0 0 -0.08];
L(2).r = [ -0.216 0 0.026];
L(3).r = [ 0 0 0.216];
L(4).r = [ 0 0.02 0];
%define link inertial as a 6-element vector
%interpreted in the order of [Ixx Iyy Izz Ixy Iyz Ixz]
L(1).I = [ 0.195 0.195 0.026 0 0 0];
L(2).I = [ 0.588 1.886 1.470 0 0 0];
L(3).I = [ 0.324 0.324 0.017 0 0 0];
L(4).I = [ 3.83e-3 2.5e-3 3.83e-3 0 0 0];
% set limits for joints
L(1).qlim=[deg2rad(-160) deg2rad(160)];
L(2).qlim=[deg2rad(-125) deg2rad(125)];
L(3).qlim=[deg2rad(-270) deg2rad(90)];
%build the robot model
rob = SerialLink(L, 'name','rob');
qready = [0 -pi/6 pi/6 pi/3   ];
m = [1 1 1 1 0 0];   % mask matrix
T0 = fkine(rob, qready);
t = [0:.056:2];
% do inverse kinematics
qreach =  rob.ikine(T0, preach, m); 
[q,qd,qdd]=jtraj(qready,qreach,t);
%compute inverse dynamics using recursive Newton-Euler algorithm
tauf = rne(rob, q, qd, qdd);
% forward dynamics
[t1,Q,Qd] = rob.fdyn(2,tauf(5,:),q(5,:), qd(5,:));

But due to
qreach =  rob.ikine(T0, preach, m); 

it shows error
Index exceeds matrix dimensions.
Error in SerialLink/jacobn (line 64) U = L(j).A(q(j)) * U;
Error in SerialLink/jacob0 (line 56) Jn = jacobn(robot, q); % Jacobian from joint to wrist space
Error in SerialLink/ikine (line 153) J0 = jacob0(robot, q);
Can anybody explain me why this is happening and how to resolve it.
Thanks.
","kinematics, inverse-kinematics, matlab, manipulator"
Is there any error analysis of Irobot Create 2?,"I am curious about the travelling distance/turning angle accuracy of Irobot Create 2. E.g., if the program let Create 2 go forward for 10 m, will Create 2 go forward exactly 10 m?
What is the possible error for Create 2 for both travelling distance and turning angle? I did not find related information in the Irobot create 2 manual. 
Is there any one could help me with the above questions?
","irobot-create, errors, sensor-error"
What is the best way to compute the probabilistic belief of a robot equipped with a vision sensor?,"I am trying to implement 'belief space' planning for a robot that has a camera as its main sensor. Similar to SLAM, the robot has a map of 3D points, and it localizes by performing 2D-3D matching with the environment at every step. For the purpose of this question, I am assuming the map does not change. 
As part of belief space planning, I want to plan paths for the robot that take it from start to goal, but in a way that its localization accuracy is always maximized. Hence, I would have to sample possible states of the robot, without actually moving there, and the observations the robot would make if it were at those states, which together (correct me if I am wrong) form the 'belief' of the robot, subsequently encoding its localization uncertainty at those points. And then my planner would try to connect the nodes which give me the least uncertainty (covariance).
As my localization uncertainty for this camera-based robot depends entirely on things like how many feature points are visible from a given locations, the heading angle of the robot etc.: I need an estimate of how 'bad' my localization at a certain sample would be, to determine if I should discard it. To get there, how do I define the measurement model for this, would it be the camera's measurement model or would it be something relating to the position of the robot? How do I 'guess' my measurements beforehand, and how do I compute the covariance of the robot through those guessed measurements?
EDIT: The main reference for me is the idea of Rapidly exploring Random Belief Trees, which is an extension of the method Belief Road Maps. Another relevant paper uses RRBTs for constrained planning. In this paper, states are sampled similar to conventional RRTs, represented as vertices as a graph, but when the vertices are to be connected, the algorithm propagates the belief from the current vertex to the new, (PROPAGATE function in section V of 1), and here is where I am stuck: I don't fully understand how I can propagate the belief along an edge without actually traversing it and obtaining new measurements, thereby new covariances from the localization. The RRBT paper says ""the covariance prediction and cost expectation equations are implemented in the PROPAGATE function"": but if only the prediction is used, how does it know, say, whether there are enough features at the future position that could enhance/degrade the localization accuracy?
","computer-vision, planning, probability"
Kalman Filter GPS + IMU,"I know this probably has been asked a thousand times but I'm trying to integrate a GPS + Imu (which has a gyro, acc, and magnetometer) with an Extended kalman filter to get a better localization in my next step. I'm using a global frame of localization, mainly Latitude and Longitude. I kinda 'get' the kalman equations but I'm struggling in what should be my actual state and what should be my sensor prediction.
I have in one hand the latitude and longitude of my GPS and in euler degrees the roll, pitch and yaw of my IMU (which is already fused by some algorithm on board the chip I think) in euler degrees. I think I can throw the pitch and roll away.
And I know I have to show a function for my state $$ x_t =  f (x_{t-1}, \mu_{t-1}) $$ and a function that predicts what my sensor is seing at the step $t$
$$ \mu_{t} = h(x_t)$$
The thing is I dont know on what these functions should depend on, should my state be about the gps? In that case how can I predcit the next yaw read since I don't think I can get the rotation from a difference from gps location.
On the other side if my state is the yaw, I need some kind of speed, which the GPS is giving me, in that case would kalman work? Since I'm using the speed from the GPS to predict the next GPS location.

Long story short I dont know what my state and sensor prediction should be in this case.
Thanks In advance.
Edit: I have an ackerman steering mobile robot with no encoders which has mounted a GPS and and IMU (gyr acc and mag). The imu fuses thes values into euler degrees and the GPS gives me lat and longitude.
","localization, kalman-filter, imu, gps, magnetometer"
How can I compare robot and human hands?,"I want a comparison between human hands and robot hands, with respect to grasping and squeezing objects. 
How can I perform such a comparison, using sensors, as one would with a benchmarking database? Is there a standard, or ready-to-use, system?
","sensors, robotic-arm"
Addressing the sample impoverishment in particle filter,"I have implemented a particle filter algorithm for the state estimation of a mobile robot.
There are several external range sensors(transmitters) in the environment which gives information on the distance (radius) of the robot based on the time taken for the receiver on the robot to send back its acknowledgement. So, using three or more such transmitters it will be possible to triangulate the position of the robot.
The particle filter is initialized with 15000 particles and the sensor noise is relatively low (0.02m).
Update Phase: At each iteration a range information from an external sensor is received. This assigns higher weights to the particles along the radius of the external sensor. Not all the particles are equally weighted since the process noise is low. Hence in most of the cases, the particle relatively closer to the robot gets lower weight than an incorrect one that happens to be along the radius. The weight is a pdf.
Resampling Phase: At this stage, the lower weighted particle(the correct one) that has negligible weight gets lost because the higher weighted particle gets picked up.
All this happens at the first iteration and so when the range information from another sensor arrives, the robot is already kidnapped.
Googling around, said that this problem is called as sample impoverishment and the most common approach is to resample only when the particle variance is low. (Effective Sample Size < number of particles / 2)
But, when the particles are assigned negligible weights and there are relatively very few particles with higher weights, the diversity of the particles are lost at resampling phase. So, when the variance is higher resampling is done which removes the lower weighted particle and hence the diversity of the particles is lost. Isnt this completely the opposite of the above idea of ESS?
Is my understanding of sample impoverishment correct? Is there a way this issue can be fixed?
Any pointers or help would be highly appreciated.
","mobile-robot, localization, particle-filter, probability"
Motor Controller Configuration,"With a group of students we are building an exoskeleton for paraplegics. I am part of the Software & Control department and we are designing the motor controller configuration. We have several requirements:

Size is one of our main requirements. We want the Exoskeleton to be
as slim as possible, so this is true for all the components as well.
We want the motor controller to be as small as possible. 
The motor controller has to work with a brushless DC motor
A setpoint will be sent from a microcontroller to the motor
controller Two or three absolute encoders need to be connected,
this depends on the design of the joint. 
**We are either going to
use position or torque control 
We need one encoder for the motor
angle, and one for the joint angle. We might implement Series
Elastic Actuation in our joint and then we want to be able to
measure the deflection in the spring and thus need two encoders
for that.
A continuous power output in the range of 700 - 1200 Watt

While exploring several off-the-shelf possibilities, we came accross the  Elmo Gold Twitter and the IOMI Pro. One of the problems of these boards, is the amount of absolute encoders that can be connected. Both the Elmo and the IOMI board can either have 1 or no absolute encoders connected.
We came op with a solution, so that we are able to connect multiple encoders. In our exoskeleton we are going to use an EtherCAT Master-Slave configuration. The High-Level control (e.g. state-machine) is on the EtherCAT Master and sends joint angle setpoints. Our idea is to use a EtherCAT Slave as a sort of second controller, which gets the joint setpoint and the joint encoder values, calculates the motor setpoint and sends this out to a certain off-the-shelf controller like the Elmo Gold Twitter or the IOMI Pro. 

My question is the following: Is this even a good solution? And what are other solution to this problem? Are there even better alternatives for a motor controller? Might it be a better idea to build and program your own motor controller? (please bare in mind that we have limited experience in that area)
I thank you all in advance for your reply!
Cheers!
Nathan
","motor, microcontroller"
Orthogonal projection of laserscanner data,"I recently discovered this ROS-package: http://wiki.ros.org/laser_ortho_projector .
Which is basically exactly what I need. However I am not using ROS, so I need to do what is been done in this package myself.
Basically the information I have is the range measurement r and the angle theta for every measurement point of a 360 degree laserscan + I have the orientation in roll, pitch, yaw angles of the laserscanner. However yaw is not important for me and could be ignored.
I really can't get my head around how to project those points to the ground plane. I mean it is easy for the measurement point which align with the roll and pitch axes, but I don't know what to do with the points in between.
One solution I thought of is this:

Convert the measurement point (r, theta) in cartesian coordinates (x,y,z) - vector
Use rotations matrices: create rotation matrix for rotation around roll axis with roll angle, and adequately for the pitch axis. Multiplay bot matrices and then multiply it with (x,y,z) - vector.
Now the orthogonal projection of the of the measurement would be the (x,y,z) - vector with z=0.
Convert (x,y) - vector back to polar coordinates (r, theta).

However, especially step 2 is very complicated, because the rotation matrices change according to the sign of the roll and pitch angles, right?
I would like to note that the absolute value of role and pitch angles will always be < 90°, so there should not be an unambiguity with rotations..
Is there an easier (or maybe more elegant) way to solve my problem?
My guess is that this problem must have been solved basically for every robot application which uses a 2D-laserscanner that is not fixed to one axis. 
But I can not find the solution anywhere.
So I would be very glad if anyone of you could point me in the right direction.
","quadcopter, slam"
What sensors and algorithms are used in Digital Pen for tracking hand writing?,"Could anyone tell what are the sensors used in a digital pen which or specifically Equil smart pen and smart marker which can track hand writings. Is it MEMS based??If Yes, Is it MEMS accelerometer only or a combination of MEMS sensors like gyroscope and magnetometer and acceleometer? What algorithms are used here?  
","accelerometer, algorithm, gyroscope"
Harmonic drives in hobby projects,"After a fair bit of research into industrial robot arms I've learned that a lot of them use components like harmonic drives and sometimes cycloidal gearboxes. 
Two main questions:

How feasible are these for use in a hobby project? I do have several thousand dollars at my disposable to build a 6 DOF arm. Arm goals at a glance: 1m total reach, 5kg payload, reasonable speed.
What sources would you recommend? Online the sites seem very specialized, gotta go through sales reps, etc, which kind of worries me regarding the prices lol. Plus for Harmonic Drives I only found one vendor.

Side/related question: would buying from China work out alright or would I not get what I'm looking for (namely: good torque at <1 arcmin backlash)? E.g. 
Another example.
--
I have little experience with this technology (precision eccentric gearing), but would definitely like to explore it and purchase test equipment to construct a basic arm. Thanks for any help, looking for a decent starting point.
EDIT -- I would like to use one of these special gearboxes with a NEMA stepper motor if possible (and if it saves cost) since we have a bunch of those lying around, heh.
EDIT2 -- Curious if this item would actually have some sort of strain wave gearing inside of it.
If so this seems very affordable and could provide decent precision on part of a robot arm.
","motor, robotic-arm, gearing"
Intelligent robotic arm and its control system,"As I know, most of robotic arms are specific-purpose and usually work under supervision of an expert, such as surgery robot. So, is this relevant for a robotic arm to be intelligent and autonomous? If so, how its control system may be different from non-intelligent ones?
","control, robotic-arm, artificial-intelligence"
"Generate synthetic accelerometer data based on (x,y,z) coordinate","I would like to create a simulation model (basically a signal generator) which will allow me to generate the 3 output signals of an accelerometer based on 3 location input signals (x,y and z). I would like a more realistic model of the data produced by an accelerometer (with some noise and bias offsets).
How can I convert the series of points into a simulated accelerometer output?
Specifically:
I have a series of positions which describe a trajectory in 3D space...If an accelerometer was moving along the trajectory described by the series of positions, I am interested in knowing (simulating!) the data that the accelerometer would produce as the result of moving along the described trajectory. 
I could just calculate the 2nd derivative of the trajectory, but that would probably be too ideal. I am looking for a model which is more realistic. 
","accelerometer, simulation"
Does simulink accepts robotics toolbox in matlab?,"I'm trying to make a model for a robotic arm on simulink and then excute it on arduino controller but i want also to use robotics toolbox to calculate forward and inverse kinematics so i need to add matlab function in my simulink model i tried it but it keeps giving errors but when i try it away from simulink it gives rsults ! is that because robotics toolbox works only with matlab but not with simulink? 

the code in matlab function
 function [theta1, theta2, theta3, theta4] = fcn(x, z, y)


L(1)=Link([0,0.03,0,pi/2,0]);
L(2)=Link([0,0,0.12,0,0]);
L(3)=Link([0,0,0.1,-pi/2,0]);
L(4)=Link([0,0,0,0,0]);
robot = SerialLink(L,'name','robot');

robot.tool=transl(0,0,.08);
robot.base=transl(0,0,.07);


    T=transl(x,y,z);
    q=robot.ikine(T,[0,0,0,0], [1 1 1 0 0 1]);
    q(1)=fix(q(1)*(180/pi));
   q(2)=fix(q(2)*(180/pi));
    q(3)=fix(q(3)*(180/pi));
    q(4)=fix(q(4)*(180/pi));

          theta1=q(1)
    theta2=q(2)
    theta3=q(3)
    theta4=q(4)


but when i try it in command window away from simulink it gives results for x=1 y=1 z=1 and higher values
   Warning: solution diverging at step 245, try reducing alpha 
 > In SerialLink/ikine (line 260) 
  Warning: solution diverging at step 459, try reducing alpha 
 > In SerialLink/ikine (line 260) 
 Warning: solution diverging at step 533, try reducing alpha 
 > In SerialLink/ikine (line 260) 
 Warning: ikine: iteration limit 1000 exceeded (row 1), final err 1.790820 
 > In SerialLink/ikine (line 179) 

 theta1 =

  -21996


theta2 =

-1050694


theta3 =

 2711696


theta4 =

 2848957

but for x=0.2  y=0.2 z=0.2 it gives that error
      .
      .
      .
      .
> In SerialLink/ikine (line 260) 
Warning: solution diverging at step 567, try reducing alpha 
> In SerialLink/ikine (line 260) 
Warning: solution diverging at step 570, try reducing alpha 
> In SerialLink/ikine (line 260) 
Warning: solution diverging at step 574, try reducing alpha 
> In SerialLink/ikine (line 260) 
Warning: solution diverging at step 578, try reducing alpha 
> In SerialLink/ikine (line 260) 
Error using tr2angvec (line 97)
matrix not orthonormal rotation matrix

Error in SerialLink/ikine (line 191)
        [th,n] = tr2angvec(Rq'*t2r(T));

","robotic-arm, matlab, simulator"
"Connected battery, esc and motor does not power up the arduino","I am currently doing my first arduino project and i am having trouble finishing it.
I have a 3s lipo battery connected with an esc (120A) that is connected to a motor (270KV). From the esc i am connecting two jumper cables that goes to GND and pin 9.
I do not have a jumper cable on the red wire from the ESC.
This is how it looks:

Below you can see a link to a sketch that I found online. The only difference compared to my schematic is that I have an Arduino Uno.

When I insert the battery and switch the ESC to ""on"" the ESC starts up correctly and the fan starts to go. But the arduino does not get any power. It is still ""OFF"".
I also noticed that my motor has 4 cables. 3 ""bigger"" cables that goes to the ESC's 3 big cables. And then a 4th one hanging loose right now because i do not quite know what to do with it. I also noticed there is a hole in the ESC where I can insert this. The hole has 6 ""inputs"" however where as the loose wire from the motor has 5. Therefor I am a bit concerned if that should be connected there or not.
So to summarize, the problem is that the Arduino does not turn ""ON"" with my current schematic. 
Any help, tips is very appreciated!!
","arduino, motor, esc"
Illegal position data with Mitsubishi Melfa RV-2F-Q robotic arm,"I have a project where I have to move a Mitsubishi Melfa RV-2F-Q robot to a position/orientation from an external source, so there are no pre-defined points available.
The problem I keep running into is that even if I give it a reachable position (within operating range), or even a position that is almost the same as its current one, it fails to move there with the error: 
L2802 Illegal position data (dstn)
What causes this and how do I avoid it?
","robotic-arm, errors"
Paradox: I can't use accelerometer measurements to obtain information about my states in a quadcopter?,"I'm currently developing an EKF to estimate the position and orientation of a quadcopter. My state vector is comprised of 3D position, 3D velocity, 3 euler angles and the angular velocity vector. 
Right now I'm looking into the measurement equation for the accelerometer.
If I understood correctly, an accelerometer measures ""proper acceleration"", instead of coordinate acceleration, that is, it measures the acceleration of the body w.r.t to a free-falling coordinate system.
If this is the case, and supposing the only forces acting on the body are the upward thrust given by the propellers, $\vec{T}$, and earth's gravitational force, $m\vec{g}$, then I understand that the only acceleration that would be measured by the accelerometer is the one caused by $\vec{T}$ (since the free-falling frame has no way of measuring the acceleration caused by $m\vec{g}$, because it is also being accelerated by it).
If this also the case, then I note that the vector $\vec{T}$, when expressed in the body coordinate frame (i.e. a coordinate frame fixed at the center of mass of the body, and always aligned with the body's orientation) does not depend on any of the states whatsoever. For example, if the propellers are assembled such that $\vec{T}$ is always perpendicular to the plane where the propellers are, then $\vec{T}$ in the body frame is specified as $(0,0,\alpha)^T$, where $\alpha$ is the magnitude of the thrust given. 
Which leads me to conclude that (since the measured acceleration doesn't depend on the states) I can't use accelerometer measurements to obtain more information about any of my states (??). This conclusion seems paradoxical to me, and that's why I ask this here. Could someone please point the mistake in my reasoning, or elucidate why this is not a paradox?
","quadcopter, kalman-filter, accelerometer, ekf, orientation"
Issue with open loop gain computation,"
Is (G1(z)*C1(z))+(G2(z)*C2(z))+(G3(z)*C3(z))  the right way of computing open loop gain for the attached block diagram. The system order differs from the order achieved using series(G(z),C(z)). Could any one help?
",control
Issue with series command in MATLAB,"I have a controller and a plant in series. The controller is 3 input 3 output MIMO system and the plant is also 3 input 3 output system. The bode of the open loop gain, i.e., 
$$D(z)=C(z)*G(z)$$
appears to be different when using 
$$series(C(z),G(z))$$ versus 
$$
D(z) = C_1(z)G_1(z) + C_2(z)G_2(z) + C_3(z)G_3(z) \\
$$
Theoretically, I believe both are same. However, the latter method gives a different bode with undamped peak, unlike the one using the series command.
The approach using 
$
D(z) = C_1(z)G_1(z) + C_2(z)G_2(z) + C_3(z)G_3(z) 
$
is most suitable, according to me, as it gives more clarity. 
Can anyone share idea on why this misbehaviour exists?
","control, matlab"
EtherCAT slave sensor node,"We are currently working on a project where we want to use EtherCAT as a communication protocol between a central system (master) and several nodes (slaves). We want these slaves to have the following:

GPIO for sensors
Local processing (microprocessor ~200MHz+)
Programmable (e.g. through USB)
EtherCAT connection

We've looked a lot at off-the-shelf solutions and came to several EtherCAT modules which can handle the communication, such as the EasyCAT PRO, Anybus M40 and BECKHOFF F1111. We could connect these modules to an off-the-shelf microprocessor (e.g. Beaglebone)
However, we are also looking into an integrated, more powerful solution because we do not believe this can handle everything we want to do. The TI AM3359 ICE suits our purposes and we have bought and tested it. However we were wondering if there are smaller, off-the-shelf solutions, since this one has a lot of stuff that we do not need (e.g. screen and CAN connection) and it requires making our own PCB. 
This is therefore my question. Do you guys know of any EtherCAT slave sensor node that can facilitate our needs and nothing more? We have been looking a lot but cannot find anything of this kind.
","sensors, communication"
What is the difference between DC motor with encoder and DC with out encoder?,"My question is: what is the difference between DC motor with encoder and DC without encoder? As long as I can control the speed of DC motor using PWM, for example on the Arduino, what is the fundamental difference?
",motor
Understanding and implementing belief space planning,"I am currently working on state estimation/navigation for a system with multiple robots. As of now, what I have is each robot localizing itself with a Kalman filter, given vision based measurements. As next steps, I am aiming to do two things:

Extend this filtering framework to span over all robots so that they can cooperate and improve each other's localization
Along with the above, construct a path planning framework such that they can navigate in a way that their localization accuracy is always maximized, thereby eliminating the problem of losing position etc.

To this end, I've been reading about multi-robot state estimation and planning strategies, and have come across belief space planning: or planning under uncertainty. While the math intuitively makes sense, I am having issues with how to implement these techniques in my real world scenario, especially for multiple robots. I have experience using algorithms such as EKF, UKF etc., and sampling based planning strategies like PRM/RRT, but I am having trouble with the probabilistic link between these two.
So far, I've been looking into research papers, but as someone who's mainly a programmer, I'm trying find something more approachable that will help me link the (somewhat abstract) math to my specific problem: for instance, helping me define terms such as 'joint belief of the entire group', using the data I have in hand. What are my best options, and are there any better resources I can consult?
","localization, planning, multi-agent, probability"
Can I create an autonomous robot that can adapt its strategy using machine learning?,"BACKGROUND:
I am creating a robot to score the most points in 30 seconds while running autonomously. Naturally, two thoughts come to mind: Linear programming and machine learning.
Linear programming would provide a stable, simple method of scoring points. However, it is limited by what it can do, and optimizing scores would require reworking the entire bot.
PROBLEMS:
The robot and code itself can change, but it is not time efficient due to the fact that it would require completely reworking the code and robot by a single programmer. The robot has to work with what its got.
QUESTION:
Can I create Android Java classes to allow my robot to work the field as an AI and tweak its strategy or course of action based on stats from previous rounds? (Self-Supervised Learning) If it is possible to do, how would I do it?
SPECS:
Max robot size of 18'' cubes
Android Marshmallow
ZTE speed or Motorola moto g gen 2 phones
Multiple inputs from a controlling phone and various sensors
Output to multiple motors and servos
","machine-learning, automatic"
UR3 TCP/IP communication protocol,"I'm currently programming a C socket for communicating via (Ethernet) TCP/IP with an UR3 robot which control software is version 3.3. I'm able to get raw data from the ports 30001,30002, and 30003. I can deserialize the data thanks to the source of a ROS driver. But I wish to have more information about deserialization, and Universal Robots seems to not provide such information. Do anybody knows where to get info about the TCP packets that the robot sends?
","robotic-arm, communication, c"
ElasticReconstruction (subroutine FragmentOptimizer) runtime error: *CHOLMOD warning: matrix not positive definite* with invalid output,"Recently I was trying to run code (ElasticReconstruction) of this paper to learn offline 3D reconstruction: 

Robust Reconstruction of Indoor Scenes, CVPR 2015, Sungjoon Choi,
  Qian-Yi Zhou, and Vladlen Koltun 

//project homepage here: http://redwood-data.org/indoor/index.html
I've tried it's pre-built toolchains and also built from source code on my own. When testing the algorithm using synthetic mock oni data, both the pre-built and my build get stuck in running FragmentOptimizer with the following error:
$ $numpcds
bash: 7: command not found  
    ##↑ only 7 fragments generated since my circlular scanning contains only 361 frames

zhangxaochen@zhangxaochen-PC /cygdrive/d/Users/zhangxaochen/Documents/indoor-executables-1.1/bin
$ ./FragmentOptimizer.exe --slac --rgbdslam ../sandbox/init.log --registration
../sandbox/reg_output.log --dir ../sandbox/pcds/ --num $numpcds --resolution 12
 --iteration 10 --length 4.0 --write_xyzn_sample 10

Processing 4 :     4 ... Done.
Data error score is : 1.#R
CHOLMOD warning: matrix not positive definite
Regularization error score is : 1.#R
......

After reading the --help info, I noticed that my synthetic test data is so small that a smaller blacklistpair threshold should be assigned (e.g. 3000), rather than the default value 10000. However there is still no luck. 1.#R and 1.#QNAN000 still occurs if I use --slac as the cmdline parameter, yet if I change it to --rigid, output of FragmentOptimizer seems to be valid(not sure) while the latter step Integrate fails to create a consistent 3D model of the whole scene.
Idk if someone has succeeded in running this code. I've uploaded a package of my input and output here: https://drive.google.com/file/d/0B4vahSr3aGadM2dWaWRZS0NyNDg/view?usp=sharing
Anyone who would check or test it will be much appreciated!!
PS: FYI, I've seen a recently opened issue here: https://github.com/qianyizh/ElasticReconstruction/issues/7 and followed it up, with no progress yet...
","slam, errors, 3d-reconstruction"
Integrating GPS into Graph SLAM (how orientation fixed?),"I'm working on Graph SLAM to estimate robot poses (x, y, z, roll, pitch, yaw). Now I want to integrate GPS measurement (x, y, z, of course no angles).
I implemented GPS as pose's prior. But I have a problem.

Position(x, y, z) is perfectly corrected by graph optimization
But orientaiton(roll, pitch, yaw) is very unpredictable(unstable) after optization.

i.e. It looks like position is fitted by the sacrifice of orientation.
I'm very confused about what's the right way of integrating GPS into graph SLAM. GPS should be handled as prior? or landmark? or one of pose-vertices?
...Thanks for your help in advance.
PS
I use g2o as a graph-optimization library. In g2o, I implemented GPS measurement with EdgeSE3_Prior. GPS's quality is RTK so it's enough precise.
","localization, slam, ros, gps, pose"
I found the matrices but can't find the inverse kinematics angles,"I need to find inverse and forward kinematics of Mitsubishi RV-M2 as a homewrok. I found the forward kinematics part. But I got stuck in inverse kinematics.
Teacher said we can think that wrist joints is not moving.(To make equations easier I guess.) This is why I thought tetha4(in T4) and tetha5(in T5) should be 0.
Here is my MATLABcode
syms t1 t2 t3 d1 a1 a2 a3 d5 px py pz r1 r2 r3 r4 r5 r6 r7 r8 r9 % sybolic equations

T1=[cos(t1)     -sin(t1)        0       0;
    sin(t1)     cos(t1)         0       0;
    0            0              1       d1;
    0            0              0       1;];

T2=[cos(t2)     -sin(t2)        0       a1; 
    0            0              -1      0; 
    sin(t2)     cos(t2)         0       0;
    0            0              0       1;];

T3=[cos(t3)     -sin(t3)        0       a2;
    sin(t3)     cos(t3)         0       0;
    0             0             1       0;
    0             0             0       1;];

T4= [0           -1             0       a3;   
     1             0             0       0;
     0             0             1       0;
     0             0             0       1;];

T5=[1             0             0       0; 
    0             0            -1      -d5; 
    0             1             0       0;
    0             0             0       1;];

Tg= [r1 r2 r3 px;
    r4 r5 r6 py;
    r7 r8 r9 pz;
    0 0 0 1;];

left= inv(T1)* Tg;
left=left(1:4,4);
left=simplify(left)
right= T2*T3*T4*T5;
right=right(1:4,4);
right=simplify(right)

This gives us

I find t1 using this and results are matching with forward kinematics equations.  But I couldn't find t2 and t3. How can  I do that? Is there a formula or somethig?
",kinematics
Forward Kinematics od Differential Robot,"r = 1;
w1 = 4;
w2 = 2;
l = 1;


R = [0 -1 0; 1 0 0; 0 0 1];
X = [(r*w1)/2 + (r*w2)/2; 0 ; (r*w1)/(2*l) - (r*w2)/(2*l)];

A = R*X;
disp(A)

I am getting the solution for the matrix as [0; 3; 1] which is exactly what I expect. I would like to input a series of w1 and w2, Lets say I have a data file 1.xlsx and 2.xlsx with ten values each. I want to load 1.xlsx into w1 and 2.xlsx into w2 and get ten answers for X. How can I do that?
","matlab, forward-kinematics, differential-drive"
Can I use a FTC regulation phone to use the camera on multiple tasks?,"PREFACE:

THIS QUESTION REGARDS FTC ROBOTICS
I am aware that a question similar to this exists on SE and I have looked at it, but the solutions provided did not solve my problem, and the nature of my question has additional constraints and criteria involved that make this a distinct problem.

PROBLEM:

I have been put in charge of creating java classes to add to an existing app, used to control a robot (FTC robotics, for those who are wondering). The features I have been given to add to the app include:

Shape detection (provided by FTC, all I need to do is tweak some
files)

Automatically record and save a video in the background of the app using the back camera


QUESTION:

I was wondering, if this is even possible, how I would go about both recording video and using shape detection? How do I even record a video from the background of an app? If i cannot achieve both I was told to prioritize recording, as we have substitutes for shape detection.

SPECS:

Motorola moto g 2nd gen with Android Marshmallow

Or as a fallback:

ZTE speed with Android Marshmallow
","wheeled-robot, computer-vision, cameras"
"""Updated"" Help in iRobot create serial communication in Arduino","I am programming an iRobot Create to follow serial commands using Arduino Uno. I have written the library, and found the serial commands to move the robot forward in the iRobot manual, but I couldn't find the bytes for other movements (backward, right and left).
Could you please help me with this. How can I move the robot backward, right and left. I will upload my code library.
#include ""iRobot.h""

#if defined(ARDUINO) && ARDUINO >= 100  //to check if the arduino is plugged and the its number is above 100

#include ""Arduino.h""

#include ""SoftwareSerial.h""  // so we can use all pins
SoftwareSerial softSerial = SoftwareSerial(10, 11);

#endif


iRobot::iRobot() //constructor to set the pins
{
_rxPin = 10;
_txPin = 11;
}


void iRobot::begin()  //needs to be called inside setup function
{

delay(2000); // Needed to initialize the iRobot, the delay is to ensure that each command before this is excucted or there will be overlap

// define pin modes for software tx, rx pins for iRobot
pinMode(_rxPin, INPUT);
pinMode(_txPin, OUTPUT);


softSerial.begin(19200); //we set the data rate received by the irobot

Serial.begin(19200); // set the data rate sent from the arduino

//these two line are necessary from the irobot manual
softSerial.write(128); // This command starts the communication.
softSerial.write(131); // set mode to safe, it will stop of there is a cliff or a wheel drops or

Serial.write(""Enter Command: ""); // here, if we start serial monitor, we can enter the command
}

void iRobot::runIt()   //needs to be called inside loop function
{
if (Serial.available())
{
String data = String(Serial.read()); //this will read the command, each word will call a function

if(data == ""forward"")
  goForward();
if(data == ""backward"")
  goBackward();
if(data == ""left"")
  goLeft();
if(data == ""right"")
  goRight();
}
}


void iRobot::goForward()
{
softSerial.write(137); // Opcode number for DRIVE, it's understood by the irobot that 137 means drive

// Velocity (-500 – 500 mm/s)
softSerial.write((byte)0);
softSerial.write((byte)200);


//Radius (-2000 - 2000 mm)
softSerial.write((byte)128); // we should adjust this to make the robot go straight or slightly right or left
softSerial.write((byte)0); // we should adjust this to make the robot go straight or slightly right or left
}

void iRobot::goBackward()
{
softSerial.write(137);
//we should change the bytes to make the robot drive backward
//negative vaule of velocity drive the robot forward
// Velocity (-500 – 500 mm/s)
softSerial.write((byte)0);
softSerial.write((byte)200);


//Radius (-2000 - 2000 mm)
softSerial.write((byte)128); // we should adjust this to make the robot go straight or slightly right or left
softSerial.write((byte)0); // we should adjust this to make the robot go straight or slightly right or left

}

void iRobot::goLeft()
{
softSerial.write(137);
 //we should change the bytes to make the robot drive left
 //radius value should be positive
// Velocity (-500 – 500 mm/s)
softSerial.write((byte)0);
softSerial.write((byte)200);


//Radius (-2000 - 2000 mm)
softSerial.write((byte)128); // we should adjust this to make the robot go straight or slightly right or left
softSerial.write((byte)0); // we should adjust this to make the robot go straight or slightly right or left
 }

 void iRobot::goRight() 
 {
softSerial.write(137);
 //we should change the bytes to make the robot drive right
 //radius value should be negative
// Velocity (-500 – 500 mm/s)
softSerial.write((byte)0);
softSerial.write((byte)200);


//Radius (-2000 - 2000 mm)
softSerial.write((byte)128); // we should adjust this to make the robot go straight or slightly right or left
softSerial.write((byte)0); // we should adjust this to make the robot go straight or slightly right or left
}

Update:
I have connected the robot to the Arduino and tried the code. Unfortunately, the robot didn't move
This is my Arduino code:
 #include <Arduino.h>
 #include <iRobot.h>
 iRobot irobot;

 void setup() {
  irobot.begin();
 }  

 void loop() {
  irobot.runIt();
 }

I have connected the the pins 10, 11 and GND on the Arduino to pins 3, 4 and 7 on the robot.


","arduino, irobot-create, serial"
Error while running rosserial node in ROS,"I have an Arduino connected to ROS through serial port. I wrote an ardiuno code to drive motors. below is my code.
//Library to communicate with I2C devices
    #include ""Wire.h""
    #include <Messenger.h>
    //Contain definition of maximum limits of various data type
    #include <limits.h>
     //Messenger object
    Messenger Messenger_Handler = Messenger();

    //////////////////////////////////////////////////////////////////////////////////////
    //Motor Pin definition
    //Left Motor
    #define USE_USBCOM
    #define INA_1 7
    #define INB_1 12    
      //PWM 1 pin 
    #define PWM_1 5

    //Right Motor
    #define INA_2 11 
    #define INB_2 10

    //PWM 2 pin 
    #define PWM_2 6
    #define RESET_PIN 4

    ///////////////////////////////////////////////////////////////////////////////////////
    //Motor speed from PC
    //Motor left and right speed
    float motor_left_speed = 0;
    float motor_right_speed = 0;
    /////////////////////////////////////////////////////////////////

    //Setup serial, motors and Reset functions
    void setup()
    {

      //Init Serial port with 115200 baud rate
      Serial.begin(57600);  

      //Setup Motors
      SetupMotors();
       SetupReset();
      //Set up Messenger 
      Messenger_Handler.attach(OnMssageCompleted);
       }
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Setup Motors() function

    void SetupMotors()
    {

     //Left motor
     pinMode(INA_1,OUTPUT);
     pinMode(INB_1,OUTPUT); 

     //Right Motor
     pinMode(INA_2,OUTPUT);
     pinMode(INB_2,OUTPUT);  

    }

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Setup Reset() function

    void SetupReset()

    {

      pinMode(RESET_PIN,OUTPUT);

      ///Conenect RESET Pins to the RESET pin of launchpad,its the 16th PIN
      digitalWrite(RESET_PIN,HIGH);


    }
    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //MAIN LOOP

    void loop()
    {

        //Read from Serial port
        Read_From_Serial();


        //Update motor values with corresponding speed and send speed values through serial port
        Update_Motors();

    delay(1000);
      }
 ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Read from Serial Function

    void Read_From_Serial()

    {
       while(Serial.available() > 0)
        {

           int data = Serial.read();
           Messenger_Handler.process(data);

        } 

    }
   ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //OnMssg Complete function definition

    void OnMssageCompleted()
    {

      char reset[] = ""r"";
      char set_speed[] = ""s"";

      if(Messenger_Handler.checkString(reset))
      {


         Reset();

      }
      if(Messenger_Handler.checkString(set_speed))
      {

         //This will set the speed
         Set_Speed();
         return; 


      }
    }
   //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Set speed
    void Set_Speed()
    {

      motor_left_speed = Messenger_Handler.readLong();
      motor_right_speed = Messenger_Handler.readLong();


    }
    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Reset function
    void Reset()
    {

      delay(1000);
      digitalWrite(RESET_PIN,LOW);

    }
   ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //Will update both motors
    void Update_Motors()
    {

      moveRightMotor(motor_right_speed);
      moveLeftMotor(motor_left_speed);

     }

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    //Motor running function


    void moveRightMotor(float rightServoValue)
    {
      if (rightServoValue>0)
      {

     digitalWrite(INA_1,HIGH);
     digitalWrite(INB_1,LOW);
     analogWrite(PWM_1,rightServoValue);

      }
      else if(rightServoValue<0)
      {
     digitalWrite(INA_1,LOW);
     digitalWrite(INB_1,HIGH);
     analogWrite(PWM_1,abs(rightServoValue));

      }

      else if(rightServoValue == 0)
      {
     digitalWrite(INA_1,HIGH);
     digitalWrite(INB_1,HIGH);


      }
    }
   ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    void moveLeftMotor(float leftServoValue)
    {
     if (leftServoValue > 0)
      {
    digitalWrite(INA_2,LOW);
    digitalWrite(INB_2,HIGH);
    analogWrite(PWM_2,leftServoValue);
      }
      else if(leftServoValue < 0)
      {
     digitalWrite(INA_2,HIGH);
     digitalWrite(INB_2,LOW);
     analogWrite(PWM_2,abs(leftServoValue));

      }
      else if(leftServoValue == 0)
      {

       digitalWrite(INA_2,HIGH);
       digitalWrite(INB_2,HIGH);

       }  

    }

when i load the code to arduino and start rosserial node using command 

rosrun rosserial_python serial_node.py /dev/ttyACM0

ROS throws below error

[ERROR] [WallTime: 1475949610.718804] Unable to sync with device;
  possible link problem or link software version mismatch such as hydro
  rosserial_python with groovy Arduino

I have tried changing the baud rate and fixing it similar in ROS and arduino but nothing helped
","mobile-robot, arduino, ros, first-robotics"
Simulation software for KUKA LBR iiwa robot?,"I have been working with KUKA LBR iiwa 7 R800 robot, with the KUKA's IDE, which is the 'Sunrise.Workbench'. Since it does not have any virtual platform to verify the code (simulate), it's been quite difficult, as I need to test each code by deploying to the robot.
Can anyone suggest if there is any simulation software available where I can test the code written using the Robotics API in Sunrise.Workbench?
I came across V-REP simulation software, but, not sure if I can use my code in the workbench platform.
Appreciate if anyone can shed some light on it.
Thanks in advance.
","robotic-arm, simulation"
Hand Eye Calibration,"I'm trying to use a dual quaternion Hand Eye Calibration Algorithm Header and Implementation, and I'm getting values that are way off. I'm using a robot arm and an optical tracker, aka camera, plus a fiducial attached to the end effector. In my case the camera is not on the hand, but instead sitting off to the side looking at the arm.
The transforms I have are:

Robot Base -> End Effector
Optical Tracker Base -> Fiducial

The transform I need is:

Fiducial -> End Effector


I'm moving the arm to a series of 36 points on a path (blue line), and near each point I'm taking a position (xyz) and orientation (angle axis with theta magnitude) of Camera->Fiducial and Base->EndEffector, and putting them in the vectors required by the HandEyeCalibration Algorithm. I also make sure to vary the orientation by about +-30 degrees or so in roll pitch yaw.
I then run estimateHandEyeScrew, and I get the following results, and as you can see the position is off by an order of magnitude. 
[-0.0583, 0.0387, -0.0373] Real
[-0.185, -0.404, -0.59] Estimated with HandEyeCalib
Here is the full transforms and debug output:
# INFO: Before refinement: H_12 =
-0.443021 -0.223478  -0.86821  0.321341
 0.856051 -0.393099 -0.335633  0.470857
-0.266286 -0.891925   0.36546   2.07762
        0         0         0         1
Ceres Solver Report: Iterations: 140, Initial cost: 2.128370e+03, Final cost: 6.715033e+00, Termination: FUNCTION_TOLERANCE.
# INFO: After refinement: H_12 =
  0.896005   0.154992  -0.416117  -0.185496
 -0.436281    0.13281  -0.889955  -0.404254
-0.0826716   0.978948   0.186618  -0.590227
         0          0          0          1


expected RobotTipToFiducial (simulation only):   0.168   -0.861    0.481  -0.0583
expected RobotTipToFiducial (simulation only):   0.461   -0.362    -0.81   0.0387
expected RobotTipToFiducial (simulation only):   0.871    0.358    0.336  -0.0373
expected RobotTipToFiducial (simulation only):       0        0        0        1


estimated RobotTipToFiducial:   0.896    0.155   -0.416   -0.185
estimated RobotTipToFiducial:  -0.436    0.133    -0.89   -0.404
estimated RobotTipToFiducial: -0.0827    0.979    0.187    -0.59
estimated RobotTipToFiducial:       0        0        0        1

Am I perhaps using it in the wrong way? Is there any advice you can give?
","robotic-arm, stereo-vision, calibration"
"Implementation of the Control of a Quadrotor, as taught in Aerial Robotics in Coursera","I completed the course of Aerial Robotics in Coursera and I want to implement what I learned in a real quadrotor. The thing is that when I see the equations given like this:

For the sake of the argument let's assume I have implemented the PD Controllers and every moment I find u1 (the sum of the forces applied to the quadrotor) and u2 (the sum of the moments applied to the quadrotor). I then ask myself: How can I find what force and moment should specifically each one of the motors produce? And here I am stuck as I can't find an answer.Could anyone help?  
","quadcopter, control, motor, microcontroller"
Stereo Camera calibration with different camera types,"I'm trying to perform stereo camera calibration, rectification and disparity map generation. It's working fine with normal sample data. However, I'm trying to use the dual cameras on an iPhone 7+, which have different zoom. The telephoto lens has 2X zoom compared to the wide angle camera. I ran the images through the algorithm, and it is succeeding, although with a high error rate. However, when I open up the rectified images, they have a weird spherical look to the edges. The center looks fine. I'm assuming this is due to the cameras having different zoom levels. Is there anything special I need to do to deal with this? Or do I just need to crop any output to the usable undistorted area? Here is what I'm seeing:

EDIT:
I tried using the calibration result from these checkerboard images to rectify an image of some objects, and the rectification was way off, not even close. If I rectify one of my checkerboard images, they are spot on. Any ideas why that happens?
These are what my input images look like that result in the spherical looking output image. They are both taken from the exact same position, the iPhone was mounted to a tripod and I used a bluetooth device to trigger the shutter so the image wouldn't get shaken, my code automatically takes one image with each lens. I took 19 such images from different angles, all images show the full checkerboard. The more zoomed in image is the one that rectified to the top spherical looking image.
This is the code I am running. I compiled it and ran it using the sample images, that worked fine. Ran it with my own images and here I am.  https://github.com/sourishg/stereo-calibration


I might just need to crop the result to a certain area. Regardless, it doesn't seem to work cropping or not when I use a picture I took of normal objects. Here is the output of an image of normal objects I ran through the filter:


","cameras, stereo-vision, opencv"
Ornstein Uhlenbeck vs Epsilon Greedy,"There are many methods of exploring in a Reinforcement Learning setting but two of the most used ones are Ornstein Uhlenbeck (OU) processes and epsilon-greedy approaches. Could anyone elucidate the major advantages/disadvantages of using one over the other? 
One of the things associated with OU processes is that you need to two additional parameters to bias exploration which might mean additional tuning. I'd be glad if someone could help!
",reinforcement-learning
Line follower for coloured lines,"I am building a line-following robot with a Raspberry Pi Zero, using the explorer PHat.
The robot is supposed to follow black, red, green and blue lines and react to the colour, so it should drive faster on a red line and slower on a blue line.
I do not have much experience with line followers, so I am not sure what kind of hardware I need.
My questions are:

Is it possible to follow a red, blue or green line with IR LEDs? Most of the line followers obviously use IR LEDs (like TCRT5000), but they are supposed to only follow black lines.
I have a RGB sensor which works quite well with the explorer PHat and I am able to recognize colours very accurately. Is it possible to use this single sensor as a line follower? As the robot should be able to drive on a curvy course a single sensor is probably not enough? 

","raspberry-pi, line-following"
On-site waste-sorting robot feasibility and availability,"I am thinking about a project at my university for doing on-site waste sorting. The problem with having one waste bin for recyclables, compost and landfill and doing the sorting at a facility, is that the organic materials can destroy paper and other recyclable materials. 
I have searched quite a bit but all of the robotic solutions available that I have found are for facilities. I am looking for a robotic bin to be deployed in replacement of the traditional waste and recycling bin.
The budget is approximately $1000.

$1000 -- is that materials cost only, or does it include assembly and
maintenance costs?: materials and assembly not maintenance for all 4 bins plus robotic sorter
it must be bin-sized (whatever that means): let's say 3 ft (height) x
2 ft (length) x 2 ft (width) per bin and there are 4 bins -- recycle
paper, recycle plastic, compost, landfill
Do you have weight requirements so the consumer can move the bin to
the curb, or are you planning to have the robot separate the
materials into other, mobile, bins?: There are no weight
requirements. We should be able to use a forklift to move it. What
would be nice is to have a single waste entry hole which customers
use. The device should internally sort the waste into the 4 bins
listed above. The entry hole and sorter should be 1 ft (height) x 8
ft (length) x 2 ft (width) to fit directly on top of the set of 4
adjacent bins.
What power is available?: It can be plugged into a wall outlet (in
case there is one nearby) but should also be able to use a
rechargeable battery (in case there isn't).
What about environmental concerns, especially if this is to be
located outside? Don't forget noise constraints and safety concerns.:
The whole point of this is to reduce waste and help the environment.
Assume the noise it can make can be as loud as a heater, AC unit, or
fan. The entire system should be one box with one waste entry hole --
the rest should be blackbox-ed, so it should be safe.
And, most importantly, what characteristics of the materials are you
planning to use for doing the actual sorting?: the shorter should be
able to detect pure recyclable plastic vs recyclable paper vs
organic/food material vs pure waste using either computer vision or
chemical techniques or both.
What size requirements are there for the products themselves?: The
waste whole should be .75 ft x .75 ft so assume the waste is less
than .75 ft^3
For example, how to detect organic garbage from non-organic (and do
it many times a day without human intervention to ""resupply
chemicals"") can be a topic of research that could take a couple of
years itself: Yes this is a good point. However, my question is more
focused on whether it's possible to use the robots already
commercially available today to solve this problem.

I read through how to ask but so here is my specific question: Is there a commercially available robot today that does or can be retro-fitted to do this on-site waste sorting?
","computer-vision, industrial-robot"
CNC: Linear motion without timing belt or lead screw,"Is it possible to build a CNC whose Linear motion system does not contain any timing belt(pulley) or lead screw(threaded rod).
I was wondering whether I could directly control the Linear motion by securing wheels of a slider onto aluminum rails & directly connecting the wheels to a stepper motor.
The main objective of this question is to find the cheapest method for controlled Linear motion.
","cnc, stepper-driver, linear-bearing"
How to implement a particle filter when sensors can't identify landmarks?,"I'm attempting to build a robot that leverages a particle filter to identify where it is relative to a map that is known. The robot only has IR sensors, so while it is able to determine its distance from landmarks, it does not know what landmark it is ""looking"" at.
I'm following this very helpful book to build my particle filter. In incorporating the sensor measurements, it is assumed that you know both the distance to a landmark and which specific landmark you are looking at. What would need to be done if you know the map and distance measurements, but not the specific landmark that you're observing? Would this require SLAM? Or could you simply increase the probability for particles that are about that distance from a landmark?
","slam, wheeled-robot, kalman-filter, particle-filter, two-wheeled"
Inverse kinematics,"I'm developing a 2 DOF SCARA with inverse kinematics. It works fine at any desired point, but how can I draw a line? Is there an efficient algorithm to do this?
",inverse-kinematics
Sensor for tracking relative position of human,"I am making a robot that needs to continuously track the relative position of a human, up to 15 meters away and with at least 300 degrees coverage. Currently I am using a Hitechnic IRSeeker v2 sensor and made a beacon wristband with 6 TV remote IR LEDs. But the maximum distance I can get is around 3 meters.
I ordered some 3 watts IR LEDs to boost the power, but the size of the wristband will be a problem because it will not run on a CR2032 battery.
I also bought some IR remote receivers. But I am not sure if the reflection from the wall will give false results.
Is what I am trying to do possible? Is a beacon 15m away feasible using this technology? 

If it is, then what do I need to modify in my current implementation?
If not, are there any other technologies that I should be considering to track the relative position or direction of human, 15 meters away with at least 300 degrees coverage?

",sensors
Pick and place using only vision information and forward kinematics,"Is it possible to do pick and place tasks using only forward kinematics and object detection information from a camera?
I want to avoid having to do inverse kinematic calculations for my robot. Is there a way I can avoid them for pick and place object sorting?
","robotic-arm, computer-vision, manipulator"
"Position Accuracy, 3-axis accelerometer, gyro in a small X,Y field","I'm working in the vertical plane about 1.5m x 0.6m. I want to track position over a 30s time frame (flexible). I'm hoping to be accurate to within 1cm. This is similar to an older question in the link below. Clearly compass data isn't going to be of help and I really can't use cameras or non on-board sensors. Is my project doable? What sort of hardware am I looking at? I don't need the position in real time and plan to do the data processing post movement in a cellphone app.
Thank you for the help.
Eric
P.S. Years ago I did some work with kalman filters and computer vision and micro controller programming but most of my experience is in analog electronics so a lot of this robotics is newish :/
How much accuracy could I get position tracking with a 3-axis accelerometer and gyro sensor, and compass, and how would I do it?
",accelerometer
Driving sensorless three phase BLDC motor with Arduino and IMU,"I have started to work on a little project of mines, which consists in implementing the stabilization a single axis gimbal using Arduino.
The gimbal is driven by a sensorless three phase brushless DC (BLDC) motor, while on it's shaft there is a generic payload provided with an IMU board (3 axes gyros + 3 axes accelometers), which can give feedback to the Arduino about the angular rates and accelerations.

I have googled a bit about this topic and there are so many solutions out there, the only thing I really do not understand is about the control of the BLDC motor.

Can I use a sensorless control of the motor, by sensing the back EMF
even if the motor is spinning very low?
How can I energize properly the phases of the BLDC motor if it is sensorless?
Can I use the IMU for finding out how to spin the BLDC motor properly without counter rotations?

Could you give me any help, please?
","arduino, control, motor, brushless-motor"
Dealing with fixed transformations while solving inverse kinematics,"I am trying to solve inverse kinematics (using the Jacobian pseudoinverse method) for a 7 DoF arm, but because of the way the robot is mounted, the base frame does not coincide with the frame of the first joint, so there is a transformation between base and frame 0. As the Jacobian expresses the joint-end effector velocity relationship w.r.t. the base frame, and also because my target poses etc. are expressed w.r.t the base frame, I encoded the transformation as an extra row in my DH parameters, but these angles are always fixed. Hence, I ended up with 8 rows in my DH although I have only 7 joints.
Because of this, my inverse kinematics algorithm, when trying to minimize the end effector pose error, continuously attempts to change the angle of the ""first"" joint which really isn't a joint at all. Hence, although the algorithm thinks the end effector has reached the target position, in real world it would not, because that base-robot transformation would be invalid for my setup. If I force this angle to be constant after every update of the iteration, the algorithm fails to converge and gets stuck at some pose. So I am guessing my approach for encoding the fixed base-first joint transformation is wrong? How are these transformations usually dealt with?
","inverse-kinematics, manipulator, rotation"
Definition of payload for industrial robots,"I'm currently looking for an industrial robot for a depalletizing application. I had a look at some datasheets but I'm not quite sure how the maximum payload is defined. e.g. the Kuka Agilus weighs 52kg and looks rather strong, but is listed with only 6kg. Is this really the highest weight that the robot can lift or is this the heaviest object that the robot can move with its maximal velocity and I can move heavier stuff at lower speeds?
",industrial-robot
Series Elastic Actuator (Passive compliance) vs Impedance or admittance control,"I have designed a 2 Degree of Freedom robot using dc motor and gearbox. However, I wanted the robot to be compliant as it'll be used in an unstructured environment with humans. I wanted to know if going SEA is a better option then using an impedance or admittance controller. For the moment I have been able to model the environment as spring and damper. But this model is not very robust. Is it a good idea to do a combination of passive and active compliance? I read in G.A. Pratt's report, and he states that SEA has drawbacks when small motions are required, but isn't this the problem with all compliance control. What are the advantages of SEA over conventional active compliance control methods?
","control, motor, robotic-arm"
"Confusion between wrist, tool and end effector","I have a confusion between the wrist frame, the tool frame and the end effector as used in the unmodified DH conventions. Can you differentiate with diagrams?
",dh-parameters
Is RobotBASIC outdated?,"I found this website http://robotbasic.org/ and it talks about a language used for programming things related to robotics, and I want to make sure whether or not it's worth investing any time or energy into compared to other languages before I just wipe it from my browser bookmarks for good. Nowadays, are there better languages and methods for going about the same things that it talks about? 
I mean, the site looks pretty old, like something from the late 90s or pre-2010, plus I never heard of it anywhere except for this site, so I wonder if it's just not relevant any more if it ever was.
","control, software"
EKF state propagation model using variables that are not part of state vector,"I am trying to understand the EKF theory. Can the state transition function depend on variables that are not part of the state space?  For example, the state propagation below depends on the quaternions that keep changing. If I get the quaternions from a very dependable source and I dont want to filter them, can I take them out of the state space? In that case, when calculating the Jacobian I will treat the quaternions as constants even though its a dynamic value from some external sensor. What are the implications of this approach? 
$$
\begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}_{k+1} = \begin{bmatrix}
x\\
y\\
z\\
\end{bmatrix}_{k} + \begin{bmatrix} 
Rotation Matrix\\
using\\
Quaternions
\end{bmatrix} * \begin{bmatrix} 
u \\ 
v \\ 
w \\
\end{bmatrix}_{k}
$$
The state space I am using is 
$$
\begin{bmatrix}
x& y& z& u& v& w
\end{bmatrix}^T
$$
where x y z are the position coordinates and u v w are the linear velocities.
","kalman-filter, ekf, jacobian"
Mobile robot identification,"The problem: line follower robots - non linear systems - use linear PID regulation algorithm in order to bring error to zero. However, using linear regulator is not the best way to drive non linear system.
There is something like global linearization of non linear systems - an algorithm that can bring regulation error to zero. In order to use it, one has to know kinematics of robot: Coriolis, inertion, gravity and friction matrixes. Those were once measured in EDDA manipulator and are now used in science, and that is how I learned about global linearization.
The question: I'd like to identify kinematical dynamical parameters of my line follower robot. I already have kinematical model, since it is simple (2,0) platform. Has anyone got information about good sources on physical parameters identification of mobile robot like this?
","mobile-robot, pid, research"
"Magnetic, low insertion force connector","I'm building a robotic tea-maker/watchdog robot and have a power problem. I would like to be able to have the robot approach a socket and insert the power cord of a cheap immersion heater (120V, 300W, see links below) to turn the heater on. However, the force and precision required to plug it into the wall is beyond the capabilities of my stepper motors/Arduino. 
My solution was a magnetic breakaway power cord like the charger on a Mac but at higher voltage. Deep fat fryers have suitable ones (120V, high power, see links below). However, the problem is I need both sides of the connector, and I can only find the magnetic breakaway power cord, not the opposite side, which would normally be built into the deep fat fryer. I don't fancy buying a whole fryer just to get one little part...
Any ideas? Alternatives to a breakaway cord? Anyone know of any (cheap) 120V induction chargers? I'll resort to a mechanical on/off switch and just leave the robot plugged in if I have to, but I was hoping for something a bit sleeker.
Links:

Immersion heater
Fryer cord 

","mobile-robot, force, connector"
Robot Arm Simulation,"I don't know if this is the right place to ask this question but I'll give it a try. A friend of mine is using unity 3D to simulate a robot arm,  however he's having some troubles when he needs to rotate the robot arm. The arm can already grab stuff with its hands, however seems like sometimes randomly crashes when is rotating the wrist.
Here's an image of the robot arm:

That's the arm he's working on for the simulation, and you can see that every part of the arm is connected through the unity's inheritance system so it can rotate every piece of the robot arm along with the nested parts that follows the inheritance path. However something seems like is failing when it rotates the wrist of the arm.
Three main questions:

How should be achieved the arm's rotation properly?
How should be achieved picking up things with the hand of the arm properly? 
How could you make it move by itself on a given point that can reach?

I'm not asking for code or anything like that, just how these 3 things should be done properly in order to have fully functional robot arm simulation like the main concepts behind it.
","robotic-arm, inverse-kinematics, simulation"
ROS Joy node with Arduino,"I'm working with ROS Indigo under Ubuntu 14.04, most the time following the tutorials. 
My purpose: read the ROS node Joy in Arduino Uno and do things when button are pressed (turn some led on). 
When I was able to have a working arduino sketch publishing/subscribing with ros (rosserial, USB port is correct and always the same), to have the joy node working (checked with rostopic echo joy) and to have all the electric parts in the right place on the breadboard, I made this sketch
#include <stdint.h>
#include <stdlib.h>  
#include <ros.h>
#include <std_msgs/Float32.h>
#include <sensor_msgs/Joy.h>

void joydata ( const sensor_msgs::Joy& joy){

  if ((joy.axes[14]) >= 0.5){
    digitalWrite(3,HIGH); 

     }     
}

ros::NodeHandle  nh;
ros::Subscriber<sensor_msgs::Joy> sub1(""joy"", joydata);    

void setup()
{
    pinMode(3, OUTPUT); //set up the LED
    nh.initNode();
    nh.subscribe(sub1);
}

void loop()   {
 nh.spinOnce();
 delay(1);
}

And even if the Arduino subscriber seems to be right checked with rqt_graph (see the image)
 
the thing doesn't work. But with the rostopic pub command for simple numeric inputs the same techniques work. 
Then I tried to mirror the signal from Joy to a publisher through Arduino: 
What I read with rostopic echo joy
header: 
  seq: 163
  stamp: 
    secs: 1478877490
    nsecs: 840985264
  frame_id: ''
axes: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.08682194352149963, 0.0]
buttons: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

And what I read if I publish with arduino a variable x = joy (copied and pasted in different moments, but all stamps are empty) rostopic echo publisher
header: 
  seq: 3744
  stamp: 
    secs: 0
    nsecs:         0
  frame_id: ''
axes: []
buttons: []

What I am doing wrong? Is only a problem of bandwidth or something like that (too much data for the rosserial communication) ?
Thank you all in advance. 
PS I made the same question on ROSanswers before finding this community from Stack Overflow. I hope that at least 1 community will help me with issue. 
","arduino, ros"
Denavit Hartenberg,"My problem says that for the articulated arm shown determine:


The algorithm of the end effector position in terms of q1, q2, q3, q4 and q5 with de Modified Denavit Hartenberg convention and
The coordinate of the end effector EF for the following data table.

I have already made the algorithm, can I have some help with all the matrices?
And also with the graphic forward kinematics?

",forward-kinematics
What is minimum torque required for CNC stepper motors and spindle for aluminium milling?,"I am planning to buy CNC mechanical skeleton without motors, spindle and controller. I will be using the CNC mainly for aluminium milling. Are there any specifications for minimum torque requirement for stepper motors and spindle to perform aluminium milling ?
","stepper-motor, cnc"
How do I interface with a drone?,"I recently bought a drone(quadcopter). I like how the drone works but now I would like to create an application that will allow me to control the drone remotely from my PC or Phone.
How does a computer or phone interface to an aerial vehicle?
Some of my initial thoughts were 

Get a RF detector and detect what signals are being sent to the drone and replicate it using a new transmitter.
Replace the control circuit in the drone with an Arduino and send corresponding signals to it to fly the drone

Both these methods seem to be kind of far fetched and hard, but I'm not sure how this could otherwise be done.
","quadcopter, software, radio-control, wireless"
How to convert G code to motor command?,"In cnc machine, programs change G-code to commands to stepper motors using parallel port.
I want to know what is the G-code and how can it be converted to stepper motor commands?
The programs doing this are not open-source, So can I find open source project doing the same?
",cnc
What is the difference between a CNC router versus a CNC mill?,"People at the RepRap 3d-printing project often mention CNC routers or CNC mills.
Both kinds of machines almost always have a motorized spindle with stepper motors to move the spindle in the X, Y, and Z directions.
What is the difference between a CNC router versus a CNC mill?
(Is there a better place for this sort of question -- perhaps the Woodworking Stack Exchange?)
",industrial-robot
Changing the logic on a PLC while it is running - exact timings,"Following my last question which confirmed that you can change the logic on a PLC while it is running, I'm now trying to understand the timings with which this happens.
Say that a PLC is sent a command to update its logic (I'm assuming that this can be done without using the PLC programming software, but could be wrong), and that the new, pending program code is stored in an area of memory which program execution then switches to when all of the new logic has been downloaded onto it. My questions are this:
1) Does a command need to be sent to switch to the new logic, or does this happen automatically once it has been downloaded?
2) Will the PLC switch to the new logic at the start of the next scan cycle (i.e. before that scan cycle's input scan), or at the start of the logic scan.
3) Would it always be that the new logic takes effect the scan cycle after it has finished being downloaded, or could there be a delay? I am trying to look for timing relationships between the networking data and the updated PLC logic, so need to be strict.
If anyone knows of any documentation for commands to update a PLC's logic while it is still running could they please point me to it?
Many thanks.
",plc
Hi i am working on this project regarding localization of a mobile robot using laser scanner and odometry ( encoder sensors) using labview,"I have a Sick laser scanner LMS 100 attached with my robot which scans the environment. Data acquistion is already done for the laser scanner and now  Encoder sensor attached on the wheel gives the distance travelled by the robot. A map in indoor room is already given in our case which consists of 3 landmarks.I am working in Labview environment.My question is now how can i localize the robot based on distance and angle? 
",slam
Inverse kinematics: how to specify TCP constraints in an iterative algorithm?,"I am trying to write some simple code to perform IK for a 6 DoF redundant robot using the Jacobian pseudo-inverse method. I can solve IK for a desired pose using the iterative method, and I want to now focus on applying constraints to the solution. Specifically I'm interested in 

Keep the end effector orientation constant as the robot moves from initial to final pose  
Avoid obstacles in the workspace

I've read about how the redundancy/null space of the Jacobian can be exploited to cause internal motions that satisfy desired constraints, while still executing the trajectory, but I am having trouble implementing this as an algorithm. For instance, my simple iterative algorithm looks like
error = pose_desired - pose_current;
q_vel = pinv(J)*error(:,k);
q = q + q_vel;

where $q$ is 'pushed' towards the right solution, updated until the error is minimized. But for additional constraints, the equation (Siciliano, Bruno, et al. Robotics: modelling, planning and control) specifies 
$$
\dot{q} = J^\dagger*v_e - (I-J^\dagger J)\dot{q_0}
\\
\dot{q_0} = k_0*(\frac{\partial w(q)}{\partial q})^T  
$$
where $w$ is supposed to be a term that minimizes/maximizes a chosen constraint. I don't understand the real world algorithmic implementation of this 'term' in the context of my desired constraints: so if I want to keep the orientation of my end effector constant, how can I define the parameters $w$, $q_0$ etc.? I can sense that the partial derivative signifies that it is representing the difference between how the current configuration and a future configuration affect my constraint, and can encourage 'good' choices, but not more than that.
","inverse-kinematics, matlab, manipulator"
Motor for a hydraulic pump in a hydraulic system,"I'm not entirely sure if this is the right area to post this question, but looking at the other subjects on StackExchange, this seems to be the best fit.
I am a complete beginner to hydraulic systems, and I've wanted to learn more about this area. I'm designing a hydraulic system that involves using hydraulics to push/pull objects using pistons. I have looked at what the basic requirements are for a hydraulic system, but there is one thing that escapes me.
I come from an electronic background, and I noticed that the hydraulic pumps (for example, this one) seem to lack a motor to drive the fluid. Am I wrong? If not, I've been looking everywhere for a motor that can/should be attached to said pump, but I cannot seem to find anywhere that sells them. Is it just a simple DC motor (with correct specs), or should there be a specific motor designed for hydraulic pumps?
Looking around, I came across this, but looking through the specs, I don't see a power requirement, and being used to seeing power consumption in datasheets, I'm not even sure it is a motor!
",motor
How is whole body control different from computed torque control?,"From my understanding Whole body control is built upon Computed Torque control. What is the difference between the two? What is the way for whole body control if the dynamic model is not available?
","control, humanoid"
High voltage motor control with arduino,"I'm trying to control a higher voltage motor than an arduino can source with a pin, with an arduino. I am trying to hook it up to a transistor. The battery pack is not supposed to be 4.8V, it's 6V, 4 D batteries.
Here is the setup:

Here is the arduino code I'm trying to run to it:
int motorpin = 2;

void setup()
{
    pinMode(motorpin, OUTPUT);
}

void loop()
{
    digitalWrite(motorpin, HIGH);
    delay(500);
    digitalWrite(motorpin, LOW);
    delay(500);
}

Code gives me no errors, but no motor movement happens. What would make this work? Thanks.
","arduino, motor"
Derivative of a Jacobian matrix,"I have an RRR planar robot:

Its forward kinematics transform is:
$$
{}^{0}T_3 = \\ \left[\begin{array}{cccc} \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & - \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & 0 & \mathrm{l_2}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) + \mathrm{l_1}\, \cos\!\left(\mathrm{\theta_1}\right) + \mathrm{l_3}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right)\\ \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & 0 & \mathrm{l_2}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) + \mathrm{l_1}\, \sin\!\left(\mathrm{\theta_1}\right) + \mathrm{l_3}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right)\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1 \end{array}\right]
$$
With the joint parameters $q = \left[\begin{array}{ccc} \mathrm{\theta_1} & \mathrm{\theta_2} & \mathrm{\theta_3} \end{array}\right]^T$ and the end-effector position $X = \left[\begin{array}{ccc} \mathrm{x} & \mathrm{y} & \mathrm{\theta} \end{array}\right]^T$. $\theta$ is constrained to 0. (Also the image is misleading, $\theta$ is actually $\theta_1 + \theta_2 + \theta_3$.)
The jacobian matrix is
$$
J = \\ \left[\begin{array}{ccc}  - \mathrm{l_2}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) - \mathrm{l_1}\, \sin\!\left(\mathrm{\theta_1}\right) - \mathrm{l_3}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) &  - \mathrm{l_2}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) - \mathrm{l_3}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & - \mathrm{l_3}\, \sin\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right)\\ \mathrm{l_2}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) + \mathrm{l_1}\, \cos\!\left(\mathrm{\theta_1}\right) + \mathrm{l_3}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & \mathrm{l_2}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2}\right) + \mathrm{l_3}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right) & \mathrm{l_3}\, \cos\!\left(\mathrm{\theta_1} + \mathrm{\theta_2} + \mathrm{\theta_3}\right)\\ 1 & 1 & 1 \end{array}\right]
$$
I'm trying to find $\ddot{q}$:
$$
[\ddot{q}] = J^{-1}(q) \cdot \left([\ddot{X}] - \dot{J}(q) \cdot J^{-1}(q) \cdot [\dot{X}] \right)
$$
My question is: how can I find $\dot{J}$? What is it?
","kinematics, inverse-kinematics, jacobian"
How does fast slam creates grid maps?,"I've implemented fast slam using landmark detection and the map stored is a feature map, made of landmarks positions. 
I would like to create a grid map, and my questions are about how does the robot create a grid map in slam:

Another landmark class is used, or the occupancy grid is the landmark itself? In other words, the grid map generation is made separatly of the feature map?
About the alignment of the map with the previous maps measures, this is similar to the fast slam 2.0 due to the fast  that considers the robot pose and the measurement of t-1?

Thanks in advance
","mobile-robot, localization, slam, particle-filter, occupancygrid"
Do you have to stop a programmable logic controller to update its logic?,"Must the operation of a PLC (programmable logic controller) be halted while its logic is being changed or can new logic be downloaded to it during runtime, without memory loss, giving a seamless transition from one operation cycle to other?
","industrial-robot, plc"
What kinematic equations should I use for Kalman filter state propagation in IMU based quadcopter navigation?,"I am working on implementing a Kalman filter for position and velocity estimation of a quadcopter using IMU and vision. First I am trying to use the IMU to get position and velocity.
In a tutorial [1] the process model for velocity estimation using IMU sensor data is based on Newton's equation of motion
$$
v = u + at \\
\\ 
\begin{bmatrix} 
\dot{x} \\ 
\dot{y} \\ 
\dot{z} 
\end{bmatrix}_{k+1} = \begin{bmatrix} 
\dot{x} \\ 
\dot{y} \\ 
\dot{z} 
\end{bmatrix}_{k} + \begin{bmatrix} 
\ddot{x} \\ 
\ddot{y} \\ 
\ddot{z} 
\end{bmatrix}_k \Delta T
$$
while in the paper [2] the process model uses angular rates along with acceleration to propagate the linear velocity based on the below set of equations.

$$
\begin{bmatrix} 
u \\ 
v \\ 
w \\
\end{bmatrix}_{k+1} = \begin{bmatrix} 
u \\ 
v \\ 
w \\
\end{bmatrix}_{k} + \begin{bmatrix} 
0& r& -q \\ 
-r& 0& p \\ 
-p& q& 0 
\end{bmatrix} \begin{bmatrix} 
u \\ 
v \\ 
w \\
\end{bmatrix}_{k} \Delta T+ \begin{bmatrix} 
a_x \\ 
a_y \\ 
a_z \\
\end{bmatrix}_{k} \Delta T + \begin{bmatrix} 
g_x \\ 
g_y \\ 
g_z \\
\end{bmatrix}_{k} \Delta T
$$
where u, v, w are the linear velocities | p, q, r are the gyro rates while a_x,a_y,a_z are the acceleration |  g_x,g_y,g_z are the gravity vector
Why do we have two different ways of calculating linear velocities? Which one of these methods should I use when modeling a quadcopter UAV motion?
[1] http://campar.in.tum.de/Chair/KalmanFilter
[2] Shiau, et al. Unscented Kalman Filtering for Attitude Determination Using Mems Sensors Tamkang Journal of Science and Engineering, Tamkang University, 2013, 16, 165-176
","quadcopter, kalman-filter, kinematics"
"What are these mechanical parts attached to robot arm structures, seen on larger robots?","Between the shoulder and elbow pitch joints, I see two types of connecting structures on larger robots. I've attached a picture. 

My questions:
1) What are each of the respective mechanical parts called?
2) What is their purpose for the robot arm?
Thanks!
",robotic-arm
Where does Gazebo set the GAZEBO_MODEL_PATH environment variable?,"I'm starting out with Gazebo (1.5) at the moment and am following a tutorial off the internet. In order to get Gazebo to find the model, the author advocates manually exporting the GAZEBO_MODEL_PATH environment variable via 
export GAZEBO_MODEL_PATH=[...]/models:$GAZEBO_MODEL_PATH

But that will only work for the current terminal. So I wanted to change the environment variable permanently. 
The Gazebo User Guide claims that GAZEBO_MODEL_PATH, along with all the other environment variables, is set by /usr/share/gazebo-1.5/setup.sh but my (virgin) Gazebo install doesn't list it:
export GAZEBO_MASTER_URI=http://localhost:11345
export GAZEBO_MODEL_DATABASE_URI=http://gazebosim.org/models
export GAZEBO_RESOURCE_PATH=/usr/share/gazebo-1.5:/usr/share/gazebo_models
export GAZEBO_PLUGIN_PATH=/usr/lib/gazebo-1.5/plugins
export LD_LIBRARY_PATH=/usr/lib/gazebo-1.5/plugins:${LD_LIBRARY_PATH}
export OGRE_RESOURCE_PATH=/usr/lib/i386-linux-gnu/OGRE-1.7.4

# This line is needed while we're relying on ROS's urdfdom library
export LD_LIBRARY_PATH=/opt/ros/fuerte/lib:${LD_LIBRARY_PATH}

But when I start Gazebo, GAZEBO_MODEL_PATH is already set to $HOME/.gazebo/models, so it must be set somewhere. I guess I could probably simply add GAZEBO_MODEL_PATH to the setup.sh script, but since it is set somewhere, I'd still like to know where and whether it is better practice to set it in there.
",gazebo
Naze32 - Channel limitation with LED strip,"I dont know if this is the place to ask this question but got stuck on Naze32 quadcopter with Flysky 6 channel radio. I need to use LED strip and guide says have to use channel/pin 5 of receiver. I am already using pin 5 for AltHold mode and pin 6 for AutoTune mode. I have to use these most of the time. But also want to use LEDs too. Please suggest how to achieve both things on 6 channel or its impossible?  Thanks
",quadcopter
How to convert this arduino code snippet to AVR GCC?,"I recently engaged with a university robotics project (based on ROS) and my main processing controller is Raspberry Pi and the system stability controller is Atmega 32 microcontroller (It is responsible for driving motors and check the communication protocols ex:-i2c,rs232 are working in good manner). So the motor controller of this robot is a i2c type one and and it drives the motors according to the i2c signals that coming from the i2c port of the Atmega 32 microcontroller. The main controller communicate with the Atmega 32 using rs232 protocol. So I found an arduino code as below,
// This function is called once when the program starts.
void setup() {
     // Choose a baud rate and configuration. 115200
     // Default is 8-bit with No parity and 1 stop bit
     Serial.begin(115200, SERIAL_8N1);
}

// This function will loop as quickly as possible, forever.
void loop() {
    byte charIn;
    if(Serial.available()){ // A byte has been received
        charIn = Serial.read(); // Read the character in from the master microcontroller
        Serial.write(charIn); // Send the character back to the master microcontroller
    }
}

The communication between processing board and microcontroller as below diagram

There is an already available arduino library called rosserial for ROS. But I want this in AVR GCC.What I want is convert this code to traditional avr gcc code that work on atmel studio 6 
","arduino, ros, avr, i2c, rs232"
Appropriate control scheme for gripper end-effector,"I want to use a gripper end-effector (3 fingers, single actuator tendon-driven with force sensors in fingers) to grip and hold fragile objects such as an egg. I can not seem to figure out which control scheme would be most appropriate for the situation. Would using an impedance control with negligible dynamic interaction be effective or a hybrid control with force/position control?
My idea was to simply tense the tendon (slowly) encompassing the object until the force sensor gives a feedback that contact has been made (+ a little grip force), what category of control scheme would that fall into?
","control, sensors, force-sensor"
Feature tracking not working correctly on low Resolution images,"I am using FAST for feature detection and calcOpticalFlowPyrLK for feature tracking in images. I am working on low resolution images (590x375 after cropping) taken from Microsoft kinect. 
// feature detection
int fast_threshold = 40;
bool nonmaxSuppression = true;
// FAST (Features from Accelerated Segment Test) corner detector
FAST(img_1, keypoints_1, fast_threshold, nonmaxSuppression);

// feature tracking
vector<float> err;
Size winSize=Size(21,21);
TermCriteria termcrit=TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01);
calcOpticalFlowPyrLK(img_1, img_2, points1, points2, status, err, winSize, 3, termcrit, 0, 0.001);

I ran this on consective images of steady scene taken from same camera position at rate of 30fps. To eyes, images looks same but somehow the calcOpticalFlowPyrLK in not able to track same features from one image to another. Position (x,y coordinates) should be same in detected feature and tracked feature. Somehow it isn't.

Is FAST a good way to get features in low resolution images? 
Should I change parameters?
Is calcOpticalFlowPyrLK expects some movement?

Find comeplete code in here.

","computer-vision, opencv"
how to add a Matlab code to a simulink model?,"I want to program an arduino using matlab by ""arduino support package"" and I want to use simulink in normal input output operations but also use Matlab language in another part so is it possible to make a code consist of simulink model and matlab language and add this code to arduino ? and if it possible how to make it ?
","arduino, matlab"
"In vision based localization, is it possible to make multiple vehicles cooperate to improve the estimation of each other?","I am currently working on a project that involves structure from motion using multiple cameras on multiple aerial vehicles (each vehicle has a monocular camera: think of it as a distributed stereo), and I am trying to extend this to include localization as well. My pipeline currently goes: robots at known locations -> take pictures -> reconstruct.
When it comes to localizing the vehicles as well using this incrementally built map, the standard approach that comes to mind is to apply the PNP algorithm on each camera (assuming the reconstructed scene is visible to all cameras) which results in the 3D pose: but this doesn't necessarily take advantage of the fact that multiple cameras exist, apart from the fact that they are used in reconstructing the environment. Is there anything I can exploit using multiple cameras/vehicles that would result in enhanced localization accuracy of all of the vehicles as compared to a ""single vehicle performing PNP on a known map"" scenario?
","localization, slam, multi-agent"
Bullet vs ODE to simulate robotic arm,"For a research project at university I have been tasked with simulating a ""robot arm"". The simulation is to be compared with the real life version for accuracy. The arm will be lifting other objects and building simple structures in the demonstration.
My supervisor has asked that we build this simulator from scratch so I am currently selecting a physics engine. 
In this link, it seems bullet is not accurate enough, albeit this was in 2010, for the original poster's needs in robotic simulation.
This comparison believes bullet is better in general but says that documentation is lacking, which is important to me as this is my first time using a physics engine such as this.
so thoughts on these or any other physics engine that may be more suitable?
","robotic-arm, simulator"
"Question about ROS, how to explain to someone what ROS is?","Over the summer, I have configured ROS navigation stack on a mobile robot (with radar and Kinect) so that it can autonomously navigate in the unknown environment. I also wrote a python program that allow the robot to track human motion, also using the open source library. 
Currently, I am applying to software job. I thought this experience is very relevant to software programming. But, I am actually stuck on how to explain what ROS is. And when I use the packages (eg, navigation stack) on a robot, am I actually doing coding? Can I say ROS is just an API?
",ros
Motor-controller with path output,"I'm entirely new to robotics and have the following design requirement for a project I'm working on:

Given a robot capable of positioning an object in three dimensional space ( vis. a plastic extruder or syringe, etc ), allow the robot's end user to manually move that object through a path so that the robot's motor/controller will output the path taken.

I'm sure there must be motor-controller packages with such a feature, but I have no idea what keywords to search for so as to learn more about this type of hardware.
The basic idea is to allow the end user to give the robot instructions by manually moving the object through a procedure, then recording that procedure, manipulating it and playing it back. 
Can anyone give me some keywords to get me started in my research?
","control, motor"
State estimation of mobile robot,"For a mobile robot - four wheels, front wheel steering - I use the following (bicycle) prediction model to estimate its state based on accurate radar measurements only. No odometry or any other input information $u_k$ is available from the mobile robot itself.
$$
\begin{bmatrix}
      x_{k+1} \\
      y_{k+1} \\
      \theta_{k+1} \\
      v_{k+1} \\
      a_{k+1} \\
      \kappa_{k+1} \\
    \end{bmatrix} = 
    f_k(\vec{x}_k,u_k,\vec{\omega}_k,\Delta t) =
    \begin{bmatrix}
      x_k + v_k \Delta t \cos \theta_k \\
      y_k + v_k \Delta t \sin \theta_k \\
      \theta_k + v_k \kappa_k \Delta t \\
      v_k + a_k \Delta t \\
      a_k \\
      \kappa_k + \frac{a_{y,k}}{v_{x,k}^2}
    \end{bmatrix}
    +
    \begin{bmatrix}
      \omega_x \\
      \omega_y \\
      \omega_{\theta} \\
      \omega_v \\
      \omega_a \\
      \omega_{\kappa}
 \end{bmatrix}
$$
where $x$ and $y$ are the position, $\theta$ is the heading and $v$, $a$ are the velocity and acceleration respectively. Vector $\vec{\omega}$ is zero mean white gaussian noise and $\Delta t$ is sampling time. These mentioned state variables $\begin{bmatrix} x & y & \theta & v & a \end{bmatrix}$ are all measured although $\begin{bmatrix} \theta & v & a \end{bmatrix}$ have high variance. The only state that is not measured is curvature $\kappa$. Therfore it is computed using the measured states $\begin{bmatrix} a_{y,k} & v_{x,k}^2\end{bmatrix}$ which are the lateral acceleration and the longitudinal velocity. 
My Question: 
Is there a better way on predicting heading $\theta$, velocity $v$, acceleration $a$, and curvature $\kappa$?

Is it enough for $a_{k+1}$ to just assume gaussian noise $\omega_a$ and use the previous best estimate $a_k$ or is there an alternative?
For curvature $\kappa$ I also thought of using yaw rate $\dot{\theta}$ as $\kappa = \frac{\dot{\theta}}{v_x}$ but then I would have to estimate the yaw rate too.


To make my nonlinear filter model complete here is the measurement model:
$$
\begin{equation}
   \label{eq:bicycle-model-leader-vehicle-h}
   y_k = h_k(x_k,k) + v_k =
   \begin{bmatrix}
     1 & 0 & 0 & 0 & 0 & 0 \\
     0 & 1 & 0 & 0 & 0 & 0 \\
     0 & 0 & 1 & 0 & 0 & 0 \\
     0 & 0 & 0 & 1 & 0 & 0 \\
     0 & 0 & 0 & 0 & 1 & 0 \\
   \end{bmatrix}
   \begin{bmatrix}
     x_k \\
     y_k \\
     \theta_k \\
     v_k \\
     a_k \\
     \kappa_k \\
   \end{bmatrix}
   +
   \begin{bmatrix}
     v_x \\
     v_y \\
     v_{\theta} \\
     v_v \\
     v_a \\
   \end{bmatrix}
 \end{equation}
$$

More Info on the available data:
The measured state vector is already obtained/estimated using a kalman filter. What I want to achive is a smooth trajectory with the estimate $\kappa$. For this it is a requirement to use another Kalman filter or a moving horizon estimation approach.
","mobile-robot, kalman-filter, motion, movement"
What exactly is $\omega_k$ in the stage cost of moving horizon estimation?,"In (nonlinear) moving horizon estimation the aim is to estimate an unknown state sequence $\{x_k\}_{k=0}^T$ over a moving horizon N using measurements $y_k$ up to time $T$ and a system model as constraint. All the papers I've seen so far 1,2,3 the following cost function is used: 
$$
\phi = \min_{z,\{\omega_k\}_{k = T-N}^{T-1}} \sum_{k = T-N}^{T-1} \underbrace{\omega_k^TQ^{-1}\omega_k + \nu_k^TR^{-1}\nu_k}_{\text{stage cost } L_k(\omega,\nu)} + \underbrace{\mathcal{\hat{Z}}_{T-N}(z)}_{\text{approximated arrival cost}}
$$
The noise sequence $\{\omega_k\}_{k = T-N}^{T-1}$ should be optimized/minimized in order to solve for the unknown state sequence using the prediction model:
$$
x_{k+1} = f(x_k,\omega_k)
$$
whereas the measurement model is defined as 
$$
y_k = h(x_k) + \nu_k
$$
with additive noise $\nu$.
The question is what exactly is $\omega_k$ in the stage cost?
In the papers $\nu$ is defined to be 
$$
\nu_k = y_k - h(x_k)
$$
However, $\omega_k$ remains undefined.
If I can assume additive noise $\omega$ in the prediction model, I think $\omega_k$ is something like the follwoing:
$$
\omega_k = x_{k+1} - f(x_k)
$$
If this should be correct, then my next Problem is that I don't know the states $x_k$, $x_{k+1}$ (they should be estimated).
EDIT
Is it possible that my guess for $\omega_k = x_{k+1} - f(x_k)$ is ""wrong"" and it is enough to just consider:

the measurement $\nu_k = y_k - h(x_k)$ in the cost function $\phi$
the prediction model as constraint?

And let the optimization do the ""rest"" in finding a possible solution of a noise sequence $\{\omega_k\}_{k = T-N}^{T-1}$? In this case which Matlab solver would you suggest? I thought of using lsqnonlin since my problem is a sum of squares. Is it possible to use lsqnonlin with the prediction model as constraint?

1 Fast Moving Horizon Estimation of nonlinear processes via Carleman linearization
2 Constrained state estimation for nonlinear discrete-time systems: stability and moving horizon approximations
3 Introduction to Nonlinear Model Predictive Control and Moving Horizon
Estimation
",algorithm
Joint angle correction using LM,"I have a camera mounted on a rotational joint. I need to calibrate the extrinsics of this camera. I can fix the camera at an estimated angle (facing the ceiling). Then I want to get the real angle.
For that I track key-points in the ceiling while moving my robot forward. Supposing that odometry is perfect, I will see a difference between real key-points shift and estimated shift from the odometry.
I thought about using Levenberg Marquardt to find the optimal solution which is the angle and of my camera in the robot frame but what would my equation look like?
","cameras, odometry, joint"
Help finding robot tracks,"I have a Robot with tracks. One of the tracks broke and I need to find a replacement, the tracks use the same plastic interconnects/pieces as this:

They were very popular years back. Does anyone know the brand/name?
","mobile-robot, tracks"
How to connect ethernet based Hokuyo scanner?,"This is a very basic beginner question, I know, but I am having trouble connecting to the Hokuyo UST-10LX sensor and haven't really found much in terms of helpful documentation online.
I tried connecting the Hokuyo UST-10LX directly to the ethernet port of a Lubuntu 15.04 machine. The default settings of the Hokuyo UST-10LX are apparently:
ip addr: 192.168.0.10
netmask: 255.255.255.0
gateway: 192.168.0.1
So, I tried going to the network manager and setting IPv4 settings manually, to have the ip addr be 192.168.0.9, netmask of 255.255.255.0, and gateway to 192.168.0.1. I also have a route set up to the settings of the scanner.
I then go into the terminal and run:
rosrun urg_node urg_node _ip_address:=192.168.0.10

and get this output:
[ERROR] [1444754011.353035050]: [setParam] Failed to contact master at [localhost:11311].  Retrying...

How might I fix this? I figure it's just a simple misunderstanding on my end, but through all my searching I couldn't find anything to get me up and running :(
Thank you for the help! :)
EDIT:
HighVoltage pointed out to me that I wasn't running roscore which was indeed the case. I was actually running into problems before that when I still had roscore up, and when I tried it again, this was the output of the rosrun command:
[ERROR] [1444828808.364581810]: Error connecting to Hokuyo: Could not open network Hokuyo:
192.168.0.10:10940
could not open ethernet port.

Thanks again!
","sensors, ros, rangefinder, linux"
Self Driving Robotics Project : ROS or Python,"So, I am working on building a simple self driving tank (small) that needs to navigate a large hall.  I plan to use ultrasonics, LIDAR and a Kinect.  I am pretty happy with how I will build all of this.  My main question is would this be easier to do in ROS or write it in Python.  I have very basic knowledge of ROS but have been programming for many years (Java, Objective C etc).
I assume I will need to load in a basic map of static objects / floor plan.  Use SLAM etc (which I see is possible in Python).
Sorry if this is a vague question. My hoping is someone on here who has used ROS a lot will turn round and say its the way to go
","ros, python, self-driving"
What relay or other gizmo do I need to switch between two circuits?,"I have built a fighting bot but improvements could be made. I currently use 1 channel to switch spinners on and off and another for self righting.  The spinners cannot turn during self righting as they are against the floor so I have to switch them off with one switch and activate self righting with another to avoid the spinner motors burning out and then once self righted reverse both switch positions again.  Currently each circuit has a normally off relay (5v from the receiver controlling 24V to load), in an ideal world I would have got a relay that allows one circuit 'on' and the other 'off' and then the opposite when given the signal by the receiver e.g. when a single switch on the remote control is 'off' then circuit A is 'off' and circuit B is 'on', when the remote switch is 'on' circuit A in 'on' and circuit B is 'off' - this would free up a remote channel and also ensure the two circuits can never be closed at the same time - with me so far? anyway turns out that this type of relay does not exist for control by a remote receiver so what I am trying to achieve is the following:

So it's not actually a relay as that suggests one voltage controlling another so what is this illusive gizmo I seek actually called?  Can I buy one on Amazon?
I'm trying to avoid going down the route of IF / THEN gates on a separate PCB, the daft thing is that all I want is an electrical version of exactly what the pneumatic actuator does - when powered air goes down one hose and when off goes down another.
Thanks in advance.
After looking at some of the answers - (thanks everyone) it allowed me to find this:

 from http://www.superdroidrobots.com/shop/item.aspx/dpdt-8a-relay-rc-switch/766/
","control, wireless, circuit"
Platooning leader follower code,"I'm trying to develop a platoon leader follower formation for two robots in Matlab. The paper I'm trying to follow is this.

I've got next code, where I want follower robot to follow the leader robot's path, just in a very simple way, no kinematics. But I cannot get it. Does anybody know which is my error?
x1=linspace(0,10,100); //x1 and y1 represent the leader's path
y1=sin(x1);
plot(x1(1:100),y1(1:100));
hold on;

x2=zeros(1,100);
y2=zeros(1,100);

x2(1)=-5; //x2, y2 represent the follower's position
x2(2)=-4;
landa=0.1; //represents the euclidean distance between robots

theta_leader(2)=atan((y1(2)-y1(1))/(x1(2)-x1(1)));
theta_follower(2)=atan((y2(2)-y2(1))/(x2(2)-x2(1)));
alfa(2)=atan((y1(2)-y2(2))/(x1(2)-x2(2)))-theta_follower(2);
phi(2)=pi-(theta_leader(2) - alfa(2) - theta_follower(2));

for i=3:100
     landa(i)=0.1;
      x2(i)=x1(i)*cos(theta_leader(i-1))-landa(i)*cos(alfa(i-1)+theta_follower(i-1));
      y2(i)=y1(i)*sin(theta_leader(i-1))-landa(i)*sin(alfa(i-1)+theta_follower(i-1));

      theta_leader(i)=atan((y1(i)-y1(i-1))/(x1(i)-x1(i-1)));

      alfa(i)=atan((y1(i)-y2(i))/(x1(i)-x2(i)))-theta_follower(i-1);
      phi(i)=pi-(theta_leader(i) - alfa(i) - theta_follower(i-1));
      theta_follower(i)=phi(i)-alfa(i)+theta_leader(i)-3.1415;
  end

  plot(x2,y2,'or');

",untagged
Kalman filter: 3D measurement error to individiual components,"I'm working on a Kalman filter for estimating the position of a point in 3D space. I know that I can measure its 3D position directly with a variance of about 2 mm (in other words: the variance of the norm of the measured x, y, z vector is about 2 mm).
I'd like to fill my measurement noise covariance matrix based on this, so my question is:
How does this relate to the variance of the individual x, y, z measurements? I'm looking for three equal variances, assuming independency. 
","kalman-filter, noise, precise-positioning"
Why can't you buy continuous servos with absolute positioning?,"I've been looking at parts for a beginners robotics kit (I teach at a museum) and have been wondering about servos.
You can buy continuous servos with relative position encoders. But I can't find continuous rotation servos with absolute position encoders. Do these exist? If not, why not?
I understand that some forums don't like shopping questions, but I suspect that this part doesn't exist and I'd like to understand why.
Also, I understand that most servos use a potentiometer as a position encoder and that these don't turn more than 1 rotation, but there are other types of encoders that seem like they would do the job.
Thanks for the help!
","servos, quadrature-encoder"
Platform design/construction,"Is there a website like easyeda for platform/cnc/laser cutting design? Such that its easy to cutouts, holes, .etc. for a nice looking robotics platform all packaged in a web-ui? I've been searching for a while and haven't been able to find something like it to make stack-able levels for my robot. If not, what software/bases do you use for your robot construction?
","design, platform"
Calculating screw breakaway torque,"I'm trying to use motor-sizing tool developed by oriental motor to choose good servo motor for my cnc
The tool requires breakaway torque of my screw as input, I searched online but I got people measuring it using wrench.
I'm working on simulating the machine before I buy anything so I don't have the screw to test anything on i.
This is the screw I want to use and all the specs are available in the link, I need to calculate the breakaway torque mathematically so is there any way?
","servomotor, torque, cnc"
Software simulation vs. real world scale model simulation,"There are high quality software robotics simulators like Gazebo available today. What is the difference between a pure software simulation and a real world (say RC) scale model simulation? Is it possible to skip the scale model simulation and only do SW simulation and then build a full scale final product right away? Does scale model simulation have any advantages over say Gazebo?
I don't have any direct experience with developing a product in robotics but if I try to think then I guess the SW simulation may primarily be used to develop the very basics of a product and then scale model simulation may take over or complement SW simulation. My personal view is that any (even small scale) real world simulation/testing is beneficial because the features of real world (noise, dust etc.) can hardly be simulated in SW. Is this true? Also small scale model will be many times cheaper than full scale one.
I am considering an autonomous car (self-driving) as one of the possible products resulting from such simulation. I read that vision is one of the weak parts of SW simulators. I can think that sensing in general may be a weak part of SW simulators since any real sensor is imperfect and noisy which Gazebo may not take into account...
","simulation, gazebo, autonomous-car, self-driving, scale-model"
Control a 2.4 Ghz AR Drone from the computer,"I had a Doyusha Nano Spider R/C mini-copter, it's controlled by a 4ch joystick 2.4 Ghz.
I look for a low cost method to control it from the computer. The software is not a problem, but how can I transform the WIFI or the Bluetooth signal of the computer to an R/C signal compatible with the mini-copter receptor?
Or is there another solution that is low cost?
","control, quadcopter, wireless"
Transforming Point Cloud to get Top Down Image,"My task:
I have a task where I am asked to track parcels(carton boxes) of different dimensions moving on a conveyor. I am using Asus Xtion pro camera mounted on top of a conveyor in any inclined angle. I am looking for a model free object tracker that will detect boxes in the scene, track them & gives their 6 DOF? My target object is just a box and I want to eliminate all other things in the scene.
My approach:

I do Point cloud pre-processing like down-sampling, pass through filtering and segmentation. All these should give me a final point cloud containing only the objects on the conveyor.
I planned to make the ""z"" values in each point(depth value) as zero, thereby making the point cloud of the box to be flat on the ground.
I planned to transfer the view of the camera from any inclined position to a top down view so that I can view any number of carton boxes moving on the conveyor from a top down view. I feel the top down view will prevent perspective viewing problems

The process flow of step 2 and 3 is shown below.


After the top down view of the point cloud is achieved, I need to convert the 3rd point cloud to 2nd image, so that I can perform object tracking with so many OpenCV based tracking algorithms available.

A Sample point cloud is shown below in different views
Original View from camera:

Point Cloud View 1:

Point Cloud View 2:

Point Cloud Target/Desired View for converting to 2nd: (The box is the target. All the ground plane and unnecessary points would be eliminated)

Is my approach correct? How will I achieve steps 2,3 and 4?
","ros, computer-vision, machine-learning, opencv, point-cloud"
Calculating Required Torque,"Say I had an object with 4 motors/wheels attached (in a fairly standard arrangement).
I need to calculate the amount of torque required from the motors to be able to move the object of x kilograms consistently (without skipping any steps) at a velocity of y, travelling up a slope of angle z.
I'm guessing this would also depend on factors like the grip of the tyre and such?
","motor, motion, torque, wheel"
Should I use an arduino to control my balancing robot's motors?,"I'm building a 2-wheel balancing robot from old parts. DC motors from an old printer, wheels from a BBQ, bodywork from an old optical drive etc.
The brain of the robot is a Raspberry Pi 3. I'm using an L298N motor driver to control 2x 12-35v DC motors. Balance and movement will be 'sensed' using a 10DOF L3GD20 LSM303D Gyro, Accelerometer & Compass board. I'm currently using an Arduino with PWM for multi speed and direction of each motor. A PID loop will keep the robot balanced.
It looks like this:
Raspberry Pi > Arduino > L298N > Motors
So far everything is good (don't worry I'm not going to ask ""Where do I start??"" :D)
My question is this: should I continue to control the sensors and motors' driver using the Arduino, having the Pi issue higher level commands to the Arduino, OR should I just let the Pi do all the leg work?
I'm trying to keep power consumption to a minimum but I want to keep the processing overhead of the balancing and movement away from the Pi. Additionally, I've read that an Arduino is better at this sort of work due to the fact that it has a built in clock, where a Pi doesn't. I assume the PID loop will be slowed if the device (the Pi) is working on other processing tasks like navigation and face recognition etc.
Are my assumptions correct and which direction would guys steer towards?
Your knowledge and wise words would be very much appreciated!!
","arduino, motor, raspberry-pi, pwm, balance"
How to update an EKF when no inputs are available?,"I'm using an  Extended Kalman filter where the motion model is a function of the states and the inputs, with additive white noise, i.e. 
$$ x_k = f(x_{k-1},u_{k-1}) +\delta_{k-1} \quad , \quad \delta_{k-1} \sim N(0,\Delta_{k-1})$$ 
If $x_{k-1}$ and $u_{k-1}$ are know, then the prediction step is done as
$$\hat{x}_{k|k-1} = f(\hat{x}_{k-1|k-1},u_{k-1}) $$
$$ f' = \frac{\partial F}{\partial x_{k-1}}\Big|_{x_{k-1}=\hat{x}_{k-1|k-1}~,~u=u_{k-1}} $$
$$ P_{k|k-1} = f'P_{k-1|k-1}f $$
However, at some time steps I won't know the value of $u$, the input. What is the optimal way to perform the prediction step in this scenario? 

My thoughts so far are to set
$$\hat{x}_{k|k-1} = \hat{x}_{k-1|k-1} ~,$$
since I have no new information to update it... but no idea how to estimate the covariance matrix $P_{k|k-1}$.
","kalman-filter, filter"
Correct way to implement an EKF,"I implemented an EKF for a mobile robot, it is implemented as the upper control block shown:

The gray block are the dynamics, which calculate a $dp$ and add it to the previous value of $p$, and, $p$ is the measurement to the EKF. The robot seems to give good positions $p'$, but I was wondering if the block diagram shown in the lower part is better, if so, why?
","control, ekf"
Connect to video stream with Java app instead of console and mplayer,"I'm building a quadcopter using Raspberry Pi. There is the Pi Camera connected to the Raspberry Pi which is streaming the captured video. I can connect to this stream via Wi-Fi on my notebook (Linux) by using the console command ""nc"" and then show it by ""mplayer"".
What I want to do though is avoid the console commands and connect to this stream directly through my Java application. The reason is I want to do some image processing operations with this video so I need to have it in my application.
Is there anyone able to help me?
","quadcopter, raspberry-pi, cameras, linux"
How to dimension a 3 DOF robot arm with RRR configuration?,"How can the maximum length and mass of the linkages of a RRR type robt arm be calculated, if the motor's mechanical characteristics are given?
","motor, industrial-robot, arm"
Can you have a career in robotics if you hate mechanics?,"I'm a first year electronics engineering student. I love almost all the aspects of robotics - the electronics, algorithms, control theory etc. I can't stand the mechanical aspect of robotics though.
Can I have a fulfilling career in Robotics if I hate mechanics but love all other parts of robotics? I'm ready to learn mechanics if I absolutely have to, but would strongly prefer not to learn any more than the absolute basics.
","software, electronics, mechanism"
How to fuse the single axis gyro yaw with odometry to estimate pose?,"I have a single axis gyro and the odometry from encoders.
I want to fuse the two sensors to improve the angular estimation precision.
However we need to run the fusing algorithms on the embedded platform with low computation.
I also read the paper“Mobile Robot Position Determination Using Data integration of odometry gyroscope”, which uses ukf.
Another paper "" Implementing the Unscented Kalman Filter on an Embedded System: a Lesson Learnt"", says the ukf on embedded need several hundreds ms.
So, I turn to use EKF, but I haven't find useful links and code.
Now I have just implement the paper ""Gyrodometry: A New Method for Combining Data from Gyros and Odometry in Mobile Robots"", but I am not satisfied with it.
",odometry
Use of ADC with Sharp Infrared Sensor,"I have a Sharp GP2D12 and I've been using it with my Arduino just fine. I have however had some experience in the past where this sensor (which is analog) was fed into an ADC (like ADC0831) to go to a Basic Stamp. I was wondering what the purpose of this is given that both the Arduino and the Basic Stamp support analog inputs if I am not mistaken, thus it seems weird having an extra and necessary link in the chain. Does the ADC provide more resolution? What is the point of it in this circumstance?
","arduino, sensors"
Is there a sensor that can measure through an object?,"Is there a sensor that can measure through an object, for example a mattress. (This is not what I want to do but it is a good illustration) I want to mount a sensor on the ceiling above the top bunk of a bunk bed and want to measure the presence of a person on the bottom bunk (when no one is in the top bunk). I have read about thermal and ultrasonic sensors however it does appear that they would be able to measure on the other side of a mattress.
",sensors
Kuka KR-C4 Documentation,"Is there an official documentation from Kuka that explains at a beginner level how to start programming Kuka Robots equipped with KR-C4 controllers?
","software, industrial-robot, kuka"
How can the inverse kinematics problem be solved?,"The forward kinematics of a robot arm can be solved easily. We can represent each joint using Denavit–Hartenberg transformation matrices.
For example, if the $i^{th}$ joint is a linear actuator, it may have the transformation matrix:
$T_i = \left[\begin{matrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&d_i\\
0&0&0&1
\end{matrix} \right]$
where the extension length is defined by $d_i$
whereas, a rotating link may be:
$T_i = \left[\begin{matrix}
1&0&0&L\\
0&\cos\alpha_i&-\sin\alpha_i&0\\
0&\sin\alpha_i&\cos\alpha_i&0\\
0&0&0&1
\end{matrix} \right]$ where $\alpha$ is the angle, and $L$ is the length of the link.
We can then find the position and orientation of the end effector by multiplying all the transformation matrices: $\prod{T_i}$.
The question is, how do we solve the inverse problem?
Mathematically, for a desired end effector position $M$, find the parameters $d_i$, $\alpha_i$ such that $\prod{T_i} = M$. What methods exist to solve this equation?
","inverse-kinematics, kinematics, joint, arm"
Is there a KUKA IDE?,"I'm learning kuka krc robot language, so far so good, and am wondering if there is an  IDE for writing the code i want to program to the robot, something like

giving me suggestions if I mistype a variable
that suggest me methods available in the kuka language
maybe let me debug the code etc etc

any option?
","programming-languages, kuka"
Relay Fix at Normally Open,"I am using my RF module to send a High signal to relay but just once by a push button.As soon as I release it the Relay goes to NC . How can I Fix it to stay at NO . I thought of using a Flip-Flop.
",radio-control
Robot autonomous variable terrain with yaw sensor,"I am programming a robot to drive over variable terrain obstacles autonomously. The variable terrain could potentially knock the robot off of its initial heading, but I would like to design an autonomous sequence to correct for any change in direction. I am using a very accurate sensor with compass and yaw. What is the best way to have it correct for any changes and maintain its heading? Side to side motion does not have to stay perfect, but the heading needs to stay the same.We are currently correcting it by overpowering one side of the wheels (depending on direction of correction needed) until the heading is correct again, but this seems to be a slightly antiquated method, so I'm looking for a cleaner and more smooth method.
","compass, automation"
What sensors can track small objects (5 cm - 25 cm) in bright sunlight?,"ToF (time-of-flight) cameras seem susceptible to bright outdoor conditions.
Are there any sensors made for bright outdoor conditions and could be embeddable in a robot that could detect and/or track small objects (5cm - 25cm) with a range of 10m - 100m?
Would radar work?
","computer-vision, lidar"
2 way Switch powerbar,"For some important reasons the plugged cable 1 must not be on at the same time as the the plugged cable 2. It must be idiot proof...
I'm talking about regular house power north america.
What is the specific name of it. I've done some research and I can't find anything. I thinking about crafting the thing with 2 powerbar and a custom button, but I'm looking for something less homemade.
Thanks
",power
See CC3D actual configuration,"I just received my first hobby-grade quadcopter.
It's the Eachine racer 250 and comes preassembled with transmitter and receiver also included.
It comes with some kind of CC3D flight board, most people say it's not the original one, but can be configured with the same software.
It is actually flying very well right out of the box so I'm not sure if I want to touch the FC config.
I'm mostly interested in learning to fly in manual/acro mode, the transmitter seems to have a switch with 3 flight modes, first 2 looks like low/high rates in self-level mode, I expect the third to be the acro mode, but I'm not sure right now, I couldn't test it because of the weather, it could be a third higher rate?.
So, is there any way I can look at the actual FC config without changing anything? what software do I need? and are the flight modes actually set on the FC or transmitter so I could be able to see and edit them? 
",quadcopter
What is the top speed of a four legged robot?,"In  Neal Stephenson's novel ""Snow Crash"", a robotic/cybernetic creature known as a ""Rat Thing"" looses its bounds and runs to rescue another character.  The Rat Thing is portrayed at moving over 700 mph and causing sonic shock waves.  Can such a speed be reached by purely mechanical means? 
",kinematics
How are huge industrial robots powered?,"Take a look at this monster lifting a car.
Now I don't think one can get that power by plugging into a wall outlet. So how are these huge robots (their servos) powered? Gasoline? How is power stored, if it needs to be? 
","power, industrial-robot"
Learning Materials for Beginners in Robotics and Quadrocopters,"I am a web developer. I am fascinated by Quadrocopters and i am trying to learn how to build one and basically i am trying to jump into robotics fields. I don't have much electric circuit and electronics knowledge so i did some research on how to build and what type of knowledge you would require to develop such flying machine. So i started learning basics of electronics from Lessons In Electric Circuits by Tony R. Kuphaldt
The books are very interesting but i could not find a technique so that i can implement what i learn from the books. Basically i am just going through the stuffs, and understanding them little by little. What i want to know is that what is the right way and effective way to learn electronics and electric circuit from your experience and i should i do now so that i can increase my learning speed so that i can achieve my goal.
While i was researching i came across topics such as mathematical modelling and modelling the quadrocopters first and them implementing them on real. How can i gain such knowledge to model something mathematically and implement such in real life? How much math and what areas of mathematics do i need to learn and how can i learn such? 
Now you have idea what i want to learn and achieve. Can you please suggest me a road map or steps i need to take to gain such knowledge and skill to develop myself, so that in near future i would be able to build such flying machines on my own. 
",quadcopter
Motion planning in the presence of drift,"Let’s assume at time $t$ a moving robot (e.g. PIONEER 3-DX), in order to get to the point at time $t+1$, changes it’s steering angle by a huge amount (e.g. 90 degrees).  In this situation, due to the increasing the centrifugal force with respect to the static force between the tires of the robot and the road, the mobile robot begins to ""drift"" and causing a difference between the predicted robot pose and the real robot pose at time $t+1$. Now the question is, apart from the robot localization methods(e.g. Particle filter), is there any other way to compute the amount of drift in position and orientation of a moving vehicle in a given surface? For example, an equation that given the robot parameters (e.g. mass, velocity, torque, and etc.) and the coefficient of static friction at time $t$, computes the amount of drift, or say slip ratio, at time $t+1$.
","mobile-robot, control, wheeled-robot, kinematics, robotc"
Tring to run 12 V DC geared motor using Samsung Li Ion ICR16850 batteries,"I am trying to run this motor. 
Using the batteries stated in the title. The motor requires 12 V and I am supplying 11.98V to the motor, through a motor driver. After a while, the motor keeps slowing down and the battery voltage drops down to 5-6 V, but after I remove the battery from the motor driver it again shows 11.9V.
Is this battery capable enough to run my motors, or do I need a new one?
","motor, battery"
Finding rotation quaternion,"I am trying to use a quaternions for robotics and there is one thing I don't understand about it. Most likely because I don't understand how to define position with quaternions and how to define rotation with quaternions if there is any difference..
Please watch my ""understanding steps"" and correct if I am wrong somewhere.
Lets assume I we have 2 vehicle positions described by 2 rotation quaternions:
$$
q_1 = w_1 + x_1i + y_1j +z_1k = \cos(\pi/4) + \sin(\pi/4)i
$$
This quaternion is normalized and represents rotation over the $x$ axis for $\pi/2$ angle as I understand it.
$$
q_2 = w_2 + x_2i + y_2j + z_2k = \cos(\pi/4) + \sin(\pi/4)k
$$
And this one represents rotation for the same angle $\pi/2$ over the $y$ axis.
$q_1*q_2 = q_3$ which would be the same rotation as if we made $q_1$ first and $q_2$ second. 
$$q_3 = \frac{1}{2} + \frac{i}{2} +\frac{j}{2} +\frac{k}{2}$$
QUESTION 1: Given $q_2$ and $q_3$ how can I find $q_1$?
QUESTION 2: How to find a rotation angle over some other vector, given rotation quaternion? For example I want to find on what angle does $q_3$ turned over $2i+j-k$ quaternion.
",kinematics
Dynamically detect changing obstacles,"So the idea is that there would be one robot acting as overwatch, which would detect all of the obstacles in an area (which are not necessarily static), and then send the data about the obstacles' positions to another robot that would navigate around the obstacles to a goal.
My initial thought was to have the overwatch robot be in an elevated position in the centre of the area, then sweep around using an ultrasonic sensor. This way, it could keep track of the obstacles in a set of polar coordinates (distance, angle). But then I realised that this method doesn't account for collinear obstacles.
So the question is, what is the best way to detect a bunch of non-static obstacles within an area?
As a side note, I have seen a system similar to this, where there was a robot detecting obstacles (in that case, a crowd of people) and another robot pathfinding around the obstacles (the people), but I'm unsure exactly how that system was detecting the obstacles.
","sensors, computer-vision, sonar, ultrasonic-sensors"
What microcontroller and parts will work for the robot project described bellow?,"I want my robot to contain about 20 MB of data or a little more, this will be mostly in text, some pictures so that it can recognize one or two particular objects.
So obviously, I want a camera connected to it and I want it to be able to save images (no need to display them, just save, I would access them through a PC later. However, I do want it to have an output text screen, i.e.: I want it to be able to display text on a screen, just plain text.)
I also want it to be controlled by speech recognition, so it has to be able to accept voice input. Also, iI want it to be capable of producing voice output, but only one or two prerecorded sounds. 
Lastly, an infrared sensor will be used for distance gauging and a push button one for contact. 
Summary of needs:
Microcontroller and parts required to:

store at least 20 MB of data, text (the microcontroller needs to be
able to access this text and run algorithms on it, GET data only, not
change it), and images (images only for image recognition)
allow saving images taken by the camera
speech recognition as input
output audio (only one or two prerecorded sounds) 
output plain text on a screen
equip an infrared sensor for measuring distance and contact
push buttons for detecting contact

If relevant, I am using tracked wheels, two DC motors, the robot will move upon detecting an object using the camera. I will decide what motor controller to use when one of you good fellows let me know what microcontroller would be adept for my wants. 
Note: for speech recognition input, I will use an advanced sound sensor, and for the infrared and push button sensors I obviously know what I'm using, same with the camera.
I only mentioned these ones so you would know what I need the microcontroller to be capable of to use. But as for the data storage and saving photos and screen, I have no idea what I can use as I have never used anything similar in a robot before.
","sensors, microcontroller, speech-processing"
What is the torque/force required to rotate the base of a robot arm?,"Take a look at the following simple example robot arm:

I want to know the torque required on that bottom motor to rotate the arm. Since it's not exactly rotating directly against gravity like the other joints I'm not sure how to analyze. Assume we know the mass of every part of the robot arm and the distances to the base.
For reference, I am planning on securing my base rotation motor's shaft through a ball bearing rotary table to the rest of the arm. I'm considering the torque required on that motor to properly pick the right component and make sure I get enough speed as well. So understanding how to analyze the forces would really help!
","motor, robotic-arm, torque, force, rotation"
Programming Robots with JavaScript,"As somebody who is spending the majority of his time programming in JavaScript, what's the best route to get into small-robotics without needing to deviate too much from my current language focus?
Are there any project kits or tools that make use of the JavaScript language that might make the field more approachable for developers like myself? I would even be interested in virtual environments where all code is executed in a simulation.
","software, programming-languages"
Pitch angle is either +90° or -90°,"Not exactly a robotics based question but mechanics is involved.
I have a wearable device that gives output in Quaternions which I can read serially via Labview. My task is to develop a threshold based fall detection system based on these values which I am not familiar with. 
Here is a sample data I read from the device
id: 4 distance: 1048 q0: 646 q1: -232 q2: -119 q3: 717 

I was able to find the Euler angles from the quaternions. I obtain a Rotation matrix from the Quaternion. From the rotation matrix, I derive the Roll, Pitch and Yaw. The coordinate system is North-East-Down. But my Pitch angle remains at positive or negative 90 degree. The fact is I didn't write the conversion code . I am attaching the
. 
Please have a look at the code and help me if you could
","gyroscope, matlab, labview"
What are good strategies for tuning PID loops?,"Tuning controller gains can be difficult, what general strategies work well to get a stable system that converges to the right solution?
","control, pid, tuning"
How to power this servomotor?,"I would like to use and test this type of servo motor, significantly stronger than the ones I've been using previously.
However the datasheets seem to be limited and I can't exactly tell how to use the driver.

Would it be fair to assume that to use this system, I would get say a 500 W (or higher) power supply and be able to plug into my (North American) wall outlet to use this motor? And if so, would that give me enough power? I believe NA wall outlets give something like 12 A @110 V max, or 1.32 kW. 
How could I tell if this is enough for the above servo, or might I have to upgrade to using an industrial power line? 
What if I want to power multiple of these motors at the same time? Surely 1 power outlet wouldn't work. 
[semi-related bonus question]  What is the difference between ""2 phase"", ""3 phase"" etc in this stepper/servo motor?

","power, servos, servomotor"
Area coverage of autonomous underwater robot,"I'm currently working on an autonomous underwater cleaning robot and would like your input on some navigation algorithms
Problem:
Wash the inside of big, open and filled tank, e.g. water storage tank (Figure 1), with an autonomous robot. The inside of the tank is divided in sections, such that the robot has a limited washing area. The washing are can be square, rectangular, parallelogram or other shapes. The inside of the tank may have unknown obstacles, e.g. inlets and outlets.
Since the robot will function under water, there are a few limitations when it comes to sensors.
Available sensors:

Wheel encoders
Distance measurements, e.g. ultrasonic, sonar, for obstacle avoidance
Boundary detection
Bumper with mechanical switch for collisions
Pressure sensor
Camera. Not necessary clean water so low visibility is an issue, i.e. difficult/impossible with visual odometry

Non-available sensors:

INS
GPS

Current solutions:

Random walk within boundaries (Figure 2). Either driving in straight lines (Figure 2), spirals (Figure 7) or a combination of lines and spirals (Figure 6)
Parallel swaths with 30% overlap (Figure 3). Requires cm precision on position estimate to guarantee coverage. Figure 4 shows an identical simulation but with one wheel radius 0.5mm larger than the other. This shows that it is not sufficient to only rely on wheel encoders for positioning as it will drift. With perfect positioning the parallel swath algorithm is 2x more efficient than random walk

One possibility is to add an acoustic navigation system, but it would be too expensive.
This problem is similar to the lawn mower and vacuuming robots. However, it seems like most of the products use random walk or a similar approach. Does anyone know a more efficient algorithm to cover the area based on the information provided?
For all the simulations, the red line represents the robot’s movement and the black is washed area. 
Looking at other similar questions I couldn't find the answers I've been looking for:

How to localise a underwater robot? 
Robot wire follower + how to position on wire 
Working of Autonomous Lawn mower(ALM) in an unbounded area without a perimeter wire 
What algorithm should I implement to program a room cleaning robot? 

Figure numbering:
1 2
3 4
5 6
7

References:

OmniClimbers: Omni-directional magnetic wheeled climbing robots
for inspection of ferromagnetic structures

","mobile-robot, algorithm, underwater, coverage"
Assigning parameters in perpendicular axes: D-H is a must,"So I was given a course assignment to assign frames and write D-H parameters for this robot using only 5+1 frames (with Frame $\{5\}$ at $P$ and Frame $\{0\}$ at $O$).

And I assigned them like this:

My question is:
From Frame $\{1\}$ to Frame $\{2\}$, what are joint distances $a$ and $d$?
The best answer I could get was 0. But obviously it should be zero for one axis and $a_1$ for the other. What's wrong?
I have read a similar question here. But the answer points me to another method which is impossible for me.
Edit:
No matter I put $a_1$ in $a$
$$(\alpha,a,d,\theta)=(-90^\circ,a_1,0,\theta_2-90^\circ)$$
or in $d$
$$(\alpha,a,d,\theta)=(-90^\circ,0,a_1,\theta_2-90^\circ)$$
The joint distance $a_1$ does not appear in $z$.
What it gave out is
$$\left( \begin{array}{cccc}
\sin{\theta_2} & \cos{\theta_2} & 0 & a_1 \\
0 & 0 & 1 & 0 \\
\cos{\theta_2} & -\sin{\theta_2} & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right)
\text{    or    }
\left( \begin{array}{cccc}
\sin{\theta_2} & \cos{\theta_2} & 0 & 0 \\
0 & 0 & 1 & a_1\\
\cos{\theta_2} & -\sin{\theta_2} & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right)$$
Obviously, $a_1$ should appear in $Z$-translation instead!
","dh-parameters, frame"
Localization of a Robot to find it Coordinates according to the Known Map,"I am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.
The robot I am trying to make is supposed to be used in rescue operations. The information I would know is the position of the person (the coordinates of the person in a JSON file that can be changed anytime except during the challenge) to be rescued from a building on fire. I would also know the rooms of the building from a map, but I don't know where the robot may be placed inside the building to start the rescue operation.
That means I have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. I can use gyroscope, accelerometer, magnetometer and ultrasonic sensors to do the localising job. I cannot use a GPS module or a camera for this purpose.
The object to be rescued (whose location is known in terms of coordinates & can be changed anytime) is surrounded by walls from 3 sides. Hence, adding more walls in this map.
According to my research particle filter is the best method used for localization of robot. But how can I deal with the landmarks (walls) that are fixed as shown in the map image and that are variable depending on the location of the object to be rescued being provided in the JSON file?
I can do the path planning from a known position to the target position, but I'm not sure how to determine the starting position.
More about JSON file:
(1) json file containing the coordinates of the object to be rescued can change. (2) it won't change during the challenge. (3) json file will be provided to me in an SD card that my robot has to read. I have successfully written the code that will allow the robot to read the json file and hence the coordinates of the object to be rescued.
Here is the map of the building which is known to me.

","localization, particle-filter"
how to manipulate arm robot?,"I want to make a robot arm with a gripper that can go to any x y z coordinates near it. the robot itself will be simple just servo motors and Arduino but I don't know how to make reach the desired coordinates, any ideas?
","arduino, robotic-arm, servomotor, manipulator"
Is there any robot stability control equation that describes the relation between velocity of the robot and its orientation?,"Let’s assume at time t a moving robot (e.g. PIONEER 3-DX) changes it’s steering angle by $25^{\circ}$. Obviously, in order to maintain stability and avoid overturning,  the robot must reduce its velocity between the time interval $t$ and $t+1$. But the question is by how much?
","mobile-robot, wheeled-robot, kinematics, industrial-robot, robotc"
How to move root locus to the left?,"I'm trying to control system that has 3 poles (2 in the right half plane)
I sketched root-locus but the two positive poles are going right and never be in the left half plane at any value of k
The state space of system is following
A =
0   1   0
-3.20205979037663e-08   0   -31.1556786564355
83333.3331597258    0   -173598.323255178

B = 
0
-3.11556786564355e-08
83333.3331597350

C = 
-0.999999999999890  0   0

D = 
0

How can I control system like this?
Note: I'm using matlab
","control, matlab"
FlySky FS-T6 loses trim settings,"I am working on a project involving custom made quadcopter using MultiWii flight controller.
I would like to share a problem I am currently facing with my transmitter(FlySky FS-T6).
I sometimes randomly lose my trim settings i.e. my Roll and Pitch values remain no more centerd at 1500.
This happens sometimes when I upload the MultiWii software, and sometimes the trim settings change automatically after the flight.
I don't have much clue what I am doing wrong here.
Do anyone of you have faced a similar problem or know how to resolve this issue?
","quadcopter, multi-rotor"
Object Detection and Pose Estimaion,"I am working on a robotics project for fun and can't really wrap my head around a solution to my perception problem. 
Setup:
My robot will have a stereo vision setup and will have to detect certain objects and align itself to those objects in a certain pose.  The robot will know what the width and height of those objects are.  The robot will be using a tx1 for computation, so implementation needs to be pretty fast.  Also, the environment's lightning will change a lot so using color for detection isn't a great option. 
My plan: To use convolutional neural networks to detect those objects of interest.  I have been able to program a network to detect those objects in 2D however, I am stuck on how to detect the pose of the objects in 3D.  My idea has been to detect the object with the neural network and once I have that region of interest, get the point cloud.  Then fit a 3D model using ICP.  Once the 3D model is fit I can get the pose of the object. 
I have also seen people using 3D correspondence grouping for this but would that work on a non dense point cloud that stereo vision generates?
I am pretty novice in this area and would love to get advice on from some more advanced robotics practitioners.  
Thanks for you time!
","mobile-robot, computer-vision"
On robocup 2d simulation league,"For some experiments, I need to have source code of a robocup 2d simulation team. Where can I find that? Is there any online test bed for this league for testing algorithms?
","simulation, soccer"
How can we solve the problem of robot size in sensor based motion planning?,"As you know in Bug algorithm there is a simplifying assumption that says the robot has no size and can fit between any arbitrarily small gap in the map. How can we overcome the challenge of robot size. As we don't have any per-calculated map (in sensor based navigation), can we still tackle the problem in configuration space?
For example, let's assume that the type of the sensor is Hokuyo URG-04LX Laser Rangefinder. Hence we can visualize the sensor measurements by the visualization matrix $V$:
\begin{equation}
V_i  =\begin{pmatrix}cos(\theta_i )* d_i, \quad sin(\theta_i )* d_i\end{pmatrix}\\
\end{equation}
Where $D= [d_1,d_2,\dotsc, d_n]$ is the set of distances, and $\theta_i$ can be calculated as:
\begin{equation}
\theta_i = \theta_{i-1} + {0.36}^\circ,\qquad \theta_1 = 0
\end{equation}
All the information we have about the robot's surrounding at each moment is $V$. I strongly believe that there is no well-formed formula which can define the robot size in this configuration, and also as we don't have any map, but a simple visualization, growing the obstacles by the radius of the robot size in the configuration space just doesn't make sense.
","mobile-robot, motion-planning, navigation, path-planning"
How to get started on making a dispenser for Indian spices?,"I want to make a dispenser system for Indian spices.

Dispense in multiple of teaspoons (5 ml) 
Handle powdered spices and small seeds like mustard/cumin

I ordered a spice carousel from amazon and was hoping I'll be able to add an actuator to it, but it needs a lot of force to click the dial.
I'm wondering that I would need something similar to sugar dispensers in coffee machines. To start with I want to design individual units, but eventually want to make a solution that can dispense 6 different spices.
I'll appreciate if anyone gives any idea on how to get started on this, thanks in advance!
","motor, robotic-arm, actuator"
What kind of torque is needed for a small 5-6 axis robotic arm?,"I'm new to robotics and I'm looking to make a 5-6 axis robotic arm out of stepper motors but I honestly don't know how much torque I should have for each part. Below I have described in more detail what my current plan is but I'm really not sure as to how much I really should be spending on each of these joints.
My general plan for this project was to make a arm that when fully extended would only be around 40-50(max) cm long. It would be consisted of light weight aluminum and I am hoping for it to weigh only a couple of pounds when done.  
Here is my current list of actuators for each of the joints:
(Bottom = 1, Top = 6)  

1st joint, (Nema 23 CNC Stepper Motor 2.8A)  
2nd and 3rd joints 
4th, 5th and 6th joints 

My real questions is, is this overkill or is it not enough for what I'm really trying to make. I really don't need it to be able to pick up a lot of weight, at most 1 to 2 kilos but I highly doubt I will ever be picking up anything more than that. Anyway I just wanted to see if this was sufficient enough for my project. 
","robotic-arm, stepper-motor, actuator"
Analytic evaluation of discrete LQR expected reward,"The discrete LQR problem seeks to control a linear system:
$$
x_{k+1} = A x_k + Bu_k + v_k
$$
for zero-mean Gaussian noise $v$.
The control is chosen to minimize the cost $J = \sum_{k=0}^K x^TQ x + u^T R u + 2x^T N u$. Solutions to the optimal control problem are well known.
Are there analytical expressions for the expected cost? For example, in the case where the initial state follows a Gaussian $x_o \sim \mathcal{N}(\mu, \Sigma)$?
",control
How to calculate Euler Angles from gyroscope output?,"I am using a tri-axis accelerometer and tri-axis gyroscope to measure the linear acceleration of a body. I need to get the orientation of the body in euler form in order to rotate the accelerometer readings from the body frame into the earth frame. Please help I'm so stuck
","accelerometer, gyroscope, frame"
Quadcopter takeoff when low throttle /w PID,"I'm making my own quad controller from scratch. I have fixed one axis on a ""pendulum"" so that only one axis can move freely ( + mode).
Let's say that I have 0 throttle. As you know PID output is added or taken away from throttle value on each motor.
My values of PID output are one instance let's say 5. (Quadcopter is laying at 5 degrees angle). Even in that case, one motor will spin at 5 speed, while other will be at -5 (won't move). 
I'm asking beacuse when I add throttle to my quad and if I angle it at 10 degrees, it bounces on the other side and then again back, untill I add at least half throttle, which is kida agressive and I can't imagine this scenario with 4 motors.
So is there any way I can compensate this values?
Thanks for the help! 
","quadcopter, pid"
I want to know whether the Create2 can interface a LIDAR?,"I want to know if the Create 2 can interface a laser radar?
For example an RPLidar.
","irobot-create, laser, lidar"
Making connections from Stepper Motor to Smoothieboard,"I have a smoothieboard and I just recently purchased a fair few amount of Stepper Motors for a project I am working on. Before buying these I have been playing around with another Stepper motor the Nema 17 which came with a 4 pin connector on the end of the wires. With the new motors I have bough which are higher torque, it doesn't come with a 4 pin connector installed. I need to hook them up to my smoothieboard and this is currently restricting me from doing so. The wires at the end of these motors are just loose and I am not sure where to purchase the 4 pin connectors I need. Thanks, any help is appreciated!
",stepper-motor
How to test how much current 9 servos can draw at a time,"Plus how should I use a multimeter to test it. 
P. S.  Really sorry for a noon question
","mobile-robot, servomotor, current"
Accelerometer deterministic errors,"I am working on MEMS accelerometers, and I want to understand the difference between, 

Cross axis sensitivity
Axis Misalignment
Non Orthogonality

In literature, people use it interchangeably. 
These physical parameters cause deterministic errors, which I want to correct during calibration
",accelerometer
Should a MG996R Servo's extreme position change over time?,"This question is a follow on from my previous question, Overheating/Jamming MG996 servo.
I have recently purchased my first ever servo, a cheap unbranded Chinese MG996R servo, for £3.20 on eBay.

After mounting the servo horn and the bracket, I realised that I had not mounted the horn in a tout a fait 0° orientation, rather the angle between the bracket and the servo side was approximately 20°. However, after switching the servo on and off a couple of times, with each time allowing the servo to perform, say, about 10 sweeps each time, I quickly noted that the servo’s extreme positions were changing over time, so that the initial extremes and then the extremes after about 5 on and off cycles, had changed by about 15°, so that now, 0° and 180° the bracket is now parallel with the body of the servo. 
I was quite surprised at this, as I had assumed that the 0° and 180° positions would be fixed, and not change over time, or vary each time that it was switched on and off.
Seeing as there should be a stop peg on the gear connected to the potentiometer inside, how is this even possible?
",rcservo
STM32_OC_Timing and IRQHandler,"I created a program to simple time base delay (in seconds). I have problem:
How to read a interrupt flag from channel 1 etc.?
When I use if(__HAL_TIM_GET_FLAG(&htim2, TIM_FLAG_CC1) != RESET) an error occurs.
When the interrupt occurred , uC should clear flag and set Blue LED in Discovery Board.
Here is my program:
Main.c
/* Includes */
#include ""stm32f3xx_hal.h""

/* Private variables */
TIM_HandleTypeDef htim2;

/* Private function prototypes */
void SystemClock_Config(void);
static void MX_GPIO_Init(void);
static void MX_TIM2_Init(void);

int main(void)
{

  /* MCU Configuration----------------------------------------------------------*/

  /* Reset of all peripherals, Initializes the Flash interface and the Systick. */
 HAL_Init();

  /* Configure the system clock */
  SystemClock_Config();

  /* Initialize all configured peripherals */
  MX_GPIO_Init();
  MX_TIM2_Init();

  /* Infinite loop */

  while (1)
  {
      HAL_GPIO_WritePin(GPIOE,GPIO_PIN_11,GPIO_PIN_RESET);
  }
}

/** System Clock Configuration*/

void SystemClock_Config(void)
{

  RCC_OscInitTypeDef RCC_OscInitStruct;
  RCC_ClkInitTypeDef RCC_ClkInitStruct;

  RCC_OscInitStruct.OscillatorType = RCC_OSCILLATORTYPE_HSE;
  RCC_OscInitStruct.HSEState = RCC_HSE_ON;
  RCC_OscInitStruct.HSEPredivValue = RCC_HSE_PREDIV_DIV1;
  RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;
  RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;
  RCC_OscInitStruct.PLL.PLLMUL = RCC_PLL_MUL9;
  HAL_RCC_OscConfig(&RCC_OscInitStruct);

  RCC_ClkInitStruct.ClockType = RCC_CLOCKTYPE_SYSCLK|RCC_CLOCKTYPE_PCLK1;
  RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;
  RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;
  RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV2;
  RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV1;
  HAL_RCC_ClockConfig(&RCC_ClkInitStruct, FLASH_LATENCY_2);

  HAL_SYSTICK_Config(HAL_RCC_GetHCLKFreq()/1000);

  HAL_SYSTICK_CLKSourceConfig(SYSTICK_CLKSOURCE_HCLK);

}

/* TIM2 init function */
void MX_TIM2_Init(void)
{

  TIM_ClockConfigTypeDef sClockSourceConfig;
  TIM_MasterConfigTypeDef sMasterConfig;
  TIM_OC_InitTypeDef sConfigOC;

  htim2.Instance = TIM2;
  htim2.Init.Prescaler = 7199; //72Mhz/7200 
  htim2.Init.CounterMode = TIM_COUNTERMODE_UP;
  htim2.Init.Period = 65535;
  htim2.Init.ClockDivision = TIM_CLOCKDIVISION_DIV1;
  HAL_TIM_Base_Init(&htim2);

  sClockSourceConfig.ClockSource = TIM_CLOCKSOURCE_INTERNAL;
  HAL_TIM_ConfigClockSource(&htim2, &sClockSourceConfig);

  HAL_TIM_OC_Init(&htim2);

  sMasterConfig.MasterOutputTrigger = TIM_TRGO_RESET;
  sMasterConfig.MasterSlaveMode = TIM_MASTERSLAVEMODE_DISABLE;
  HAL_TIMEx_MasterConfigSynchronization(&htim2, &sMasterConfig);

  sConfigOC.OCMode = TIM_OCMODE_TIMING;
  sConfigOC.Pulse = 20000; //0.0001[s] * 20000 = 2 [s] DELAY
  sConfigOC.OCPolarity = TIM_OCPOLARITY_HIGH;
  sConfigOC.OCFastMode = TIM_OCFAST_DISABLE;
  HAL_TIM_OC_ConfigChannel(&htim2, &sConfigOC, TIM_CHANNEL_1);

  sConfigOC.OCMode = TIM_OCMODE_TIMING;
  sConfigOC.Pulse = 30000;
  sConfigOC.OCPolarity = TIM_OCPOLARITY_HIGH;
  sConfigOC.OCFastMode = TIM_OCFAST_DISABLE;
  HAL_TIM_OC_ConfigChannel(&htim2, &sConfigOC, TIM_CHANNEL_2);
  HAL_TIM_Base_Start_IT(&htim2);

  HAL_TIM_OC_Start_IT(&htim2,TIM_CHANNEL_1 );
  //HAL_TIM_OC_Start_IT(&htim2,TIM_CHANNEL_2 );
}

/** Configure pins as 
        * Analog
        * Input 
        * Output
        * EVENT_OUT
        * EXTI
     PC9   ------> I2S_CKIN
*/
void MX_GPIO_Init(void)
{
  GPIO_InitTypeDef GPIO_InitStruct;

  /* GPIO Ports Clock Enable */
  __GPIOF_CLK_ENABLE();
  __GPIOC_CLK_ENABLE();
  __GPIOE_CLK_ENABLE();

  /*Configure GPIO pin : PC9 */
  GPIO_InitStruct.Pin = GPIO_PIN_9;
  GPIO_InitStruct.Mode = GPIO_MODE_AF_PP;
  GPIO_InitStruct.Pull = GPIO_NOPULL;
  GPIO_InitStruct.Speed = GPIO_SPEED_HIGH;
  GPIO_InitStruct.Alternate = GPIO_AF5_SPI1;
  HAL_GPIO_Init(GPIOC, &GPIO_InitStruct);

 /*
  * Configure GPIO pin : PE8 BLUE LED
  */

  GPIO_InitStruct.Pin=GPIO_PIN_8;
  GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;
  GPIO_InitStruct.Pull=GPIO_NOPULL;
  GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;
  HAL_GPIO_Init(GPIOE,&GPIO_InitStruct);

  GPIO_InitStruct.Pin=GPIO_PIN_12;
  GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;
  GPIO_InitStruct.Pull=GPIO_NOPULL;
  GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;
  HAL_GPIO_Init(GPIOE,&GPIO_InitStruct);

/*
 * COnfigure GPIO pin : PE11 GREEN LED
 */

  GPIO_InitStruct.Pin=GPIO_PIN_11;
  GPIO_InitStruct.Mode=GPIO_MODE_OUTPUT_PP;
  GPIO_InitStruct.Pull=GPIO_NOPULL;
  GPIO_InitStruct.Speed=GPIO_SPEED_HIGH;
  HAL_GPIO_Init(GPIOE,&GPIO_InitStruct);
}

low level implementation:
void HAL_TIM_Base_MspInit(TIM_HandleTypeDef* htim_base)
{
  if(htim_base->Instance==TIM2)
  {
    /* Peripheral clock enable */
    __TIM2_CLK_ENABLE();
    /* Peripheral interrupt init*/
    HAL_NVIC_SetPriority(TIM2_IRQn, 0, 0);
    HAL_NVIC_EnableIRQ(TIM2_IRQn);

  }

}

void TIM2_IRQHandler(void)
{

  /* USER CODE BEGIN TIM2_IRQn 0 */
  HAL_GPIO_WritePin(GPIOE,GPIO_PIN_8,GPIO_PIN_SET);

  // HAL_GPIO_TogglePin(GPIOE,GPIO_PIN_12);
  HAL_TIM_IRQHandler(&htim2); //THis function is implemented by StmCubeMX , WHAT IS THIS?
}

So how should my TIM2_IRQHandler look like? Each channel generate delay in +1 sec. When I am debugging this program, when LED is set the period is equal to 1s (time for set LED). 
","microcontroller, c"
Is it possible to detect road surface or navigation area using LRF data or other laser data in a Park?,"I would like to detect road surface in a park. In the park, only small grass are covered with the both side of the road. That means there is road and the both side of the road are covered by small grass.
Is it possible to detect road surface (not grass) using LRF data or other laser sensors? 
If not, why?
if yes, which is better-Camera or laser sensor?
","wheeled-robot, computer-vision, navigation, stereo-vision, laser"
MRPT - Solid red light in Kinect Xbox,"I am trying to build and install the MRPT tools in Debian 8 to use it with the Kinect Xbox v1. By now I can launch the sample codes from MRPT and they seem to work but the images from the Kinect (both RGB and depth) don't work properly. In fact when I connect the Kinect the green light blinks but when I run the code it changes to solid red. 
The calibration file I am using is the default from MRPT. I am working with Debian 8 and the libfreenect libraries.
I am not sure whether this could be a problem with the libusb. I have made one snapshot so you could see what I mean. 
The file is attached to this post.
Any clue about why Kinect could be failing?

","mobile-robot, computer-vision, kinect, c++"
Implementing pure pursuit problem in robot navigation?,"I would like to implement pure pursuit waypoints navigation. we know that,
look ahead distance=look ahead gain*vehicle forward velocity
How can I calculate look ahead velocity gain/look ahead gain? 
How can I calculate velocity profile for each waypoints?
","mobile-robot, wheeled-robot, navigation, line-following"
Why people use camera instead of laser sensor for robot navigation?,"I am working on Robot localization and navigation in urban environments. I want to use Camera. But I am a little bit confused about LRF data or other laser data.
Why people want to use camera?
why not LRF or other laser data?
Can anyone explain please in favor of Camera?
","computer-vision, stereo-vision, laser, lidar"
Estimating state of moving object from other moving object,"What is the best way to estimate the state 
[x-position;
y-position; 
heading (yaw angle); 
velocity; 
acceleration; 
curvature (or yaw rate)]

of a moving leading vehicle with sensors mounted on a follower/ego vehicle?
The following measurements of the leading vehicle are obtained via radar sensors mounted on the ego vehicle.


x-y-position in the ego coordinate frame 
heading in ego coordinates
relative and absolute velocity and acceleration


No information about curvature (yaw rate). This should be estimated which is possible using the lateral acceleration and longitudinal velocity.
For estimation I think of using EKF or nonlinear moving horizon estimation.
Considering that no prediction about the moving leader vehicle can be made because the control inputs are unknown. Only the measurments (update step) and the movements of the ego vehicle are available (incroporate in prediction).
What kind of model would be appropriate for the whole scenario?

just a model for the leading vehicle? (e.g. bicycle model)
just a model for the following ego vehicle?
or a combination of both vehicles?

Option 1 would be perfect in combination with a simple bicycle model if there was an outside observer who is not moving. Option 2 is not really an option because the configuration of the leader vehicle should be estimated. Option 3 seems to me the right way because of the following thoughts:
Looking from the ego coordinates: is it correct correct that a motion change of the ego vehicle will seem as if the measured locations of the leading vehicle changed? If so will I need a coordinate transformation or is it better to use a model in global coordinates, then transform the measurements (which are in ego coordinates)?. The approach using global coordinates seems counter intuitive because the final estimate should be used for the follower/ego vehicle as reference trajectory.
Can you give me a hint which coordinate frame (global or ego) to use, which model to use in the prediction step and if my thoughts on motion changes are correct?
Or do you know any sources that address this or a similar issue?

For the process model I thought of
x_k+1 = x_k + v_k * cos(heading_k)
y_k+1 = y_k + v_k * sin(heading_k)
heading_k+1 = heading_k
v_k+1 = v_k 
a_k+1 = a_k 
curvature_k+1 = curvature_k + a_y_k/v_x_k

of course there should be some process noise added, especially for heading, velocity acceleration and curvature because these measurements are rather inaccurate.
For heading, velocity and acceleration I would use the previous estimates (or measurements) since no other source except the measurements from the sensors are available.
Curvature is computed using the acceleration in (global?) y-direciton and the (global?) velocity in x-direction curvature = a_y/v_x
The measurement model looks probably something like this:
y = [1 1 1 1 1 0; 
     0 1 0 0 0 0; 
     0 0 1 0 0 0; 
     0 0 0 1 0 0;
     0 0 0 0 1 0;
     0 0 0 0 0 0]*x

where x is the state vector [x-position; y-position; heading (yaw angle); velocity; acceleration; curvature or (yaw rate)]

The trajectory must be smooth and should be exactly like the driven path of the leader vehicle. So I think some sort of estimation will be necesseray in order to avoid following the measurements in straight lines, which would lead to an unsteady trajectory.
The mesurements of the heading, velocity and acceleration have a rather high variance depending on the situation.
","mobile-robot, kalman-filter, movement"
Do I need the 5V to 3.3V level shifter?,"I would like to follow the instructions here, to allow myself to control the Create2 with a Raspberry Pi 3. 
However, this source, says that I need a level shifter to protect my Pi's circuitry from the Create's serial logic signal.

I'm fairly certain that I don't need the level shifter, since it looks like it's more just used to integrate the camera with the Create 2. But I am not in a rush to burn out either of these devices, so I'd like some verification.
","raspberry-pi, irobot-create, serial"
Switching from wall power to battery and back again,"I'm in the final phase of my BeerBot* project, and I'm looking for the best way to power it.
BeerBot will spend most of his life sitting in a docking station, keeping his battery charged and running off of wall power.  When called upon, he needs to be able to disconnect from the docking station and switch to battery power without interrupting power to his brain.  Upon returning to his docking station, he needs to switch back to wall power again.
I have a NiMH charge circuit, and I have a circuit for powering the robot from a wall outlet, I'm just not sure how to switch from one to the other without interrupting power.  For my testing thus far, I've just been shutting the robot down and manually switching between wall power and battery power, and connecting the battery to an external charger as needed.  This is obviously not a permanent solution, since BeerBot needs to be always at the ready - shutting him down to switch over to battery power every time I need a beer would defeat the whole purpose of BeerBot*.
So my question is, what is the best way to keep a robot powered from the wall, simultaneously charging the battery, and switching between wall power and battery power automatically when disconnected?  I guess this is analogous to the way a laptop works - switching to battery when unplugged and and keeping the battery charged while plugged in.
I don't care if it's a ready-made solution or a circuit I can build, I'm just looking for a good way to do this without frying my robot or setting my house on fire.  I'm mostly a software guy, so I know just enough about circuit design to be dangerous (and I've fried enough electronics to know that power supplies can be dangerous...).  I've been looking at power management ICs, but I'm thinking that might be overkill since I already have a charging circuit and a wall-power circuit, I just need a way to switch between them.  Can I just connect both and use a couple of diodes to keep current from flowing in directions it shouldn't, like this?

or do I need something more complex?  
.
*If you're curious, BeerBot is a semi-autonomous beer-seeking robot.  He sits quietly in the corner listening for commands.  Currently his only command comes from a modified Easy Button on my desk, which sends a wireless signal to BeerBot.  When he hears the signal, he wakes up, drives autonomously to the refrigerator, opens the door with his robotic arm, uses computer vision to detect a beer bottle (placed strategically on the bottom shelf where he can reach), grabs a cold one, closes the refrigerator door, brings the beer to my desk, and returns to his corner to await my next request.  Phase 2 will incorporate a bottle opener.
","battery, power"
Do multiple LIDAR systems in same area interfere?,"LIDARs use a pulse of light to measure distance, usually from the time of flight to reflection and back. With a collection of these measurements they can determine their surroundings in two or three dimensions. LIDARs are used as one of the sensor systems for self-driving cars, in addition to cameras and other radar systems.
Robotic cars are still in the testing phase, but at some point in the future we can expect a busy intersection filled with them, trying to navigate their way through it. With multiple scanners per car, and possibly multiple beams per scanner, interfering signal sources could go over a hundred even in smaller stages.
When time of flight is used to measure the distance to the reflection, the interfering signals would produce multiple ""distances"", and it would most likely require multiple scans of the same point to average some kind of a reliable value from all the noise.
How do LIDAR systems differentiate their own signals from other sources? With the example of robotic cars, could this interference lead to an error state where traffic could gridlock from lack of valid data? Or is this even a problem?
","sensors, lidar"
Complimentary filter issues,"I'm trying to implement the complimentary filter to get Euler angles using accelerometer and gyroscope data. Attached is the MATLAB code that I have along with a data set.
The data corresponds to moving the sensor from 0-90 degrees while attached to a goniometer. The sensor has an inbuilt algorithm that outputs Euler angles too and I'm trying to test the accuracy of this algorithm as it tends to overshoot the angle estimates.
The problem with the complimentary filter is that the angles move between (around) negative 40 and positive 40 degrees instead of changing between (around) 0-90 degrees.
Can anyone please point out what is wrong and why the complimentary filter isn't working well.
clc;
clear all;
close all;
M=importdata('Multiple_Sensors_747000.csv');
A=M.data;
[m n]=size(A);
a=1;
t=m/60;
angle=0;

for i=1:3723
    Acc(a,:)=M.data(i,6:8); % Reading Accelerometer data
    R_norm(a)=sqrt(Acc(a,1)^2+Acc(a,2)^2+Acc(a,3)^2); % Normalized accelerometer 
    Racc_norm1(a,1)=acos(Acc(a,1)/R_norm(a));         % Angle from accelerometer in X
    Racc_norm1(a,2)=acos(Acc(a,2)/R_norm(a));         % Angle from accelerometer in Y
    Racc_norm1(a,3)=acos(Acc(a,3)/R_norm(a));         % Angle from accelerometer in Z

    Gyroscope(a,:)=M.data(i,3:5);                     % Reading gyroscope data
    gyro(a,:)=Gyroscope(a,:)*(1/t);                   % Integrating gyroscope data
    r(a,1)=sum(gyro(:,1));                            % Angle from gyroscope data
    r(a,2)=sum(gyro(:,2));
    r(a,3)=sum(gyro(:,3));
    angle=0.99*(angle+(gyro(a,:)))+(0.01*Racc_norm1(a,:)); % Complimentary filter equation
    ang(a,:)=angle;
    a=a+1;
end

","accelerometer, gyroscope, matlab, filter"
Are there any advantages to using a LIDAR for SLAM vs a standard RGB camera?,"I've been thinking about building a small UAV with an onboard LIDAR, just for fun. I'm interested in SLAM and autonomous flight indoors and thought that I would need a lidar to get a 3D map of the environment. Now, I've spent some more time looking into SLAM techniques and have seen very impressive results with simple RGB cameras, not even necessarily stereo setups. For instance, these results of the CV group of TU Muenchen. They are capable of constructing 3D pointclouds from simple webcams, in real-time on a standard CPU.
My question: are there cases where you'd still need a LIDAR or can this expensive sensor be replaced with a standard camera? Any pros/cons for either sensors?
I'm going to list some pros/cons that I know/can think of:

LIDARs are better at detecting featureless objects (blank walls) whereas a vision-based SLAM would need some features. 
Using LIDARs would be computationally less intensive than reconstructing from video
The single RGB camera 3D reconstruction algorithms I found need some movement of the camera to estimate depth whereas a LIDAR does not need any movement.
Using a single camera for SLAM would be cheaper, lighter and possibly have a better resolution than a LIDAR.

","slam, computer-vision, stereo-vision, lidar"
"There seems to be so many robotic arms on the market, where can I get a list of pros/cons of each?","Doing some research on robotic arms and thinking about getting one, but I was wondering where can I get info on the pros/cons of each one and how they compare to each other. Another important factor is which one has the largest community following.
",robotic-arm
Seminal work in multi-robot task decomposition,"Multi-Robot task decomposition implies that there is a mission that needs multiple mobile robots like guarding the president's car with a team of drones. This high-level layman interpretation of the mission has to be broken down into algorithmic language. Basically the missions have to be decomposed into atomic tasks which robots can be instructed to do in terms of their existing abilities and intelligence.
Here are some papers i found on this problem:-
[1]
[2]
Can someone point some seminal papers on it that tackle the algorithmic aspects of this problem? Pointing to open source codes in any language will also be preferred.
Note there is a closely related problem of multi-robot task allocation which i am not referring to.
","mobile-robot, motion-planning, algorithm, multi-agent, reference-request"
Robotics with Kinect,"I want to learn robotics and really interested in making a robot based on Kinect sensor.
I see so many projects like this one, and just wondering how it works on top level. I downloaded Kinect SDK and did some basic tutorials, but I just don't think that Microsoft SDK is the library to use for real robotics projects. Any suggestions where to start and what library to use? Any good books in particular or online resources?
",kinect
Calculate input parameters of CTRV model,"I am using Constant Turn Rate and Veloctity (CTRV)  model to predict the position of the vehicle.The CTRV model assumes a circular path between two consecutive time steps of the car as depicted in the figure, where the vehicle moves from point A to point B. Also, the yaw rate and velocity are assumed constant between two timesteps.

The displacement between the two points is denoted by S and R is the radius of the curvature. The five input parameters of the CTRV model are [ x, y, yaw angle, velocity and yaw rate]. The previous x, y position of the car is known to me. The laser data of the surrounding environment is available for each timestep. With this laser data, I have already calculated the Rotation matrix and translation vector between two time steps using ICP. 
Now my question is how to calculate the input parameters of the CTRV model with the available data?
The followings things are clear to me: -

With the help of rotation angle I can calculate: - 
 Theta = Displacement (S) / Radius (R)
The velocity can be calculated using the timesteps :-
 Velocity = Displacement (S) / t
The yaw rate can be calculated using :-
 Velocity = Yaw rate * Radius (R)

Considering the first equation I am not sure how to calculate the displacement S if Radius is not known to me. 
Pardon me if my queries are a bit naive. I am still in the process of learning. Any help from your side will be much appreciated :)
","mobile-robot, slam, kalman-filter"
Cant get frame_id ar_track alvar pose msg,"I am trying to get pose of the camera and set it with respect to the world frame. I want to get the frame_id from msg, so that I can set multiple cameras with respect to world frame dynamically.
I am using Asus Xtion pro live, so I launch ar_track_alvar with pr2_indiv_no_kinect.launch.
This is what I have done,
Launch file,
<launch>
    <arg name=""marker_size"" default=""4.4"" />
    <arg name=""max_new_marker_error"" default=""0.08"" />
    <arg name=""max_track_error"" default=""0.2"" />
    <arg name=""cam_image_topic"" default=""/camera1/rgb/image_rect_color"" />
    <arg name=""cam_info_topic"" default=""/camera1/rgb/camera_info"" />    
    <arg name=""output_frame"" default=""/camera1_link"" />

    <node name=""ar_track_alvar"" pkg=""ar_track_alvar"" type=""individualMarkersNoKinect"" respawn=""false"" output=""screen"" args=""$(arg marker_size) $(arg max_new_marker_error) $(arg max_track_error) $(arg cam_image_topic) $(arg cam_info_topic) $(arg output_frame)"" />
</launch>

My ros Node,
#include <ros/ros.h>
#include <tf/transform_datatypes.h>
#include <ar_track_alvar_msgs/AlvarMarkers.h>
#include<tf/transform_broadcaster.h>
#include <tf/transform_listener.h>
#include<iostream>
#include <string>

void cb(ar_track_alvar_msgs::AlvarMarkers req) {

  tf::TransformBroadcaster tf_br;
  tf::TransformListener listener;
  static  tf::Transform transform;

  if (!req.markers.empty()) {
    tf::Quaternion q(req.markers[0].pose.pose.orientation.x, req.markers[0].pose.pose.orientation.y, req.markers[0].pose.pose.orientation.z, req.markers[0].pose.pose.orientation.w);
    transform.setOrigin( tf::Vector3(ceil(req.markers[0].pose.pose.position.x), ceil(req.markers[0].pose.pose.position.y), ceil(req.markers[0].pose.pose.position.z)) );
    transform.setOrigin( tf::Vector3(req.markers[0].pose.pose.position.x, req.markers[0].pose.pose.position.y, req.markers[0].pose.pose.position.z) );
    transform.setRotation(tf::Quaternion( req.markers[0].pose.pose.orientation.x, req.markers[0].pose.pose.orientation.y, req.markers[0].pose.pose.orientation.z, req.markers[0].pose.pose.orientation.w));

    try{

        // this doesn't prints the frame id.
        ROS_INFO(""req.header.frame_id . . . . . .. .  .. "", req.header.frame_id.c_str());

      if(req.header.frame_id.compare(""/camera1_link""))
      {
          ROS_INFO(""this gets printed . . "");
        // this works . . I mean string comparision returns true.
        // I want to set frame_id to the tf tree.

       //   listener.waitForTransform(req.header.frame_id, ""world"", ros::Time::now(), ros::Duration(1.0));
      //    tf_br.sendTransform(tf::StampedTransform(transform.inverse(), ros::Time::now(), ""world"", req.header.frame_id));

      }

    }
    catch (tf::TransformException ex){
      ROS_ERROR(""%s"",ex.what());
      ros::Duration(1.0).sleep();
    }
  }
}

int main(int argc, char **argv) {
  ros::init(argc, argv, ""camera_tf_pose"");
  ros::NodeHandle nh;
  ros::Subscriber sub = nh.subscribe(""ar_pose_marker"", 1, &cb);

  ros::spin();
  return 0;

}

Output of rosrun camera_tf_pose camera_tf_pose
[ INFO] [1475225608.355125575]: req.header.frame_id . . . . . .. .  .. 
[ INFO] [1475225608.355185064]: this gets printed . . 
[ INFO] [1475225608.454772325]: req.header.frame_id . . . . . .. .  .. 
[ INFO] [1475225608.454802236]: this gets printed . . 
[ INFO] [1475225608.555007653]: req.header.frame_id . . . . . .. .  .. 
[ INFO] [1475225608.555137160]: this gets printed . . 

Output of rostopic echo /ar_pose_marker
   markers: 
  - 
    header: 
      seq: 0
      stamp: 
        secs: 1475225585
        nsecs: 290621273
      frame_id: /camera1_link
    id: 14
    confidence: 0
    pose: 
      header: 
        seq: 0
        stamp: 
          secs: 0
          nsecs:         0
        frame_id: ''
      pose: 
        position: 
          x: 0.310138838061
          y: -0.0777276835864
          z: -0.00489581265903
        orientation: 
          x: 0.158053463521
          y: -0.431284842866
          z: 0.021097283333
          w: 0.888013170859

when I uncomment the following lines,
 //   listener.waitForTransform(req.header.frame_id, ""world"", ros::Time::now(), ros::Duration(1.0));
      //    tf_br.sendTransform(tf::StampedTransform(transform.inverse(), ros::Time::now(), ""world"", req.header.frame_id));

I get the following output:
[ INFO] [1475225923.155792267]: req.header.frame_id . . . . . .. .  .. 
[ INFO] [1475225923.155849162]: this gets printed . . 
Warning: Invalid argument passed to canTransform argument target_frame in tf2 frame_ids cannot be empty
         at line 122 in /tmp/binarydeb/ros-indigo-tf2-0.5.13/src/buffer_core.cpp
Warning: Invalid argument passed to canTransform argument target_frame in tf2 frame_ids cannot be empty
         at line 122 in /tmp/binarydeb/ros-indigo-tf2-0.5.13/src/buffer_core.cpp
Warning: Invalid argument passed to canTransform argument target_frame in tf2 frame_ids cannot be empty
         at line 122 in /tmp/binarydeb/ros-indigo-tf2-0.5.13/src/buffer_core.cpp
Warning: Invalid argument passed to canTransform argument target_frame in tf2 frame_ids cannot be empty
         at line 122 in /tmp/binarydeb/ros-indigo-tf2-0.5.13/src/buffer_core.cpp
Warning: Invalid argument passed to canTransform argument target_frame in tf2 frame_ids cannot be empty
         at line 122 in /tmp/binarydeb/ros-indigo-tf2-0.5.13/src/buffer_core.cpp

Please help me with this issue. Thanks in advance
","ros, c++"
Determine what the rotation axis is given a rotation matrix,"How do I find out around which axis the coordinate system has to rotate, if the rotation matrix is given?
$ {^{a}R_{b} } $ = $ \left(\begin{matrix} 0 & 1 & 0 \\ -1 & 0 & 0 \\ 0 & 0 & 1 \\\end{matrix}\right)$ 
$ {^{a}R_{c} } $ = $ \left(\begin{matrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\\end{matrix}\right)$ 
For $ {^{a}R_{b} } $ I thought, that it has to be a rotation around the z-axis, because 
$R(z,\theta) =  \left(\begin{matrix} cos(\theta) & -sin(\theta) & 0 \\ sin(\theta) & cos(\theta) & 0 \\ 0 & 0 & 1 \\\end{matrix}\right)$ 
the values at the positions $a_{13}, a_{23},a_{33},a_{32},a_{31}$ of $ {^{a}R_{b} } $ and $R(z,\theta)$ are identical.
So I solved $cos(\theta) = 0$ =>$\theta = 90° $ => 90° rotation around z-axis.
But how do I solve it, if there is more than 1 rotation, like for $ {^{a}R_{c} } $?  
",joint
Motor sizing: Should a slope be considered a continuous or peak condition?,"I have a 30kg robot that needs to climb a 20° slope, this is the ""worst"" situation it can be in, it can take 30+ seconds to get up the slope and may needs to be able to stop in the middle and start up again.
When sizing a motor should the operating conditions required to climb the slope be considered:

Continuous conditions, as it takes a while to go up, and to stop and start again in the middle of the slope?
Peak conditions, since this slope is the hardest obstacle the robot will encounter?

This is important because, in the range of power needed for the slope ( calculations yield 24Nm torque per motor taking into account 60% efficiency) motors are getting quite expensive and therefore I am trying to find something appropriate without completely oversizing for safety's sake.
","mobile-robot, motor, wheeled-robot"
Localising a robot placed at an unknown position in a known environment,"I am a third-year electrical engineering student and am working on an intelligent autonomous robot in my summer vacations.
The robot I am trying to make is supposed to be used in rescue operations. The information I would know is the position of the person (the coordinates of the person in a JSON file) to be rescued from a building on fire. I would also know the rooms of the building from a map, but I don't know where the robot may be placed inside the building to start the rescue operation.
That means I have to localise the robot placed at an unknown position in a known environment, and then the robot can plan its path to the person who has to be rescued. But, since this is not my domain I would like you to guide me on what is the best method for localising given that I can use an IMU ( or gyro, accelerometer, magnetometer) and ultrasonic sensors to do the localising job. I cannot use a GPS module or a camera for this purpose. 
I, however, do know how to do path planning.
As far as my research on the Internet is concerned I have found a method called ""Kalman filtering"" that maybe can do the localising job. But there are I think some other filtering methods as well. Which one should I use? Or is there any other simpler/better method out there of which I don't know yet?
I am also attaching the map of the building which is known to me.

Edit:
The terrain is flat, and I would like to know where the robot is on the map like at coordinate 0,4 etc.
","mobile-robot, localization, imu, accelerometer, gyroscope"
Inter-processor communication for robotic arm,"I'm building a hobby 6-DOF robotic arm and am wondering what the best way is to communicate between the processors (3-4 AVRs, 18 inches max separation). I'd like to have the control loop run on the computer, which sends commands to the microprocessors via an Atmega32u4 USB-to-??? bridge.
Some ideas I'm considering:

RS485


Pros: all processors on same wire, differential signal more robust
Cons: requires additional chips, need to write (or find?) protocol to prevent processors from transmitting at the same time

UART loop (ie, TX of one processor is connected to RX of next)


Pros: simple firmware, processors have UART built in
Cons: last connection has to travel length of robot, each processor has to spend cycles retransmitting messages

CANbus (I know very little about this)

My main considerations are hardware and firmware complexity, performance, and price (I can't buy an expensive out-of-box system).
","microcontroller, electronics, communication, arm"
How to construct rotary turn table (S/J1 axis) from ball bearing?,"For my robotics arm project I have recently come across the following ball bearing at low discount price which I think could make a great rotary table for my arm.
However it does not seem to have any mounting connectors, and the inner and outer rings are the same height. 
My mechanical creativity is a bit limited, but my goal is to drive (spin) it with a large stepper motor (I have 34 and 42 series available which, even ungeared, should be enough for my smaller arm). On top of the rotary table that I'm trying to create will sit a U-channel that will hold the rest of the robot arm. So in the bottom of that channel I will drill holes and connect it to... something.
If anyone could point me to, recommend, or create a simple design - I have access to basic metalworking tools and scrap aluminum - that would be absolutely fantastic. My main goal is to work with what I have: the bearing, stepper motors, aluminum, other things at a standard hardware store.

BTW, here is how I refer to the standard robot axes (of a 6-DOF industry bot) by name.
","robotic-arm, industrial-robot, rotation"
Determining pose from ar_track_alvar message in ROS,"I am using the ar_track_alvar package in Indigo to detect AR Tags and determine their respective poses. I am able to run the tracker successfully as I can visualize the markers in RViz. I give the following command to print the pose values

rostopic echo /ar_pose_marker

and I get the following output indicating that the poses are determined.

header: 
  seq: 0
  stamp: 
    secs: 1444430928
    nsecs: 28760322
  frame_id: /head_camera
id: 3
confidence: 0
pose: 
  header: 
    seq: 0
    stamp: 
      secs: 0
      nsecs: 0
    frame_id: ''
  pose: 
    position: 
      x: 0.196624979223
      y: -0.238047436646
      z: 1.16247606451
    orientation: 
      x: 0.970435431848
      y: 0.00196992162831
      z: -0.126455066154
      w: -0.205573121457


Now I want to use these poses in another ROS node and hence I need to subscribe to the appropriate ROS message('ar_pose_marker""). But I am unable to get enough information on the web on the header files and functions to use in order to extract data from the published message. It would be great if somebody can point to a reference implementation or documentation on handling these messages. It might be useful to note that ar_track_alvar is just a ROS wrapper and hence people who have used ALVAR outside of ROSmay also give their inputs.
UPDATE:
I tried to write code for the above task as suggested by @Ben in the comments but I get an error. The code is as follows
#include <ros/ros.h>
#include <ar_track_alvar_msgs/AlvarMarker.h>
#include <tf/tf.h>  
#include <tf/transform_datatypes.h>

void printPose(const ar_track_alvar_msgs::AlvarMarker::ConstPtr& msg)
{   
    tf::Pose marker_pose_in_camera_;    

    marker_pose_in_camera_.setOrigin(tf::Vector3(msg.pose.pose.position.x,
                             msg.pose.pose.position.y,
                             msg.pose.pose.position.z));

}

int main(int argc, char **argv)
{
    ros::init(argc, argv, ""pose_subscriber"");

    ros::NodeHandle nh;

    ros::Subscriber pose_sub = nh.subscribe(""ar_pose_marker"", 1000, printPose);

    ros::spin();

    return 0;

}

And I get the following error
/home/karthik/ws_ros/src/auto_land/src/pose_subscriber.cpp: In function ‘void printPose(const ConstPtr&)’:
/home/karthik/ws_ros/src/auto_land/src/pose_subscriber.cpp:17:53: error: ‘const ConstPtr’ has no member named ‘pose’
    marker_pose_in_camera_.setOrigin(tf::Vector3(msg.pose.pose));
                                                     ^
make[2]: *** [auto_land/CMakeFiles/pose_subscriber.dir/src/pose_subscriber.cpp.o] Error 1
make[1]: *** [auto_land/CMakeFiles/pose_subscriber.dir/all] Error 2
make: *** [all] Error 2

Any suggestions?
","ros, pose"
Using an AR.Drone's IMU to perform PDR,"My task is simple: I want to move my drone manually around a room (IMPORTANT: I DO NOT WANT TO FLY IT) and I want to see its position update on a map using IMU data. 
I've attempted something in ROS and Rviz, but the position on the map does not update when I pickup my drone from spot A and place it 5 meters away at spot B. It does however update when I fly the drone from A to B.
My question is this : is there someway I can do dead reckoning using the drone while not flying it? Or is flying the drone necessary to perform dead reckoning using the drone's IMU?
EDIT: Here's what my Rviz shows when I pickup the drone from A and put it at B. As you can see, orientation is updated, but the position remains the same. However, when I fly the drone, I can see the red arrow move around the map.

Any help would be greatly appreciated!
EDIT : The duplicate question just asks for general solutions. Here I've found a solution, I just can't get it to work; I need help debugging.
","quadcopter, localization, imu, dead-reckoning"
Bilateral Stabilization,"I'm starting out in more complex robotics, which in this case includes the bilateral stabilization of a humanoid robot. 
I know that when a human walks, his/her pelvis moves in correspondence, which allows for a basic stabilization when being on one leg in between steps.
I'm wondering about how to replicate this type of effect in a humanoid robot.
Thank you for your time!
",humanoid
ROS : Performing 2-D Pedestrian Dead Reckoning on an AR Drone and displaying location on a floorplan,"I'm trying to find a way where I can estimate the location of my drone on a floorplan. Note that right now, I will just be moving the drone around manually and not flying it.
I read up on PDR and what I want to do is this:
Provide an initial location of my drone on the floorplan, and as I move the drone around, using information from the IMU/accelerometers, I want to update the position of my drone on the floorplan.
I've worked with ROS a bit and I want to know if there are packages in ROS that could do this. For now, I'm looking for rough estimates and not perfect solutions.
Thanks!
","localization, ros, dead-reckoning"
The SVD dont give me correct Result?,"I work actualy in RGBD Visual Odometry and I have used for this the Dataset of TUM. I used ORB features Extractor and then I match the features between frames. After that I use SVD to recover Rotation Matrix and translation vector. Finnaly, I recover pose by this Equation Pos = R*Pos + T;
But result is very aleatory like (0,0,0) then (-314,-1000,5) ---(500,8,600 )
Here is the Code:
Matx33f R; //Rotation
Matx31f T;  //Translation
cv::Matx31f meanX(0, 0, 0), meanY(0, 0, 0);
BGR1 = imread(""C:/Users/Madali/Downloads/rgbd dataset freiburg2 pioneer 360/rgbd_dataset_freiburg2_pioneer_360/rgb/"" + *it, CV_LOAD_IMAGE_ANYCOLOR);
depth = imread(""C:/Users/Madali/Downloads/rgbd dataset freiburg2 pioneer 360/rgbd_dataset_freiburg2_pioneer_360/depth/"" + *itdepth, CV_LOAD_IMAGE_ANYCOLOR);
cvtColor(BGR1, Gray, CV_RGB2GRAY);
orb->detectAndCompute(Gray, noArray(), features, descriptors_1);

//drawKeypoints(BGR1, features, BGR1, Scalar(25, 147, 90));
depth2 = imread(""C:/Users/Madali/Downloads/rgbd dataset freiburg2 pioneer 360/rgbd_dataset_freiburg2_pioneer_360/depth/"" + *(itdepth+1), CV_LOAD_IMAGE_ANYCOLOR);

BGR1 = imread(""C:/Users/Madali/Downloads/rgbd dataset freiburg2 pioneer 360/rgbd_dataset_freiburg2_pioneer_360/rgb/"" + *(it+1), CV_LOAD_IMAGE_ANYCOLOR);
cvtColor(BGR1, Gray, CV_RGB2GRAY);
orb->detectAndCompute(Gray, noArray(), features2, descriptors_2);

matcher->match(descriptors_1, descriptors_2, matches);

double max_dist = 0; double min_dist = 100;

// --Quick calculation of max and min distances between keypoints
for (int i = 0; i < descriptors_1.rows; i++)
{
    double dist = matches[i].distance;
    if (dist < min_dist) min_dist = dist;
    if (dist > max_dist) max_dist = dist;
}

//-- Draw only ""good"" matches (i.e. whose distance is less than 2*min_dist,
//-- or a small arbitary value ( 0.02 ) in the event that min_dist is very
//-- small)
//-- PS.- radiusMatch can also be used here.
std::vector< DMatch > good_matches;

for (int i = 0; i < descriptors_1.rows; i++)
{
    if (matches[i].distance <= max(2 * min_dist, 0.03))
    {
        good_matches.push_back(matches[i]);
    }
}
cv::Mat1f X(good_matches.size(), 3); //N*3 matrix of points
cv::Mat1f Y(good_matches.size(), 3);; //N*3 matrix
//-- Draw only ""good"" matches

for (size_t i = 0; i < good_matches.size(); i++)
{

    float Z = depth.at<float>(features[good_matches[i].queryIdx].pt.y,
        features[good_matches[i].queryIdx].pt.x)/5000;
    (Z>5) ? (Z = 0) : X(i, 2) = Z;

    X(i, 2) = Z;
    X(i, 0) =Z*(features[good_matches[i].queryIdx].pt.x-cx)/fx;
    X(i, 1) = Z*(features[good_matches[i].queryIdx].pt.y-cy) / fy;
    //cout << Z << "" "" <<cx << endl;

    Z = depth2.at<float>(features2[good_matches[i].trainIdx].pt.y,
        features2[good_matches[i].trainIdx].pt.x) / 5000;
    (Z>5) ? (Z = 0) : Y(i, 2) = Z;

    //cout << Z << "" "" << cx << endl;

    Y(i, 0) = Z*(features2[good_matches[i].trainIdx].pt.x-cx)/fx;
    Y(i, 1) = Z*(features2[good_matches[i].trainIdx].pt.y-cy)/fy;
    Y(i, 2) =Z;
    line(BGR1, features[good_matches[i].queryIdx].pt, features2[good_matches[i].trainIdx].pt
        , Scalar(0, 255, 0), 2);
}


for (int i = 0; i < X.rows; i++)
{
    meanX(0) += X(i, 0);
    meanX(1) += X(i, 1);
    meanX(2) += X(i, 2);

    meanY(0) += Y(i, 0);
    meanY(1) += Y(i, 1);
    meanY(2) += Y(i, 2);
}

meanX *= 1.0f / X.rows;
meanY *= 1.0f / X.rows;

for (int i = 0; i < X.rows; i++){
    X(i, 0) -= meanX(0);
    X(i, 1) -= meanX(1);
    X(i, 2) -= meanX(2);

    Y(i, 0) -= meanY(0);
    Y(i, 1) -= meanY(1);
    Y(i, 2) -= meanY(2);
}
cv::Mat1f A;

//Rotation 
A = Y.t()*X;
cv::SVD svd(A);
cv::Mat1f Rmat;
Rmat = svd.vt.t()*svd.u.t();

if (determinant(Rmat) < 0)
{
    Rmat.at<float>(0, 2) *= -1;
    Rmat.at<float>(1, 2) *= -1;
    Rmat.at<float>(2, 2) *= -1;
}
Rmat.copyTo(R);
//Translation

T = (-R*meanY )+ meanX;
system(""cls"");
Pos = R*Pos + T;
cout << Pos << endl;

","computer-vision, kinect, odometry"
Choosing motor to buy after calculations,"I am building a two wheeled robot which will have:

max speed of 10km/h and acceleration of 0.5m/s²
can climb 12° slopes
weighs at most 30 kg 
wheel diameter is 50cm

By doing the calculations myself, and checking with many online automatic calculator I find that in the ""worst"" possible conditions I would need :

Around 9.5Nm of torque
Around 100Rpm for speed
Which results in approximately a 100W motor.

When I look for suitable motors online I come across a lot of sites recommending different motors depending on robot weight.
An example would be.
Where this kind of motor is recommended for up to 150lbs robots and it is a 30-50W motor (I believe they use 4 of them).
Is the power I'm looking for in my case too ""overkill""? 
Seeing as this is an indoor robot but which will need to be able to go up slopes to reach different areas, do I need to find a motor that corresponds exactly to my numbers or do I need to get something even more powerful since efficiency is never 100%? 
","mobile-robot, wheeled-robot"
Rotation matrix sign convention confusion.,"In rotation matrix, Why do we rotate the first and third rotation in the opposite direction of the 2nd rotation, this is confusing. 
Image is attached with this.

In this image we can note that for x and Z rotation non zero elements are same. But for Y rotation sign of sin(theta) changed. 
why is so?
","inverse-kinematics, rotation"
Inexpensive lidar/radar rangefinder good up to 100m?,"I've been looking around for a lidar/radar rangefinder for a few months now and can't seem to find anything reasonable for my application.  My requirements are pretty straightforward:

100m range
I2C or other serial protocol
Accuracy +/- 0.25m
Sampling > 50hz

This isn't a spinning/mapping application, so sampling rate isn't very demanding, and I don't think I'm asking much regarding accuracy, but I can't find anything for less than $600.  It seems like there's a bit of a hole in the market?
The closest ive come is the lidar lite from pulsed lite (now Garmin), but its only good up to 40m, and I've had one backordered for 4 months now!  If they offered this product in an 80 or even 60m application it would help me, but 40m isnt quite enough.
I would assume the technology/demand just isn't out there that anyone is cheaply Producing this device, but I see lots of lidar based rangefinders for golf courses/gun sitting/etc that are around $100 in price and work in these ranges.  What's different about these technologies? Example.  What technology does this use and are there sensors out there that I can buy direct?
Thanks
Also found this, but seems specific to altimeters. Is there something about this device that would only make it applicable for drone altimeter applications?  I know there must be considerations for the incoming environmental light and reflectiveness of the surface it's detecting,so possibility it wouldn't work in an outside application where environmental lighting will vary?  And also this device doesn't call itself 'lidar', so let's ask a very basic question:
Are all laser rangefinders considered 'lidar'?  Surprisingly I haven't found this answer.
","laser, lidar, rangefinder"
Gauss Newton Method for Accelerometer calibration,"I am following the code obtained in matlab file exchange for the paper 
http://ieeexplore.ieee.org/document/4655611/
He calculates the Hessian matrix as follows using Jacobian Matrix J,
**H = inv(J'*J); % Hessian matrix,** 

How is this relation true. 
And also the g value used is 1 to construct the COST function to be minimized using LEAST SQUARES , but should'nt it be the local gravity value at the particular place you are carrying out the calibration?? 
Link to the code :
MATLAB Code 
Please throw some light on this. 
Thank You.
","accelerometer, calibration"
How much weight can the iRobot carry?,"I'd like to attach some extra equipment to the iRobot. Does anyone have any idea of a weight limit before mobility is restricted or it just breaks? Could it possibly carry a few extra pounds? 
",irobot-create
CC3D PWM control signal characteristic (to be simulated by Raspberry PI),"My goal is to control drone by Raspberry PI. The Raspberry PI uses camera and OpenCV, sends control commands to AVR microcontroller which will generate the PWM control signal. Meaning that it will simulate pilot with transmitter-receiver setup.
In other words (to make it more clear). Raspberry tells the Atmega8 that the drone needs to go more forward. Atmega8 generates custom PWM signals on 8 pins. Those signals are sent directly to CC3D pins responsible for roll, pitch etc. Atmega8 replaces controller receiver in this setup. It generates signal not based on user input but on what Raspberry tells it.
In order to do that I need the parameters (period, voltage etc.) of the PWM signal that CC3D accepts to properly simulate it. I have found this topic:
CC3D - Replacing RC emitter with an RPi
He has the same problem as I do and he found the solution. Unfortunately I can't send pm and I can't comment because I'm new to the site... so basically there is no way for me to contact him.
So any help would be appreciated.
","quadcopter, arduino, raspberry-pi, uav, avr"
Stabilizing a Drone,"I have my IMU and I can get attitude (pitch, roll, yaw) as well as gyro (x, y, z)
As far as I can tell, attitude is all I need to stabilize my drone.
Is there any reason to implement gyro or will attitude suffice?
-- EDIT --
It seems the benefit of gyro is an immediate response to an outside factor effecting drone stability.
Example:
A gust of wind hits your drone. The attitude will not be quick enough combat the effects of wind. As far as I can tell, the gyro will be the best way to combat extreme wobble and outside influences such as wind / tether (leash).
","quadcopter, imu"
Introducing new particles in particle filters for localization,"Standard particle filters can produce bad localization result if the initial particle generation step produces no particle that is close to location (and bearing) of tracked object. The accuracy depends on large number of particles to create at least one particle that's very close to state of tracking object.
Could we introduce in resampling stage a small number of completely random new particles? For example, 99% of particles are randomly selected with weighted probability, while 1% are new particles with random state.
My reasoning is that new particles that are bad guesses would quickly disappear, while good guesses would improve accuracy beyond what was possible with fixed particle pool. Does this improvement to particle filters make sense?
","localization, particle-filter"
What are some ways to estimate torque/load on a robotic arm joint,"My main problem is measuring joint torque/load economically. There are ways to detect the electric current drawn which is proportional to the torque. Since my motor is a BLDC motor, there will be a driver which drives the 3 phases and hence  the current drawn will also include losses, which would be variable. Is there a proper way to estimate the load by sensing current?
Is there any way to reliably get the torque/load on a motor joint, what are the common ways to do so without spending a ton of money on torque sensors?
Do to link me to any papers/research on this subject, if you'd want to.
I need to judge the load on the joints of a 6 dof robotic arm programmatically and detect unexpected impacts.
","motor, robotic-arm, brushless-motor, torque, current"
Why are robots made to mimic humans actions? Isn't it better to make them better than Humans?,"Robots can be better than humans. If we attach a wheel to a robot, it can move faster than a human with legs.
But why then is a lot of research going on to make robots look, walk and talk like humans. Wouldn't it be better to make robots better than humans and not to be limited by the human body parameters?
","research, humanoid"
why is quadrotor motion planning hard?,"With introduction of incremental sampling algorithms, like PRM and RRT planning in higher dimensional spaces in reasonable computation time has become possible though it is PSPACE-hard. But why is a quadrotor motion planning problem still difficult even with simplified quadrotor model? 
I was solving a dynamic car problem with OMPL, which produced solution within 10s but I set a planning time of 100s for quadrotor, but it still does not find a solution.
","mobile-robot, quadcopter, motion-planning, planning, rrt"
LIDAR Points as Landmarks,"I am currently trying to implement a GraphSLAM/SAM algorithm for LIDAR. From papers I've read, you generate a directed graph from expected LIDAR measurements to landmarks similar to the image below (taken from the Square Root SLAM Paper by Dellaert).

However in practice the point clouds you obtain from LIDAR are similar to this (taken from the KITTI car collected dataset):

It seems algorithms such as SIFT for 3D point clouds aren't as accurate yet. Is there a commonly used technique to efficiently find features in consecutive point clouds to find landmarks for SLAM algorithms without using >30,000 points in a point cloud?
","localization, slam, lidar"
Need super-basic help with motor encoder,"So I have this motor: https://www.servocity.com/23-rpm-hd-premium-planetary-gear-motor-w-encoder
and this motor shield for my arduino uno:
https://www.pololu.com/docs/pdf/0J49/dual_vnh5019_motor_driver_shield.pdf
And no background in electronics to speak of. I cannot find a basic enough resource to tell me where to attach the wires, or really what they each do. The ones that are simple enough for me only deal with motors with a couple of wires. Information on motors with six wires go immediately to discussing things that are way beyond me, or at least use terms that I don't know.
So the motor has six leads, and the only info I can find on them in the documentation are this: Black (Motor -), Red (Motor +), Green (Ground), Brown (Channel B), Yellow (Channel A), and Orange (Sensor Voltage +).
I have the black and red hooked up to M1A and M1B on my shield, so I am able to turn the motor on, vary its speed and direction. I also had all the other wires hooked up to various pins mentioned in the Demo sketch associated with the shield.
I have spent a week trying to figure this out on my own, but I am not getting anywhere (except that I know that hooking up orange to almost anything triggers a fault).
So in addition to the four wires that are not connected to anything, I have these pins mentioned in the sketch that are also not connected to wires: D2 M1INA, D4 M1INB, D6 M1EN/DIAG, D9 M1PWM and A0 M1CS.
I hope someone can provide a pointer to a resource or a good, plain-language explanation for what these wires and pin descriptions mean. Thanks in advance!
servocity
","arduino, motor, encoding, wiring"
Irobot Create 2 drawing power from Pi,"I am controlling an IRobot Create 2 with a raspberry pi, which I am powering with one of these:
https://www.adafruit.com/product/1566
(I know, I know, I should have just gotten power from the battery thats already in the roomba)
Anyways.  I am trying to read electric current values flowing through the roomba as it cleans.  I should be getting negative values (which I did previous to using the battery).  My code was working, although I know that doesn't mean much.
After much thought, I was wondering if maybe the roomba is drawing power through the serial port from the raspberry pi, giving it a slight charge even though it is not on the charger.
Is this possible, and is there a way to confirm or deny my suspicions?
Any help is appreciated.
","raspberry-pi, irobot-create, battery, serial"
How has the Mars Exploration Rover (MER) has been working for 11 years if it was designed for 90 Sol?,"The Mars Exploration Rover (MER) Opportunity landed on Mars on January 25, 2004. The rover was originally designed for a 90 Sol mission (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.
How it has been working for 11 years? Can anyone explain hardware-related aspects and how smart this Rover is ?
I need to know how this rover is getting services on Mars regarding to hardware related issues?(if any hardware is not working properly, how it can be repair on the Red Planet?)  
","control, robotic-arm, microcontroller, wheeled-robot, mechanism"
Quadcopter PID Tuning for Altitude Hold/Position Hold along Z axis,"Good day, I have just finished tuning the Pitch and Roll PID's. I did this by setting the throttle such that the quad is weightless. I did the tuning of each axes separately.
I would just like to ask what is the best way to tune the PID for maintaining an altitude setpoint.

Is it best to turn off the Pitch and Roll PID controllers while tuning the altitude PID or is it best to have them already active while tuning the latter controller?

I am going to use a cascaded PID controller using the Velocity along the z-axis calculated from the accelerometer output for the inner PID loop (150Hz) and the altitude measurement of the HC-SRO4 ultrasonic sensor (20Hz) for the outer PID loop.
","quadcopter, pid, stability, real-time, sonar"
The velocity profile of my robot is fluctuating,"I am presently doing a robotics project. I am using USARSIM (Urban Search and Rescue Simulation) to spawn a robot. I am trying to create different behaviors, like: 

goal following behavior; 
obstacle avoidance behavior, and; 
wall following behavior for my robot. 

I first generate the robots in USARSIM. Then I specify a goal location to the robot and provide it with a speed. The robot then moves to the goal location at the specified speed. USARSIM provides me the (x, y, z) coordinates of the vehicle at every time stamp. Based on the the coordinates received, I am trying to calculate the instantaneous speed of the robot at every time stamp. The instantaneous speed graph is fluctuating a lot. 
In a specific case, I am providing the robot with 0.2 m/s. The velocity profile is shown below. I am unable to understand the reason behind it.

Here are some observations that I have made. 

As I increase the speed of the robot, the variations are decreasing.
Suppose, I provide a straight trajectory to the robot, it doesn't follow the straight trajectory. Does it explain why my velocity profile is fluctuating a lot ? 


Please let me know if any one can provide me a possible explanation for the variance in my velocity profile.
",mobile-robot
Servo motor eventual shake,"I've been developing an inspection rover with two miniature cameras on two dual axis gimbals.  I've tried analog, digital and now brushless servos from various R/C parts makers and all of them eventually develop a flutter or shake.  I am getting ready to step up to a more expensive servo that will be almost 30X more expensive than the typical $30 R/C servos I started with.  The loads seem to be easily controlled at first but after a few weeks the shaking starts and just gets worse until I replace the servo.  I've tried two different R/C controllers and that hasn't helped and I can't seem to control by programming my Spektrum DX-9 controller.  Any suggestions?
",servomotor
Qi Battery Pack,"I am trying to build a bot that is ""always on"". I will be using a qi platform and am looking for a battery that charges through Qi. I've found several batteries that can charge your phone through Qi, but I am looking for one that charges itself through Qi. Any clue where I can find one like that? As a second question, do you think such an idea would work? Any foreseeable problems?
",battery
ROS/Gazebo: URDF Error: No valid hardware interface element found in joint,"I am fairly new to ROS and Gazebo. I have posted this question on the ROS/Gazebo forums, but they appear to be dead. 
I am using ros-kinetic and gazebo7.3 on Ubuntu 16.04. I have been following this tutorial (Tutorial) and have adapted it slightly for my own vehicle (4 wheels instead of 2, skid drive plugin instead of diff drive).
I receive errors when running roslaunch on my own project, and I also get the same errors when using the same command on the completed project provided by the creators of the tutorial, Complete Project Github. I assume that means that I am missing something in ros or Gazebo.
Running:
roslaunch jaguar4x4_gazebo jaguar4x4_world.launch
gives the following errors (repeated for each of the four joint and trans):



[ERROR] [1472045218.311147687, 0.142000000]: No valid hardware interface element found in joint 'left_back_wheel_hinge'.

[ERROR] [1472045218.311359894, 0.142000000]: Failed to load joints for transmission 'left_back_trans'.

Also: 

[ERROR] [1472045218.646822285, 0.365000000]: Exception thrown while initializing controller leftfrontWheel_effort_controller. Could not find resource 'left_front_wheel_hinge' in 'hardware_interface::EffortJointInterface'. [ERROR] [WallTime: 1472045219.647578] [1.363000] Failed to load leftfrontWheel_effort_controller [INFO] [WallTime: 1472045219.647938] [1.363000] Loading controller: leftbackwheel_effort_controller [ERROR] [WallTime: 1472045220.651591] [2.366000] Failed to load leftbackwheel_effort_controller

This error is repeated for each of the 4 wheels.
I have seen similar error messages posted online, however, the solutions don’t seem to fix mine (I can't post any more links due to my reputation level).
(1). I already use the hardwareInterface tags
(2). I have already installed ros_control
If anyone has any ideas on how to fix these errors I would appreciate it. 
EDIT -------------------------------------------------------------------
I think maybe there is an issue with the way that I have created my joints and the use of the hardwareInterface tags.
In my macros.xacro file I create my joints and link the transmission separately (I repeat this twice for the front and back wheels): 

   <joint name=""${lr}_front_wheel_hinge"" type=""continuous"">
      <parent link=""chassis""/>
      <child link=""${lr}_front_wheel""/>
      <origin xyz=""${+wheelPos-chassisLength+2*wheelRadius} ${tY*wheelWidth/2+tY*chassisWidth/2} ${wheelRadius}"" rpy=""0 0 0"" />
      <axis xyz=""0 1 0"" rpy=""0 0 0"" />
      <limit effort=""100"" velocity=""100""/>
      <joint_properties damping=""0.0"" friction=""0.0""/>
    </joint>


  <transmission name=""${lr}_front_trans"">
    <type>transmission_interface/SimpleTransmission</type>
    <joint name=""${lr}_front_wheel_hinge"" /> 
    <actuator name=""${lr}_front_Motor""> 
      <hardwareInterface>EffortJointInterface</hardwareInterface>
      <mechanicalReduction>10</mechanicalReduction>
    </actuator> 
  </transmission>

Doing it this way gives the errors mentioned above, but my model in Gazebo appears. 
If I try to merge both of these blocks so that there is just one joint tag wrapped by the transmission tags then I get the following error and my model does not appear in Gazebo:

[ERROR] [1473672367.041892175]: Failed to find root link: Two root links found: [footprint] and [left_back_wheel]

I don't understand why I get this error because I have a joint between my chassis base link and the world in my Jaguar4x4.xacro file : 
 <link name=""footprint"" /> 

 <joint name=""base_joint"" type=""fixed"">
   <parent link=""footprint""/>
   <child link=""chassis""/>
 </joint>

I now get a number of errors when trying to combine the joint and transmission blocks, so I imagine that this is not the best way to go?
","ros, gazebo"
Mechanism to oscillate a human leg?,"I'm working on creating a robotic device capable of oscillating an adult human leg using the knee as pivot, being able to constrain the amplitude of oscillation by controlling the mechanism with a microcontroller and some other electronic components.
The only idea I have to do this is by placing a pair of servomotors and some gears that will rotate the structure that holds the leg and then control the PWM, however I don't think I will be able to find servos strong enough to lift both the structure and the leg, so it would be very useful to hear some suggested mechanism and motors to accomplish this device. 

","motor, design, mechanism"
How to precisely land a copter in a defined place?,"Say, I have a flat square landing place for a copter, made of e.g. aluminium or plastic, with marks on it making a square, looking like this from the top:
-------------------------------------
|                                   |
|   *                           *   |
|                                   |
|                                   |
|                                   |
|                  |                |
|                --+--              |
|                  |                |
|                                   |
|                                   |
|                                   |
|   *                           *   |
|                                   |
-------------------------------------

The copter is smaller than the landing field, to land properly. Say, it has some sensors or can be manipulated by the computer that powers logic of the landing square. The square itself can have sensors too.
I want the copter to land as precisely as possible into the place marked by land signs.
What ways are possible to implement this functionality knowing that it's a DIY prototype where I have no access to secret technology, but only to that is available in online electronic components stores?
I think of these:

Having a camera either on the drone or on the landing plate that
finds the marks and calculates if these marks are in the right place
or if the drone must move along X or Y axis. Looks as an overkill and
will be hard to configure it for difficult conditions like raining,
night, snow.
Using laser transmitters and receivers.
Some solution I am not aware of but which is still simple and efficient enough.

In what direction should I look to find a solution for this problem?
","quadcopter, sensors, uav"
What algorithm should I use for a line follower?,"I actively take part in robotics competitions with my school's robotics club, and all of the line followers we use, are implemented using a PID algorithm (PD actually), but recently my electronics teacher told me to get my feet wet with fuzzy logic, he explained to me the reasons for why to use fuzzy logic (a mathematical model of a robot is hard to come up with, fuzzy logic is more ""human-friendly"" than PID, etc.) My question is if I should really bother with fuzzy logic or stay with PID, or maybe to change to another less known but better algorithm. The tracks that the robot has to follow are regular white over black continues ones, with no weird extras or anything like that, but most of them have marks on the sides of the beginning and end of every turn, but apart from that nothing else. So should I stick with PID or change to something else? Any advice would be very helpful, thanks.
","pid, algorithm, line-following"
Extracting as many possible end configurations as possible,"I am trying to implement a path planner to generate a path that moves the robot from q_start to q_goal.   
Q_goal is extracted from a stereo camera mounted on the tool, from
which I extract x,y,z coordinates of the desired position, the rotation can be arbitrary. 
The robot I am using is an industrial ur5 robot arm, the software I use is capable of performing Jacobian based inverse kinematics given a transformation matrix with rotation and translation. 
my inverse kinematics provide me with only one solution, which is ok, but doesn't provide me flexibility for path planning...
How do I using inverse kinematics determine all possible q-configurations that fulfills my criteria of having the desired x,y,z coordinates? 
","robotic-arm, inverse-kinematics"
"Getting pitch, yaw and roll from Rotation Matrix in DH Parameter","I've calculated a DH Parameter matrix, and I know the top 3x3 matrix is the Rotation matrix. The DH Parameter matrix I'm using is as below, from https://en.wikipedia.org/wiki/Denavit%E2%80
Above is what I'm using. From what I understand I'm just rotating around the Z-axis and then the X-axis, but most explanations from extracting Euler angles from Rotation matrixes only deal with all 3 rotations. Does anyone know the equations? I'd be very thankful for any help.
","kinematics, forward-kinematics, dh-parameters"
"Guidelines for a ""stopwatch based"" craft","I'm a swimmer and would like to create a small object that would help me folow my trainning plannification.
In other words I want to create a custum stopwatch, with 5 colored led that will be on or off according to the trainning program.
This is my training:

0:00 to 5:00 : Low Effort (Green light)
5:00 to 15:00 : Medium Effort (Yellow light)
15:00 to 20:00 : High Effort (Orange Light)
20:00 to 25:00 : Medium Effort (Yellow light)
25:00 to 26:00 : Maximum Effort (Red light)
26:00 to 30:00 :  Low Effort (Green light)
30:00 Trainning done (Should turn off automaticaly.)

The whole thing must be waterproof.
Must work on battery.
I'm asking for some guidelines, I'm not asking for a complete patent(would be appreciated though) but the main idea how to do it. So I can avoid wasting my time on research and tests.
I don't want to spend too much time on the project less than 100 hours maybe. I have studied in computer science and I have most of the materials I'll need. I can buy what is missing.
I'm a little bit noob on this kind of project, I want to know how you do it.
Thanks for help.
",battery
Power solution for Raspberry Pi robot,"I am building a Pi car with 4 gear motors (190-250mAh each max). Now I want to use my 10000mAh USB power bank to power up raspberry pi along with the 4 gear motors.
I can power up the raspberry pi directly but I want to use my power bank as the only source of power for the Pi Car. How can I connect both my RPi and motor controller L298N to my USB power bank?
","motor, raspberry-pi, power"
Motion primtive: machine learning vs. handcrafted,"On Google Scholar there are a lot of papers which explain the advantages of motion primitive: instead of searching inside the state-space (configuration space of a robot) the solver has to search inside the plan space of motion primitives. Sometimes this concept is called lattice graph.
Even if all papers are convinced about motion primitive in general, there is room for speculation about how exactly this idea should be implemented. Two different school of thought are out there:

Machine Learning for generate motion primitive. This is based on q-learning, neural networks and motion capture. The projekt ""poeticon"" (Yiannis Aloimonos) is a good example.
Handcoded motion primtive. This concept is based an manual coded Finite States-Machines (FSM) which can only solve a concrete example like ""pushing the box"". Additional functionality has to be implemented by hand.

The question is, which concept is better on real life examples? 
",machine-learning
Off position robot model - Inverse Kinematics,"I had to make a Unity3D robot model(ABB IRB 1600-6R/6DOF), that given a desired end effector transformation matrix, it would calculate and rotate the robot joints to the appropriate angles(Inverse Kinematics Computation). I found some code in Robotics Toolbox for MATLAB that, lets say that you trust me, actually calculates the needed angles(its the general ""offset"" case in ikine6s.m) - but for a different zero angle position than my chosen one, which is corrected using  the appropriate offsets.
So, I have set my 3D robot model in Unity3D correctly, angles are correct, I give the same parameters in Robotics Toolbox in MATLAB and the results are the same, I plot the robot stance in MATLAB to see it-it's on position-, I then run the code in Unity3D and the robot model seems to move to the stance I saw in MATLAB but it is off position- the end effector is away from its desired position.
Am I missing something?
The scaling is correct. I have subtracted a translation (equal to the distance of the bottom of the model's base contact to the floor, to the start of the next link- as MATLAB doesnt calculate it) from the Y component of the desired position of the end effector(in its homogenous transformation matrix I use as the rotation part, the identity matrix, so we do not care about that part).
Here are some pictures showing my case(say Px, Py, Pz is my desired EE position):
MATLAB-This is the plot of the results of the MATLAB ikine6s with input Px, Py, Pz in the corresponding translation part of the desired homogenous transform matrix:

Unity3D-This is what I get for the same input and angles in Unity3D-the EE is off position(should be half inside white sphere): 

","inverse-kinematics, 3d-model"
Automated high-precision weighing of individual parts,"I'm trying to automate a process wherein individual small parts must be weighed one after another for identification / sorting.
The parts would be fed to the scale using a conveyor. A mechanism is in place to feed the parts one at a time (i.e. sufficient spacing between the parts).
I know that it is possible to have weighing idlers on industrial conveyors but their precision and repeatability seem to be inadequate for precision weighing. The parts range in weight from 100mg to 30g so the only type of scale that I've found that has sufficient precision (10mg or better) are precision/laboratory scales. For example: Torbal Precision Scales.
I can imagine a solution wherein a lightweight flat surface (e.g. plexiglass) could be laid atop the scale plater upon which the feed conveyor would deposit the parts to be weighed. Then, after a stable weight has been read by computer via the integrated RS-232 port, a motor controlled ""sweeping mechanism"" could brush off the part from the plexiglass plater onto a discharge conveyor system for routing into one of many possible part bins.
Given this, I have two questions:

Is there a technology that I'm unaware of that could achieve individual small part weighing that would be simpler than stated solution?
Would the plexiglass platter (used to create a larger flat surface) cause too much pre-loading to the load cells, thereby affecting the scales performance / durability in any way?

","mechanism, automation"
Quadcopter wont start,"I have a skyline FX-6
After several minutes of flying for one of the first times, it stopped flying. I thought it was because the battery was empty.
But then I charged the battery and it wont start flying anyway.
What can be the problem and how do I fix it?
",quadcopter
"Difference between an underactuated system, and a nonholonomic system","What's the difference between an underactuated system, and a nonholonomic system? I have read that ""the car is a good example of a nonholonomic vehicle: it has only two controls, but its configuration space has dimension 3."". But I thought that an underactuated system was one where the number of actuators is less than the number of degrees of freedom. So are they the same?
",kinematics
Differential GPS or Simple GPS for Robot navigation and odometry?,"I am building a rover which can navigate autonomously. I do not want to use wheel encoders for generating robot odometry since it causes drifts due to slippage etc. 
I want to use GPS for generating odometry. Will it be useful for me to use the differential GPS approach or shall I use a simple GPS?
I am thinking of using DGPS because it enhances the accuracy to great extent, But at the same time, I think that in generating odometry, absolute gps values are not of importance. Instead, relative gps values are important to find for example, the distance covered by the robot.
Any suggestions on this ? Thanks
","navigation, odometry, gps"
How can I make a motion tracking camera?,"I'm looking into CCTV, and am interested in minimising costs by having a motion tracking camera cover an area that would otherwise utilise multiple cameras.
There is already something like this on the market manufactured by a company called NightWatcher (I think).  However, it does not track, it merely senses using 3 PIR's and points the camera in 1 of 3 positions. Ping ponging between them if the subject is between sensors.
I like the idea of this, but not the drawbacks, and was wondering if there was anything I could do with an arduino or similar to achieve a better result.
I stumbled across this, but am not entirely sure about it. Also this is for outside application, and that thread is for indoor (if that makes a difference).
https://robotics.stackexchange.com/a/1397/9751
Edit...
Just in case I have mislead you, I want to have a unit where sensors detect movement and then a camera to face that position.
",cameras
What types of actuators do these industrial bots use?,"I have a particular example robot that interests me:
http://www.scmp.com/tech/innovation/article/1829834/foxconns-foxbot-army-close-hitting-chinese-market-track-meet-30-cent
See first image, the bigger robot on the left, in particular the shoulder pitch joint that would support the most weight. I'm curious because I know it's really hard to balance strength and precision in these types of robots, and want to know how close a hobbyist could get.
Would they be something similar to this: rotary tables w/ worm gears?
http://www.velmex.com/products/Rotary_Tables/Motorized-Rotary-Tables.html 
Looking for more actuation schemes to research, thanks!
","motor, robotic-arm, actuator, torque"
How to properly calibrate a magnetometer in IMU for precise yaw?,"EDIT: Moved to ElectricalEngineering StackExchange Community
I'm using Sparkfun Razor IMU 9DOF sensor which incorporates accelerometer, gyroscope, and magnetometer, for giving the Euler's angles (yaw, pitch, and roll). I'm using the firmware at this link. It has Processing sketch for calibration of magnetometer, but it doesn't give the precise measurements. Especially, the yaw is imprecise. I'm using this sensor for measuring azimuth and altitude of stellar objects. The altitude is mostly correct, but azimuth (yaw) isn't. 
I have several questions:

Is there a better way to calibrate magnetometer? Is the calibration sufficient, without using the Madgwick or Kalman filter?
Is there some nonlinearity present in the sensor? Since the yaw offset isn't constant, it changes (around -12 degrees up north to almost correct value at southwest). And if it is, how could I measure that nonlinearity and apply to the yaw measurements?
If I have to use Madgwick or Kalman, do I have to apply them on quaternions? I believe that applying them at the final yaw measurements wouldn't do the job.

","kalman-filter, imu, calibration, magnetometer, precise-positioning"
Auto Stabilising Flight,"At the moment this project is purely hypothetical but my friend and I were looking to make a model airplane which could stabilize its flight to be a straight line. Basically, we want there to a button on the controller or a separate transmitter which, when activated, would cause the plane to fly in a straight, horizontal line in whichever direction it was facing. The instrumentation we believe we would need is a three axis accelerometer, so that it can level the plane to fly horizontally with the ground and keep the roll and yaw steady. My question is, would this work? When I talked it over with my dad (who does a lot of this kind of thing) he suggested that we might need a Kalman filter to keep the instrumentation from gradually drifting off course but, being a high school student, that sounds a little intimidating. Any comments on feasibility or improvements would be greatly appreciated.
","kalman-filter, automation"
Need a pushing mechanism,"I'm new to robotics (and maybe it's not even robotics), and want to build a ""pushing mechanism"" to automatically push a ""pop up stick"" up from my kitchen bench (see product here: http://www.evoline.no/produkter/port/).
By design, the ""pop up stick"" is made to be dragged up by hand, but I want to install a mechanism under the bench, to do it by a trigger of some kind.
I'm a programmer, and I have some experience with Raspberry Pi. Does anybody have any hints of what products I could use to build this?
Thanks :)
","robotic-arm, microcontroller, raspberry-pi, industrial-robot"
Why don't cheap toy robotic arms move smoothly?,"Why don't cheap toy robotic arms like this move smoothly?  Why can't it even move itself smoothly, (even without any load)?
In other words - what do real industrial robotic arms have, that cheap toys don't?
",robotic-arm
OpenNI vs OpenNI2 file difference,"I am relatively new to c++ and to OpenNI2 so forgive me if this is fundamental. I have received software from someone to use in my final year project. His software has multiple source and header files and makes use of OpenCV and OpenNI.
Note: I am using Visual Studios 2015, on a windows 64bit Laptop.
My error now is that there is one final header file I am unable to include because it cannot be found anywhere in the installed OpenNI2 program files. I have been able to add all the libraries and do the 'linker'and include stuff correctly in the project properties.
The file is : 'XnCppWrapper.h' , which through googling i have found is available in the OpenNI program files, however i have OpenNI2 installed.
Will i be able to download and install OpenNI to only use this one file and use OpenNI2 aswell? Or will my whole project need to work off one or the other?
","computer-vision, c++, opencv, openni"
I can't get motors to turn with Raspberry pi,"I want to turn some motors using my raspberry pi. I am able to turn an LED on and off using the 3.3V GPIO pin. For the motors, I tried using a L293D chip as per the instructions on this link.
What happened is that the very first time I set the circuit up for one motor, it worked perfectly. But then, I moved the pi a little and the motor has since refused to work. I even bought a new pi and still no luck with the circuit. I then bought a L298N board that fits smugly on top of the GPIO pins of the pi and followed the instructions on the this video
Still no luck, the motor just won't run with either pi. I am using four AA batteries to power the motor and a connecting the pi to a power supply from the wall. What could possibly be the problem here?
","motor, raspberry-pi"
Base for home robot of a bigger size,"I am trying to build a robot. But a bigger robot than a Raspberry Pi connected to some tiny something as big as a can of black Coke. I am planning to build a robot of a size 1.2-1.5m. I already have chosen some torso, arms and so, but, the problem is the base (bigger wheels, able to cope with weight of 10kg minimum).
I was thinking about using iRobot Create 2 platform, but it is not that robust and cannot pass a bit higher doorsteps. I want it let it go outdoors eventually. Do you know of some product or similar project? Every base which is sold in shop is very small, which can be used on a table and not something which could serve as a good robust mechanism with wheels, servos and everything...
",wheeled-robot
Communication in SWARM robotics,"Hey so I am trying to research into SWARM robotics, and trying to find helpful information or even articles/papers to read on the process of setting up communication protocols between different robots. For instance using a LAN connection, does each robot need to have a wireless adapter, and how would I begin setting up a network for say 5-10 smaller robots?
More generally could someone help me understand how devices connect and communicate across networks? I understand the basics of IP addressing, but I haven't researched into further complexities.
Any advice or direction is appreciated.
",wireless
Is the crazyflie control board considered a microcontroller,"I am currently doing a project for school and we are told that we must use a micro controller that ends up controlling some external hardware, now i know the crazyflie is controlling the motors which counts as external hardware but is it a micro controller? My second question is i want to purchase the kit so i can assembly it myself however I saw that you can use an expansion board so you need not solder and also i plan on not buying a remote its possible to control the crazyflie via my iPhone correct? I would appreciate it if someone could answer my questions. Thank you in advance
",quadcopter
How is PIV control performed?,"I'm considering experimenting with PIV control instead of PID control. Contrary to PID, PIV control has very little explanation on the internet and literature. There is almost a single source of information explaining the method, which is a technical paper by Parker Motion. 
What I understand from the control method diagram (which is in Laplace domain) is that the control output boils down to the sum of:

Kpp*(integral of position error)
-Kiv*(integral of measured velocity)
-Kpv*(measured velocity)

Am I correct? Thank you. 
","control, servos, pid"
Can I use a 3D gimbal system as a simplistic quadcopter IMU(3 axis accelerometer)?,"i have 3d gimbal system and i want to use this sensor in place of IMU(3 axsis accelerometer) in Quadcopter 
","quadcopter, accelerometer, gyroscope"
Technique to increase POV resolution,"I have thought of a technique to increase the resolution of a POV (persistence of vision) display. In an usual POV display, the LEDs are arranged in a strip and spun in a circle. There are two limiting factors to increasing the radial resolution along the circumference of any one circular path that an LED follows. One is, depending on the speed of the POV wheel, the minimum time required (decided by the microcontroller) to change the LED's color in case of a RGB. The other factor is the LED's width, that increases the 'bleeding' of color from one pixel to the neighboring pixel if the LED changes color or brightness too fast. 
If one were to fix a slit in the front of an LED, |*|  <-- like so, would this help improve the resolution of the POV display; by doing this one would in effect be reducing the width of the 'pixel' along the circumference on which the led would be traversing.
Thus if one were to use a fast enough microcontroller and a narrow enough slit, one could probably obtain a very high resolution along one dimension at least.
To be clear I've not yet implemented this, and am just looking for any experienced person who can tell if this will work or not.
","microcontroller, electronics"
Autonomous Navigation without Distance Sensors,"I'm doing a project with the iRobot Create 2. I want it to be able to map out a room and navigate to a point for example. My problem is that the robot doesn't have any distance sensors. What it can do is detect if there is an obstacle ahead of it or not (0 or 1) and it can measure how far it has traveled in millimeters. Any good techniques out there or best to buy an IR sensor?
","mobile-robot, irobot-create"
Determining position from a 2D map and LIDAR,"We need to determine the 2D position of my robot. To do so, we have a LIDAR at a known high, with an horizontal plane, which gives us the distance to the nearest point for each angular degree (so 360 points for one rotation). The environment is known, so I have a map of the position of every object that the LIDAR is susceptible to hit.
My question is, based on the scatter plot that the LIDAR is returning, how can we retrieve the position of my robot in the map ? We would need the x, y position in the map frame and also the theta angle from the map frame to my robot frame.
We have tried to match objects on map with groups of points based on their distance between each other and by identifying those objects and the way the LIDAR ""sees"" them to retrieve the robot position. But it is still unsuccessful.
To put it in a nutshell, we want to make SLAM without the mapping part. How is it possible, and with what kind of algorithms ?
A first step could be to stop the robot while acquiring data if it seems easier to process.
","localization, lidar, precise-positioning"
"In EKF-SLAM, why do we even need odometry when there is a more reliable sensor?Also, are all SLAM algorithms feature-based?","In the book of SLAM for dummies, why do we even need the odometry when the robot would use the data retrieved from the laser scanner which is more accurate than odometry? Why not just rerly on the laser scanner and do away from the odometry? Is there any contribution by the odometry that the laser scanner does not have? Also, are all SLAM algorithms feature-based?
","localization, slam, mapping"
Can mapping be done in real life applications without also solving the localization problem at the same time (i.e. SLAM)?,"I know that Occupancy Grid Mapping requires the assumption that the robots' pose is always known when generating a map.  However, I also know that in reality, position and orientation usually treated as uncertain in practical applications.  Assuming that my target mapping environment is inside a home, is there a way I can overcome inherent robot pose uncertainty in a real world application of Occupancy Grid Mapping without resorting to implementing a SLAM?  That is, what is a low-cost way to increase the certainty about my pose?  
Or is Occupancy Grid Mapping only useful in theory and not in practice?  
Update:
It is clear to me, from the responses given, that occupancy grid mapping is just one possible way to represent a map, not a method in and of itself.  The heart of what I really want to know is:  Can mapping be done without also solving the localization problem at the same time (i.e. SLAM) in real life applications? 
","slam, mapping, occupancygrid"
What is the torque transmission efficiency using a bycicle chain/setup for robot?,"For this robot the gear attached to the motor is linked to the gear attached to the wheels by a bicycle chain (I am using a bicycle wheel and transmission set as the parts for the robots movements).
How does using a bicycle chain affect the power transmission efficiency, how does this impact the torque?
","mobile-robot, torque, gearing"
Understanding and correct drift when using BreezySLAM (aka tinySLAM / CoreSLAM),"I was looking for a Python implementation of SLAM and stumbled upon BreezySLAM which implements tinySLAM aka CoreSLAM. 
My robot is equipped with the hokuyo urg-04lx-ug01. 
I have odometry hence passing it to the updater: 
self.slam.update(ls_array, (dxy_mm, dtheta_deg, dt));

As I start moving the robot starts discovering room A and then room B & C already the map seems to have rotated. I come back to room A and return the initial pose end=start using the same path. Now I noticed room A has significantly rotated in relation to the other room. Consequently the map isn't correct at all, neither is the path travelled by the robot. 

Wasn't the SLAM supposed to store and keep the boundaries for the first room it discovered?
Why this rotation may be happening?
How could I try to troubleshoot this issue with the data I have collected (odometry, calculated position, liDAR scans)?
Can I tune SLAM to do a better job for my robot?

SLAM is pretty new to me, so please bear with me, any pointers on literature that may clarify and moderate my expectations of what SLAM can do.

Extra
... and Here the best video I found to understand particle filter
","localization, slam, mapping"
Quadrature encoder signal from dc motor is very noisy,"I'm starting out with robotics, got my first DC gear motor with quadrature encoder (https://www.pololu.com/product/2824):

I ultimately plan to hook it up to a motor driver connected to a Tiva Launchpad. However, since I'm a noob, and curious, I am starting by just playing with it with my breadboard, oscilloscope, and voltage source. E.g., when I plug in the motor power lines into my (variable) voltage source the axis spins nicely as expected between 1 and 12 V.
The problems start when I try to check how the encoder works. To do this, first I plug a a 5V source into the encoder GND/Vcc, and then try to monitor the encoder output.
While the motor is running, I check the Yellow (encoder A output) cable (referencing it to the green (encoder GND) cable).  I made a video that shows a representative output from one of the lines (no USB on my old oscilloscope so I took a video of it using my phone).
As you would see at the video, the output doesn't look anything like the beautiful square waves you typically see in the documentation. Instead, it is an extremely degraded noisy sin wave (at the correct frequency for the encoder). The amplitude of the sin is not constant, but changes drastically over time. Strangely, sometimes it ""locks in"" and looks like the ideal square wave, for about a second or two, but then it gets all wonky again.
Both of the lines (encoder A and B output) act this way, and they act this way at the same time (e.g., they will both lock in and square up at the same time, for those brief glorious moments of clarity). Both of my motors are the same, so I don't think it's that I have a bad motor.
I have also checked using Vcc=12V, but it made no difference other than changing the amplitude of the output.
Note I already posted this question at reddit:
https://www.reddit.com/r/robotics/comments/502vjt/roboredditors_my_quadrature_encoder_output_is/
","motor, quadrature-encoder"
Shield IMU from magnetic interferences,"I experienced some drifting when coming near to magnetic fields with my IMU, so I wondered if it is possible to completely shield the IMU from external influences. Would this be theoretically possible or does the IMU rely on external fields like the earth magnetic field? Are there maybe alternatives to IMUs that are less susceptible to magnetic interferences? I only need the rotational data of the sensor and don't use translational output.
","sensors, imu, rotation"
3 Axis Gimbal Stabilization System can replace with 3 Axis Accelerometer,"i have ""TAROT ZYX-GS 3-Axis Gimbal Stabilization System ZYX13"" sensor that gives me the value of Roll tilt and Pan.The 3 axis accelerometer give me the value of x y and z.so can we use the Gimbal stabiliztion system in place of accelerometer
","quadcopter, sensors, accelerometer"
Simple equation to calculate needed motor torque,"Suppose I have a DC motor with an arm connected to it (arm length = 10cm, arm weight = 0), motor speed 10rpm.
If I connect a 1Kg weight to the very end of that arm, how much torque is needed for the motor to do a complete 360° spin, provided the motor is placed horizontally and the arm is vertical?
Is there an simple equation where I can input any weight and get the required torque (provided all other factors remain the same)?
","motor, torque"
How to create a model for temperature control?,"I have a heated compartment, inside which, there is another object heated up by independent heater. I want to control temperatures of both chamber and the object. 
I could achieve this by simple PID (or PI) controllers for both chamber and object, but I would like to try more thoughtful approach :) I have two temperature sensors, and two PWM outputs for heaters. How do I identify a model for an object I want to control?   
","control, pid, automation"
What is the best way to plug in more than 3 stepper motors into a Arduino Uno board?,"I'm developing a 5 axis robotic arm with stepper motors and I am getting around to ordering most of my parts. I plan on using 5 EasyDriver shields to drive all of my motors. I am also planning on using just a basic arduino uno board to go with it. So here are my questions:
Is there any alternative instead of buying a ton of Easy Drivers and connecting all of them to a single board?
And if there isn't, then how would the setup look to use more than 3 stepper motors? This is the most useful picture I found, however It only shows 3 and while I know I could plug in a 4th I am unsure whether I could plug in a 5th.
",stepper-motor
"Do simple, non-sonic, omni-directional rangefinding beacons exist?","I am on a robotics team that plans to compete in a competition where one of the rules is that no sort of sonic sensor is allowed to be used. I guess that limits it to some sort of EM frequency right?
Ideally, my team is looking for a simple beacon system, where beacon A  would be attached to the robot, while beacon B would be attached to a known point on the competition space. Then, beacon A can give information about how far away B is. After some searching, I could only turn up laser rangefinders that required pointing at the target. I am a CS student, so I'm not familiar with the terminology to aid searches.
Another nice property would be if the beacons also gave the angle of beacon A in beacon B's field of view, although this is not necessary, since multiple beacons could be used to obtain this information.
We have an Xbox 360 Kinect working, and able to track things and give distances, but it looses accuracy over distance quickly (the arena is about 6 meters long), and this beacon should be as simple as possible. We ONLY need it for a relative position of our robot.
Alternate Solution:
Another way to solve this would be for an omni-directional beacon to only give angle information, two of these could be used to triangulate, and do the job just as well.
","localization, electronics, laser, rangefinder"
Absolute positioning without GPS,"Using an IMU a robot can estimate its current position relative to its starting position, but this incurs error over time. GPS is especially useful for providing position information not biased by local error accumulation. But GPS cannot be used indoors, and even outdoors it can be spotty.
So what are some methods or sensors that a robot can use to localize (relative to some frame of reference) without using GPS? 
","localization, gps, sensors, slam"
What is difference between RoboEarth and KnowRob?,"I am not able to clearly differentiate between the two platforms:

RoboEarth, and;
KnowRob.

",theory
Is it possible to use a LiPo charger as a lab bench power supply?,"I recently thought about building a lab bench power supply, it comes in cheaper and I love to build things...
But then I also have a LiPo charger an iMax B6AC, that I had bought for my quadcopter, then came the idea of whether I can use the charger as a lab bench power supply...
My questions is, could this work and how could I make it work?
",power
What type of motor can be hooked up on a bike?,"As the title briefly explains, my question is, what type of motor is powerful enough be on a cycle?
My plan is to convert a cycle into an electric bike by mounting a motor and controlling it through either a Raspberry Pi or an Arduino board.
","motor, electric"
How to make a simple Arduino Insect Robot?,"I want to make a simple Arduino based programmable insect robot.
I want it to walk on legs and legs will be made of hard aluminum wire. It needs to have 4 legs. I am planning to use Arduino Nano for that. I just had few questions like:

How to arrange servos and wire to have motion?
I also want it to turn sideways?
Where can I read good theory on making insect like robots?

",arduino
"Quadcopter Position Measurement (Accelerometer, GPS or Both)?","I previously thought that an accelerometer on a quadcopter is used to find the position by integrating the data got from it. After I read a lot and watched this Youtube video (specifically at time 23:20) about Sensor Fusion on Android Devices, I seem to get its use a little correct. I realized that it's hard to filter out the considerable noise, generated from error integration,  to get useful information about the position. I also realized that it is used along with the gyroscope and magnetometer to for fused information about orientation not linear translation.
For outdoor flight, I thought of the GPS data to get the relative position, but is it so accurate in away that enables position measurement (with good precision)? How do commercial quadcopters measure positions (X,Y and Z)? Is it that GPS data are fused with the accelerometer data?
","quadcopter, accelerometer, navigation, sensor-fusion"
Scale factor of a 3d robot model relative to the real measurements of a robot,"I have some measurements of a real life robot, and a 3d model of that robot (lets say in Unity) and I want to know the scale factor, plus I dont want to find the 3d models measurements and then divide with the real world ones to find the scale factor, in order not to get more confused with more mathematics than I am already. So, is there a methodology or will I have to do it as I mentioned?
",3d-model
What is the difference between CC3D Revolution Mini and CC3D Revolution,"I recently came across this doubt... As the title suggests what's the difference between the two flight controller. They have a big price difference and size difference, that's all I know...
Do they all function the same way, so does the two flight controller differ in size?
Answers appreciated,
Sid
","quadcopter, microcontroller"
Is it a bad-design decision to implement high number of moving parts in an automation-robot?,"I'm currently designing an autonomous robotic system to manipulate clothes using computer vision and complex moving hardware. My current design incorporates quite a number of moving parts. My biggest worry is a frame (140 x 80 x 40 cm) rotates from 0 to 90 degrees every time it manipulates a piece of cloth. Other than this the design involves various other moving parts to achieve successful manipulation of the cloth. It seems like the hardware is capable of achieving the task despite the high number of complex and moving parts. 
So the question is, what are the design considerations I should take in designing an automated system. Should I think of a alternative design with less number of parts? Or proceed with the current design if it does the job>? Sorry I am in a position where I can't disclose much information about the project. 
Thank you.   
","computer-vision, automation"
"Once matching features are computed between a stereo pair, how can they be tracked?","I am currently working on a SLAM-like application using a variable baseline stereo rig. Assuming I'm trying to perform visual SLAM using a stereo camera, my initialization routine would involve producing a point cloud of all 'good' features I detect in the first pair of images.   
Once this map is created and the cameras start moving, how do I keep 'track' of the original features that were responsible for this map, as I need to estimate my pose? Along with feature matching between the stereo pair of cameras, do I also have to perform matching between the initial set of images and the second set to see how many features are still visible (and thereby get their coordinates)? Or is there a more efficient way of doing this, through for instance, locating the overlap in the two points clouds?
","slam, computer-vision, stereo-vision"
Fit robot simulator to robot,"I have odometry data $(x, y, angle)$ of a real two-wheeled robot, who received control commands $(forward speed, angular speed)$.
Now I want to code a motion model (in C++ (/ROS)) which should follow the same trajectory, given the same control commands.
Normally, the kinematics should look something like this:
$$ 
\begin{align} 
v_{fwd} &= control_{fwd} \\
v_{ang} &= control_{ang} \\
x &= x_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \cos(angle) * dt \\
y &= y_{old} + 0.5(control_{fwd} + v_{fwd,old}) * \sin(angle) * dt \\
angle &= angle_{old} + 0.5(control_{ang} + v_{ang,old}) * dt
\end{align} 
$$
And I thought about just setting 
$$ 
\begin{align} 
v_{fwd} &= control_{fwd} + k_1 v_{fwd,old} + k_2 v_{fwd,old}^2 + k_3 v_{ang,old} + k_4 v_{ang,old}^2 \\
v_{ang} &= \text{ ...analog...} \\
x, y, angle &\text{ unchanged}
\end{align} 
$$
and then just search the minimum of the squared distance of the computed trajetory to the real one - depending on the values of $k_i$. This would mean either a good optimization algorithm or just brute-forcing / randomly testing a lot of values.
Is this the way to go here? I tried the 2nd approach, but the results so far are not that good. 
So, as you might guess now, I'm pretty new at this, so any help is appreciated.
","wheeled-robot, kinematics, algorithm"
"Accelerometer, gyro, and magnetometer sensor fusion for material resource survey","As a hardware engineer, I have studied quite a lot on sensor spec such as Accel, Gyro and Magnetometer including custom made fluxgate. I have studied matrix and quadarion (complex number) and so on.
I moving into calibration arena, I seen lot of article on calibration but not sure which is best solution to fix offset and mis-alignment axis. Can anyone point to best open source code.
I'm not interested in output results related to flight such as quadchopter and GPS, but more interested in directional math for drilling pipes, where toolface, inclination, azimuth and position is most important. What the best thesis or paper that cover this topic and open-source or example code (in C) for this application. Do I need Kalnman filter or such advance post data capture processing. Any tip how to avoid getting too involved with maths
","control, kalman-filter, imu, calibration, precise-positioning"
Running a cycle on brushless outrunner motors?,"Is it possible to convert a cycle into an electric bike by using brushless outrunner motors that usually are for RC planes, multicopter, helicopter, etc?
If it is possible, what specs do my motors need to be to provide enough power to bring my cycle to speed?
Will I need a gear system?
","motor, power, electric"
Do DH parameters change for a scaled robot 3d model?,"I have the actual DH parameters of a robot:
d1 = 0.4865 m
d2 = 0.600 m
d3 = 0.065 m
a1 = 0.150 m
a2 = 0.475 m

all other di's and ai's are zero.
Can I use these for an inverse kinematics analytic closed form computation or should I measure the virtual distances in the 3d environment?
I am actually asking if the theta angles that will be yeld after the computations are dependent on the scale of those distances.
EDIT: Scale factor is unknown
","inverse-kinematics, dh-parameters, 3d-model"
Can I reuse the hall sensors in a brushless motor as an encoder?,"I have upgraded the motors in my robotic arm to sensored, brushless RC car motors. The hope was to reuse the Hall sensors to double as a rotary encoder, by tapping 2 Hall sensors and treating the 2 bits as a quadrature signal (a crude quadrature since 2 of the 4 states will be longer than the other 2).
This works when none of the motor phases are powered and I just rotate the motor manually. But once the stator coils are energized, the encoder no longer counts correctly: When running at low power, the counting is correct, but when running under high power, the count is monotonic (only increases or decreases) no matter if I run in reverse or forward.
I'm almost certain this is because of the stator coils overpowering the permanent magnets on the rotors. So is there still a way to use the Hall sensors as an encoder?
Sorry if this is an obvious question. I'd love to research this problem more if I had more time.
Update:
I've measured the wave forms with my DSO quad and see the expected 120 degree separated signals (the measurement for phase C gets more inaccurate over time because I only had 2 probes, so I measured phases A & B first, then A & C, and then merged them.
When ESC speed is 0.1:

When ESC speed is 0.3:

Previously, I was using a hardware quadrature counter (EQEP module on a BeagleBone). At speed=0.3, this was counting backwards no matter if I do forward or reverse!
I then implemented quadrature counting on an LPC1114FN28 uController. The result was still bad at high speeds (count didn't change at all). The logic was:
void HandleGPIOInterrupt()
{
  const uint8_t allowableTransitions[4][2] = {1, 2, 3, 0, 0, 3, 2, 1};
  static int prevState = -1;
  int state = phaseA | (phaseB * 2)
  if (prevState != -1)
  {
    if (allowableTransitions[prevState][0] == state)
    {
       ++rotations;
    }
    else if (allowableTransitions[prevState][1] == state)
    {
      --rotations;
    }
  }
  prevState = state;
}    

Then I got the idea to change the code to not update prevState until an expected state happens (to deal with glitches):
  int state = phaseA | (phaseB * 2)
  if (prevState != -1)
  {
    if (allowableTransitions[prevState][0] == state)
    {
       ++rotations;
       prevState = state;
    }
    else if (allowableTransitions[prevState][1] == state)
    {
      --rotations;
      prevState = state;
    }
    else
    {
        // assume transition was a glitch
    }
  }
  else
    prevState = state;

Now the counting finally is correct in both directions, even at speeds higher than 0.3!
But are there really glitches causing this? I don't see any in the waveforms?
","brushless-motor, encoding, hall-sensor"
How to know what type of stepper motors to use when designing a robot,"I am talking about robots like this one: http://www.meccanotec.com/step781b.JPG
How would a person know what type of motor to use in design of such a robot? What I want to understand is that stepper motors have different step sizes, different torques among other things. How do we determine what type of stepper motor is most suitable to be used in a given robot?
","robotic-arm, stepper-motor"
Combine individually working cartesian coordinates,"
I am trying to control a Dobot arm.  The arm moves with angles whereas I need to work with cartesian coordinates. From inverse kinematics equations and polar coordinates I have implemented x,y and z coordinates working very well on their own. But I can not combine the coordinates in order to work all together. When I add them up it is not going to the desired place. How can I combine these coordinates? I got some help from (https://github.com/maxosprojects/open-dobot) but could not manage to successfully move dobot. 
Edit: I've written the codes in Qt and also I've added the triangles used for angle calculations.
//foreArmLength=160mm rearArmLEngth=135mm
float DobotInverseKinematics::gotoX(float x) //func for x-axis
float h=qSqrt(qPow(lengthRearArm,2)-qPow(x,2)); //height from ground
QList<float> zEffect=gotoZ(h); //trying to find the effect of x movement on z-axis
float cosQ=h/lengthRearArm; //desired joint angle
float joint2=qRadiansToDegrees(qAcos(cosQ));    
//move in range control
if(joint2 != joint2)
{joint2=0;
    qDebug()<< ""joint2NAN"";}
return joint2;

QList<float> DobotInverseKinematics::gotoY(float y) //func for y-axis
QList<float> result ;
float actualDist=lengthForeArm+distToTool; //distance to the end effector
float x=(qSqrt(qPow(actualDist,2)+qPow(y,2)))-actualDist; //calculating x movement caused by y movement
float joint1=qRadiansToDegrees(qAcos(actualDist/(actualDist+x))); //desired joint angle
float joint2=gotoX(x);  //the angle calculation of y movement on x axis
if(joint1 != joint1)
{joint1=0;
    qDebug()<< ""joint1NAN"";}
result.append(joint1);
result.append(joint2);
return result;

QList<float> DobotInverseKinematics::gotoZ(float z) //func. for z-axis
QList<float> result ;
float joint3=qSqrt(qPow(160,2.0)-qPow(z,2.0))/ 160; //desired joint angle
float temp=160-qSqrt(qPow(160,2.0)-qPow(z,2.0));
float joint2=qSqrt(qPow(lengthRearArm,2)-qPow(temp,2.0))/lengthRearArm; //desired joint angle
if(joint3 != joint3)
{joint3=0;
    qDebug()<< ""joint3NAN"";}
joint2=qAcos(joint2)*(180/M_PI);
joint3=qAcos(joint3)*(180/M_PI);
result.append(joint2);
result.append(joint3);
return result;

","robotic-arm, inverse-kinematics, c++"
Examples of Zeno Behaviour in the Real World,"Zeno Behaviour or Zeno Phenomenon can be informally stated as the behavior of a system making an infinite number of jumps in a finite amount of time.
While this is an important Control system problem in ideal systems, can Zeno behavior exist in real systems? Any examples?
If so, why don't noise or external factors deviate a system from achieving Zeno?
","mobile-robot, control"
Equations of motion of 3D pendulum-like system,"I'm trying to get the equations of motion of a 3D pendulum system (spherical pendulum), however I don't want to describe the system using spherical coordinates (which there is lot of information about). 
Instead I want to describe the system using x,y,z coordinates of the mass as well as the euler angles phi, theta, psi (the roll, pitch and yaw of the mass). 
That is, I want to assume that the mass has a position and an orientation in relation to the inertial frame. 
Furthermore this is a 3D pendulum system, where the mass, which is symmetric, is actuated (notice that this is a simplification of the system needed to actuate the mass, where only the resulting forces and torques are taken into account):

There is a force F acting in the x-axis direction of the mass reference frame
There is a torque T acting about the z-axis of the mass reference frame
There is also the gravitational force acting on the center of mass (in the negative direction of z-axis of the inertial reference frame

In order to clear misconceptions about how this forces are generated, think of the mass as a differential drive robot using fans instead of wheels.
The cable connecting the mass to the anchor point is assumed rigid and works as a distance constraint modeled by:
$\|r_{anchor} - r_{mass}\| = cable\_length$
Where $r_{anchor}$ is the position of the anchor point to which the mass is connected through the cable and $r_{mass}$ is the position of the mass. This constraint makes it so that it is similar to having two spherical joints: one at the anchor point and another at the mass. Futhermore the cable is assumed to have no mass. 
It's important to note that this rigid connection (calbe) is meant to be modeled by the distant constraint refered above.

I'm looking for help solving this system to obtain its equations of motion.
Thanks in advance
","kinematics, dynamics"
"Odometry motion model for Kalman filter, but is the error zero mean?","I was planning on using the odometry model in the prediction stage of an Extended Kalman Filter.
State transition equations:
$$ f(X_t,a_t) = \begin{bmatrix}
x_{t+1} = x_t + \frac{\delta s_r + \delta s_l}{2} \cdot \cos(\theta_t) +u_1
\\ y_{t+1} = y_t + \frac{\delta s_r + \delta s_l}{2} \cdot \sin(\theta_t) + u_2
\\ \theta_{t+1} = \theta_t + \frac{\delta s_r + \delta s_l}{b} \cdot \sin(\theta_t)+u_3
\end{bmatrix} $$
with $\delta s_r$ and $\delta s_l = \frac{n}{n_0} \cdot 2 \cdot \pi \cdot r$

$X_t = \begin{bmatrix} x_t & y_t & \theta_t\end{bmatrix}^T$ state matrix containing XY-coordinate and heading $\theta$ of vehicle in global reference frame



$\delta s_r$ and $\delta s_l$ distance travelled by respectively right and left wheel



$b$ distance from center of the vehicle to the wheel



$n$ encoder pulses count during sampling period t



$n_0$ total pulses count in 1 wheelturn



$r$ wheel radius



$u_1,u_2$ and $u_3$ random noise N(0,$\sigma^2$)


Now I doubt if this noise indeed does have a zero mean?
Wheelslip will always make the estimated distance travelled shorter than the measured distance isn't it?
","kalman-filter, odometry, noise"
Measure 2 diffrent battery voltages on arduino,"Is it possible to measure the voltage of 2 different batteries on arduino? Currently I am able to use a resistor divider / voltage divider of 2x 10K resistors to an analog pin to read the voltage of the battery supplying the arduino.
Currently the system looks like 6v battery -> 5v power regulator to Arduino -> resistor divider attached to 6v (unregulated) battery. GND is common throughout.
How could I measure the voltage of another battery given that it will be on a different circuit? e.g. different ground loop.
","arduino, power"
Ultrasonic Sensor through a column,"I am trying to measure the height of water inside a column. The column is 50mm in dia and 304mm long. I have mounted the sensor just above the column.

To measure the accuracy of the sensor, I filled the column up to a known value (a) and got the average sensor reading (b). a+b should give me the height of the sensor from the base of the column.
Repeating this for different values of a, I got very different values for (a+b). see attached chart.

My question is

Is the sensor expected to have error of this order? 
Is my setup of confining the sensor through a column producing such errors. 
Any other ideas to get the water column height. Please note that during the actual test, the water inside will be oscillating (up and down).I am thinking of making a capacitive sensor using aluminium foil. Water will work as the dielectric and the level of water will determine the capacitance.

P.S. I also did some open tests (not through a column) to get the distance of a fixed object, and it was quite accurate.
Any help is appreciated.
Arduino Code
#include <NewPing.h>

#define TRIGGER_PIN  7  // Arduino pin tied to trigger pin on the ultrasonic sensor.
#define ECHO_PIN     8  // Arduino pin tied to echo pin on the ultrasonic sensor.
#define MAX_DISTANCE 200 // Maximum distance we want to ping for (in centimeters). Maximum sensor distance is rated at 400-500cm.

double DB_ROUNDTRIP_CM = 57.0;

NewPing sonar(TRIGGER_PIN, ECHO_PIN, MAX_DISTANCE); // NewPing setup of pins and maximum distance.

void setup() {
  Serial.begin(9600); // Open serial monitor at 115200 baud to see ping results.
}

void loop() {
  delay(100);
  unsigned int uS = sonar.ping();
  double d = uS / DB_ROUNDTRIP_CM;


  Serial.println(d);
}

","arduino, ultrasonic-sensors"
Heading and Yaw Rate Measurements,"I am working in the field of automated vehicles mainly in the domain of passenger and commercial vehicles. I have been studying whatever I can get regarding the measurement the state (relative position, relative velocity, relative heading and roation, a.k.a. yaw rate) of surrounding objects especially other vehicles using sensors.
While everything else is possible to measure precisely using on-board sensors, I have found out that not much literature is available for measuring the vehicle heading and yaw rate of other vehicles which is baffling to me given the extreme precision of laser based sensing (albeit using time stamps).
I am looking for:

Reference to literature with experiments for estimation of yaw rate and vehicle heading.
As I can see from the literature available (or the lack thereof), no direct way of measuring yaw rate is available but by using LIDAR or Camera with consecutive time stamps or scans of data. However, this inherently requires the data to be correct. Hence, I would think that due to the inaccuracies involved, this method is not used! Is this correct?
Are there any commercially available sensors that give accurate heading and yaw rate information of other vehicles?

Sources and research papers would be most welcome!
Edit: By this inherently requires the data to be correct I mean, given the high sensitivity to error in heading or yaw rate at high vehicle speeds, the values computed using sensor information is not accurate enough to be put to use in practice!
",localization
GPS observation equations for Kalman filter?,"In the design of an Extended Kalman Filter for the position estimation of a vehicle, I am searching for the observation equations for inserting GPS data (longitude, latitude) into the update step of the filter.
The state vector of my filter $X_t = \begin{bmatrix} x_t & y_t & \phi_t\end{bmatrix}$ contains the X and Y coordinate of the vehicle in the local reference frame and the angle under which the vehicle is standing relative to the X-axis.

The observation equations should look like this:
$$H(X_t) = \begin{bmatrix} longitude = f(x_t,y_t) \\ latitude = f(x_t,y_t)\end{bmatrix} $$
Can anybody fill them in?
","kalman-filter, gps"
"Conversion GPS (longitude,latitude) to (x,y) of local reference frame?","I would like to use GPS data as measurement input for an extended kalman filter. Therefore I need to convert from GPS longitude and lattitude to x and y coordinate. I found information about the  equirectangular projection given these formulas: 
$$\ X = r_{earth} \cdot \lambda \cdot cos(\phi_0) $$
$$\ Y = r_{earth} \cdot \phi $$
However I think these formulas are only for use when the axis x- and y-axis of my local frame are parallel to north and south axis of the earth.

But my vehicle is starting in my local reference frame in the origin and heading straight in y-direction. In whatever compas angle I put my vehicle, this should always be the starting position. 
I can measure the angle $ \alpha $ to north with a compass on my vehicle.
Now what is the relationship between (longitude,lattitude) an (x,y) of my local frame?
","kalman-filter, gps"
Inverse Kinematics Computation -- why are alpha angle values not included,"Given a desired transform matrix of the end effector relevant to the base frame of the P560:

John J. Craig, in his book, Introduction to Robotics
Mechanics and Control, computes the inverse kinematic solutions of a Puma 560, with (correct me if wrong) Modified DH parameters and gets the following set of equations for theta angles:

and I noticed that there are no alpha angles in these calculations.
So my question is why aren't the alpha angle values not used in the calculation for the desired pose with the given end effector transform. Why is it independent of the axes twist angles of the robot?
",inverse-kinematics
Replacing just the Wheels on Create2,"Is it possible to replace just the wheels on the create2 robot? Is it a standard shaft/coupling?
","irobot-create, roomba"
Handling of a 4WD robot frame as a 2 wheel differential drive,"I have a 'Baron' robot frame with 4 static wheels, all driven by a motor. At the moment I'm thinking of handling it like a 2 wheel differential drive. Left and right wheels would receive the same signal. Actually you can interpret it as a tank on caterpillars, exept there is no link between the two tires. 
Does anyone have a different idea about this? 
Ps: The purpose of the robot will be to know it's exact location. I will use a kalman filter (EKF) to do sensor fusion of the odometry and an IMU with accelero, gyro and magnetometer. So in the kalman filter I add the odometry model of a differential drive robot.
",differential-drive
How can a DMP be used for simulating physics?,"I read a paper from 2015, ""Structural bootstrapping - A novel, generative
mechanism for faster and more efficient acquisition of action-knowledge
"", which introduces a concept called, ""Structural bootstrapping with semantic event chains and dynamic movement primitives,"" which confused me a little bit. 
According to my knowledge a robotarm is controlled by a PDDL-like planner. The PDDL file is a ""qualitative physics engine"" which can predict future events. The paper says the ""qualitative physics engine"" consists of dynamic movement primitive (DMP) which are learned from motion capture data. 
My question is: How can a DMP be used for simulating physics?
","motion-planning, algorithm, machine-learning"
Is there a portable and accurate sensor to measure the position of the hand relative to the body?,"My team has been working on a wearable glove to capture data about hand movements, and use it as a human-computer interface for a variety of applications. One of the major applications is the translation of sign language, shown here: https://www.youtube.com/watch?v=7kXrZtdo39k
Right now we can only translate letters and numbers, because the signs for them require the person to hold their hand still in one position ('stationary' signs). I want to be able to translate words as well, which are non-stationary signs. Also the position of the hands really matters when signing words, for example it matters whether the hand is in front of the forehead, eyes, mouth, chest, cheeks, etc.
For this we need a portable and highly accurate position sensor. We have tried getting position from a 9-DOF IMU (accelerometer, gyroscope, magnetometer) but as you might guess, there were many problems with double integration of the noise and accelerometer bias.
So is there a device that can provide accurate position information? It should be portable and wearable (for example worn in the chest pocket, headband/cap, etc...be creative!).
EDIT (more details):
I'm going to emphasize certain aspects of this design that weren't clear before, based on people's comments:

My current problem of position detection is due to errors in double integration of the accelerometer data. So hopefully the solution has some incredibly powerful kalman filter (I think this is unlikely) or  uses some other portable device instead of an accelerometer.
I do not need absolute position of the hand in space/on earth. I only need the hand position relative to some stable point on the body, such as the chest or belly. So maybe there can be a device on the hand that can measure position relative to a wearable device on the body. I don't know if such technology exists; I guess it'd use either magnets, ultrasound, bend sensors, or EM waves of some sort. Be creative :)

","sensors, sensor-error, precise-positioning"
is it possible to get all possible solutions of inverse kinematics of a 6 DOF arm?,"I would like to know if there is any way to get all the possible solutions of inverse kinematics of a 6 DOF robotic arm?
I have found some good Matlab codes but gives only one solution like in Peter corke's book .
Thank you in advance. 
",inverse-kinematics
unable to install ros kinetic in ubuntu 16.04,"I am trying to install ros kinetic kame in ubuntu 16.04 , but after trying the first step setup your sources. list. 
I am getting  cannot create /etc/apt/sources.list.d/ros-latest.list: Permission denied what to do now
","ros, irobot-create"
IR distance sensor,"I am trying to make a IR distance sensor. I have seen this online. My goal however is to see the distance between a IR transmitter and my IR sensor. In the example above he uses the IR led's ambient light and timing to track the distance. Is there a way to find the distance between lets say a IR remote and a sensor? It would only have to be accurate to about 1 meter. I am also open to other ideas of accurately tracking distance between two objects weither that be bluetooth/ir/ultrasonic 
",sensors
NameError: name 'TK' is not defined,"I reference the following article.
http://www.egr.msu.edu/classes/ece480/capstone/spring15/group02/assets/docs/nsappnote.pdf
I have followed article's code, 
but it appears:

How can i solve this problem?
",irobot-create
"Difference between SLAM and ""3D reconstruction""?","I'm reading this paper: http://arxiv.org/abs/1310.2053 (The role of RGB-D benchmark datasets: an overview) and see the following words:

Thanks to accurate depth data, currently published papers could
  present a broad range of RGB-D setups addressing well-known problems
  in computer vision in which the Microsoft Kinect ranging from SLAM
  [10, 12, 19, 17, 35, 11] over 3d reconstruction [2, 33, 38, 32, 1]
  over realtime face [18] and hand [30] tracking to motion capturing and
  gait analysis [34, 41, 8, 7, 4]

I thought of the term SLAM and 3D Reconstruction being the same thing, while the paper says the opposite with a bunch of citations (which still haven't tell the two apart).
In my opinion, Mapping in SLAM is the same term as 3D Reconstruction, while Localization is the essential part for Mapping. So I don't find a difference between SLAM and 3D Reconstruction, am I wrong (or is the author misclassfying)?
","slam, 3d-reconstruction"
Stereo vision for outdoor obstacle detection,"I'm trying to detect obstacles for a distance of up to 10 meters in an outdoor environment. Up to meaning that I also want to be able to detect obstacles that are close to the robot. I am thinking of doing this using stereo vision, but I am unsure if this is in fact even possible (before I buy expensive hardware). So is it possible? Has anyone had any success?
If this isn't possible, then what kind of sensors could give me a decent point cloud for such a range (outdoors)? I need a sensor that will fit a medium size robot. Also it needs to be not overly expensive since I have a limited budget.
Thanks
","sensors, stereo-vision"
I-Robot Create 2 Connectors,"Am I correct in assuming that the I-Robot Create 2 does not have the 25-pin connector like the original version of I-Robot Create has? Thanks much...Rick
",irobot-create
view angle of distance sensor,"I need a distance sensor (IR or Optical or any other) with 90 degree view angle to sense a rectangle surface.
in this case sensor must putting at the same level of surface area. please help me to solve this.

",mobile-robot
What is the difference between ROSberry Pi builds?,"I went to go install ROS for my Rassberry Pi and found that there are 5 different variants. What is the difference between them and where can I go to learn about these differences for future updates?
Link to the ROSberryPi downloads I'm talking about:
http://wiki.ros.org/ROSberryPi/Setting%20up%20ROS%20on%20RaspberryPi
","ros, raspberry-pi"
Need help for a quadcopter PID,"I'm trying to make a quadcopter with Arduino.
I already have the angles (roll pitch and yaw) thanks to an IMU. They are in degrees and filtered with a complementary filter.
I want to apply a PID algorithm for each axis but I dont know if the inputs should be angles (degrees) or angular velocities in degrees per second so as to calculate the errors with respect referencies.
Which will be the difference? Which will be the best way? 
Finally, another question about a PID code: I have seen that many people don't include time in their codes. For example, their derivative term is kd×(last error-actual error) instead kd×(last error-actual error)/looptime and something similar with the integrative term. Which is the difference?
Thank you in advanced.
","quadcopter, pid"
Determining the graspable range of a robot arm,"I have a 6 DOF robot arm, and I want to do some object grasping experiments. Here, the robot is rigidly mounted to a table, and the object is placed on a different adjacent table. The robot must pick up the object with its gripper parallel to the normal of the object's table, such that it is pointing directly downwards at the point of grasping.
Now, the two tables have adjustable heights. For any given height difference between them. there will be a fixed range of positions over which the robot arm can achieve this perpendicular pose. What I am trying to figure out, is what the optimum relative distance of the tables is such that this range of positions is maximum.
Is there a way to compute this analytically given the robot arm kinematics? Or is there a solution which applies to all robot arms (e.g. it is optimum when the tables are at the same height)?
If it is important, the arm is the Kinova MICO: https://www.youtube.com/watch?v=gUrjtUmivKo.
Thanks!
","robotic-arm, kinematics"
Smart Home Model with Raspberry Pi,"I'm still new to RPi and I am currently trying to do a smart home model. 
I planned to use RPi only to control 5 servos (which will be controlling the open/close of the doors by setting the angle) and 5 LEDs. 
Will I need to use an external circuit to supply the power for the servos or is it fine to just connect them to the RPi?
","raspberry-pi, rcservo"
Battery damaged?,"Could you please see the attached battery images and tell me if it is safe to continue using this battery or should I discard it?

","battery, lithium-polymer"
Will this pseudocode work as a basis for a flight controller?,"I'm programming a flight controller on an Arduino. I've researched how other people have written theirs but without notes it's often so obfuscated that I've decided it will be easier and better to write my own.
This is my pseudocode thus far, will this work?
all of this will happen inside the constant Arduino loop

Read RX signal
Calculate desired pitch, roll, and yaw angles from RX input
Signal ESCs using PWM in order to match desired pitch, roll, and yaw from RX input
Gather IMU values (using Kalman filter to reduce noise)
Compare filtered IMU values vs. RX input to find errors in desired outcome vs. actual outcome
Use PID algo to settle errors between IMU vs. RX
Rinse and repeat

Suggestions are greatly appreciated
","arduino, microcontroller, uav"
How is the absolute flash size calculated in a microcontroller?,"I am working with an STM32F103C8 which has a flash size of 64kBytes.
Now i am using ChibiOS 2.6 and the build file is a binary file of 82kBytes.
Using ST-Link Utility, the program is getting dumped into the microcontroller's flash. 
My question is how come a 82kB code fits in the 64kB Flash?
How is the size of that .bin calculated? I am attaching a picture of the display. 
I did a compare ch.bin with device memory and it doesn't report any errors found.
All parts of the code work just fine, i don't see any problems anywhere tried all the features of the code, nothing breaks or behaves abnormally.
Could someone please explain this?
Thanks!

",microcontroller
Sensors in Collaborative Robots,"I'm currently doing some research on collaborative robotics. One area of interest is the type of sensor(s) used in these kind of machines. 
I had a look at some robots by FANUC and Universal Robots and I've noticed that they do not come equipped with sensors; they are sold as an add-on.
Is this inherent with collaborative robots? Do customers need to buy sensors as an add-on - which has advantages and disadvantages. 
Thanks for your help.
","sensors, industrial-robot"
Directly observing lidar laser rays,"I am working with SICK lidars and have to mount/unmount them quite often on my robot. The mounting process is very tedious especially when it comes to making sure that the lidars are horizontal. I thought about using IR goggles (like the night vision ones) and some fog machine (like the one in nightclubs) in order to see the surface covered by the lidar's rotating laser ray. As a result I would expect to see something like this but planar. 
Before thinking about trying to get my hands on such hardware I wanted to ask:
Do sick laser have enough intensity to be observed by such goggles?
Does anybody tried such an approach?
","laser, lidar"
What is the interpretation of unsampled particles in particle filters?,"I implemented Particle Filters few years back. I was experimenting few things with Particle Filters.
I learned that we can resolve Robot Kidnapping problem by introducing new particles.
What if we left some particles unsampled e.g. 1% of the population

What is the interpretation of unsampled particles in this context?
How they can effect our localization output?


","localization, algorithm, particle-filter"
Q Learning And Kohonen Maps For Line Follower Robot,"I'm trying to build a line follower robot and I'm interested in predicting the curves on the track.
I have 8 binary sensor array(qre1113).
My goal is to make a system that it can generalize what it learned about the curves and give me predictions about where should be at the line to pass it as fast as possible.
How can I integrate a system like Q learning and how can I train it?
And also how can I combine this system with a Type C PID controlller ?
There is a paper about it ot you are willing to explain
This is a important project for me and i am kinda running on clock so quick help would be appreciated
",differential-drive
Need help regarding development of Extended Kalman Filter for sensor-data fusion of odometry and IMU data,"I'm trying to develop an Extended Kalman Filter (EKF) for the positioning of a wheeled vehicle. I have a 'Baron' robot frame with 4 static wheels, all driven by a motor. On the 2 rear wheels I have an encoder. I want to fuse this odometry data with data from an 'MPU9150' 9 DOF IMU. 
This is my mathlab code for the what I call 'medium-size' EKF. This uses data from encoders, accelerometer in x and y axis and gyroscope z-axis.
Medium-size EKF

Inputs:   x: ""a priori"" state estimate vector (8x1)
           t: sampling time [s]
           P: ""a priori"" estimated state covariance vector (8x8)
           z: current measurement vector (5x1) (encoder left; encoder right; x-acceleration, y-acceleration, z-axis gyroscope)
Output:   x: ""a posteriori"" state estimate vector (8x1)
           P: ""a posteriori"" state covariance vector (8x8)
State vector x: a 8x1 vector $\begin{bmatrix} x \rightarrow X-Position In Global Frame \\ \dot x \rightarrow Speed In X-direction Global Frame \\ \ddot x \rightarrow Acceleration In X-direction Global Frame \\ y \rightarrow Y-Position In Global Frame \\ \dot y \rightarrow Speed In Y-direction Global Frame \\ \ddot y \rightarrow Acceleration In Y-direction Global Frame \\ \theta \rightarrow Vehicle Angle In Global Frame \\ \dot \theta \rightarrow Angular Speed Of The Vehicle \end {bmatrix}$
Measurement vector z: 
  a 5x1 vector $\begin{bmatrix} \eta_{left} \rightarrow Wheelspeed Pulses On Left Wheel \\ \eta_{right} \rightarrow Wheelspeed Pulses On Right Wheel \\ \dot \theta_z \rightarrow GyroscopeMeasurementInZ-axisVehicleFrame \\ a_x \rightarrow AccelerometerMeasurementX-axisVehicleFrame \\ a_y \rightarrow AccelerometerMeasurementY-axisVehicleFrame \end {bmatrix}$

function [x,P] = moodieEKFmedium(x,t,P,z,sigma_ax,sigma_ay,sigma_atau,sigma_odo,sigma_acc,sigma_gyro)

% Check if input matrixes are of correct size
[rows columns] = size(x);
if (rows ~= 8 && columns ~= 1)
    error('Input vector size incorrect')
end
[rows columns] = size(z);
if (rows ~= 5 && columns ~= 1)
    error('Input data vector size incorrect')
end

% Constants
n0 = 16;
r = 30;
b = 50;

Q = zeros(8,6);
Q(3,3) = sigma_ax;
Q(6,6) = sigma_ay;
Q(8,8) = sigma_atau;
%[Q(1,8),Q(3,6),Q(6,3)] = deal(small);

dfdx = eye(8);
[dfdx(1,2),dfdx(2,3),dfdx(4,5),dfdx(5,6),dfdx(7,8)] = deal(t);
[dfdx(1,3),dfdx(4,6)] = deal((t^2)/2);

dfda = zeros(6,6);
[dfda(3,3),dfda(6,6),dfda(8,8)] = deal(1);

dhdn = eye(5,5);

R = zeros(5,5);
[R(1,1),R(2,2)] = deal(sigma_odo);
R(3,3) = sigma_gyro;
[R(4,4),R(5,5)] = deal(sigma_acc);
%[R(2,1),R(1,2)] = deal(small);


% Predict next state
% xk = f(xk-1)
xtemp = zeros(8,1);
xtemp(1) = x(1) + t*x(2)+((t^2)/2)*x(3);
xtemp(2) = x(2) + t*x(3);
u1 = normrnd(0,sigma_ax);
xtemp(3) = x(3) + u1;

xtemp(4) = x(4) + t*x(5)+((t^2)/2)*x(6);
xtemp(5) = x(5) + t*x(6);
u2 = normrnd(0,sigma_ay);
xtemp(6) = x(6) + u2;

xtemp(7) = x(7) + t*x(8);
u3 = normrnd(0,sigma_atau);
xtemp(8) = x(8) + u3;

x = xtemp

% Predict next state covariance
% Pk = dfdx * Pk-1 * transpose(dfdx) + dfda * Q * transpose(dfda)
P = dfdx * P * transpose(dfdx) + dfda * Q * transpose(dfda);

% Calculate Kalman gain
% Kk = P * transpose(dhdx) [dhdx * P + dhdn * R * transpose(dhdn)]^-1
dhdx = zeros(5,8);
if(x(2) == 0 && x(5) == 0)
    [dhdx(1,2),dhdx(2,2)] = deal(0);
    [dhdx(1,4),dhdx(2,4)] = deal(0);
else
    [dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(5)^2)));
    [dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(5)/sqrt(x(2)^2+x(5)^2)));
end
%[dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(5)^2)));
%[dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(5)/sqrt(x(2)^2+x(5)^2)));
dhdx(1,6) = (t*n0*b)/(2*pi*r);
dhdx(2,6) = -(t*n0*b)/(2*pi*r);

dhdx(4,3) = sin(x(7));
dhdx(4,6) = -cos(x(7));
dhdx(4,7) = (x(3)*cos(x(7)))+(x(6)*sin(x(7)));

dhdx(5,3) = cos(x(7));
dhdx(5,6) = sin(x(7));
dhdx(5,7) = (-x(3)*sin(x(7)))+(x(6)*cos(x(7)));

Kk = P * transpose(dhdx) * (dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn))^(-1)

% Update state
H = zeros(5,1);
n1 = normrnd(0,sigma_odo);
H(1) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))+(((t*n0*b)/(2*pi*r))*x(6)) + n1;
n2 = normrnd(0,sigma_odo);
H(2) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))-(((t*n0*b)/(2*pi*r))*x(6)) + n2;
n3 = normrnd(0,sigma_gyro);
H(3)= x(8) + n3;
n4 = normrnd(0,sigma_acc);
H(4)=(x(3)*sin(x(7))-(x(6)*cos(x(7))))+n4;
n5 = normrnd(0,sigma_acc);
H(5)=(x(3)*cos(x(7))+(x(6)*sin(x(7))))+n5;

x = x + Kk*(z-H)

% Update state covariance
P = (eye(8)-Kk*dhdx)*P;

end
This is the filter in schematic :

These are the state transition equations I use : 
$$\ x_{t+1} = x_{t} + T \cdot \dot x_{t} + \frac{T^{2}}{2} \cdot \ddot x_{t}$$
$$\ \dot x_{t+1} = \dot x_{t} + T \cdot \ddot x_{t} $$
$$\ \ddot x_{t+1} = \ddot x_{t} + u_{1} $$
$$\ y_{t+1} = y_{t} + T \cdot \dot y_{t} + \frac{T^{2}}{2} \cdot \ddot y_{t}$$
$$\ \dot y_{t+1} = \dot y_{t} + T \cdot \ddot y_{t} $$
$$\ \ddot y_{t+1} = \ddot y_{t} + u_{2} $$
$$\ \dot \theta_{t+1} = \dot \theta_{t} + T \cdot \ddot \theta_{t} $$
$$\ \ddot \theta_{t+1} = \ddot \theta_{t} + u_{3} $$
These are the observation equations I use :
$$\ \eta_{left} = \frac{T \cdot n_{0}}{2 \cdot \pi \cdot r} \cdot \sqrt{\dot x^{2} + \dot y^{2}} + \frac{T \cdot n_{0} \cdot b}{2 \cdot \pi \cdot r} \cdot \dot \theta + n_{1}$$
$$\ \eta_{right} = \frac{T \cdot n_{0}}{2 \cdot \pi \cdot r} \cdot \sqrt{\dot x^{2} + \dot y^{2}} - \frac{T \cdot n_{0} \cdot b}{2 \cdot \pi \cdot r} \cdot \dot \theta + n_{2}$$
$$\ \dot \theta_{z} = \dot \theta + n_{3}$$
$$\ a_{x} = \ddot x \sin \theta - \ddot y \cos \theta + n_{4}$$
$$\ a_{y} = \ddot x \cos \theta + \ddot y \sin \theta + n_{5}$$
Small-size EKF
I wanted to test my filter, therefore I started with a smaller one, in which I only give the odometry measurements as input. This because I know that if I always receive the same amount of pulses on the left and right encoder, than my vehicle should be driving a straight line. 

Inputs:   x: ""a priori"" state estimate vector (6x1)
           t: sampling time [s]
           P: ""a priori"" estimated state covariance vector (6x6)
           z: current measurement vector (2x1) (encoder left; encoder right)
Output:   x: ""a posteriori"" state estimate vector (6x1)
           P: ""a posteriori"" state covariance vector (6x6)
State vector x: a 6x1 vector $\begin{bmatrix} x \rightarrow X-Position In Global Frame \\ \dot x \rightarrow Speed In X-direction Global Frame  \\ y \rightarrow Y-Position In Global Frame \\ \dot y \rightarrow Speed In Y-direction Global Frame \\  \theta \rightarrow Vehicle Angle In Global Frame \\ \dot \theta \rightarrow Angular Speed Of The Vehicle \end {bmatrix}$
Measurement vector z:
  a 2x1 vector $\begin{bmatrix} \eta_{left} \rightarrow Wheelspeed Pulses On Left Wheel \\ \eta_{right} \rightarrow Wheelspeed Pulses On Right Wheel \end {bmatrix}$

% Check if input matrixes are of correct size
[rows columns] = size(x);
if (rows ~= 6 && columns ~= 1)
    error('Input vector size incorrect')
end
[rows columns] = size(z);
if (rows ~= 2 && columns ~= 1)
    error('Input data vector size incorrect')
end

% Constants
n0 = 16;
r = 30;
b = 50;

Q = zeros(6,6);
Q(2,2) = sigma_ax;
Q(4,4) = sigma_ay;
Q(6,6) = sigma_atau;
%[Q(1,8),Q(3,6),Q(6,3)] = deal(small);

dfdx = eye(6);
[dfdx(1,2),dfdx(3,4),dfdx(5,6)] = deal(t);

dfda = zeros(6,6);
[dfda(2,2),dfda(4,4),dfda(6,6)] = deal(1);

dhdn = eye(2,2);

R = zeros(2,2);
[R(1,1),R(2,2)] = deal(sigma_odo);
%[R(2,1),R(1,2)] = deal(small);


% Predict next state
% xk = f(xk-1)
xtemp = zeros(6,1);
xtemp(1) = x(1) + t*x(2);
u1 = normrnd(0,sigma_ax);
xtemp(2) = x(2) + u1;
xtemp(3) = x(3) + t*x(4);
u2 = normrnd(0,sigma_ay);
xtemp(4) = x(4) + u2;
xtemp(5) = x(5) + t*x(6);
u3 = normrnd(0,sigma_atau);
xtemp(6) = x(6) + u3;

x = xtemp

% Predict next state covariance
% Pk = dfdx * Pk-1 * transpose(dfdx) + dfda * Q * transpose(dfda)
P = dfdx * P * transpose(dfdx) + dfda * Q * transpose(dfda);

% Calculate Kalman gain
% Kk = P * transpose(dhdx) [dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn)]^-1
dhdx = zeros(2,6);
if((x(2) < 10^(-6)) && (x(4)< 10^(-6)))
    [dhdx(1,2),dhdx(2,2)] = deal((t*n0)/(2*pi*r));
    [dhdx(1,4),dhdx(2,4)] = deal((t*n0)/(2*pi*r));
else
    [dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(4)^2)));
    [dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(4)/sqrt(x(2)^2+x(4)^2)));
end
%[dhdx(1,2),dhdx(2,2)] = deal(((t*n0)/(2*pi*r))*(x(2)/sqrt(x(2)^2+x(4)^2)));
%[dhdx(1,4),dhdx(2,4)] = deal(((t*n0)/(2*pi*r))*(x(4)/sqrt(x(2)^2+x(4)^2)));
dhdx(1,6) = (t*n0*b)/(2*pi*r);
dhdx(2,6) = -(t*n0*b)/(2*pi*r);

Kk = P * transpose(dhdx) * ((dhdx * P * transpose(dhdx) + dhdn * R * transpose(dhdn))^(-1))

% Update state
H = zeros(2,1);
n1 = normrnd(0,sigma_odo);
H(1) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))+(((t*n0*b)/(2*pi*r))*x(6)) + n1;
n2 = normrnd(0,sigma_odo);
H(2) = (((t*n0)/(2*pi*r))*sqrt(x(2)^2+x(4)^2))-(((t*n0*b)/(2*pi*r))*x(6)) + n2;

x = x + Kk*(z-H)

% Update state covariance
P = (eye(6)-Kk*dhdx)*P;

end
Odometry observation equations
If you would wonder how I come to the observation equations for the odometry data: 

$\ V_{vl} = V{c} + \dot \theta \cdot b \rightarrow V_{vl} = \sqrt{ \dot x^{2} + \dot y^{2}} + \dot \theta \cdot b$
Problem
If I try the small-size EKF, using a Matlab user interface, it does seem to drive a straight line, but not under a heading of 0° like I would expect. Eventhough I start with a state vector of $\ x= \begin{bmatrix}0\\0\\0\\0\\0\\0\end{bmatrix}$ meaning starting at position [0,0] in the global coordinate frame, with speed and acceleration of zero and under an angle of 0°.

In the top right corner you can see the measurement data which I give as input, which is 5 wheelspeed counts on every wheel, every sampling period. (Simulating straight driving vehicle)
In the top left corner you see a plot of the X and Y coordinate (from state vector) at the end of one predict+update cycle of the filter, labeled with the timecycle.
Bottom left corner is a plot of the angle in the state vector. You see that after 12 cycles the angle is still almost 0° like I would expect.
Could anyone please provide some insights in to what could be wrong here? 
Solutions I've been thinking on

I could use the 'odometry motion model' like explained in this question. The difference is that the odometry data is inserted in the predict step of the filter. But if I would do this, I see 2 problems: 1) I don't see how to make a small-size version of this for testing purposes, because I don't know which measurements to add in the update-step and 2) for the medium-size version I don't know how to make the observation equations as the state vector doesn't imply velocity and acceleration.
I could use the 'odometry motion model' and in the update step use the Euler-angle, which can be linked to $\ \theta $. This Euler-angle I can obtain from the Digital Motion Processor (DMP), implemented in the IMU. Then it is no problem that angular velocity is not in the state matrix. But than I still have a problem with the acceleration observation equations. 

","kalman-filter, imu, sensor-fusion, odometry"
Standard equation for steering differential drive robot,"I am writing a code in Arduino IDE for NodeMCU Board to control a differential drive 2 wheeled robot.
I am able to steer only one direction for some reason and the steering response time is a little awkward.
Is there perhaps a better strategy for the code that I am using?` 
I am using an app called Blynk that has a virtual joystick that controls that feeds the data through Virtual Pins. V1 param 0 and 1 are x and y. x would be left to right on the joystick and y would be forward and back. 
Information about the App is available here: http://www.blynk.cc/. I have it working for the most part, but there is some latency since it is through a cloud service.
The main problem I am stuck on is steering while driving forward and backward. 
    //#define BLYNK_DEBUG
//#define BLYNK_PRINT Serial    // Comment this out to disable prints and save space
#define BLYNK_PRINT Serial    // Comment this out to disable prints and save space
#include <ESP8266WiFi.h>
#include <BlynkSimpleEsp8266.h>


int motorA ;
int motorB ;
int X=0;
int Y=0;
int Steer=0;
int maximo=0;

// You should get Auth Token in the Blynk App.
// Go to the Project Settings (nut icon).
char auth[] = ""b41ff7f1659b4badb694be4c59601c2c"";

void setup()
{
  // Set console baud rate
  Serial.begin(9600);


 Blynk.begin(auth,""100Grand"",""Mob4life"");

 pinMode(motorA, OUTPUT); 
 pinMode(motorB, OUTPUT);
 pinMode(0,OUTPUT);
 pinMode(2,OUTPUT);
 pinMode(4,OUTPUT);
 pinMode(5,OUTPUT);

}

 BLYNK_WRITE(V1) 
{
  int X1 = param[0].asInt();
  X=X1;
  int Y1 = param[1].asInt();
 Y=Y1;

}

 BLYNK_WRITE(V0)//      slider  de 100 a 255!!!!
{
 int vel = param.asInt(); 
 maximo=vel;
}

void loop()
{

  if(X == 128  &&  Y == 128)  //  Stop
  {
   motorA = 0;
   motorB = 0;
   analogWrite(5, motorA);  
   analogWrite(4, motorA);
   analogWrite(0, motorB);  
   analogWrite(2, motorB);
   } 

   if(Y > 130 && X > 127 && X < 129)   //Forward
  {
    motorA = Y;
    motorB = Y;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA);
    digitalWrite(0,LOW);
    motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB);
    digitalWrite(2,HIGH);
  }

  else if(Y < 126 && X > 127 && X < 129)   //Reverse
  {
    motorA = Y;
    motorB = Y;

    motorA = map(motorA, 450,maximo,126, 0);
    analogWrite(5, motorA);
    digitalWrite(0,HIGH);
    motorB = map(motorA, 450,maximo,126, 0);//something is wrong with HIGH signal
    analogWrite(4, motorB);
    digitalWrite(2,LOW);
  }

   if(Y > 130 && X < 126)   //Steer Left
  {
    motorA = Y;
    motorB = Y;
    Steer = map(X, 450,maximo, 126,0);
     Steer = X / maximo;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA * (1 + Steer));
    digitalWrite(0,LOW);
   motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB * (1 - Steer));
    digitalWrite(2,HIGH);
  }

   if(Y > 130 && X > 130)   //Steer Right
  {
    motorA = Y;
    motorB = Y;
    Steer = map(X, 450,maximo, 126,0);
    Steer = X / maximo;

    motorA = map(motorA, 450,maximo,130, 255);
    analogWrite(5, motorA * (1 - Steer));
    digitalWrite(0,LOW);
    motorB = map(motorA, 450,maximo,130, 255);
    analogWrite(4, motorB * (1 + Steer));
    digitalWrite(2,HIGH);
  }

  Blynk.run();
}

Any help would be appreciated. Thanks!`
",differential-drive
Malfuction in motors using L298N,"I have purchased the undermentioned robot chassis with DC motors supported with plastic gears from a local store. There is a 3A battery holder and when I connect robot to that and put it on the ground both the motors working fine and smooth. But when I connect the L298N motor controller which was made by myself and tested with proteus, only one motor is working. When I switch the wires of both motors, still the motor which worked before is working and the other motor never runs unless I manually give a little rotation to the wheel.
I use a PIC18F4431 to control the motor controller and tried USB power and a 5V regulator created by myself and the result is the same in both occasions. What could be the issue here? If I tried oiling the malfunctioning motor will it work? I heard one of my friends having the same issue, one motor is not working. But both motors work with just two pen torch batteries and can't think of a valid reason that the motor is faulty. May be my motor controller? But when I swap the wires from faulty to the working one, the working one still works as I've mentioned above.


",mobile-robot
How to check which Gazebo/ODE functions are being called?,"I'm trying to take a simple event in which the Atlas steps on the ground plane. I want to see which functions ODE calls and the functions ODE uses to determine the constraint forces. I'd like to see this happen while the simulation is running. Is there a way I could do that? I'd like to know what constraint equations and constraint forces ODE is using for that particular case. Thanks.
","ros, dynamics, gazebo"
battery question for DW558 Explorer (small quadrocopter),"I recently bought a DW558 Quadrocopter (http://www.gearbest.com/rc-quadcopters/pp_110531.html).
After few minutes, the battery is dead. Which is understandable since the battery is so tiny. It is a 3.7V 250mAh battery, included with the Quadrocopter. I was thinking about buying spare batteries for it, and I have few questions about this:

1: Can I buy any kind of battery 3.7V 250mAh of the same size or is there any other property I have to pay attention?
2: Can I buy batteries of 3.7V and 350mAh (100 more than the included battery) and expect my Quadrocopter to be more ""energic""? Is it bad to buy batteries with more mAh ?
2b: If I buy few 3.7V 350mAh batteries, will I be able to charge them with the same charger I got with my 3.7V 250 mAh batteries or do I have to buy a specific charger for these too?

(these are the batteries I want to buy, any comment is greatly appreciated: 350mAh batteries x5 http://www.gearbest.com/rc-quadcopter-parts/pp_196991.html and/or 4x 250mAh batteries + charger http://www.gearbest.com/rc-quadcopter-parts/pp_331372.html)
Thank you very much for your input. I think I just discovered my new hobby and I can't wait to have my spare batteries!
","quadcopter, battery"
quaternion implementation,"I am trying to implement quaternions and i am using CC2650 sensortag board from TI. This board has MPU9250 from invensense which has Digital Motion Processor (DMP) in it. This DMP gives quaternion, but for my understanding i implemented my own quaternion. I used Gyroscope and acceleorometer values coming out of DMP (which are calibrated ) to calculate angle of rotation. I feed this angle, in 3 directions (x,y,z), to my quaternion. I am not able to match my quaternion values with DMP quaternion values. In fact it's way off, so wondering what I have done wrong.
Following are detailed steps that i did :
1)  Tapped Gyro sensor values from function “read_from_mpl”.
2)  Converted gyro values in to float by diving by 2^16. As gyro values are in Q16 format.
3)  Now used Gyro values of 3 axis and found out resultant using formula : 
Gr = sqrt(Gx^2+Gy^2+Gz^2)
Where Gx,Gy and Gz are Gyro values along x-axis,y-axis and z-axis respectively.
4)  Now Angle is derived using above found resultant Gr by : 
*Angle = Gr*1/sample_rate*
       Where sample_rate is found using API call ,mpu_get_sample_rate(&sample_rate)
5)  This Angle is fed to angle_to_quater function which basically converts angle to axis and then quaternion multiplication.
/* Angle to axis and quaternion multiplication: */
temp.w = cos((Angle*1.0/RAD_TO_DEG)/2);
temp.x = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.y = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.z = sin((Angle*1.0/RAD_TO_DEG)/2);
temp.x = temp.x *gyro_axis[0];//gyro_axis[0]=Gx
temp.y = temp.x *gyro_axis[1]; //gyro_axis[0]=Gy
temp.z = temp.x *gyro_axis[2]; //gyro_axis[0]=Gz
/* quaternion multiplication and normalization */
res = quat_mul(*qt,temp);
quat_normalize(&res);
*qt = res;*   

6)  I also added  doing angle calculations from accelerometer as follows : Here also accelerometer is converted to float by dividing by 2^16, as acceleorometer values also in Q16 format.
*//acc_data[0]->Ax, acc_data[1]->Ay, acc_data[2]->Az
temp = (acc_data[0]*acc_data[0]) + (acc_data[1]*acc_data[1]);
acc_angle[0]=atan2(acc_data[2],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[2]*acc_data[2]);
acc_angle[1]=atan2(acc_data[0],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[0]*acc_data[0]);
acc_angle[2]=atan2(acc_data[1],temp)*RAD_TO_DEG;*

*Find resultant angle of this also as :
inst_acc_angle = (sqrt(acc_angle[0]*acc_angle[0] + acc_angle[1]*acc_angle[1] + acc_angle[2]*acc_angle[2]));*

7)  Then complimentary filter is :
*FinalAngle = 0.96*Angle + 0.04*inst_acc_angle;
This Final Angle is fed to step 5 to get quaternion.*
Quaternion multiplication is done as below and then normailized to get new quaternion (q). 
quater_mul :
q3.w = -q1.x * q2.x - q1.y * q2.y - q1.z * q2.z + q1.w * q2.w;
q3.x =  q1.x * q2.w + q1.y * q2.z - q1.z * q2.y + q1.w * q2.x;
q3.y = -q1.x * q2.z + q1.y * q2.w + q1.z * q2.x + q1.w * q2.y;
q3.z =  q1.x * q2.y - q1.y * q2.x + q1.z * q2.w + q1.w * q2.z;

quat_normalize:
double mag = pow(q->w,2) + pow(q->x,2) + pow(q->y,2) + pow(q->z,2);
mag = sqrt(mag);
q->w = q->w/mag;
q->x = q->x/mag;
q->y = q->y/mag;
q->z = q->z/mag;

When i check my quaternion values with DMP, they are WAY off. Can you please provide some insights in to what could be wrong here. 
Source code :
acc_data[0]=data[0]/65536.0;
acc_data[1]=data[1]/65536.0;
acc_data[2]=data[2]/65536.0;
double temp = (acc_data[0]*acc_data[0]) + (acc_data[1]*acc_data[1]);
acc_angle[0]=atan2(acc_data[2],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[2]*acc_data[2]);
acc_angle[1]=atan2(acc_data[0],temp)*RAD_TO_DEG;
temp = (acc_data[1]*acc_data[1]) + (acc_data[0]*acc_data[0]);
acc_angle[2]=atan2(acc_data[1],temp)*RAD_TO_DEG;*

gyro_rate_data[0]=data[0]/65536.0;
gyro_rate_data[1]=data[1]/65536.0;
gyro_rate_data[2]=data[2]/65536.0;

float inst_angle = (sqrt(gyro_rate_data[0]*gyro_rate_data[0] +  gyro_rate_data[1]*gyro_rate_data[1] + gyro_rate_data[2]*gyro_rate_data[2]));
gyro_rate_data[0] = gyro_rate_data[0]/inst_angle;
gyro_rate_data[1] = gyro_rate_data[1]/inst_angle;
gyro_rate_data[2] = gyro_rate_data[2]/inst_angle;
inst_angle = inst_angle *1.0/sam_rate;
float inst_acc_angle = (sqrt(acc_angle[0]*acc_angle[0] + acc_angle[1]*acc_angle[1] + acc_angle[2]*acc_angle[2]));
inst_angle = WT*inst_angle + (1.0-WT)*inst_acc_angle;

angle_to_quat(inst_angle,gyro_rate_data,&q);

/* The function for angle to quaterinion and multiplication,normalization */
void angle_to_quat(float Angle,float *gyro_axis,struct quat *qt)
{
    struct quat temp;
    struct quat res;
    temp.w = cos((Angle*1.0/RAD_TO_DEG)/2);
    temp.x = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.y = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.z = sin((Angle*1.0/RAD_TO_DEG)/2);
    temp.x = temp.x *gyro_axis[0];
    temp.y = temp.x *gyro_axis[1];
    temp.z = temp.x *gyro_axis[2];
    res = quat_mul(*qt,temp);
    quat_normalize(&res);
    *qt = res;
}


This variation is coming when i am keeping my device stationary.
Y-Axis : Resultant of all 3 gyro axis.
X-axis : The number of samples. (have not converted them to time)
Sample_rate is 3Hz.
",sensor-fusion
Working of Autonomous Lawn mower(ALM) in an unbounded area without a perimeter wire,"I have an Autonomous Lawn mower(ALM) which can mow a certain lawn area when that area is bounded by a perimeter wire. Even when that perimeter wire is removed, it has to mow the above mentioned area accurately without slipping into a neighboring area.
Constraints and problems:

The ALM is an open loop system.
Differential GPS was tried, but it did not yield proper results.
Any iterative pattern of area coverage can be used provided the error in each iteration is not added cumulatively which can result in unpredictable error in the end.

I do not expect full fledged solution. But I need a starting point to understand motion planning particularly for unbounded robotics to solve this problem. 
I searched on internet to know about the knowledge sources about motion planning but could not get good results. Can anyone guide me to know about such sources preferably books and articles on internet which can help me to solve this problem?
EDIT:
Addition of information:
 
The above picture shows the irregular lawn area which does not have any enclosures and perimeter
wire
1.The red mark shows the center point of lawn .
2.The grey area is the initial scaled down area which resembles in shape to the larger area .I could not draw the grey area which exactly resembles the larger green area .
3.The grey lines are the contours which from the tracks to be followed by the lawn mower
Idea description:
1.Using planimeter app for onetime , the shape and dimension of the lawn area (green area) can be known
Link:https://play.google.com/store/apps/details?id=com.vistechprojects.planimeter&hl=en
2.Center of polygon can be found by using the method in the following link
http://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon
3.Calculation of area of grey shape in the above figure .
4 . Grey shape is the least possible area which can be grazed by the ALM . Grey shape is similar to the green area shape and it is formed when Green area is scaled down
To determine the scale down factor which is a numerical value ‘ n’ (n<1) 
Where Grey area = n * Green area
Once the Grey area is known , the number of contours or tracks to be grazed by ALM have to be determined manually .
The width of contour is equal to the distance between the blades on the either end i.e. the width which can be grazed by ALM in a single stroke .
Green area = Grey area + area of track 1 + area of track 2 + area of track3 + . . . . . . + area of track n
5.Once the lawn mower is switched on ,it should reach the center of the lawn (red mark showed in the above figure)
6.Then, ALM should graze the least possible area or grey area .
7.After that ALM should Switch to contour circumscribing the grey area . It should continue circumscribing in each track till all the tracks are completed( decision has to be made by validating against the calculated and preset value ' No.of tracks' in ALM)   
In this way entire lawn can be mowed without the need of perimeter wire and also ALM would not mow the neighbor’s lawn 
Challenges :
a. Enable ALM to reach the center point of the lawn
a. To make ALM mow the grey area accurately
b. To make the ALM switch from one track to track .
c. To bypass the obstacle in track and return to the same track .
When i mentioned this idea to my colleague ,he mentioned the about possible cumulative addition of error in each iteration resulting in an unpredictable error in the end .
I intend to minimize the error and fix the boundary as correct as possible. 
In fact this deviation should be predictable before it can be corrected .
",motion-planning
Robot positioning using IMU quaternion data?,"I want to use a MPU9150 to give me the position (XY) and heading (angle) of a wheeled robot. This MPU9150 from invensense has a Digital Motion Processor in it which can give me a quaternion. 
But how do I convert this quaternion data to an XY-coordinate and an angle so I can plot the position of my vehicle?
","wheeled-robot, imu, sensor-fusion"
Need help regarding odometry using Encoder motor and raspberry pi,"I am doing project on odometry using raspberry pi. I know that encoder motor will tell me how much distance my robot has covered, but I have no idea ho to implement completely. I just need guideline about which steps to follow. Till now I have interfaced motor with raspberry pi and counted the number of rotation. I have questions as follow?
How to plot map of odometry using which language and library?
If you know anything, just give me guideline about steps to follow.
","motor, raspberry-pi, odometry"
What are the specifications of the digital compass used in iPhone 6S,"What are the specifications of the digital compass used in the iPhone 6S?
I am trying to measure yaw angle using the magnetometer.  I observed the magnetometer/digital compass in the iPhone is really very stable. The north direction is always the same, while the magnetometer I am using (or the magnetometer used in Nexus) needs to be calibrated again and again to function properly.
I found that the digital compass AK8963C is used in the iPhone 6, but it needs calibration.  So I am not sure what is inside iPhone 6S because it works without a calibration procedure.
","imu, sensor-fusion, magnetometer"
Lidar problems in a multi-robot setup,"Consider multiple mobile bases driving around in some area. In order to get meaningful data from the lidar of each base, the sensors should be mounted as horizontal as possible. Due to safety regulations, the lidars should also be mounted at a height of 15 cm from the floor. When I checked the data sheet of SICK lidars, it shows that all models use the wavelength 904 nm. Does that mean that mobile bases equipped with lidars with a coplanar scan lines will end up mutually blinding each other? 
If it is the case, how is this problem solved? (I don't consider tilting the lidars a solution as it defeats the purpose of having ""2D"" lidars where even if the tilting angle is known, what the lidar observes becomes dependent on the robot's pose and distance from eventual obstacles)
","sensors, lidar, rangefinder"
Angle to a circle tangent line,"I want to simulate the detection of a moving object by a unicycle type robot. The robot is modelled with position (x,y) and direction theta as the three states. The obstacle is represented as a circle of radius r1 (r_1 in my code). I want to find the angles alpha_1 and alpha_2from the robot's local coordinate frame to the circle, as shown here:

So what I am doing is trying to find the angle from the robot to the line joining the robot and the circle's centre (this angle is called aux_t in my code), then find the angle between the tangent and the same line (called phi_c). Finally I would find the angles I want by adding and subtracting phi_c from aux_t. The diagram I am thinking of is shown:

The problem is that I am getting trouble with my code when I try to find the alpha angles: It starts calculating the angles correctly (though in negative values, not sure if this is causing my trouble) but as both the car and the circle get closer, phi_c becomes larger than aux_t and one of the alphas suddenly change its sign. For example I am getting this:
$$\begin{array}{c c c c} 
\text{aux_t} & \text{phi_c} & \text{alpha_1} & \text{alpha_2} \\ \hline
\text{-0.81} & \text{+0.52} & \text{-1.33} & \text{-0.29} \\
\text{-0.74} & \text{+0.61} & \text{-1.35} & \text{-0.12} \\
\text{-0.69} & \text{+0.67} & \text{-1.37} & \text{-0.02} \\
\text{-0.64} & \text{+0.74} & \text{-1.38} & \text{+0.1} \\
\end{array}$$
So basically, the alpha_2 gets wrong form here. I know I am doing something wrong but I'm not sure what, I don't know how to limit the angles from 0 to pi. Is there a better way to find the alpha angles?
","mobile-robot, kinematics, matlab, geometry"
How to find Friction or Viscous force b (nmsec) in DC motor,"PLease guide me 
How to find Friction or Viscous force b (nmsec) in DC motor for a particlar speed.
The motor is connected with a gear and the ration is 26:1
I want to find for 200 rpm and the motor no load speed is 4900rpm
please guide me
","quadcopter, wheeled-robot, brushless-motor, stepper-motor"
Path planning of 2 arm 4dof Robot,"I am working on path planning for a 2 arm 4dof (2 dof for each arm) robot. I am currently using a centralised planning methodology (considering the multi robot system as a single one with higher dof, 4 in this case) and A* algorithm to find the shortest path. The problem with this algorithm is its high computation time.Is there any way to reduce the computation time while still obtaining the shortest route ?
Note:decentralised path planning is not good enough for my case.
","robotic-arm, motion-planning, path-planning"
Generalized Voronoi Diagram,"I need to compute the Voronoi diagram for a map with some obstacles but I can't find any pseudo-code or example in MATLAB.
The ""voronoi"" function in MATLAB works with points, but in this case the obstacles are polygons (convex and non-convex). You can see the map in the attached image.

Because the obstacles are polygons I found that the Voronoi algorithm needed is the GVD (Generalized Voronoi Diagram).
Can anyone help with code or examples on internet explaining how to compute this GVD?
","mobile-robot, motion-planning, geometry"
Approach to using PID to get a differential robot driving straight,"Consider a differential drive robot that has two motorized wheels with an encoder attached to each for feedback. Supposed there is a function for each DC motor that takes a float from -1 to 1 and sets the PWM signals to provide a proportional amount of power to that motor. Unfortunately, not all motors are created equal, so sending each motor the same PWM signal makes the robot veer left or right. I'm trying to think about how to drive the robot straight using the encoders attached to each motor as input to a PID loop.
Here's how I would do it: I would take the difference between the left and right encoders, bound the error between some range, normalize it to be from [-1, 1], and then map it to the motor powers 0 to 1. So if I and D were zero, and we get an error of 1 (so the left motor has turned much more than the right motor), then left motor would be set to 0, and the right motor set to 1 (causing a hard left). 
Are there any issues with this? What is a better approach?
","pid, differential-drive"
Using robotic simulator for prediction step in probabilistic localization approaches,"Probabilistic localization approaches like Kalman or Monte Carlo benefit from an accurate prediction step. The more accurate the prediction step, the more accurate is the belief of the robots pose. In most approaches probabilistic motion models are applied, mainly because robot dynamics are more difficult to model. Still some approaches rely on dynamic models in order to increase the accuracy.
Therefore, I was wondering if it’s reasonable to utilize a robotic simulator like V-REP or Gazebo for the prediction step. The advantages I see in doing so are the following:

the robots kinematic is solved by default, simply through modeling it in the robotic simulator
the robots dynamics are taken into account
nonlinear behaviors like slippage or collision can be modelled up to a certain extend
the robots workspace is taken into account, by modeling its environment (if the robot drives against a wall previous models would predict it behind the wall, which won’t happen in a robotic simulator)

With the shown advantages I hope to achieve a more accurate prediction.
However there might be some problems using a robotic simulator. For a start it has to ensure real time behavior and there will be delay in the prediction due to the communication with the simulator.
I was looking for some papers which pick up on that idea but couldn’t find any. Are there any approaches similar to my idea? If not, are there any reasons why nobody is using a robotic simulator for the prediction? What are your opinions about my proposal?
","mobile-robot, localization, motion, simulator"
"PID Gains: Drop in control loop rate, need to retune?","Good Day,
I am working on an autonomous quadcopter. May I ask if there is a significant difference if my control loop dropped from 500Hz to 460Hz due to added lines of code that would require retuning of the PID gains? And if retuning is required, is it correct to assume that only the I and D gains should be retweaked since they are the only constants which are time dependent? Thank you :)
","quadcopter, mobile-robot, control, pid, stability"
CC3D - Replacing RC emitter with an RPi,"I am trying to control a quadcopter using te OpenPilot CC3D board and a RaspberryPi. The main idea was first replace the signals from the RC emitter to the CC3D RC receiver for an RPi connected directly to the RC receiver inputs of the CC3D.
As far as I know the RC signals to the CC3D are PWM so the RPi should be able to control the channels using RPIO library to create the PWM by software. 
But after make some tests I haven't find any way to move the motors. I am using the Ground Control System (OpenPilot Software) to configure the CC3D.
I am not sure whether I need to send the PWM signals in any order or something like that. I am also not sure how the Flight Mode Switch works, I suposse it works the same way as the other channels, using PWM.
Anyone have made anything similar to this? 
","raspberry-pi, quadcopter, uav"
CompressedImage to an Image in a node,"Update
Hey I have the following subscriber on Nvidia TX1 board running on an agricultural robot. we have the following issue with subscribing to Sensor_msgs::Compressed:
ImageConverter(ros::NodeHandle &n) :  n_(n), it_(n_)
{
  image_pub_ = it_.advertise(""/output_img"",1);

  cv::namedWindow(OPENCV_WINDOW);
  image_transport::TransportHints TH(""compressed"");
  image_sub_compressed.subscribe(n,""/Logitech_webcam/image_raw/compressed"",5,&ImageConverter::imageCallback,ros::VoidPtr(),TH);
}

And the callback function
void imageCallback(const sensor_msgs::CompressedImageConstPtr& msg)

When I compile this I get an error:
from /home/johann/catkin_ws/src/uncompressimage/src/publisher_uncompressed_images.cpp:1:
/usr/include/boost/function/function_template.hpp: In instantiation of ‘static void boost::detail::function::function_void_mem_invoker1<MemberPtr, R, T0>::invoke(boost::detail::function::function_buffer&, T0) [with MemberPtr = void (ImageConverter::*)(const boost::shared_ptr<const sensor_msgs::CompressedImage_<std::allocator<void> > >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&]’:
/usr/include/boost/function/function_template.hpp:934:38:   required from ‘void boost::function1<R, T1>::assign_to(Functor) [with Functor = void (ImageConverter::*)(const boost::shared_ptr<const sensor_msgs::CompressedImage_<std::allocator<void> > >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&]’
/usr/include/boost/function/function_template.hpp:722:7:   required from ‘boost::function1<R, T1>::function1(Functor, typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type) [with Functor = void (ImageConverter::*)(const boost::shared_ptr<const sensor_msgs::CompressedImage_<std::allocator<void> > >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&; typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type = int]’
/usr/include/boost/function/function_template.hpp:1069:16:   required from ‘boost::function<R(T0)>::function(Functor, typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type) [with Functor = void (ImageConverter::*)(const boost::shared_ptr<const sensor_msgs::CompressedImage_<std::allocator<void> > >&); R = void; T0 = const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&; typename boost::enable_if_c<boost::type_traits::ice_not<boost::is_integral<Functor>::value>::value, int>::type = int]’
/home/johann/catkin_ws/src/uncompressimage/src/publisher_uncompressed_images.cpp:27:126:   required from here

The red error statement was:
/usr/include/boost/function/function_template.hpp:225:11: error: no match for call to ‘(boost::_mfi::mf1<void, ImageConverter, const boost::shared_ptr<const sensor_msgs::CompressedImage_<std::allocator<void> > >&>) (const boost::shared_ptr<const sensor_msgs::Image_<std::allocator<void> > >&)’
           BOOST_FUNCTION_RETURN(boost::mem_fn(*f)(BOOST_FUNCTION_ARGS));

I am not using BOOST, and searching around hasn't helped me solve it
","ros, c++, opencv"
Motor Choice given size constraint and load requirement,"Good day everyone :)
I am an undergraduate student working on a project involving the use of high torque small-sized DC motors for use in designing a person following trolley bag. Where in the problem is to use small sized motors but still maintain the usability and efficiency in carrying loaded luggages.
I have been looking for motors in local stores as well as in RS components as well as Element 14. However I am not sure if my choices are the right fit as I am at a loss on what specifications to look for when selecting a particular motor for this application. I have also tried to look for motors used in current products that can be used in my application such as todays electric skate boards but unfortunately have no luck on finding their suppliers.

Basically the question I would like to ask  is what specifications or calculations can I perform to select the proper motors given size constraints and weight carrying requirments. Or does anyone have suggestions on common motors that are already normally used for this application. My target maximum load capacity is 20kg.

Thank you!
","mobile-robot, motor, gearing"
"Question for those who have experience using stereo cameras/module (e.g. ZED, DUO M, Bumblebee, etc.)","This is a question for those of you who have experience using stereo cameras/modules like the ZED, DUO M, Bumblebee cameras, etc. (not TOF cameras). I can't find any sample disparity outputs out there on the internet, and I can't find any information on how they perform. Basically here are a few things I'd like to know to those of you who used any of the cameras mentioned above (and others)

What resolution and no. of disparities did you work with? 
How was the framerate? 
On what hardware?
Did the camera have an ASIC of some sort to produce the disparity maps, or did it require a host?
How was the quality?

For those who used the ZED camera, there is a promotional video on youtube. Are the disparity maps really that good?
","cameras, stereo-vision"
Typical laser scanner noise values,"I am building an application that executes graphSLAM using datasets recorded in a simulated environment. The dataset has been produced in MRPT using the GridMapNavSimul application. To simulate the laserScans one can issue the bearing and range error standard deviation of the range finder. 
Currently I am using a dataset recorded with range_noise = 0.30m, bearing_noise = 0.15deg. Am I exaggerating with these values? Could somebody provide me with typical values for these quantities? Do laser scanner manufacturers provide these values?
Thanks in advance,
","slam, laser, rangefinder"
When was the first time a robot killed a human?,"Scott Adams, creator of Dilbert, recently shared an article about a robot the police used to kill a suspect by detonating a bomb in close range.
This made me wonder -- when was the first time a robot took a human life?
Good comments were made on this which leads me to clarify that I mean a pureposeful taking of life. I shy away from the term ""murder"" because that involves legal concepts, but I mean an intentional killing.
An interesting subdivision would be between robots under active human direction (""remote control"") and those with a degree of autonomy.

",mobile-robot
Is horsepower related to torque in electric motors?,"Is torque related to size or power at all in electric motors? And what about gas motors too?
I have a go kart that is 2.5hp and it's 50cc and its about 1ft x 2ft x 1ft in size. I also see online there are .21 cubic inch gas motors for R/C cars that are also 2.5hp, the difference being that the R/C motor spins at 32k rpm while the go-kart motor spins at 12k rpm. If I were to put a gear reduction on the R/C motor, would it preform more or less the same as the go kart motor? Why is there a size difference?
Same for electric motors. I can buy an RC car electric motor that's 10hp and the size of a pop can. The CNC machine at work has a 10hp motor the size of a 5 gal bucket. Again, the only difference is the RPM.
If I were to reduce both setups so they spun at the same RPM, would they preform the same?
The only reasons I could think of is 1. Cooling and 2. RPM control (For PID loops and sensors)
","motor, power, torque, engine"
OpenRAVE output torques and simulation timestep,"I'm using OpenRAVE to simulate a quadruped, in order to get an idea of torque requirements. 
To get started I made a single DOF, single link pendulum to test controllers etc out on.
I've whipped up an inverse dynamics based PD controller using ComputeInverseDynamics(), which I set the outputs using SetDOFTorques(). I then set a desired position, with the desired velocity being zero. This all appears to work well and I can start the simulation, with the pendulum driving up to the desired position and settling. 
My concern is the value of the output torques. My pendulum is modeled as a simple box of length 1, mass manually set to 1, with a COM of 0.5.
When I run my simulation, I output the gravity component from ComputeInverseDynamics(). This gives 4.9NM, which matches up with hand calculated torques I expect from the pendulum (eg the static case) when it is driven to the desired position (from down to horizontal).
But the output torques to SetDOFTorques() are much higher and vary depending what I set the simulation timestep to.
If I maintain a controller update rate of 0.001 seconds, then for a simulation update of 0.0001 seconds, my output torque is approximately 87NM. If I alter the simulation timestep to 0.0005 seconds, keeping the controller rate the same the output torques drop down to about 18NM.
As an experiment I removed the inverse dynamics controller and replaced it with a plain PD controller, but I still see large output torques.
Can anyone shed some light on this? It's very possible I'm missing something here!
Thanks very much
Edits:
I'm adding the main section of my code. There is no trajectory generation, really. I'm just trying to get to a fixed static position.
In the code, if I keep dt fixed, and alter env.StartSimulation(timestep=0.0001), I get the issues popping up.
with env:
    robot = env.GetRobots()[0]
    robot.GetLinks()[0].SetStatic(True)
    env.StopSimulation()
    env.StartSimulation(timestep=0.0001)

dt = 0.001
w = 100
eta = 5
Kp = [w*w]
Kv = [2*eta*w]
# Desired pos, vel and acc 
cmd_p = [3.14/2]
cmd_v = [0]
cmd_a = [0]

while True:    
    with env:
        torqueconfiguration, torquecoriolis, torquegravity = robot.ComputeInverseDynamics([1],None,returncomponents=True)
        err_p = cmd_p - robot.GetDOFValues()
        err_v = cmd_v - robot.GetDOFVelocities()

        # ID Controller
        M = compute_inertia_matrix(robot, robot.GetDOFValues())
        a_cmd = (Kp*err_p + Kv*err_v + cmd_a)
        taus = torquegravity + torquecoriolis +  M.dot(a_cmd.transpose()).transpose()

        # Just PD(ish) controller
        #taus = Kp*err_p - Kv*robot.GetDOFVelocities()

        with robot:
            robot.SetDOFTorques(taus,False)     # True = use limits
            print (taus, torquegravity+torquecoriolis, a_cmd, M.dot(a_cmd.transpose()).transpose())
    time.sleep(dt)


# https://scaron.info/teaching/equations-of-motion.html
def compute_inertia_matrix(robot, q, external_torque=None):
    n = len(q)
    M = np.zeros((n, n))
    with robot:
        robot.SetDOFValues(q)
        for (i, e_i) in enumerate(np.eye(n)):
            m, c, g = robot.ComputeInverseDynamics(e_i, external_torque, returncomponents=True)
            M[:, i] = m
    return M



<?xml version=""1.0"" encoding=""utf-8""?>
<Robot name=""Pendulum"">
    <RotationAxis>0 1 0 90</RotationAxis> <!-- makes the pendulum vertical -->
    <KinBody>
        <!-- <Mass type=""mimicgeom""><density>100000</density></Mass> -->
        <Body name=""Base"" type=""dynamic"">
            <Translation>0.0  0.0  0.0</Translation>
            <Geom type=""cylinder"">
                <rotationaxis>1 0 0 90</rotationaxis>
                <radius>0.3</radius>
                <height>0.02</height>
                <ambientColor>1. 0. 0.</ambientColor>
                <diffuseColor>1. 0. 0.</diffuseColor>
            </Geom>
            <mass type=""custom"">
                <!-- specify the total mass-->
                <total>5.0</total>
                  <!-- specify the 3x3 inertia matrix-->
                  <!--<inertia>2 0 0 0 3 0 0 0 5</inertia> -->
                  <!-- specify the center of mass (if using ODE physics engine, should be 0)-->
                <com>0.1 0.0 0.0</com>
                </mass> 
        </Body>
        <Body name=""Arm0"" type=""dynamic"">
            <offsetfrom>Base</offsetfrom>
            <!-- translation and rotation  will be relative to Base -->
            <Translation>0 0 0</Translation>
            <Geom type=""box"">
                <Translation>1 0 0</Translation>
                <Extents>1 0.1 0.1</Extents>
                <ambientColor>1. 0. 0.</ambientColor>
                <diffuseColor>1. 0. 0.</diffuseColor>
            </Geom>
            <mass type=""custom"">
                <!-- specify the total mass-->
                <total>1.0</total>
                  <!-- specify the 3x3 inertia matrix-->
                  <!--<inertia>2 0 0 0 3 0 0 0 5</inertia> -->
                  <!-- specify the center of mass (if using ODE physics engine, should be 0)-->
                <com>0.5 0.0 0.0</com>
                </mass>             
        </Body>
        <Joint circular=""true"" name=""Joint0"" type=""hinge"">
            <Body>Base</Body>
            <Body>Arm0</Body>
            <offsetfrom>Arm0</offsetfrom>
            <weight>0</weight>
            <axis>0 0 1</axis>
            <maxvel>100</maxvel>
            <resolution>1</resolution>
        </Joint>
    </KinBody>
</Robot>

Here is some data for dt = 0.001 and env.StartSimulation(timestep=0.0001)
In this data,

taus is the torque command to the simulation, 
torquegravity+torquecoriolis is returned from the inverse dynamics 
a_cmd is the controller command and
M*a_cmd is the command after being multiplied by the mass matrix

The gravity and coriolis parts appear to be correct for steady state, where it should be about 4.9NM
taus, torquegravity+torquecoriolis, a_cmd, M*a_cmd
 3464.88331508,  0.48809828,  5329.83879509,  3464.39521681
 330.67177959,  1.47549936,  506.45581573,  329.19628023
-785.91806527,  2.45531014, -1212.88211601, -788.37337541
-1065.4689484,  3.23603844, -1644.16151823, -1068.70498685
-1027.47479809,  3.80261774, -1586.58063974, -1031.27741583
-877.83110127,  4.18635604, -1356.94993433, -882.01745731
-707.25108627,  4.4371714, -1094.9050118, -711.68825767
-554.34483533,  4.6006198, -859.91608481, -558.94545512
-432.22314217,  4.70818921, -672.20204828, -436.93133138
-327.797496,  4.7768792, -511.65288492, -332.5743752
-240.77203429,  4.82021019, -377.83422228, -245.59224448
-172.18942128,  4.84807059, -272.3653721, -177.03749186
-117.58895761,  4.86591166, -188.39210657, -122.45486927
-74.51920719,  4.87743369, -122.14867828, -79.39664088
-39.91183436,  4.88473444, -68.91779816, -44.7965688
-12.82321495,  4.88971433, -27.25066043, -17.71292928
 8.45349476,  4.89281357,  5.47797105,  3.56068118
 25.35468725,  4.89489884,  31.47659755,  20.45978841
 38.84080509,  4.896309,  52.22230167,  33.94449609
 48.72668147,  4.89724689,  67.42989936,  43.82943458
 56.78552877,  4.89790152,  79.82711885,  51.88762725
 65.515892,  4.89836756,  93.25772991,  60.61752444
 68.81359264,  4.89867903,  98.33063633,  63.91491362
 73.86961896,  4.89891052,  106.10878221,  68.97070844
 76.67416578,  4.89907489,  110.42321674,  71.77509088
 79.62549808,  4.89919702,  114.96354008,  74.72630105
 85.17343708,  4.89928669,  123.49869291,  80.27415039
 85.13686188,  4.89934963,  123.44232654,  80.23751225
 85.75675034,  4.89939931,  124.39592466,  80.85735103
 86.55192592,  4.89943807,  125.61921208,  81.65248785
 86.39672231,  4.89946802,  125.38039121,  81.49725429
 87.4299925,  4.89949202,  126.97000073,  82.53050048
 87.42776523,  4.8995098,  126.96654682,  82.52825543
 87.15472709,  4.8995251,  126.54646461,  82.255202
 86.97240783,  4.89953825,  126.26595319,  82.07286958
 86.98023044,  4.89954905,  126.27797137,  82.08068139
 86.75364661,  4.89955809,  125.92936696,  81.85408852
 86.9853716,  4.89956526,  126.28585591,  82.08580634
 88.01679721,  4.89957062,  127.8726563,  83.1172266
 89.2610231,  4.89957348,  129.78684557,  84.36144962
 88.47969399,  4.89957495,  128.58479851,  83.58011903
 88.77623594,  4.89957711,  129.04101359,  83.87665884
 90.87280518,  4.89957739,  132.2665043,  85.9732278
 88.9513552,  4.89957707,  129.3104279,  84.05177813
 89.14100099,  4.89957773,  129.60218964,  84.24142327

And here is some data for dt = 0.001 and env.StartSimulation(timestep=0.0005)
taus, torquegravity+torquecoriolis, a_cmd, M*a_cmd
-313.62240349,  0.98927261, -484.01796324, -314.61167611
-242.03525463,  2.00886997, -375.45249938, -244.0441246
-199.82226305,  2.79259699, -311.71516928, -202.61486003
-190.02605484,  3.39367572, -297.56881625, -193.41973056
-162.08293067,  3.8525617, -255.28537288, -165.93549237
-125.84847045,  4.17559368, -200.03702174, -130.02406413
-103.89936813,  4.40068949, -166.61547326, -108.30005762
-82.32305905,  4.5566127, -133.66103347, -86.87967175
-64.56801352,  4.66415211, -106.51102404, -69.23216563
-49.68124446,  4.73812107, -83.72210081, -54.41936553
-37.91265825,  4.78890663, -65.6947152, -42.70156488
-27.99189838,  4.82374208, -50.48560071, -32.81564046
-19.81225948,  4.84762415, -37.9382825, -24.65988362
-12.55978349,  4.8636252, -26.80524414, -17.42340869
-6.89165107,  4.87470983, -18.10209369, -11.7663609
-3.13313345,  4.88256746, -12.33184754, -8.0157009
 0.69831646,  4.88796162, -6.44560793, -4.18964516
 3.86277859,  4.89166745, -1.58290594, -1.02888886
 6.12163439,  4.8941598,  1.88842245,  1.22747459
 8.58189707,  4.89593332,  5.67071346,  3.68596375
 9.1580546,  4.89712981,  6.55526891,  4.26092479
 11.81854706,  4.89798468,  10.64701905,  6.92056238
 12.40540565,  4.89856409,  11.54898701,  7.50684156
 14.04109075,  4.89897979,  14.06478609,  9.14211096
 14.39924399,  4.89926951,  14.61534535,  9.49997448
 14.98060951,  4.89947252,  15.50944153,  10.08113699
 16.08890875,  4.89961544,  17.2142974,  11.18929331
 16.01955973,  4.89971637,  17.10745133,  11.11984337
 17.06493791,  4.89978831,  18.71561478,  12.16514961
 17.35364328,  4.89983976,  19.15969772,  12.45380352
 17.62239334,  4.89987688,  19.57310225,  12.72251646
 17.84455913,  4.89990387,  19.91485424,  12.94465525
 17.43825648,  4.89992362,  19.28974286,  12.53833286
 17.58436934,  4.89993826,  19.51450935,  12.68443108
 17.70571012,  4.8999492,  19.70117065,  12.80576093
 18.40852272,  4.89995746,  20.78240808,  13.50856525
 18.49492461,  4.89996372,  20.91532445,  13.59496089
 18.56575802,  4.89996852,  21.02429154,  13.6657895
 18.62430693,  4.89997223,  21.11436108,  13.7243347
 16.54216482,  4.89997511,  17.91106109,  11.64218971
 18.71146936,  4.89997747,  21.24844907,  13.81149189
 18.13316504,  4.89997923,  20.35874741,  13.23318581
 18.77330006,  4.89998067,  21.34356829,  13.87331939

Despite the differences in torque command (a_cmd) I still get similar performance, in that the arm drives to the right position fairly quickly.
As another experiment I set the initial position to pi/2 and just fed back the gravity term to the torque output. My understanding of this is that the arm should float, ala a gravity compensation sort of thing. But it just drops as if a small torque is applied.
Thanks again! 
","robotic-arm, torque"
SLAM with iRobot Create 2,"I have an iRobot Create 2 and have been working with it and have gotten to the point where I can control it via Bluetooth. This is great but I also want it to be able to be autonomous and navigate itself room to room for example. Are there any SLAM iRobot tutorials or any other materials you'd recommend for autonomous navigation?
","mobile-robot, slam, irobot-create"
Can we simulate a actuator with a very strong torque with a PID controller,"I use gazebo to simulate a robot arm. To control its joints, I use PID controllers. As you might know, PID are sometimes pretty hard to tune and this is the case for a robotic arm. To avoid any tuning, and because I don't need the PID values to be realistic, I set to zero the derivative and integral parameters, increase a lot the proportional gain and add a lot of damping in my joints. By doing this, I can get a well working arm but only if I disable the gravity. 
My question is the following. Do you have an idea how I could simulate a very strong actuator with not necessarily realistic parameters?
EDIT 1: Setting the integral and derivative gain is stupid. The integral gain helps in correcting the effect of the gravity. The derivative gain counters the loss of stability and speed due to the integral gain. 
This question somehow leads to another. Do you know what tuning do the robotic arm manufacturer (big arms for car industry for example). I guess that this arm use actuators with a very strong torque and a low maximum speed which reduces the need of tuning.  
EDIT 2: More info on my setup. I use gazebo 6, with ODE. The robot description is in SDF. I control the robot with a model plugin. As a PID controler I use the PID class from the common library of gazebo and get directly the JointControler associated to the model. 
Let say that I would like actuators very robust without any tuning needed. This way I could have a simulation WITH dynamics (by opposition to the SetPosition method). Do you think it is possible ?    
","pid, power, servomotor, joint, gazebo"
Difference between 3D Camera(using IR projection) and Stereo Camera?,"I am currently busy with a final year project which requires me to track people walking through a doorway.
I initially thought this may be possible using a normal camera and using some motion detection functions given in OpenCV, I have however come to the conclusion that the the camera is mounted too low for this to work effectively.(Height shown in the image below)

I have now been looking into using a 3D camera or a stereo camera to try and get around this problem.
I have seen similar examples where a Kinect(from Xbox 360) has been used to generate a depth map which is then processed and used to do the tracking, this was however done from a higher vantage point, and I found that the minimum operating range of the Kinect is 0.5m.
From what I have found, the Kinect uses an IR projector and receiver to generate its depth map, and have been looking at the Orbbec Astra S which uses a similar system and has a minimum working distance of 0.3m.
My question now:
What exactly would the difference be between the depth maps produced by a 3D camera that uses an IR projector and receiver, and a stereo camera such as the DUO/ZED type options?
I am just looking for some insight from people that may have used these types of cameras before
On a side note, am i going about this the right way? or should i be looking into Time of Flight Cameras instead? 
----EDIT----:
My goal is to count the people moving into and out of the train doorway. I began this using OpenCV, initially with a background subtraction and blob detection method. This only worked for one person at a time and with a test video filmed at a higher vantage point as a ""blob-merging"" problem was encountered as shown in the left image below.
So the next method tested involved an optical flow method using motion vectors obtained from OpenCV's dense optical flow algorithm.
From which i was able to obtain motion vectors from the higher test videos and track them as shown in the middle image below, because of the densely packed and easily detected motion vectors it was simple to cluster them.
But when this same system was attempted with footage taken from inside a train at a lower height, it was unable to give a consistant output. My thoughts of the reasons for this was because of the low height of the camera, single camera tracking is able to function when there is sufficient space between the camera and the top of the person. But as the distance is minimized, the area of the frame that the moving person takes up becomes larger and larger, and the space to which the person can be compared is reduced (or atleast that is how I understand it). Below on the right you can see how in the image the color of the persons clothing is almost uniform, Optical flow is therefore unable to detect it as motion in both cases.

I only started working with computer vision a few months ago so please forgive me if I have missed some crucial aspects.
From what i have seen from research, most commercial systems make used of a 3D cameras, stereo cameras or Time-of-Flight cameras, but I am unsure as to how the specifics of each of these would be best suited for my application.
","computer-vision, cameras, stereo-vision"
How to determine quality of ICP matches?,"In SLAM frontends which use the Iterative Closest Point (ICP) algorithm for identifying the association between two matching point clouds, how can you determine if the algorithm is stuck in a local minimum and returns a wrong result? 
The problem is defined as matching two pointclouds which are both samples of some arbitrary surface structure, and the sampled areas have an overlap of 0-100% which is unknown. I know the Trimmed ICP variant works by iteratively trying to determine the overlap, but even this one can be stuck in a local minimum. 
A naive approach would be to look a the mean square error of the identified point pairs. But without some estimate of the sampling this seems a risky thresholding. In the manual for the Leica Cyclone they suggest manual inspection of the pair error histogram. If it has a Gaussian shape the fit is good. If there is a linear fall-off the match is probably bad. This seems plausible for me, but I've never seen it used in an algorithm.
",slam
what books do you suggest for a beginner like me ?,"i am studying bacholar of dental surgery but have intrest in learning this subjet so tell me about a good book to read.
",mobile-robot
What is the difference between Kinect for Windows and Kinect for XBox?,"As I see there is a huge price gap between the two \$223 vs \$99 (at amazon).
My intention is to use one of those from Ubuntu linux to perform depth sensing, navigation etc. and naturally I prefer the cheaper. 
However I am not sure if I miss some important point while betting on the Kinect for Xbox version. 
As it seems the Windows version is overpriced because it has the license for development. Here it is stated that there are internal differences but without exact details (The minimum sensing distance seems to be better for Windows version.).
Could anyone give a comparison chart?
It would be good to know about

Connectivity: USB, special connector, ... .
Hardware differences: are they the same or do they really differ in weight, energy consumption, speed, sensing range, ...?
Driver: could I use Xbox version under Ubuntu?
API usage: could I develop on Xbox version, could I use the same/similar API on both, is the API for Xbox mature enough?
License: is it against the license of Xbox version to develop for home/hobby/educational use?

Thanks.
","sensors, kinect"
how to find maximum force of a robot joint,"I want to know if there is any equation that calculates the maximum force of a robot joint. The force that we should not exceed.
For example in human leg, if we apply a big external force to the knee, it will break. now how can i find the necessary force that will just make the leg move without breaking the knee.
I have a programme that generates robot morphologies randomly with different sizes, so I have to know the force to not exceed for each joint. I think this depend on weight, mass, inertia of each robot part.
I can not do this by trial and error because I have hundreds different morphologies.
This video shows the behaviour of robot if I apply a big force. It is in Gazebo robotic simulator.
Thanks in advance!
","simulation, joint, force"
Mounting a gimbal BLDC motor,"I'm trying to build my own motorised camera gimbal using a BLDC like this, where the shaft is hollow. Does anyone know how the camera platform should be mounted? Should a shaft be somehow pressed into the hole?
Any thought appreciated.
",brushless-motor
Velocity derivatives using Quaternions,"How to compute the angular and linear velocities Quaternions? I am new to this area and although I have studied the algebra I am unable to understand how to compute the velocities.
",kinematics
Control both Velocity and Position (Linear actuator),"I am trying to control the velocity+position of a linear actuator.
At this moment I am able to control the position or the velocity. But I'm trying to control both. What the control has to do: Let the linear actuator drive to a position i.e. 0 to 100 cm with a constant velocity of 1cm/s.
I control the actuator using a PWM signal. And I measure the velocity and position using a position sensor on the shaft.
What kind of control is preferred, PID in cascade?
If so,what would the code look like.
Any other kind of control would function better?
Thanks in advance!
EDIT:
A more describing picture.

I want a Velocity controlled Position controller.
Hopefully this will make it clear
EDIT
My first try is with a trapezoid wave. Maybe there is an easy way without to much calculation power to change it a s-curbe. Then the accelartion/jerk will be alot smoother.
        I let the microcontroller calculate 3 different formulas afterwards it will calculate it using loop iteration. This way I can use one PID for the position. The parameters in the following code will fictional:
    AccelerationLoops: 5                            //[Loops]
Velocity:      100                          //[mm/s]
DeltaPosition:     7.5                          //[mm]
Looptime:      5                            //[ms]
Loopfactor:        1000 / Looptime                  //[-]
VelocityLoop:      Velocity  /Loopfactor                //[mm/loop]
VelocityFactor:    VelocityLoop * .5 / AccelerationLoops        //[mm/loop]  (.5 found by integration)
Loops:         DeltaPosition / VelocityLoop / AccelartionLoops  //[Loops]

----Formula---
Formula1:      VelocityFactor * x^2
LastF1:        Last value of Formula1 Formula1(5)

Formula2:      VelocityLoop * x - LastF1

Formula3:      VelocityFactor * (Loops - x)^2 + DeltaPosition)

Using the parameters of above it will generate the following setpoint :
0   0,00
1   0,05
2   0,20
3   0,45
4   0,80
5   1,25
6   1,75
7   2,25
8   2,75
9   3,25
10  3,75
11  4,25
12  4,75
13  5,25
14  5,75
15  6,25
16  6,70
17  7,05
18  7,30
19  7,45
20  7,50

A big problem with the code above is that the amount of accelartion loops is a constant. It can not be changed except when you already know the amount of loops it will take.
I will be using two separate arduinos, they will be connected using a CAN-bus connection. Anyway, they won't communicate through it unless the load becomes too high. This will make Master/Slave impossible. Also the system has to be modular: adding another actuator to circuit won't be a problem. The actuator is speed controlled by using a PWM signal. The linear sensor will deliver a 0-10v signal which i will reduce to 0-5v by a simple voltage divider. The loop will be around 5 to 10 ms, will depend on the maximum looptime.
Arduino has a 10-bit(1023) ADC but use of oversampling I will probably try to increase it to 12-bit. To not decrease the reading speed I will decrease the prescaler of the ADC.
The PWM output is 8-bit(255), I am trying to find a way to further increase. Because I think 255 steps are too low for my application.
Because the Arduino has limit internal memory, pre calculating all the positions is impossible.
Thank you all for the help so far!
","arduino, pid, microcontroller"
12 volt input to 5 volt ouput of Arduino,"I accidentally ended up supplying 12 v to the Arduino 5v output pin instead of the Vin pin. Does that mean that I can't use the 5v output pin anymore i.e. its fried?
",arduino
Question about what motor to use for opening window,"first off, just to be transparent, I'm a total newbie when it comes to DC motors (and pretty much anything robotic). 
I've got a couch that's right up to a window with the lever type openings (anderson windows). With the couch, I have no clearance to turn the lever to open it. Given I've replaced most of my house switches/outlets with home automatable ones, I figured I'd see if I can build myself a small motor that I can automate to open these also. To be absolutely honest, I've got no clue where to start. I have no problem with coding the automation part, but I don't even know what kind of motors to look for that would be able to turn my knob (or rather how to actuate the thing my knob connects to)...
Help!
Thanks :)
",motor
Differential Drive Robot on uneven surfaces,"So I am building a differential drive robot and I want it to autonomously drive in a straight line on an uneven surface. I know I need a position and velocity PID. As of now, I am deciding on which sensors to buy. Should I use optical encoders, accelerometers, or something else?
I wanted to go with accelerometers due to the error encoders would face due to slippage, but I am not sure.
Some enlightenment would help!
","sensors, control, pid, differential-drive"
Sun tracking with +/- 0.1degree accuracy?,"For a school project I am looking to track the sun with +/- 0.1 degree accuracy for use with a parabolic dish reflector. Say we need a final output torque of about 20Nm, what kind of gearing/motors and feedback would you guys use to accomplish this? The sun position will be found with a solar positioning algorithm.
I am pretty new to this all but from my research stepper motors seem the easiest but brushless DC motors, from what I have read, can yield better results with smoother tracking. I am confused how you even use regular brushless dc motors to achieve precision positioning. I am aware of encoders but I don't really understand why the BLDC are preferred for this particular application, and how one would implement them.. Any ideas that can help kick start my researching?
","motor, brushless-motor, stepper-motor, servomotor"
Selecting a gear reduction: torque vs speed,"I have just sized the DC motors I want to use (corresponding to my robot and its intended applications - my figures include a 50% uncertainty factor to account for friction in reducers and other losses). Now I need to actually choose the exact motors I want to buy from the manufacturer (I am targeting maxon motors as I am not an expert and want no problem). I have a few down to earth questions about linking the mechanical needs to the electrical characteristics, among them:
Question #4:
The motor I chose (maxon brushed DC: 310005 found here) has nominal speed = 7630rpm - nominal torque = 51.6mNm. My needs are max speed = 50.42rpm / max torque = 10620 mNm. This means a reduction factor of 151 for speed and 206 for torque. Should I choose a gear closer to 151 or 206?
","motor, brushless-motor, servos, servomotor"
Should I use or not EKF for Baro-Acc altitude estimation?,"I've recently implemented a kalman filter to estimate altitude for a small robot with an IMU+Baro sensor mounted on it.
My objective is to get max precision I can have, using this two sensor, with small computing power that a MCU can provide me. I've tuned my filter and it seems to work pretty well.
Can I obtain a significant improvement using an Extended Kalman Filter instead of a normal Kalman Filter and if it worth time to implement it?
More in detail, since this request is too specific for each application, if a Model function that use Baro and Accel as states should be linearized and used in a EKF and if this can improve data reliability compared to a simply KF?
","kalman-filter, accelerometer, ekf"
PID with position and velocity goal?,"I'm trying to design a control system for a robot that tracks moving object. Thus I want to robot to match the position and velocity state of the object. I don't want robot to simply to arrive at the position, but I want to arrive at the position with the same velocity of the object. 
Object velocity and position data will be provided externally.
I'm not sure if a traditional PID controller (with velocity controls) with just a position based error is enough. Wouldn't position only state goal result in tracking that is always lagging behind?
Is PID what I want or should I be looking at something else like trajectory controls?
",pid
Stereo Vision Using Compute Module: Pi camera synchronization,"Good day,
I am currently working on an obstacle avoiding UAV using stereo vision to obtain depth maps. I noticed that the quadcopter would sometimes not steer to the correct direction.
I am using the Raspberry Pi Compute Module IO board which comes with two CSI ports used with two v1 Pi Cameras.
Issue
I soon found out that due to the latency between the cameras, the left and the right images are not in sync thus the errors in the depth map result.
Steps taken:
I noticed the image blur when moving the cameras around so I adjusted the shutter speed by setting the UV4l/raspicam driver. With the shutter speed, I also tried to increase the framerate as I've read, it improves the latency issue. In my code which uses the opencv library, I used the grab() and retrieve() commands to replace the read() command so that the frames from both cameras is grabbed at the nearest time possible however it didn't help much.
Does anyone know any possible solutions?
","computer-vision, stereo-vision, c++, opencv"
Obstacle Avoidance while Navigating,"I need some ideas for strategies or algorithms to apply on these strategies to perform obstacle avoidance while navigating.
At the moment I'm doing offline path planning and obstacle avoidance of known obstacles with an occupancy grid. And running the A* algorithm over the created matrix. After that my robot follows along the resulting trajectory. This is done by splitting the whole trajectory into sub-path. The robot adjust it's heading to the new target and follows the straight line. The robot is controlled by a fuzzy logic controller to correct deviations from the ideal line (steering) and adjusting the velocity according to the steering action and distance to the target. So far so good. And it's working very well.
As sensor system, I solely use the Google Project Tango (Motion Tracking and Area Learning for proper path following). Now I want to use the depth perception capability of the device. Getting the appropriate depth information and extracting a possible obstacle is done with a quite simple strategy. The robot analyses the depth information in front of the robot and if any object is in between the robot and the target point of the sub-path, an obstacle must be there.
Now I'm wondering how to bypass this obstacle most efficiently. The robot is only aware of the height and width of the obstacle, but has no clue about the depth (only the front of the obstacle is scanned). Feeding the occupancy grid with this new obstacle and running again the A* algorithm is not effective, because of the missing depth. One possible strategy I could imagine is estimating a depth of the length of the grid cell, re-plan and continue the navigation. If the robot faces the same obstacle again, the depth is increased by the size of one additional grid cell length. But I think this is extremely ineffective. 
The requirement is to only use the Google Project Tango and no additional sensors, such as ultrasonic to sense the sides.
Update 1
The first picture illustrates the given trajectory from the path planning (orange). The gray and blue data points are the sensed obstacles in front of the robot. The notch behind the blue obstacle is actually the wall, but is shadowed by the blue obstacle. Image 2 shows the same scene just from a different perspective.
The issue I have to treat is, how to optimally bypass the blue obstacle even I don't know how deep it is. Driving to the left and to the right only to capture better data points (to generate a 3D model) is not possible. 


Update 2
Yes, I'm using a depth sensor, the one integrated in Google Project Tango. It's a visual measurement. A infra-red laser beams a grid onto the objects and a RGB-IR camera capture these information and evaluates the appropriate depth information.
","control, motion-planning, algorithm"
ROS and Kinect data without callbacks,"I'd like to get rgb and depth data from a kinect, and I found a little tutorial here: http://wiki.ros.org/cv_bridge/Tutorials/ConvertingBetweenROSImagesAndOpenCVImagesPython. It's fine, but what I'd like is to be able to get the data on demand, and not as whenever the callback is triggered, assuming I won't try to get the data faster than it can be available. I'd appreciate any help - do go easy on the ROS jargon, I'm still learning...Thanks.
","ros, kinect"
Up to what force is a servo motor a reasonable choice as an actuator?,"I'm working on an application where I need to apply a linear or angular force to operate a linkage mechanism, but I don't (yet) know what amount of force I will need. I anticipate that it will be less than 4.5 kg (44 N). The travel distance on the linkage input should be less than 15 cm.
As I look through available servos, they seem to exist firmly in the scale-model realm of remote control vehicles, and as such I am uncertain if any will be suitable for my application. For example, one of Futaba's digital servos, the mega-high torque S9152, is listed at 20 kg/cm.
From what I understand, this means that at 1 cm from the center of the servo shaft, I can expect approximately 20 kg force. If I wanted 15 cm of travel distance I would need roughly a 10.6 cm radius, which would diminish the applied force to 20 / 10.6 = 1.9 kg, well below the 4.5 that might be required.
Question:
Is my understanding and calculation even remotely accurate? Should I be looking at other types of actuators instead of servos? They seem to become prohibitively expensive above 20 kg/cm torque. (For the purposes of this project, the budget for the actuator is less than $250 US.)
For my application, I'd like to have reasonable control over intermediate positions across the travel range, good holding power, and fairly fast operation. For this reason I have dismissed the idea of using a linear actuator driven by a gearmotor and worm drive.
I am relatively new to robotics in the usage of motorized actuators, but I've used pneumatic cylinders for many years. For this application, I can't use pneumatics.
Edit:
Per comments, some additional constraints that are important:

Linkage Details: The linkage is a planar, one degree-of-freedom, part of a portable system (similar to a scissor lift mechanism). It is for a theatrical effect where the motion is amplified and force reduced (speed ratio and mechanical advantage are < 1).
Power: It will be carried by a person. As such, the actuation needs to be battery-operated, as no tubing or wiring can tether the person. Tubing or wiring that is self-contained is okay. Because this is a portable system, battery-power will be used. The control system will be designed specifically for an appropriate actuator. Rechargeable batteries up to 12V will most likely be employed. Actuators could operate on as high as 24V. Ideally a motor would not exceed 1-2 amperes draw, but as it is not in continuous operation, this is not a hard limit.
Not Pneumatic: I've considered pneumatic actuation, using CO2 cartridges, for example, but the client would prefer not to use pneumatics. Also, the ability to stop/hold at intermediate points in the motion range is desirable, and somewhat more complicated to do with pneumatic actuators.
Speed: An ideal actuator will be able to move the input coupling 15 cm in 1-2 seconds.
Weight: Weight constraints are not well-defined. As it will be carried by a person, it should be moderately lightweight. The actuator itself should probably be less than 1kg, but certainly this can vary. (The rest of the mechanism will probably be 6-8 kg.)
Size: The primary size constraint is that everything must fit within a space measuring no more than 500 x 500 x 120 mm (H x W x D). The linkage mechanism extends from and collapses outside the enclosure, parallel to the width.
Noise: The quieter the better, but noise is the least priority.

Servos seemed like the best choice for the job, but they don't seem to be available with the sort of torque I need.
",servomotor
How does a QuadCopter Startup work ? Will they tune every copter before releasing to market?,"We know that a quadcopter needs to be tuned to its perfect PID values to minimise the pitch, roll , yaw errors and etc., Before releasing to the market will they tune every unit and ship it ? Or a any different algorithm is used which doesn’t require any tuning ? Because every motor/ESC or a chassis will not be exactly same, which will add to the noise. 
",quadcopter
Dynamic torque simulation for a 6 DOF robotic arm,"I am working on a 6 DOF robotic arm(industrial manipulator). I have the basic structural specs (dimensions, weights etc) for the links and joints with me. 
Basically, I want to simulate both static torque(Due to the weight of the arm) and dynamic torque(due to the accelerating joint's motion) Torque that the joints will need to bear for a given set of motions. 
I have looked on the web and found tools like the ROS-MoveIt Visualiser, Gazebo, V-REP which let me visually see a robotic arm and simulate the position logic and external factors like collisions etc. But I have been unable to simulate/calculate dynamic torque values from these tools.
Ideally, I'd want to define a fixed motion of the end effector(i.e. move the robot between 2 positions) and measure the Torque(both static and dynamic) during that particular move.
These torque values are essential for selecting the optimum motors and gearboxes for my design and payload.
","robotic-arm, dynamics, torque, simulation, manipulator"
Fence avoidance for manually controlled robot,"I'm trying to find known techniques for keeping a manually controlled robot within a known polygon fence. More specifically, a pilot controls a robot by issuing desired velocity vectors, and the autopilot adjusts the velocity so that the distance to any boundary is always at least the stopping distance of the robot.
My goal is to implement a system that:

Tries to follow the pilot's desired velocity as closely as possible.
Is robust to changes in position and desired velocity. At a minimum, I want the velocity to change continuously with respect to the position of the robot and desired velocity of the pilot. Informally, this means that sufficiently small changes in the position or desired velocity of the pilot induce arbitrarily small changes in the velocity.

The second point is particularly important. Suppose that the policy were to find the intersection with the boundary in the direction of the desired velocity and slow down smoothly to that point. The below figure depicts a couple of scenarios in which this would not be continuous. In this figure, the black lines represent the fence boundary, the red dot is the position of the robot, and the blue line is the desired velocity of the pilot. In figure (a), a small perturbation of the position to the left will cause a large increase in allowed velocity because the desired velocity will intersect the far edge instead of the near edge. In figure (b), a small clockwise rotation of the velocity vector will result in a large decrease in allowed velocity because the desired velocity will intersect the near edge instead of the far edge.

I have searched for relevant papers, but most of the papers I've seen have dealt with fully autonomous obstacle avoidance. Moreover, I haven't seen any papers address the robustness/continuity of the system.
:EDIT:
The robot knows its own location and the location of the boundary at all times. I also have some equations for maximum velocity that allow a smooth ramp-down to a single line boundary (though I'd be interested in seeing a better one). I would like the velocity limits to be continuous in the position and desired velocity of the pilot.
I want to continuously throttle the user's input such that a minimum safe distance between the robot and the boundary is maintained, but see the figure that I added to the question. The hard part (I think) is to make sure that small changes in position (e.g. due to sensor noise) or small changes in desired velocity (e.g. due to pilot noise) don't cause huge changes in what the autopilot allows.
I want continuity because I think it will provide a much nicer experience for the pilot while still enforcing the fence boundary. There is a trade-off with optimally but I think this is worth it. Even though the physical world smoothes any discontinuities in velocity, big changes could still cause large jerk which will be somewhat disturbing to the pilot. The goal is to not have the autopilot introduce large oscillations not intended by the pilot.
This Will be implemented On a physical system that has sensors that provide an estimation of position, and the boundary shape is known and is unchanging. The actual system that I'm targeting is a quadcopter.
","control, geometry, reference-request"
Find object using only distance,"I'm working on a extremely simple robot (very first project) that attempts to find the source of a Bluetooth signal. There are two motors that drive the platform and each has an encoder. We've already used a Kalman filter to calculate the approximate distance to the Bluetooth beacon within reasonable error.
I worked out a manual solution using some trig that solves the problem in theory, but it fails if there is any error (For example, it attempts to turn 73 degrees, but turns 60).
My question is how can I reasonably drive the motors based on the encoder data to continuously minimize the distance to the signal? Furthermore, is there a generic solution to problems like these? (I guess you might call it a stochastic ""Hotter/Colder"" problem)
Thanks in Advance.
","raspberry-pi, wheeled-robot"
Compass sensor for robot,"What's an appropriate compass sensor to use on a robot?
There are a ton of cheap digital compass sensors, and I was thinking of using an MPU9250 combined accel/gyro/magnetometer as a compass, but I'm finding these are terribly unreliable and need constant calibration via the ""wave in a figure 8 pattern"" method whenever it gets near other electronics or small magnets, which a robot obviously won't be able to do. Is there a digital compass technology that mimics traditional compasses that requires little to no calibration, appropriate for installation on a robot?
","magnetometer, compass"
TCP Communication with PCDuino,"I'm working on a robot that is controlled by an xbox controller connected to a windows computer and commands are sent to a pcduino through a tcp connection. I have it working by sending a string of 1's and 0's to tell the pcduino which motors to turn on. I'm trying to optimize it by just sending an int and using bit masks to make the decisions on the pcduino but I can't get the pcduino to receive the int correctly. I tested the windows function sending the command with sokit and its sending the correct values but the pcduino is receiving the same number even when the commands are changing.
This is what its doing:
Windows          -> PCDuino
command = 1      -> sendBuff = 73932
cmdstring = 1    -> n = 1

command = 1025   -> sendBuff = 73932
cmdstring = 1025 -> n = 4

My windows functions are:
bool Client::Send(char * smsg)
{
    int iResult = send(ConnectSocket, smsg, strlen(smsg), 0);

    if (iResult == SOCKET_ERROR)
    {
        std::cout << ""Sending Message has failed: "" << WSAGetLastError() << ""\n"";
        Stop();
        return false;
    }
    return true;
}


    bool sendCommand()
{
    cmdbuffer << command;
    cmdstring = cmdbuffer.str();

    if (!client->Send((char *)cmdstring.c_str()))
    {
        std::cout << ""Disconnected from Server. Press Enter to Exit"";
        std::cin.ignore();
        std::cin.get();
        return false;
    }
    return true;
}


PCDuino Loop Function
void loop()
{
    recBuff = 0;
    deviceFlag = 0;

    //Read Socket

/******************************************************************************/

    read(connfd, sendBuff, strlen(sendBuff));
    recBuff = atoi(sendBuff);

/******************************************************************************/

    //Set Current Device to Receive Instructions From
    checkAuto(recBuff, currDevice);

    //Find Current Device of Command
    deviceFlag = checkDevice(recBuff);

    //If Current Device and Set Device are the Same Parse Command
    if (deviceFlag == currDevice)
    {
        parseHex(recBuff);
    }
    usleep(50000);
}


I have a printf after the read call and that's where I am getting the 73932 number. I think I have everything you guys need but if there's anything else I need to add let me know. I'm stumped...I don't know if its just a casting problem or what.
Update 1
What I have before everything the setup and loop functions on the PCduino run is:
int listenfd = 0, connfd = 0;
int n;
struct sockaddr_in serv_addr;
char sendBuff[1025];
time_t ticks;

","communication, c++, c"
What does internal sparking in a motor mean?,"My Arduino + Raspberry Pi robot was working fine in the morning. I tested it, it ran perfectly, and then I switched it off. 
Now in the evening when I'm trying to run it again, with the same batteries and everything, it just doesn't move!
I stripped it down to the motor compartment and found that when I try to run my main motor, I can see sparks through the translucent plastic on the back.
Does that mean my motor is gone?
","arduino, motor, raspberry-pi, battery"
What are some pitfalls of an ultrasonic sensor?,"I'm using a HC-SR04 sensor to detect obstacles. What are the pitfalls with an ultrasonic sensor?
Here are a couple I've found during my testing:

The signal can bounce off of one wall to another and then get picked up, distorting latency
Absorbent materials sometimes don't bounce the signal back
Check the datasheet for supported range (min/max)

",ultrasonic-sensors
how to plot a line between two centroids in matlab,"
I am able to locate centroids of each blocks, but i am unable to join two blocks with a line segment by avoiding the obstacle as shown in the figure. Please need help how do i achieve this using matlab.
","computer-vision, motion-planning, navigation, matlab"
Steadier wheels - Pin them or lock springs,"When running on a hard surface, the Create will shake sometimes during turns or acceleration.
Has anyone ever removed the springs or pinned the wheels in place so they can't move up and down?
","irobot-create, roomba"
Are there any others alternatives for PID controllers for line following robots?,"Are there any better/ advanced ways of steering a line following robot other than pid controller? If so what are them? 
","pid, line-following, steering"
Perspective n Point - RPnP algorithm,"I need to caculate the pose of a camera using an image of an artificial landmkark. For this porpouse I am trying to use the Perspective n Point approach so I can calculate it using the intrinsic camera matrix, the world coordinates of the landmark (I am using 4 points) and its projection in the image.
There are some algorithms to solve this (PnP, EPnP, RPnP, etc) and I am trying to use the RPnP. I have found an implementation of this here:
http://xuchi.weebly.com/rpnp.html
I used this code but I am having some problems because I can't obtain the correct pose.
I am using the P.Corke's Robotics Toolbox for MATLAB to create a CentraCamera with a known pose and calculating the projection of the landmark in this camera, but the rotation and translation that the RPnP returns me is not the same as I defined before.
Anyone has used this RPnP algorithm to solve that kind of problems?
","computer-vision, cameras, 3d-reconstruction"
Triangulation from calibrated stereo rig,"I am using a stereo rig to do SLAM, calibrated using the MATLAB Calibration Tool. I need to compute the 2D coordinates of a landmark using the observation model obtained from triangulation (the images are rectified). 
The equations obtained from triangulation are the ones presented in the blue box here. Because I am doing SLAM in 2D the coordinates I need to use are $Z_p$ and $X_p$. The parameters needed to compute those values are $f$, $T$ and $disparity (x_L - x_R)$.
After doing the calibration intrinsics matrices $K_L$ and $K_R$ are obtained and a common intrinsic matrix for the stereo rig is calculated from $K = 1/2*(K_L +K_R)$ so I get the parameters needed in triangulation from  this common matrix.
The focal length is supplied from the manufacturer, and for my Logitech C170 is 2.3mm. The baseline $T$ from the calibration is 78.7803 mm. To compute the disparity I am obtaining SURF points and using RANSAC to discard the outliers so I get x coordinates from both rectified images.
The problem is that with those values I can't obtain correct values for $Z_p$ and $X_p$ and I am not sure why or where I am doing the wrong step. Anyone can help with this? Are those the correct steps to do triangulation from rectified stereo images?
EDIT: My stereo rig looks like the figure I attach:
If you compare the coordinates system with the one used in the link before is easy to see that my $X_r$ corresponds to the $Z_p$ from the link and the $Y_r$ corresponds to $X_p$, so the equations to calculate the distance using triangulation and with the coordinate system of the figure are:
$x_r=\frac{fb}{x_L-x_R}$
$y_r = \frac{(x_L-p_x)b}{x_L-x_R}-\frac{b}{2}$ 
Being $f$ the focal length, $b$ the baseline, $p_x$ the x coordinate of the central point and $x_L-x_R$ the disparity. The $X_r$ $Y_r$ coordinate system is situated between the two cameras, so this is the meaning of the $\frac{b}{2}$ displacement in the equations.
Calibration 
To obtain the cameras calibration I am using the Stereo Camera Calibrator Toolbox with the chessboard pattern.
After calibration, I made some tests using MATLAB functions triangulate and reconstructScene to know whether the parameteres are well calculated. The distances I obtained using this functions (which use the stereoParams object created by the calibrator) works well and I obtain distances very similar to the actual ones. So I suposse the calibration works well.
The problem, as I explained before, is when I try to calculate the distances using the equations $x_r$ and $y_r$ because I am not sure how to obtain the common matrix $K$ for the stereo rig (the calibrator gives one intrinsic matrix for each camera, so you have two matrices).
The value of the baseline given from calibration make sense, I made a measurement with a ruler and gives me 78 mm approximately.
The $f$ value I assume should be in pixels but here again the calibration gives an $f_x$ and $f_y$ value so I am not sure which one should I use.
Those are the intrinsic matrices I obtain:
Left: 
$\begin{pmatrix} 672.6879&-0.7752&282.2488\\0&674.3705&240.1287\\0&0&1 \end{pmatrix}$
Right: $\begin{pmatrix} 681.7049&0.0451&331.2612\\0&681.8235&246.1209\\0&0&1\end{pmatrix}$
Being the parameters of $K$: $\begin{pmatrix}f_x&s&p_x\\0&f_y&p_y\\0&0&1\end{pmatrix}$
","mobile-robot, slam, computer-vision, stereo-vision"
Transfer function of a quadrotor position controller,"I'm trying to find the transfer function of a quadrotor with two controller loops, following next structure:

I know how to calculate the attitude stability controller, which relate rotor speed and desired angles. However, I have no clear at all how to implement the translational controller transfer function, whose output is the desired angle that the rotors must achieve considering the position I want to translate.
Considering that two controllers are PD, how can you calculate the translational controller transfer function and include it in the system? Time domain equations in the outer loop are next, where U terms relate to the thrust axis components. Thanks

","quadcopter, control"
Using Quaternions to feed a quadcopter PID stabilizing controller to avoid Gimbal lock,"I am trying to control my F450 dji quadcopter using a PID controller. From my IMU, I am getting the quaternions, then I convert them to Euler's angles, this is causing me to have the Gimbal lock issue. However, is there a way that I directly use the quaternions to generate my control commands without converting them to Euler's angle?
This conversation here discusses a similar issue but without mentioning a clear answer for my problem.
The three errors so far I am trying to drive to 0 are:
double errorAlpha  = rollMaster  - rollSlave;
double errorTheta  = pitchMaster - pitchSlave;
double errorPsi    = yawMaster   - yawSlave;

where the Master generates the desired rotation and the Slave is the IMU.
UPDATE:
Here are some pieces of my code:
Getting the current and the reference quaternions for bot the Master and the Slave from the ROTATION_VECTOR:
/** Master's current quaternion */
double x   = measurements.get(1);
double y   = measurements.get(2);
double z   = measurements.get(3);
double w   = measurements.get(4);

/** Slave's current quaternion */
double xS  = measurements.get(5);
double yS  = measurements.get(6);
double zS  = measurements.get(7);
double wS  = measurements.get(8);

/** Master's Reference quaternion */
double x0  = measurements.get(9);
double y0  = measurements.get(10);
double z0  = measurements.get(11);
double w0  = measurements.get(12);

/** Slave's Reference quaternion.
 *  If the code has not been initialized yet, save the current quaternion
 *  of the slave as the slave's reference orientation. The orientation of
 *  the slave will henceforth be computed relative to this initial
 *  orientation.
 */
if (!initialized) {
    x0S = xS;
    y0S = yS;
    z0S = zS;
    w0S = wS;
    initialized = true;
}

Then I want to know the orientation of the current quaternion relative to the reference quaternion for both the Master and the Slave. 
/**
     * Compute the orientation of the current quaternion relative to the
     * reference quaternion, where the relative quaternion is given by the
     * quaternion product: q0 * conj(q)
     *
     * (w0 + x0*i + y0*j + z0*k) * (w - x*i - y*j - z*k).
     *
     * <pre>
     * See: http://gamedev.stackexchange.com/questions/68162/how-can-obtain-the-relative-orientation-between-two-quaternions
     * http://www.mathworks.com/help/aerotbx/ug/quatmultiply.html
     * </pre>
     */
    // For the Master
    double wr = w * w0 + x * x0 + y * y0 + z * z0;
    double xr = w * x0 - x * w0 + y * z0 - z * y0;
    double yr = w * y0 - x * z0 - y * w0 + z * x0;
    double zr = w * z0 + x * y0 - y * x0 - z * w0;

    // For the Slave
    double wrS = wS * w0S + xS * x0S + yS * y0S + zS * z0S;
    double xrS = wS * x0S - xS * w0S + yS * z0S - zS * y0S;
    double yrS = wS * y0S - xS * z0S - yS * w0S + zS * x0S;
    double zrS = wS * z0S + xS * y0S - yS * x0S - zS * w0S;

Finally, I calculate the Euler angles:
/**
     * Compute the roll and pitch adopting the Tait–Bryan angles. z-y'-x"" sequence.
     *
     * <pre>
     * See https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions#Quaternion_.E2.86.92_Euler_angles_.28z-y.E2.80.99-x.E2.80.B3_intrinsic.29
     * or  http://nghiaho.com/?page_id=846
     * </pre>
     */
    double rollMaster  =  Math.atan2(2 * (wr * xr + yr * zr), 1 - 2 * (xr * xr + yr * yr));
    double pitchMaster =  Math.asin( 2 * (wr * yr - zr * xr));
    double yawMaster   =  Math.atan2(2 * (wr * zr + xr * yr), 1 - 2 * (yr * yr + zr * zr));

and I do the same thing for the Slave. 
At the beginning, the reference quaternion should be equal to the current quaternion for each of the Slave and the Master, and thus, the relative roll, pitch and yaw should be all zeros, but they are not!
","quadcopter, pid, stability"
connecting MPU-9250 GY-9250 SENSOR MODULE to arduino uno,"i am using this sensor to make self balancing robot.At first i have soldered the header(only to vcc,gnd,scl,sda ) on the imu borad at the opposite side where there is no component mounted.then connecting it to arduino uno r3(vcc to vcc 3.3v/5v,gnd to 1 of 3 gnd,scl to scl and sda to sda(first time at those next to AREF, second time A5,A4) ) i uploaded the sketch https://github.com/adafruit/Adafruit_ADXL345/blob/master/examples/sensortest/sensortest.pde then when i opened the serial monitor i got

Accelerometer Test
FF Ooops, no ADXL345 detected ... Check your wiring!

i thought may be i have soldered the header in wrong direction(as in picture and videos at internet,they are so) so i desolder(with solder iron,no other technique) the header,but there were still some solder around the hole which i could not remove.then while checking the continuity between pins with multimiter(in resistance mode) i found the resistance to be 20k(scl-sda),220k(scl-gnd),220k(sda-gnd),between vcc and 3 other pins multimieter shows 1(range 2000k). then i soldered it on opposit side(this time where other components are mounted).the serial monitor still shows same output,and so does the muiltimeter.so where is the problem? is it with soldering ?do i need to disolder the header again and clean left out solder(with Chip Quik type desoldering technique ) on the opposite side(no component mounted)?is there any hope that i won't need to buy it again?
picture of opposite side where no component is mounted and this is after desoldering and resoldering
","imu, accelerometer, gyroscope"
How to select dc motors for a line following robot?,"What are the criteria to consider when ordering dc motors for a line following robot?
Is there a way to calculate the torque required?
","motor, line-following"
Quadcopter that can carry heavy things?,"So, while I was out drinking with a couple of my friends, one of us said something like 'man, wouldn't it be cool if the beer just came to us?' and that got me thinking.
We all have seen some crazy things people do with quadcopters (or polycopters even), but would it be possible (and not too expensive) to build a quadcopter that could carry, say, a crate of beer? (16-20kg)
I'm a bit of a tinkerer and I've built some minor things with rasp. pi's before but never tried myself at a quadcopter, because they are quite a big piece of work, but being able to fly a crate of beer right in front of me would be pretty awesome.
That aside, how strong would such a quadcopter have to be? In terms of motors, propellers, battery & frame. I'm a complete noob when it comes to RPM and the like, so I wouldn't even know where to begin. I have, of course, read through most of the available tutorials on the internet, but they don't answer my question of what exactly to look for when I want my quadcopter to be able to carry something specific.
","quadcopter, raspberry-pi"
thrust measurement,"I try to find out the relation between rpm vs. thrust for a battery+motor+propeller combination. the image shows my setup and also the measurement result. Can anyone explain how l should use this datas (I know Kv.v gives the rpm but my voltage values decreasing because of P=V.I relation etc.) 
","quadcopter, brushless-motor, esc"
Hand Eye Calibration Solver,"I have a rig for which I have a pretty good estimate of the static transformation between the camera and a joint based off of the CAD. It has some errors though and I was hoping to fix it by doing a hand eye calibration. So, I started off with generating some data based off of the transformation that I have already. From the papers that I have been reading, they all want to solve the $$AX = XB$$ problem by either converting $A$, $B$ to dual quaternions or simplifying the equation to something like 
$$ n_A = Xn_B $$ where $n_A$, $n_B$ are the eigenvectors corresponding to the eigenvalue of 1 for the $A$ and $B$ rotations.
After generating the data, I tested if my data collection was correct and I validated it by checking if $AX = XB$ for all of the $A$s and $B$s that I generated. I used the CamOdoCal library to try and solve the problem but I got this -
/hand_eye_calib_node    : 
[ 0.00196822,   -0.457069,    0.889429,    0.143463;
   -0.999965, -0.00813605, -0.00196822,    -1.74257;
  0.00813605,   -0.889394,   -0.457069,   0.0270069;
           0,           0,           0,           1]

----------------------------------------

/hand_eye_calib_node    : Actual transform
    0         0         1   0.08891
   -1         0         0 -0.070465
    0        -1         0   0.07541
    0         0         0         1

The actual transform is the one that I had based my $A$ and $B$ data on. Then I tried implementing the Tsai-Lenz and Horaud and Dornaika's Nonlinear optimization techniques using LM solver but to no avail. I do not get the correct transformation out of any of the solvers.
So, I was wondering if you could point me to a hand eye calibration library or paper that has worked.
","kinematics, calibration"
Piezo sensors and multiplexers,"I got asked to make some sort of trigger pads for the foot section of an organ working over midi to a electric piano and my friend wants it to be pressure sensitive so we can program in the note velocity when he's not using the organ sound.
https://www.youtube.com/watch?v=DuariiHWJQg
That is what I try to achive. I want the pads to not just be on/off but also be able to control the velocity of the midi note.
Im planning to use a Adruino Uno with a MUX Shield II from Mayhew Labs to get 36 analog inputs. Not exactly sure on the wiring yet but have looked at some guides and videos on google to get a feel for how it can be made. 
All these 36 piezo-""sensors"" is planned to register how hard you push the pedals and then send out a MIDI signal with a specific note correspondig to the pedal, and velocity to the electric piano so you can control the low notes with your feet.
http://www.thomann.de/se/clavia_nord_pedal_key_27.htm
Just like that but more pedals and a lot cheaper. 
Will the Arduino be able to read the analog output of the piezo sensor even though it's going through a multiplexer?
","control, electronics"
Programming a line following robot with reinforcement learning,"I am considering programming a line following robot using reinforcement learning algorithms. The question I am pondering over is how can I get the algorithm to learn navigating through any arbitrary path?
Having followed the Sutton & Barto Book for reinforcement learning, I did solve an exercise problem involving a racetrack where in the car agent learnt not to go off the track and regulate its speed. However, that exercise problem got the agent to learn how to navigate the track it trained on.
Is it in the scope of reinforcement learning to get a robot to navigate arbitrary paths? Does the agent absolutely have to have a map of the race circuit or path? What parameters could I possibly use for my state space?
","machine-learning, artificial-intelligence, reinforcement-learning, line-following"
Autonomous Indoor Positioning System Robot based on CV approach,"I have some questions regarding an IPS autonomous robot system,
Configuration: 

Mounting a camera to the ceiling of a room
Assume the room is a cube of 5mx5mx5m (LxWxH)
Assume the camera is Microsoft LifeCam Studio (CMOS sensor technology, Sensor Resolution: 1920 X 1080, 75° diagonal field of view, Auto focus from 0.1m to ≥ 10m, Up to 30 frames per second, Frequency Response: 100 Hz – 18 kHz)
A rover

Objectives:

By putting the rover in an unknown location (x,y) in the room, the system should localize the rover's position
After the rover's coordinates will be known, Navigation will be the next step
We want the rover to navigate from the known coordinates (x1,y1) (let's say point A) to another point B on the map (x2,y2)
Control signals will be sent to the rover's servos to complete the navigation task

Methodology:

Camera will capture the environment in real time
Environment will be represented as cells (occupancy grid mapping)
Assume each cell represents 5 cm in the environment
Rover will be localized by the system point A
Determine the navigation to point B
Determine the path of the rover in the grid map (ex: go x cells horizontal then y cells vertical)
Control Signal will be sent to rover's servos

Questions:

Can I use this camera for this task or I need another type of cameras ?
What are the factors affecting the system accuracy ?
(ex: Sensor Resolution - FOV - FPS - Frequency Response - Distance of the camera in the ceiling)
What's is the most important factor to consider to increase the accuracy ?
I would appreciate any opinions regarding the project

King Regards,
Thank you
","mobile-robot, localization, slam, computer-vision, mapping"
How can I switch between Autonomous mode and usercontrol?,"I want to switch from usercontrol to autonomous. When I have the program running for 120 seconds, how come it wont automatically switch in autonomous mode? Thanks!
#pragma config(Motor,  port1,           driveBR,       tmotorVex393, openLoop)
#pragma config(Motor,  port2,           driveFR,       tmotorVex393, openLoop)
#pragma config(Motor,  port3,           driveFL,       tmotorVex393, openLoop)
#pragma config(Motor,  port4,           flyRight,      tmotorVex393, openLoop)
#pragma config(Motor,  port5,           driveBL,       tmotorVex393, openLoop)
#pragma config(Motor,  port6,           flyLeft,       tmotorVex393, openLoop)
#pragma config(Motor,  port10,          Belt,          tmotorVex393, openLoop)

//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

#pragma platform(VEX)
#pragma competitionControl(Competition)
#pragma autonomousDuration(15)
#pragma userControlDuration(120)

#include ""Vex_Competition_Includes.c""

//Main competition background code...do not modify!

void pre_auton() {
}

task autonomous() {
    while(true == true) {
        motor[flyLeft] = -127;
        motor[flyRight] = 127;
        wait1Msec(500);
        motor[Belt] = -127;
    }
}

task usercontrol() {
    while (true == true) {
        motor[driveFR] = -vexRT[Ch2];
        motor[driveFL] = vexRT[Ch3];
        motor[driveBR] = vexRT[Ch2];
        motor[driveBL] = vexRT[Ch3];

        if(vexRT[Btn6D] == 1) {
            motor[flyRight] = -127;
            motor[flyLeft] = -127;
        }
        if(vexRT[Btn6D] == 0) {
            motor[flyRight] = 0;
            motor[flyLeft] = 0;
        }
        if(vexRT[Btn5D] == 1) {
            motor[Belt] = -127;
        }
        if(vexRT[Btn5D] == 0) {
            motor[Belt] = 0;
        }
    }
}

","wheeled-robot, robotc, vex"
Is this the right way to do motor mixing with PID outputs for a quadcopter?,"These are the motor mixing formulas I've written for my quadcopter's flight controller (Arduino-Mega) and I was wondering if its all right to use all three (roll-pitch-yaw) in each of the esc's signals. 
esc1 = throttle - pid_output_pitch + pid_output_roll - pid_output_yaw;  
esc2 = throttle + pid_output_pitch + pid_output_roll + pid_output_yaw;  
esc3 = throttle + pid_output_pitch - pid_output_roll - pid_output_yaw;  
esc4 = throttle - pid_output_pitch - pid_output_roll + pid_output_yaw;

","quadcopter, arduino, pid, esc"
Create 2 Reading Sensor Values,"I am trying to solve some Create 2 sensor reading problem that I am having when I came across @NBCKLY's posts (Part 1 and Part 2) that I believe are exactly what I am looking for. I copied his code from the original post into my project and updated the code from the second post as best as I could interpret...but something is not going according to plan.
For example, I am printing the angle to my serial monitor (for now) but I am constantly getting a value of 0 (sometimes 1).
Can @NBCKLY or anybody please check out this code and tell me what I'm doing wrong? I would appreciate it. Thank you very much.
int baudPin = 2;
int data;
bool flag;
int i;
int ledPin = 13;
int rxPin = 0;
signed char sensorData[4];
int txPin = 1;

unsigned long baudTimer = 240000;
unsigned long prevTimer = 0;
unsigned long thisTimer = 0;



void drive(signed short left, signed short right) {
  Serial.write(145);
  Serial.write(right >> 8);
  Serial.write(right & 0xFF);
  Serial.write(left >> 8);
  Serial.write(left & 0xFF);
}

void updateSensors() {
  Serial.write(149);
  Serial.write(2);
  Serial.write(43); // left encoder
  Serial.write(44); // right encoder
  delay(100);

  i = 0;

  while (Serial.available()) {
    sensorData[i++] = Serial.read();
  }

  int leftEncoder = int((sensorData[0] << 8)) | (int(sensorData[1]) & 0xFF);
  int rightEncoder = (int)(sensorData[2] << 8) | (int)(sensorData[3] & 0xFF);
  int angle = ((rightEncoder * 72 * 3.14 / 508.8) - (leftEncoder * 72 * 3.14 / 508.8)) / 235;

  Serial.print(""\nAngle: "");
  Serial.print(angle);
  Serial.print(""\n"");
}



void setup() {
  pinMode(baudPin, OUTPUT);
  pinMode(ledPin, OUTPUT);
  pinMode(rxPin, INPUT);
  pinMode(txPin, OUTPUT);

  delay(2000);
  Serial.begin(115200);

  digitalWrite(baudPin, LOW);
  delay(500);
  digitalWrite(baudPin, HIGH);
  delay(100);

  Serial.write(128);
  Serial.write(131);
  updateSensors();
  drive(50, -50);
}



void loop() {
  thisTimer = millis();

  if (thisTimer - prevTimer > baudTimer) {
    i = 0;
    prevTimer = thisTimer;
    digitalWrite(baudPin, LOW);
    delay(500);
    digitalWrite(baudPin, HIGH);
    Serial.print(""Pulse sent...\n"");
  }

  updateSensors();
}


#

What I am asking is why do I only get an angle of rotation of 0 or 1 degrees when the robot is moving in a circle. The angle should be incrementing while the robot is moving. 
The output I am getting on the serial monitor shows a line of what looks like garble which I assume is supposed to be the bytes sent back from the Create which is followed by ""Angle: 0 (or 1)"" What I was expecting to see was an increasing angle value. (1,2,3...360, and so on).
","arduino, sensors, irobot-create"
Low power to motors -- motor power jumper issue,"I'm currently working on my first robotics project using the Initio kit from 4tronix powered by Raspberry Pi. The setup was fairly simple, and I've been testing it out over the last couple of days. All of the sensors work as expected; however, my motor tests are failing. When I input commands to actually move the robot, I can hear the DC motors running but they're not getting enough power to do anything. In the instructions, it says if this issue is encountered, that the power selection jumper might not be set correctly and provides this diagram:

For comparison, here's how I have the wiring for the motors setup:

I'm not entirely sure what it means to have the power selection jumper being set incorrectly and would greatly appreciate it if someone could explain this to me or point out if they see anything wrong with my setup.
","mobile-robot, motor, raspberry-pi, first-robotics"
Create2 incremental encoder rollover method,"I have never yet had the Create2's incremental encoder rollover but want to write my code to be prepared for this to happen and test it. When the encoder rolls past 32767 (14.5m), does it rollover to -32768 and count there or start at 0 again and count up from there?
One other odd thing but not a big deal. When I reset the Create2, the first value is 1 not 0. 
","irobot-create, roomba"
Send Arduino sensor data to server with GPRS shield,"I'm trying to send Arduino sensor data to a server using a GPRS shield (SIM900 shield from Geeetech). I have this particular set up because the data will be updated to a website and the device will be roaming. I can't use http://www.cosm.org because to the best of my knowledge that only updates every 15 minutes, I need to update about every 5-10 seconds.
In order to connect I tried the code below to form UDP connection but it does not get sent through to the receiving IP and port. I don't know why. No errors occur on the Arduino side, and the server side has been shown to work with an iPhone app that sends a UDP message.
///connect
void connectUDP()
{
 mySerial.println(""AT+CSTT=\""APN\"""");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIICR"");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIFSR"");
 delay(3000);
 ShowSerialData();
 mySerial.println(""AT+CIPSTART=\""UDP\"",\""SERVER IP\"",\""SERVER PORT\"""");
 delay(3000);
 ShowSerialData();
 mySerial.println();

}


///send udp packet to server 
void sendUDP()
{
 for(int x = 0; x < 30; x++){
   mySerial.println(""AT+CIPSEND""); 
   delay(100);
   ShowSerialData();
   mySerial.println(""\""hello world\"""");
   delay(100);
   ShowSerialData();
   mySerial.println((char)26);
   delay(1000);
   ShowSerialData();
 }
 mySerial.println();
 //ShowSerialData();
}

The server side is as follows (written in python):
import SocketServer

PORTNO = 14

class handler(SocketServer.DatagramRequestHandler):
    def handle(self):
        newmsg = self.rfile.readline().rstrip()
    print (newmsg)
        self.wfile.write(self.server.oldmsg)
        self.server.oldmsg = newmsg

s = SocketServer.UDPServer(('',PORTNO), handler)
print ""Awaiting UDP messages on port %d"" % PORTNO
s.oldmsg = ""This is the starting message.""
s.serve_forever()

I can see a possible solution might be to change it to a TCP connection, but I don't know how to do that...
","arduino, sensors, raspberry-pi, programming-languages, python"
Can a robot or mechanical part be programmed to exert a specific force,"So I was thinking about projectiles that don't need a propellant like gunpowder I've seen coils gun but that's a little out my way. I was wondering if I know the force required to propel a object could I program a robot to exert that force to propel the object the same way (in a linear propelled fashion).
",force-sensor
Proper naming of PID regulators,"I was wondering either there is any special naming for regulators that:

Outputs unit is the same as inputs, ie. velocity [m/s] as input and velocity as output [m/s].
Outputs unit is different than inputs, ie. position as input [m], velocity as output [/m/s]

I would appreciate all help.
",pid
Switch activated by a microcontroller,"I'm working on a project where I'm using a voltage that is higher than what most microcontrollers can handle. I'm looking for a kind of switch that will connect a power source to an electromagnet and all of this controlled by my microcontroller. I also thought about using a potentiometer to control the speed of two high voltage DC motors via my microcontroller so please tell me if this is a good idea aswell. 
Thanks for your time

Zakary
","arduino, microcontroller"
Covering Up Ultrasonic Sensor,"I'm using a basic trig/echo Ultrasonic Sensor with an Arduino Uno. I get accurate readings until I cover the sensor at which point I receive very large numbers. Why is this?
Program
int trigPin = 8;
int echoPin = 9;
float pingTime;
float targetDistance;
const float speedOfSound = 776.5; // mph

void setup() {
  Serial.begin(9600);

  pinMode(trigPin, OUTPUT);
  pinMode(echoPin, INPUT);

}

void loop() {
  digitalWrite(trigPin, LOW);
  delayMicroseconds(2000);
  digitalWrite(trigPin, HIGH);
  delayMicroseconds(15);
  digitalWrite(trigPin, LOW);
  delayMicroseconds(10);

  pingTime = pulseIn(echoPin, HIGH);
  pingTime /= 1000000; // microseconds to seconds
  pingTime /= 3600; // hours
  targetDistance = speedOfSound * pingTime; // miles
  targetDistance /= 2; // to from target (averaging distance)
  targetDistance *= 63360; // miles to inches

  Serial.print(""distance: "");
  Serial.print(targetDistance);
  Serial.println("""");

  delay(100);
}

Example Output
I moved my hand from 10"" away until I cover the sensor
10.20 distance: // my hand is 10"" away from the sensor
10.01 distance:
9.51 distance:
8.71 distance:
7.85 distance:
6.90 distance:
5.20 distance:
4.76 distance:
3.44 distance:
2.97 distance:
1.65 distance:
1211.92 distance: // my hand is now pressed up against the sensor
1225.39 distance:
1197.16 distance:
1207.43 distance:
1212.66 distance:
1204.60 distance:

EDIT
I changed the amounts from inches to milimeters to get a more precise reading. I held the sensor ~100mm from a granite counter-top and quickly lowered it until the tabletop covered the front of the sensor.
distance: 103.27 // 100mm from tabletop
distance: 96.50
distance: 79.84
distance: 76.72
distance: 62.66
distance: 65.78
distance: 54.85
distance: 47.04
distance: 44.95
distance: 38.71
distance: 28.81
distance: 25.69
distance: 27.08
distance: 25.17
distance: 27.77
distance: 22.04 // sensor continues toward table but values start to increase when they would logically decrease ??
distance: 23.95
distance: 26.73
distance: 28.81
distance: 46.52
distance: 2292.85 // sensor is now flush against tabletop
distance: 2579.59
distance: 2608.75
distance: 2595.56
distance: 2591.57
distance: 2583.75
distance: 2569.87
distance: 2570.91
distance: 2600.07
distance: 30579.64 // extreme high & low values with sensor is same place against tabletop
distance: 37.66
distance: 30444.43
distance: 37.66
distance: 30674.23
distance: 38.71

","arduino, ultrasonic-sensors"
Detect physical touch/hit,"I'm making a target to an outdoor robot competition.
The target should detect if some of the robot got touched or got an hit   automatically. and the target can get hit 360 degree. 
I'm searching for the perfect sensor to detect an hit, without get false positive from a wind.
My option right now are:
1- ultrasonic sensor (bad coverage)
2- tilt sensor  (bad FP rate)
3- wooden conductive 
I would like to know if someone has other ideas (that affordable - less than 30$ dollar per target might be o.k)
Edit: the target is static, and just waiting to a robot to touch it.
Edit: The specs are:
1- The target dimension is 1 meter height, 0.5 meter width , 0.3 depth.
2- To trigger the target ,the robot should be around 10 centimeter long to any point of the target surface.
3-To trigger the target the robot needs to get close up to 10 centimeter or even press with around 1 Newton force. the robot might even throw an object that satisfy the previous condition.
4-Detection must be only from intentional touch.
5-Wooden conductive is trigger because a human is Electrically conductive. this might not be the option when we throw an object.
6- Target will be placed outdoor, so the sensor need to be wind-resistance (not extreme wind condition- just around 20-25 km/h)
7- I  prefer a sensor that detect touch (more than proximity)because it might make my solution more cheap and reliable(in factor of amount sensors as i estimate).
Thanks.
Guy
","arduino, sensors, force-sensor"
Quadcopter forward speed,"I am trying to better understand the dynamics of forward flight in multirotors.
Assuming I have a quadcopter with 4 motor/propeller combinations capable (each) of a propeller pitch speed of, say, SpeedMax= 100 mph.
In forward horizontal flight, the quadcopter will pitch down at a certain angle, let's say AlphaP, from horizontal. If AlphaP is, say, 45 degrees, and drag is neglected, wouldn't the quadcopter be capable of a max theoretical speed of sin (45)* SpeedMax ~ 70Mph?
Also, seems to me AlphaP cannot go all the way to 90 degree (quadcopter flying like a plane), as at that point the propellers would not produce any  upward thrust to maintain the copter aloft given there is no wing loading as available in a plane.  If drag was to be neglected, what factors would the optimum AlphaP be depended on, and what would that angle be, for maximum speed?
",quadcopter
Bldc motors erratic behavior with Arduino program,"I've been making my own quadcopter flight controller using Arduino Mega. This is the sample code I wrote in order to test the esc timers and motors:
byte channelcount_1, channelcount_2, channelcount_3, channelcount_4;  
int receiverinput_channel_1, receiverinput_channel_2, receiverinput_channel_3, receiverinput_channel_4, start;  
unsigned long channel_timer_1, channel_timer_2, channel_timer_3, channel_timer_4, current_time, esc_looptimer;  
unsigned long zero_timer, timer_1, timer_2, timer_3, timer_4;  
void setup() {  
  // put your setup code here, to run once:  
  DDRC |= B11110000; //Setting digital pins 30,31,32,33 as output  
  DDRB |= B10000000;; //Setting LED Pin 13 as output  
  //Enabling Pin Change Interrupts  
  PCICR |= (1 << PCIE0);  
  PCMSK0 |= (1 << PCINT0); //Channel 3 PIN 52  
  PCMSK0 |= (1 << PCINT1); //Channel 4 PIN 53  
  PCMSK0 |= (1 << PCINT2); //Channel 2 PIN 51  
  PCMSK0 |= (1 << PCINT3); //Channel 1 PIN 50  
  //Wait till receiver is connected
  while (receiverinput_channel_3 < 990 || receiverinput_channel_3 > 1020 || receiverinput_channel_4 < 1400) {  
    start++;  
    PORTC |= B11110000;  
    delayMicroseconds(1000); // 1000us pulse for esc  
    PORTC &= B00001111;  
    delay(3); //Wait 3 ms for next loop  
    if (start == 125) { // every 125 loops i.e. 500ms  
      digitalWrite(13, !(digitalRead(13))); //Change LED status  
      start = 0; //Loop again  
    }  
  }  
  start = 0;  
  digitalWrite(13, LOW); //Turn off LED pin 13  
  zero_timer = micros();  
}  
void loop() {  
  // put your main code here, to run repeatedly:  
  while (zero_timer + 4000 > micros());  
  zero_timer = micros();  
  PORTC |= B11110000;  
  channel_timer_1 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 33  
  channel_timer_2 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 32  
  channel_timer_3 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 31  
  channel_timer_4 = receiverinput_channel_3 + zero_timer; //Time calculation for pin 30  
  while (PORTC >= 16) //Execute till pins 33,32,31,30 are set low  
  {  
    esc_looptimer = micros();  
    if (esc_looptimer >= channel_timer_1)PORTC &= B11101111; //When delay time expires, pin 33 is set low  
    if (esc_looptimer >= channel_timer_2)PORTC &= B11011111; //When delay time expires, pin 32 is set low  
    if (esc_looptimer >= channel_timer_3)PORTC &= B10111111; //When delay time expires, pin 31 is set low  
    if (esc_looptimer >= channel_timer_4)PORTC &= B01111111; //When delay time expires, pin 30 is set low  
  }  
}  
//Interrupt Routine PCI0 for Receiver  
ISR(PCINT0_vect)  
{  
  current_time = micros();  
  //Channel 1  
  if (PINB & B00001000)  
  {  
    if (channelcount_1 == 0 )  
    {  
      channelcount_1 = 1;  
      channel_timer_1 = current_time;  
    }  
  }  
  else if (channelcount_1 == 1 )  
  {  
    channelcount_1 = 0;  
    receiverinput_channel_1 = current_time - channel_timer_1;  
  }  
  //Channel 2  
  if (PINB & B00000100)  
  {  
    if (channelcount_2 == 0 )  
    {  
      channelcount_2 = 1;  
      channel_timer_2 = current_time;  
    }  
  }  
  else if (channelcount_2 == 1)  
  {  
    channelcount_2 = 0;  
    receiverinput_channel_2 = current_time - channel_timer_2;  
  }  

  //Channel 3  
  if (PINB & B00000010)  
  {  
    if (channelcount_3 == 0 && PINB & B00000010)  
    {  
      channelcount_3 = 1;  
      channel_timer_3 = current_time;  
    }  
  }  
  else if (channelcount_3 == 1)  
  {  
    channelcount_3 = 0;  
    receiverinput_channel_3 = current_time - channel_timer_3;  
  }  
  //Channel 4  
  if (PINB & B00000001) {  
    if (channelcount_4 == 0 )  
    {  
      channelcount_4 = 1;  
      channel_timer_4 = current_time;  
    }  
  }  
  else if (channelcount_4 == 1)  
  {  
    channelcount_4 = 0;  
    receiverinput_channel_4 = current_time - channel_timer_4;  
  }  
}

However, my issue here is that the bldc motors i'm using don't work smoothly when connected to the Arduino. They erratically stop and even change direction of rotation at the same throttle input. I've tested them by connecting them directly to the transmitter and they work fine there with perfect rotation and speed. Can someone please help me out and tell me where I might be going wrong? 
EDIT: I do realize posting the entire Arduino code might be overkill, but I've been trying to solve this problem for three days (as of 22nd June,16) and I really do hope someone can point out any improvements/corrections in my code.
","quadcopter, arduino, brushless-motor, esc"
Single camera vision and mapping system,"Some time ago I saw a demo of a small 'toy tank' with a single camera mounted on it. This tank was able to drive around the floor and detect objects and then move/steer to avoid them.
The interesting part was that it used a single camera vision system and as far as I remember was taking advantage of the floor being flat. and then using the rate a feature was moving in the scene relative to the motors and directions of travel to evaluate and hence map the scene.
Can anyone send me pointers what to search for to get some more information on this, or some pointers to codebases that can do this.
The reason I ask is that this was a single camera system from a number of years ago (5+) and therefore (from what I remember) was a relatively low compute load.
I was intending to try this out on a Raspberry PI to build a car/tank that maps a room or set of rooms.
","raspberry-pi, computer-vision"
How does ODE determine contact points in Gazebo?,"I was looking at the contact points for the Atlas in the DRCsim package. Each foot has 4 contact points at each vertex of the rectangle. I'd like to know how these points are determined. I've tried looking at the ODE code, but C++ isn't my strong suit so I had some difficulty figuring out what was going on. What I understand is that ODE compares the geometries one by one however it's not possible to compare all points so it only compare a select few points. What I'm trying to understand is what basis are those particular points selected? Why does the Atlas have the 4 contacts set up the way they are, and not some additional points on the heel? Can I add them myself?
Thanks.
",gazebo
iRobot Create2: Granularity of drive control?,"I own an iRobot create2 on which I am planning to implement a control algorithm. After playing with the different drive commands, I noticed that changing the desired velocity values marginally doesn't seem to do anything.
Even the Drive PWM command that ranges from -255 to 255 seems to have an internal granularity that is bigger than 1.
In this video the create seems to change its driving direction nearly seamlessly, which I am not able to reproduce with the described behavior. 
Does anyone have any suggestions?
","control, motor, irobot-create"
Linearize a non linear system,"How do I linearize the following system using taylor series expansion:
$\dot x = v cos\theta \\ \dot y = v sin\theta \\ \dot \theta = u$

Here, $\theta$ is the heading direction of my robot, measured counter clockwise with respect to $x$ axis.
$v$ is the linear velocity of the robot,
$u$ is the angular velocity of the robot.
","mobile-robot, localization"
KUKA FRI program using JAVA,"I am trying to establish the FRI connection for KUKA LBR iiwa. I know how to configure the FRI connection as there are example programs available in the Sunrise.Workbench. 
A sample code is given below. My question is 'how to pass' the joint torque values (or joint position or wrench) to the controller using 'torqueOverlay' as mentioned in the code below. Since I could not find any documentation on this, it was quite difficult to figure out. Any sample code with explanation or any clues would be more than helpful.   
JAVA code:
package com.kuka.connectivity.fri.example;

import static com.kuka.roboticsAPI.motionModel.BasicMotions.ptp;

import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import com.kuka.connectivity.fri.ClientCommandMode;
import com.kuka.connectivity.fri.FRIConfiguration;
import com.kuka.connectivity.fri.FRIJointOverlay;
import com.kuka.connectivity.fri.FRISession;
import com.kuka.roboticsAPI.applicationModel.RoboticsAPIApplication;
import com.kuka.roboticsAPI.controllerModel.Controller;
import com.kuka.roboticsAPI.deviceModel.LBR;
import com.kuka.roboticsAPI.motionModel.PositionHold;
import com.kuka.roboticsAPI.motionModel.controlModeModel.JointImpedanceControlMode;

/**
 * Moves the LBR in a start position, creates an FRI-Session and executes a
 * PositionHold motion with FRI overlay. During this motion joint angles and
 * joint torques can be additionally commanded via FRI.
 */
public class LBRTorqueSineOverlay extends RoboticsAPIApplication
{
    private Controller _lbrController;
    private LBR _lbr;
    private String _clientName;

    @Override
    public void initialize()
    {
        _lbrController = (Controller) getContext().getControllers().toArray()[0];
        _lbr = (LBR) _lbrController.getDevices().toArray()[0];
        // **********************************************************************
        // *** change next line to the FRIClient's IP address                 ***
        // **********************************************************************
        _clientName = ""127.0.0.1"";
    }

    @Override
    public void run()
    {
        // configure and start FRI session
        FRIConfiguration friConfiguration = FRIConfiguration.createRemoteConfiguration(_lbr, _clientName);
        // for torque mode, there has to be a command value at least all 5ms
        friConfiguration.setSendPeriodMilliSec(5);
        friConfiguration.setReceiveMultiplier(1);

        getLogger().info(""Creating FRI connection to "" + friConfiguration.getHostName());
        getLogger().info(""SendPeriod: "" + friConfiguration.getSendPeriodMilliSec() + ""ms |""
                + "" ReceiveMultiplier: "" + friConfiguration.getReceiveMultiplier());

        FRISession friSession = new FRISession(friConfiguration);
        FRIJointOverlay torqueOverlay = new FRIJointOverlay(friSession, ClientCommandMode.TORQUE);

        // wait until FRI session is ready to switch to command mode
        try
        {
            friSession.await(10, TimeUnit.SECONDS);
        }
        catch (final TimeoutException e)
        {
            getLogger().error(e.getLocalizedMessage());
            friSession.close();
            return;
        }

        getLogger().info(""FRI connection established."");

        // move to start pose
        _lbr.move(ptp(Math.toRadians(90), Math.toRadians(-60), .0, Math.toRadians(60), .0, Math.toRadians(-60), .0));

        // start PositionHold with overlay
        JointImpedanceControlMode ctrMode = new JointImpedanceControlMode(200, 200, 200, 200, 200, 200, 200);
        PositionHold posHold = new PositionHold(ctrMode, 20, TimeUnit.SECONDS);

        _lbr.move(posHold.addMotionOverlay(torqueOverlay));

        // done
        friSession.close();
    }

    /**
     * main.
     * 
     * @param args
     *            args
     */
    public static void main(final String[] args)
    {
        final LBRTorqueSineOverlay app = new LBRTorqueSineOverlay();
        app.runApplication();
    }

}

","robotic-arm, programming-languages"
"""Smooth"" inverse kinematics model for 2-wheeled differential drive robot","I have been reading about kinematic models for nonholonomic mobile robots such as differential wheeled robots. The texts I've found so far all give reasonably decent solutions for the forward kinematics problem; but when it comes to inverse kinematics, they weasel out of the question by arguing that for every possible destination pose there are either infinite solutions, or in cases such as $[0 \quad 1 \quad 0]^T$ (since the robot can't move sideways) none at all. Then they advocate a method for driving the robot based on a sequence of straight forward motions alternated with in-place turns.
I find this solution hardly satisfactory. It seems inefficient and inelegant to cause the robot to do a full-stop at every turning point, when a smooth turning would be just as feasible. Also the assertion that some points are ""unreachable"" seems misleading; maybe there are poses a nonholonomic mobile robot can't reach by maintaining a single set of parameters for a finite time, but clearly, if we vary the parameters over time according to some procedure, and in the absence of obstacles, it should be able to reach any possible pose.
So my question is: what is the inverse kinematics model for a 2-wheeled differential drive robot with shaft half-length $l$, two wheels of equal radii $r$ with adjustable velocities $v_L \ge 0$ and $v_R \ge 0$ (i.e. no in-place turns), and given that we want to minimize the number of changes to the velocities?
","mobile-robot, inverse-kinematics"
Disable MAVLINK Heartbeat Using Telemetry,"I am using an APM 2.6 that is connected to an Odroid USB via Telemetry port (UART to USB). I am trying to get MAVLINK messages without the need to keep sending a HEARTBEAT message.  
If I switch to USB connection (not telemetry) I get MAVLINK messages continuously  without sending HEARTBEAT messages to the APM. I want to be able to do the same using the Telemetry port.
Is there any place in the firmware (ArduCopter) code that I can change? Or maybe just a parameter?
I am using 57600 Baud-Rate, I tried also 115200. And the USB cable is not connected.
","quadcopter, ardupilot, mavlink"
Arduino or Raspberry Pi?,"I want to make a object tracking quadcopter for a project. While I'm using the Arduino Mega 2560 as the flight controller, I was thinking of using an additional offboard microcontroller/board for getting data from the onboard camera,which would then send an appropriate command to the onboard Arduino.
I was hoping someone could provide clarification on the advantages/disadvantages of doing object tracking with either choice.
Thanks!   
","quadcopter, arduino, raspberry-pi"
create2 commands with an app called SERIAL,"there is an app called SERIAL, available in the app store. 
I've downloaded it on my mac and am experimenting with it, any ideas on how to send create2 OI commands using ""Serial""?
so far it seems a handy app, I've bypassed all the need for other drivers. anyone else use SERIAL/something of the like? 
*when the SERIAL terminal is open and the number 9 is pressed on my mac it seems to activate cleaning mode. thats all the communication I'm getting after hours of playing around in python and mac terminal.  
","mobile-robot, irobot-create, serial"
360 degree ultrasonic beacon sensor,"Basically, I want to detect an ultrasonic beacon in a radius around the robot.
The beacon would have a separate ultrasonic emitter while the robot would have the spinning receiver.
Are there any existing ultrasonic sensors that would meet this use case or am I stuck hacking one together myself?
Is ultrasonic even the best choice? I was hoping that the beacon would be kept in a pocket, so I figured optical sensors were out.
Edit: The beacon and robot will both be mobile so fixed base stations are not an option.
",sensors
What frequency does my quadcopter output-sense-calculate-output update loop need to stay stable?,"With a 600 mm (2 foot) motor-to-motor quadcopter, what frequency does my output-sense-calculate-output update loop need to stay stable?
I'm estimating a total takeoff weight of very roughly 2 pounds ( 0.9 kg ),
which I expect to be mostly motors and batteries.
","stability, quadcopter"
SLAM - odometry motion model,"I am making a project with a 4 wheeled differential robot to make visual SLAM using a stereo rig. I have some encoders to measure de displacement and the steering angle of the robot and I want to use the odometry motion model in the fastSLAM algorithm.
To use the odometry motion model you need to calculate the values it needs from the odometry reading (incremental encoders), $u_t=(\bar{x}_{t-1},\bar{x_t})$ where $\bar{x}_{t-1}=(\bar{x}\>\bar{y}\>\bar{\theta})$ and $\bar{x}_t=(\bar{x}'\>\bar{y}'\>\bar{\theta}')$ are the previous and the current pose extracted from the odometry of the vehicle.
My question is about how to obtain those values from the encoders. I guess that in this case I would need to obtain the equations from the geometric model for the differential robot:
$D_L=\frac{2\cdot\pi \cdot R_L}{N_c}\cdot N_L$
$D_R=\frac{2\cdot\pi \cdot R_R}{N_c}\cdot N_R$
$D_T=\frac{D_L+D_R}{2}$
$\Delta\theta=\frac{D_L-D_R}{L}$
where $D_L$ is the advance of the left wheel, $D_R$ is the advance of the right wheel, $R_L$ is the lecture from the left encoder, $R_R$ the lecture from the right encoder, $N_C$ the total number of pulses of the encoder type, $D$ is the total distance achieved by the robot and $\theta$ the angle steered. $L$ is the distance between the wheels.
Using those equations is possible to obtain the pose in every time step:
$\bar{x}_{t}=\bar{x}_{t-1}+D\cos(\theta_{t-1})$
$\bar{y}_{t}=\bar{y}_{t-1}+D\sin(\theta_{t-1})$
$\bar{\theta}_{t}=\bar{\theta}_{t-1}+\Delta\theta$
So those last are the values I need to inject to the modometry motion model and then add gaussian noise to them.
Am I right? Or is there another way of computing the pose from odometry for a differential 4-wheel robot?
","mobile-robot, slam, odometry, movement"
Stability of PID values update function for quadrotor,"A reviewer of the last paper I sent replied me that the it is very dangerous to update a PID with next kind of formula (paper is about quadrotor control):
$$
K_p (t + 1) = K_p (t)+e(t) (μ_1 (Pe(t)) + μ_4 (Pe(t)))
$$
$Pe(t)$ is the % relationship between the desired angles and the real angles, and $e(t)$ is the difference between those angles. $μ_1$ and $μ_4$ are the membership functions of a fuzzy function. I think that the reviewer is talking about the time increment update rather than the fuzzy usage and specific formula.
How can stability of this formula be tested, please?
EDIT: 
membership functions are represented in following graph:

$e(t)$ is not the absolute difference between angles, just the difference. It can be negative
","quadcopter, control, pid"
How to use the opcode to start?,"I am new to robotics.Recently came into contact code.So the teacher let me use the Serial port app for Android to enter the opcode.But the robot did not have any reaction.

I use the Communications Cable with Adapter in android phone.
App Use [DroidTerm: USB Serial port]
Serial Port Settings
Baud: 115200   (19200 also used)
Data bits: 8
Parity: None
Stop bits: 1
Flow control: None

I try to enter opcode, but no response.--> enter:128,135,134.....
But it did not show any reaction to the phone and robot.
I hope according to Opcode instructions to control robots to make the specified action.
",irobot-create
Measuring vehicle's forward and lateral acceleration using a smartphone,"I want to measure the acceleration (forward and lateral separately) using an android smartphone device in order to be able to analyse the driving behavior. 
My approach would be as follows:
1. Aligning coordinate systems
Calibration (no motion / first motion):
While the car is stationary, I would calculate the magnitude of gravity using Sensor.TYPE_GRAVITY and rotate it straight to the z-axis (pointing downwards assuming a flat surface). That way, the pitch and roll angles should be near zero and equal to the angles of the car relativ to the world.
After this, I would start moving straight forward with the car to get a first motion indication using Sensor.TYPE_ACCELEROMETER and rotate this magnitude straight to the x-axis (pointing forward). This way, the yaw angle should be equal to the vehicle's heading relativ to the world.
Update Orientation (while driving):
To be able to keep the coordinate systems aligned while driving I am going to use Sensor.TYPE_GRAVITY to maintain the roll and pitch of the system via


where A_x,y,z is the acceleration of gravity. 
Usually, the yaw angle would be maintained via Sensor.ROTATION_VECTOR or Sensor.MAGNETIC_FIELD. However, the reason behind not using them is because I am going to use the application also in electrical vehicles. The high amounts of volts and ampere produced by the engine would presumably make the accuracy of those sensor values suffer. Hence, the best alternative that I know (although not optimal) is using the GPS course to maintain the yaw angle.
2. Getting measurements
By applying all aforementioned rotations it should be possible to maintain an alignment between the smartphone's and vehicle's coordinate systems and, hence, giving me the pure forward and lateral acceleration values on the x-axis and y-axis.
Questions:

Is this approach applicable or did I miss something crucial?
Is there an easier/alternative approach to this?

","sensors, accelerometer, gps"
Task space to joint motion space conversion,"I am the moment trying to read and understand this paper Task Constrained Motion Planning in Robot Joint Space but seem to have a hard time understanding the math.  
The paper describes how to perform task constrained motion planning in cases where a frame is constrained to a specific task.
the problem the paper tackles is when sampling in joint space, randomized planners typically produce samples that lie outside the constraint manifold. the method  they proposed methods use a specified motion constrain vector to formulate a distance metric in task space and project samples within a tolerance distance of the constrain.   
Given the this I am seem to a bit confused on some simple terms they define in this paper. 
Examples. How is a task space coordinate defined ? what information does it have?
they compute the $$\Delta x = T_e^t(q_s)$$ which is transformation matrix of the end effector with respect to the task frame. 
What I don't get is why the end effector? and why the end effector with respect to the task frame?
Secondly.
Later in the paper they write down an expression that relates the task space to the joint space motion. They do it using the Jacobian, but seem to miss explaining (in my opinion) what $E(q_s)$ actually do. 
$$J(q_s) = E(q_s)J^t(q_s)$$
What is said about it in the paper is that 

Given the configuration $q_s$, instantaneous velocities have a linear
  relationship $E(q_s)$

why the need of instaneous? what is the definition of an instantaneous component? how does it differ from the information given by the jacobian?
Basically i don't understand how and why the mapping is as it is?.. 
","robotic-arm, motion-planning, jacobian"
"How can I charge a 11,1 volt LiPo akku?","I didn't found any modules to charge my 11,1 volt LiPo akku, only for 3,7 volt with 5 volt power supply. How can I handle that with a micro-USB connector on my robotplatform?
","arduino, power, lithium-polymer"
Locking the yaw direction of a laser pointer,"I have a laser pointer on a handle grip and I'm trying to keep the laser pointer's yaw direction, which can rotate at around 10deg/s. So I have the laser pointer on a stepper motor and an accelerometer/gyro in the handle, but what's a good way for maintaining its yaw direction? Could I simply turn the shaft according to the accelerometer/gyro's yaw readings or is control theory (PID) needed?
That is, if my stepper makes 4096 steps/rev, one gives 0.0879 deg. If the handle is turned by, say, 0.879 deg, then turn 10 steps in reverse (instantaneously). Would this be jerky and PID be needed?
Any thought appreciated.
","sensors, stability"
For a quadcopter: Premade flight controller or custom made?,"I'm interested in building a quadcopter. The result I'd like to obtain is an autonomous drone. I'd be interested in a GPS to allow it to remain stationary in the air, and also to fly through checkpoints.
Can this be done with a flight controller, or does it need to be programmed? I'm not too sure about what flight controllers really are.
Could someone offer any materials to help me get towards this goal.
Thanks, Jacob
","arduino, quadcopter"
How to stabilize a quadcopter,"Today was my quadcopter's first ""flight"". I'm running megapirate on a Crius AIOP v2 with a Turnigy Talon v2 frame.
I only touched the throttle stick on my remote, nothing else. When I felt the quadcopter was about to take off, I pushed the throttle just a little bit more, and the quadcopter oscillated 2 or 3 times and the just flipped over, landing on the propellers.
So, I broke 2 props, my frame feels a bit loose, I'll probably have to tighten the screws (I hope...). How can I tune the software so it will stabilize nicely after take off?
Edit :
I don't know if it was true oscillation or just random air flows making it unstable. I made some more tests yesterday and it was quite OK (even if I crashed a few times). This time, it was really oscillating but it was quite windy outside and the quadcopter managed to stabilize after all. So i'll probably have to tune my PIDs and find a way to do it without crashing.
Edit 2 : After some PID tuning, I managed to stabilize my quadcopter pretty well but it's still oscillating just a little bit. I guess I'll have to slightly change the values to get a perfect stabilization.
","quadcopter, stability"
What is the most realistic grasping simulator?,"I am looking for a physics simulator which can accurately model a robot hand picking up an object. The main requirement is for accuracy / realism, rather than speed. It needs to be able to model soft bodies, such as the rubber ""skin"" on robotic finger tips. It also needs to be a dynamics engine, such that the object is actually moved around by the hand, modelling effects such as slippage.
From the research I have already done, there are two good candidates. First, GraspIt! (http://graspit-simulator.github.io/). This is open-source, and specifically designed for grasping, rather than physics simulation in general. Second, MuJoCo (http://www.mujoco.org/). This is a more general simulator, is a commercial product, and has been adopted by some big names such as DeepMind.
I have tried using the Bullet physics engine for robot grasping simulation, but soon realised that this was not going to be strong enough, because Bullet is really designed for games, and hence sacrifices realism for speed. However, I'm much more interested in something which is as realistic as possible, even if the computation is slow.
Does anyone have any suggestions as to how I can proceed? Anybody with any experience with GraspIt! or MuJoCo?
Thanks!
","robotic-arm, simulator, simulation"
In how many ways can a six propellers drone fly or rotate?,"I thought it were twelve ways:

Six for each ways between two propellers
Six others for each rotation on these ways.

But according to Vijay Kumar Dean of Penn Engineering , it seems that I was wrong...
Then I read this article  about modeling and robust trajectory tracking control for a novel Six-Rotor Unmanned Aerial Vehicle and this one about navigation and autonomous control of a Hexacopter in indoor environments but was never able to find such an information.
I then guessed that 3 of the rotors could go one direction and three others into another which would add 6 other ways for rotating and therefore 6 others for simply flying but that is only a guess.

",multi-rotor
Stabilising an inverted pendulum,"With the problem of stabilising an inverted pendulum on a cart, it's clear that the cart needs to move toward the side the pendulum leans. But for a given angle $\theta$, how much should the cart move, and how fast? Is there a theory determining the distance and speed of the cart or is it just trial and error? I've seen quite a few videos of inverted pendulum, but it's not clear how the distance and speed are determined.
","mobile-robot, sensors, accelerometer, gyroscope"
Choice of a motor for robotic arm,"This is my first post here, so hello all. I really hope I can learn a lot from you guys.
I am trying to build a robotic arm to carry an object and put it inside of different boxes that are placed in different fixed locations.
I found a few robotic arms that can do it, but I am still trying to find the right motor for the job. I read a lot on-line about the different motors, but I am not sure which on to pick. Since the boxes are located in fixed places, the motors have to move in a precise way, so, according to my research, Servo motors are the ones I should use.  
Since it is a low budget project (I am college student), I wasn't sure which motor to choose (there are a lot of servo motors out there). I found several Servo motors on-line, for example , Analog Feedback Servo, and I was wondering what is the best servo motor I can buy for a really low cost project? I think I can spend about 10-20$ per motor (I need 5 motors).
I already have an Rpi and I know that pin 18 is the PWM pin that controls the motor's precision movement, but before I purchase a PWM controller and additional motors I need to run some testing to find how precise the motor is.
By the way, how can I calculate the amount of weight the motor can handle?
Any ideas and information will be greatly appreciated.
Thank you
","robotic-arm, raspberry-pi, servomotor, python"
How are industrial robotics components purchased?,"For hobbyists, you go to a store to buy products. The prices for these products are all clearly listed in the store catalog, and you can easily search for parts by lowest price or read customer reviews of the products.
For industrial engineers building complex machines, how do they buy components? Or don't they worry about cost, and leave it to their employer to eat the cost as a part of doing their line of work?
Is it possible to ""shop around"" for low-cost engineering components?
 
It is unclear to me how someone building a robot on their own as a small one-man startup can make the step from the world of toy robots, to larger and more industrial robotic components.
Most of the non-hobbyist stuff is hidden away and not exposed to the world. While a product catalog might be available, there are no prices listed for anything.
For larger industrial components, there does not seem to be any realistic way to shop around for the lowest price or best value, since pricing for much of the big stuff is basically unavailable.
 
For me personally, I am interested in trying to build my own powered exoskeleton on a middle class American income, so I can't afford to be paying 1000 bucks for a single electrohydraulic proportioning servo valve, when I'll need probably 50 of them. But shopping around for low cost ones is basically impossible as far as I can determine, because pricing info is generally not available or searchable from the majority of manufacturers.
",industrial-robot
How to make a robot?,"For instance, how would you hook up a electric pump communicate with a motherboard? Let's say I buy a electric pump, I hook it up to some sort of metal structure that if the pump is turned on it moves the metal structure, how would I hook up the pump to my motherboard so that I can program it? 
","control, motor, robotic-arm, microcontroller, machine-learning"
Should I use gyro or encoders for robot moving in straight line?,"I've recently succeeded in building my first collision-avoidance Arduino robot with 2 DC motors, and it works pretty well. However, it doesn't move in a straight line yet, when it should. I'm now studying which method should I implement to make the robot go straight. I've heard about using IMU or encoders with feedback control. I pretty much understand how to use the encoders, but I'm not sure about the gyro. Should I use just one of those or a combination of them?
","mobile-robot, arduino, control, gyroscope"
Sporadic sensing rates for hc-sr04 ultrasonic distance sensor,"Been working on a robot recently which uses ultrasonic sensors for an integral part of the navigation.
While testing the sensors I noticed a strange behaviour, the sensors seem to frequently stop functioning and bring the entire Arduino Mega I'm working with to a stop. The strange part is that these stops seem to be entirely random, on some occasions the sensor will read values consistently (at maybe 20 vals per second) for 10+ seconds, then all of a sudden the sensor will slow to reading only 2-3 values per second with stalls between.
I have tested several sensors and different codes for pinging distances yet the problem has persisted.
This leads me to believe the issue is with the arduino mega itself, but I am unsure how to verify this. Any advice?
Thanks in advance!
PS: other pins on the Mega seem to be working fine, i.e. analog pins for IR reflectance sensors and PWM pins for driving 2 DC motors.
","arduino, ultrasonic-sensors"
Rostock Delta Robot 3D Printer Degrees of Freedom (DOF),"What is the degrees of freedom (DOF) of the Rostock delta robot 3d printer (delta mechanism that consists of three prismatic joints)?
Here's the link to the delta mechanism I'm referring to:
https://www.youtube.com/watch?v=AYs6jASd_Ww.
Thanks in advance for your help!
","kinematics, inverse-kinematics, actuator, manipulator, 3d-printing"
Position Control vs Velocity Control vs Torque Control,"Please can somebody explain to me the difference between Position Control, Velocity Control, and Torque Control? Specifically, I am thinking in terms of a robot arm. I understand that position control tries to control the position of the actuators, such that the error signal is the difference between the current position and the desired position. Velocity control is then trying to control the velocity of each actuator, and torque control is trying to control the torque of each actuator.
However, I don't understand why these are not all the same thing. If you want to send a robot arm to a certain position, then you could use position control. But in order to move an actuator to a certain position, you need to give it a velocity. And in order to give it a velocity, you need to give it a torque. Therefore, whether the error is in position, velocity, or torque, it always seems to come back to just the error in torque. What am I missing?
","control, kinematics, dynamics"
Rotate 3d vector value into a single axis using a rotation quaternion,"I want to rotate the whole value of a 3d vector into one axis using quaternion rotations.
The reason behind is that I want to align the X and Y Axis of my smartphone with the X and Y Axis of my vehicle in order to detect lateral and longitudinal acceleration separated on these two axis. Therefore I want to detect the first straight acceleration of the car and rotate the whole acceleration value into the heading axis (X-Axis) of the phone assuming a straight forward motion.
How do I achieve this?


","sensors, accelerometer, rotation"
Double/Triple inverted pendulum always on a cart?,"All of the examples of keeping a double/triple inverted pendulum balanced using a PID controller I've seen seem to be on a cart. Like this one https://www.youtube.com/watch?v=cyN-CRNrb3E
How come the PID controller always controls a cart rather than a servo that holds the first pendulum? The second/third pendulum could be connected loosely on the first pendulum and the PID controller controls the first pendulum. Is it because servos tend to be too slow or are there other reasons?
","motor, mechanism, servos"
What information an IMU gives to a drone?,"An Inertial Measurement Unit (IMU) is an important sensor used in aerial robotics. A typical IMU will contain an accelerometer and a rate gyroscope. Which of the following information does a robot get from an IMU? 

Position
Orientation
Linear velocity
Angular velocity
Linear acceleration
Angular acceleration

I don't think it gets its orientation information from IMU. The last time I took the test, I said that all but the first two are true. I failed.
",imu
How mature is real-time programming in robotics?,"Edit: I don't know why, but this question seems to be confusing many people. I am aware of when/where/why/how to use real-time. I am interested in knowing whether people who have a real-time task would actually care enough to implement it in real-time or not.
There's no need to mention why real-time operations are important for a robot. My question is however, how much is it actually used in robotics?
Take this question for example. Only one answer mentions any platform with real-time capabilities, and it is far from the top too. ROS apparently, being a very popular platform which is not real-time.
In the real-time world however, RTAI1 seems to be the only workable free real-time platform of use. It is however, limited to Linux (no problem), badly documented and slowly developed.
So, how much is real-time behavior sought after among robotics developers? The question is, how much are developers inclined to write real-time applications when real-time behavior is actually needed? If not much, why?
For example, reflexive behavior based on tactile data, cannot go through ROS because it would lose its real-time property. But do people really come up with a real-time solution or use ROS anyway, ignoring the real-time property?
1 or similarly Xenomai
","software, platform, real-time"
"First build - Quadcopter , need help deciding hardware and connections","I am building my first drone,

Objective: - Need to control drone by wifi on phone or laptop using
  ground station software of Openpilot

I have a Arduino 2560 , cc3d Openpilot flight controller , raspberry pi with wifi bluetooth in built...
Now i am not able to understand , how to go forward , should i connect arduino with openpilot cc3d flight controller , or raspberry pi directory with cc3d flight controller ....
Do i really need arduino 2560 now ?
also how to connect r pi with cc3d flight controller , and how to mock PWM signals ?
","quadcopter, arduino, raspberry-pi"
Build a simple robot to learn ROS,"I am a beginner to ROS and I wanted to know if I could build a simple robot to learn ROS. 
I currently have the following components available:

Arduino Uno
Simple two wheeled robot chassis
Some motors
L293D motor driver
Some ultrasonic sensors
Some infrared sensors

","arduino, slam, ros, beginner, ultrasonic-sensors"
Damping vs Friction,"I am using a physics simulator to simulate a robot arm. For a revolute joint in the arm, there are two parameters which need to be specified: damping, and friction. If a torque is applied to the joint, both the damping and the friction seem to reduce the resultant force on the torque. But what is the difference between the damping and the friction?
","robotic-arm, dynamics"
Downgrade ROS from Jade to Indigo,"Is it possible to downgrade from ROS Jade to Indigo?
For those who are not yet familiar with Robot Operating System (ROS), here: ROS
",ros
"Raspberry pi 3 location in a set field, no gps","I'm developing a project which involves a raspberry pi 3 remote control rover and I need to know the exact location of the raspberry pi rover in a set field.
Let's say I have four logs, one in each corner of the square field (the goal right now is to extend this to any shape field, any number of corners), every of them equipped with (some kind of wave technology) that allows me to triangulate the position (based on signal intensity) of the raspberry pi rover.
The distance between logs should not be bigger than 30m (~100feet) and there is no line of sight guaranted.
The question is: Which kind of technology should I use, infrared, wifi, bluetooth, radio, ultrasound, etc? or, is there any better approach to this problem?
",wireless
How to go around in a circle?,"I have the mBot robot and I'm trying to get it to go to the other side of a cylindral obstacle. 
Something like this:

What I know:

Radius of the cylinder - r
Robot's distance from the cylinder
Wheel thickness - 1.5 cm
Distance between the middle of each wheel - 11.5 cm

How would I achieve the above path?
The only thing I saw was this SO question that says: 

The distance between the left and right wheel of the robot is 6
  inches.
So the left wheel should travel at a distance of 2(pi)(radius+6)
And the right wheel should travel at a distance of 2(pi) (radius-6)

The problem with my robot is that you can't tell it to go 20cm to the right, nor can you tell it to turn 90 degrees to the right.
All you can do is set each motor's speed 0-255, so there's not way to put it in the formula disatance = time x speed.
I assume I have to set each motor's speed to a different value so they would go in a circle of radius x and then just exit at the half of the circle (like shown in the picture)
","arduino, wheeled-robot"
Design in the robotics world,"Apologies if this isn't really the right place to be asking, but I was wondering whether third party design firms are ever contracted to design industrial and or consumer robots? 
If not is it something that is usually done in house, and who within an org would usually take care of this process?
Thanks.
",design
How to produce a continuous variation of a discontinuous function?,"I have a differential equation that connects the ""velocity"" of a point in the FOV of a camera with the velocities of a robot's joints, that is $$\dot s=J(s) \dot q$$ where s is a vector with the $x$,$y$ coordinates of the point in the FOV, $J$ is the interaction matrix and $q$ is the vector of the joint positions. 
If I have a certain point whose velocity I am tracking and this point remains in the FOV, then $\dot s$ is well defined. But if I change this point online, that is at the time instant $t$ I have point $s_t$ and at the time instant $t+dt$ I have the point $s_{t+dt}$, then $\dot s$ is not defined.
Can I create a filter to produce a continuous variation of $\dot s$? If not, what can I do?
More specifically, I want to perform occlusion avoidance. In order to do this I want to compute the minimum distance of each feature point of my target object from the possibly occluding object. But, obviously, this distance can be discontinuous due to the fact that another possibly occluding object can appear in the FOV nearer to my target than the previously measured.  
","cameras, visual-servoing, filter"
Solving Inverse Kinematics with Non-Linear Least Squares,"I want to write my own inverse kinematics solver, and I have been recommended to use Google's Ceres Solver to help. Now, according to the documentation, Ceres Solver is usually used for non-linear least squares problems (http://ceres-solver.org/nnls_tutorial.html). This minimises the sum of squared differences between the measured and target values, over all data. What I am confused about, is how this relates to inverse kinematics.
In inverse kinematics, with the example of a robot arm, the goal is to determine what joint angles the robot should be positioned in, in order to reach a target end effector pose. There exists a single equation which determines the end effector pose, given the set of joint angles, and we want to determine the parameters in that equation (the joint angles).
But how does this relate to the least-squares problem, where there are multiple measurements? Is the problem I am trying to solve essentially the same, except that the number of measurements is one? And in that case, is using Ceres Solver's non-linear least squares solver really necessary?
Thanks!
","robotic-arm, inverse-kinematics"
How to compile arm compatible binary from an x86 precompiled library on a pc host to be run on an arm target?,"I am using a library precompiled on x86 on my pc (x86_64). Does there exist any toolchain to compile the x86 library and in the end generate an executable for armv7l ubuntu?
","arm, linux"
Instantaneous velocity calculation from accelerometer?,"I am trying derive velocity from accelerometer (MPU9250 in sensor-tag board). I saw lot of blogs which talk about noise and exact estimation problems. I started seeing velocity derivation (integration of accelerometer data over time) yielding me towards ramp because of noise presence in MPU9250.
Is the velocity can be estimated only by accelerometer or we need assistance of another sensor such as GPS or gyroscope, etc..
Please let me know as I see my velocity calculations never converge at all.
Also I have limitation in compute power, so Kalman filter kind of estimation techniques is difficult to implement. Can you please suggest whether I am in right direction or not.
","sensors, accelerometer"
Why wouldn't my robot stop?,"I am working on an Arduino based robot which engages a braking mechanism detecting anything in front. I was using an ultrasonic sensor, to detect obstacles which worked well while the robot was on my table (i.e under construction). But when I ran it on the ground, it doesn't stops and crashes.
The robot is programmed as if anything is detected 50 cm ahead if the robot, the braking mechanism stops the wheels. But when testing, the robot just wouldn't stop.
My robot is running at an average 7.5m/s . Thinking that doppler's effect might have rendered my sensor useless, I tried a little IR sensor I had lying around (range 25 cm approx), but that didn't work as well.
What am I doing wrong here?
","arduino, motor, ultrasonic-sensors"
Can too much input current destroy my motor driver?,"I have a 18 V rated driver I'm using to drive two 12 V DC gear motors using my Arduino. I bought a new battery pack which is rated 3300 mAh 25C, 11.1V making the total current input 82.5 A. My driver is rated for 7 V min and 18 V max, no current rating is given.
My motors are 12V max current under load is 9.5 A.
So just to be sure, can using this battery destroy my motor driver?
This is the datasheet.
","motor, battery, current"
Is there an algorithm using the Kinect depth image (not the point cloud) for registration?,"I know given the intrinsics fx, fy, cx, cy (where fx, fy are the horizontal and vertical focal length, and (cx, cy) is the location of principal point of the camera if pinhole camera model assumed) of an Kinect depth camera(or other range sensor?), a depth pixel px=(u, v, d) ((u, v) is the pixel coordinate, d is the depth value) can be converted to a 3D point p:
p=(x, y, z)
x=(u-cx)/fx*d
y=(v-cy)/fy*d
z=d

so that a depth image can be converted to a point cloud, and indeed, a depth Image represents a unique point cloud physically.
SLAM systems e.g. KinectFusion use such point clouds for ICP based registration to obtain camera pose at each time and then fuse new point cloud to the previously reconstructed model.
However, my mentor told me that depth Image cannot be inveribly converted to a point cloud since it's 2D->3D mapping with ambiguity (which I disagree), and he claims that I should use the depth Image at time (i-1) and (i) for registration, not the derived point cloud.
（If I have to obey my mentor's order) I've been reading papers and found one using Gradient Descent to solve camera pose (tx, ty, tz, qw, qx, qy, qz):

Prisacariu V A, Reid I D. PWP3D: Real-time segmentation and tracking
  of 3D objects[J]. International journal of computer vision, 2012,
  98(3): 335-354.

which uses RGB Images and a known model for pose estimation. However, I've NEVER found a paper (e.g., KinectFusion and other later RGB-D SLAM algorithms) deals with depth data just as plane image but not point cloud for registration. So could someone give me some hint (papers or opensource code) about: 
How to do depth image registration without converting them to point clouds?
","localization, slam, kinect"
What software to use to send (OI) commands to Create 2. Using windows laptop and supplied Create 2 cable?,"I read most of the iRobot Create 2 Open Interface (OI). It says send these serial commands to the Create 2 to get it to do the described action, but no suggestion of what software to use to send these serial commands through the USB interface.  I did install the FTDI Drivers to enable the USB to serial connection.  Question: What serial software should I use to communicate with Create 2?  Is there a tool to verify that the supplied usb to serial cable supplied with Create 2 is functioning and if the Create 2 is functioning? (I did a reset on Create 2 using Spot and Dock buttons)
","irobot-create, serial"
Quadcopter: X-Y Velocity PID Controller,"Good day,
Introduction
I am currently working on an autonomous quadcopter project. I have currently implemented a cascaded PID controller consisting of two loops. The inner rate loop takes the angular velocity from the gyroscope as measurements. The outer stabilize/angle loop takes in angle measurements from the complementary filter (gyroscope + accelerometer angles). 
Question:
I would like to ask if it is effective to cascade a Lateral Velocity (X and Y - axis) PID controller to the the Angle Controller (Roll and Pitch) to control drift along the X-Y plane. For the outermost PID controller, the setpoint is 0 m/s with the measured velocities obtained from integrating linear accelerations from the accelerometer. This then controls the PID controller responsible for the Pitch (if Y velocity PID) and Roll (if X velocity PID).
","quadcopter, control, pid, raspberry-pi, stability"
Mobile robot algorithm implementation error,"I am working in reproducing a robotics paper, first simulating it in MATLAB in order to implement it to a real robot afterwards. The robot's model is:
$$\dot{x}=V(t)cos\theta $$
$$\dot{y}=V(t)sin\theta$$
$$\dot{\theta}=u$$
The idea is to apply an algorithm to avoid obstacles and reach a determines target. This algorithm uses a cone vision to measure the obstacle's properties. The information required to apply this system is:
1) The minimum distance  $ d(t) $ between the robot and the obstacle (this obstacle is modelled as a circle of know radius $ R $).
2) The obstacle's speed  $ v_{obs}(t) $
3)The angles $ \alpha_{1}(t)$ and $ \alpha_{2}(t)$  that form the robot's cone vision, and
4) the heading $ H(t) $  from the robot to the target
First a safe distance  $ d_{safe}$  between the robot and the obstacle is defined. The robot has to reach the target without being closer than        $ d_{safe}$ to the obstacle.
An extended angle $ \alpha_{0} \ge arccos\left(\frac{R}{R+d_{safe}} \right) $  is defined, where $ 0 \le \alpha_{0} \le \pi $ 
Then the following auxiliary angles are calculated:
$ \beta_{1}(t)=\alpha_{1}(t)-\alpha_{0}(t)$ 
$ \beta_{2}=\alpha_{2}(t)+\alpha_{0}(t)$ 
Then the following vectors are defined:
$ l_{1}=(V_{max}-V)[cos(\beta_{1}(t)),sin(\beta_{1}(t))]$ 
$ l_{2}=(V_{max}-V)[cos(\beta_{2}(t)),sin(\beta_{1}(2))]$ 
here $ V_{max}$  is the maximum robot's speed and  $ V $ a constant that fulfills  $ \|v_{obs}(t)\| \le V \le V_{max} $ 
This vectors represent the boundaries of the cone vision of the vehicle
Given the vectors $ l_{1} $  and $ l_{2}$ , the angle $ \alpha(l_1,l_2)$  is the angle between $ l_{1}$  and $ l_{2} $  measured in counterclockwise direction, with $  \alpha \in (-\pi,\pi) $ . Then the function $f$ is 
The evasion maneuver starts at time $t_0$. For that the robot find the index h:
$h = min|\alpha(v_{obs}(t_0)+l_j(t_0),v_R(t_0))|$
where $j={1,2}$ and $v_R(t)$ is the robot's velocity vector 
Then, from the two vectors  $v_{obs}(t_0)+l_j(t_0)$ we choose that one that forms the smallest angle with the robot's velocity vector. Once h is determinded, the control law is applied:
$u(t)=-U_{max}f(v_{obs}(t)+l_h(t),v_R(t))$
$V(t)=\|v_{obs}(t)+l_h(t)\| \quad \quad (1)$ 
This is a sliding mode type control law, that steers the robot's velocity  $v_R(t)$ towards a switching surface equal to the vector $v_{obs}(t)+l_h(t)$. Ideally the robot avoids the obstacle by surrounding it a 
While the robot is not avoiding an obstacle it follows a control law:
$u(t)=0$
$V(t)=V_{max} \quad \quad  (2) $    
Hence the rules to switch between the two laws are:
R10 Switching from (2) to (1) occurs whenthe distance to the obstacle is equal to a constant C, which means when $d(t_0)=C$ and this distance is becoming smaller in time  i.e. $\dot{d(t)}<0$
R11 Switching from (1) to (2) occurs when $d(t_*)<1.1a_*$ and the vehicle is pointing towards the obstacle, i.e. $\theta(t_*)=H(T_*)$
where $a_*=\frac{R}{cos\alpha_0}-R $
Ideally the result should be similar to this
But I'm getting this instead
While I understand the theory there's obviously a flaw in my implementation that I haven't been able to solve. In my opinion the robot manages to avoid the obstacle but at certain point (in the red circle), the robot turns to the wrong side, making impossible the condition $H(t) = \theta(t) $ to be achieved.
I feel that I am not measuring properly the angle alpha between the $v_{obs}(t)+l_h(t)$ and $v_{R}(t)$ , because while debugging I can see that at certain point it stops switching between negative and positive values and become only positive, leading the robot's to the wrong side. It also seems to be related with my problem here: Angle to a circle tangent line

","mobile-robot, kinematics, matlab, geometry"
augmenting room magnetic field for smartphone sensors,"is it possible to enhance (or redirect) the earth's magnetic field in a room or house so that one can write a small program that makes smartphones with hall-effect sensors detect more reliably in which direction they are pointing?
I presume a fridge magnet won't do the job...
","sensors, hall-sensor"
How to further understand the computed torque model controller,"For the following controller what do $q_{des}$ and $q_{act}$ stand for? Also, what is the general principle of this controller? 
Thanks!
","control, microcontroller, torque"
Distance calculation with two robots and two obstacles,"I have a problem with two robots and two obstacles in a space. Each robot can communicate its measurements to the other and can measure angles and distances.
The two obstacles in the environment are identical to each other.
Each robot can see both obstacles but not each other. Therefore have angle theta 1 and 2 combined with distance 1 and 2. Can the distance between the two robots be calculated?

So far I have placed circles with radius of the measured distance over each landmark (triangles in my workings), this provides 4 possible positions for each robot. Red and black circles correspond to robot 1 and blue and green to robot 2. Using the relative size of the angle measurements I can discount two of these positions per robot.
This still leaves me with two possible positions for each robot shown with the filled or hashed circles.

Is it possible to calculate which side the robot is to the landmarks and the distance between each other?
Robot 1 only has the two measurements of angle and distance and can therefore assign an id to each obstacle, but when information is transmitted to robot 2, robot 2 does not know which obstacle will have been designated an id of 1 or 2.
","localization, kinematics"
How to get manufactured part from CAD file?,"I am working through the book Learning Robotics using Python, which is for Python programmers who want to learn some robotics. Chapter 2 shows you how to use LibreCAD to design the plates and poles that form the chassis of the turtlebot-like robot. For instance, the base plate looks like this:

Then, there is nothing more about it but suddenly in a later chapter there is a picture of the fully manufactured plates assembled into a chassis, and the author acts as if this should just be something we know how to do:

How did he do that? We have these CAD drawings, and suddenly there are these plates that were manufactured via some magical process the book never discusses, he never gives things like tolerances, the material that they are supposed to be made out of, etc., the kinds of things discussed here: 
http://www.omwcorp.com/how-to-design-machined-parts.html
I know nothing about this stuff, in terms of how to go from CAD design specs to getting an actual part manufactured. What kinds of companies should I use, what is a reasonable price to expect, what is the process? 
In general, how do I go from CAD design to manufactured item? Do I find a local machine shop that specializes in robotics, bring my CAD drawings, and work with them to try to build the parts?
I am totally a noob, I hope this isn't a question like this:
http://blog.stackoverflow.com/2010/11/qa-is-hard-lets-go-shopping/
","design, software, manufacturing, chassis, hardware"
Pushing Buttons Remotely over Ethernet,"So I want to program something that will simply push a button, but controllable over ethernet.  I'm new to robotics so I don't know where to start.  What's the best way to control an actuator over a network connection?
","arduino, actuator"
kalman filter with redundant sensors,"Suppose I have one robot with two 3D position sensors based on different physical principles and I want to run them through a Kalman filter. I construct an observation matrix two represent my two sensors by vertically concatenating two identity matrices.
$H = \begin{bmatrix} 1&0&0\\0&1&0\\0&0&1\\1&0&0\\0&1&0\\0&0&1 \end{bmatrix}$ $\hspace{20pt}$
 $\overrightarrow x = \begin{bmatrix} x\\y\\z \end{bmatrix}$
so that 
$H \overrightarrow x = \begin{bmatrix} x\\y\\z\\x\\y\\z \end{bmatrix}$
which represents both sensors reading the exact position of the robot. Makes sense so far. The problem comes when I compute the innovation covariance
$S_k = R + HP_{k|k-1}H^T$
Since 
$H H^T = \begin{bmatrix}
 1 & 0 & 0 & 0 & 0 & 1 \\
 0 & 1 & 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 & 1 & 0 \\
 0 & 1 & 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 & 1 & 0 \\
 1 & 0 & 0 & 0 & 0 & 1 \\
 \end{bmatrix}$ 
then, no matter what $P$ is, I'm going to wind up with $x$ innovations from the first sensor being correlated to $z$ innovations from the second, which seems intuitively wrong, if I'm interpreting this right. 
Proceeding from here, my gain matrix ($K = P_{k|k-1} H^T S_k^{-1}$) winds up doing some pretty odd stuff (swapping rows and the like) so that, when updating a static system ($A = I_3, B = [0]$) with a constant measurement $\overrightarrow z = [1,0,0]$ I wind up with a predicted state $\hat x = [0,0,1]$.
If I separate the sensors and update the filter with each measurement separately, then $H H^T = I_3$, and I get sensible results.
I think I am confused about some technical points in one or more of these steps. Where am I going wrong? Does it not make sense to vertically concatenate the observation matrices?
I suppose that I could just set the off-diagonal 3x3 blocks of $S_k$ to 0, since I know that the sensors are independent, but is there anything in the theory that suggests or incorporates this step?
",kalman-filter
Visualizing and debugging an EKF,"I am currently debugging and tuning an EKF (Extended Kalman Filter). The task is classical mobile robot pose tracking where landmarks are AR markers.
Sometimes I am surprised how some measurement affects the estimate. When I look at and calculate the numbers and matrices involved, I can work out how the update step got executed, what and why exactly happened, but this is very tedious. 
So I wonder if anyone is using some technique, trick or clever visualization to get a better feel of what is happening in the EKF update step?
UPDATE #1 (will be more specific and show first approximation of what I have in mind)
What I am looking for, is some way to visualize one update step in a way that gives me a feel of how each component of the measurement affects each component of the state.
My very first idea is to plot the measurement and it's prediction together with some vectors taken from the K matrix. The vectors from K represent how the innovation vector (measurement - measurement prediction, not plotted) will affect each component of the state.
Currently I am working with an EKF where the state is 2D pose (x,y,angle) and the measurements are also 2D poses.

In the attached image(open it in new page/tab to see in full resolution), the (scaled) vector K(1,1:2) (MATLAB syntax to take a submatrix from 3x3 matrix) should give an idea how the first component of the EKF state will change with the current innovation vector, K(2,1:2) how the second component of EKF will change, etc. In this example, the innovation vector has a relatively large x component and it is aligned with vector K(2,1:2) - the second component of the state (y coordinate) will change the most.
One problem in this plot is that it does not give a feel of how the third component(angle) of the innovation vector affects the state. The first component of the state increases a bit, contrary to what K(1:1:2) indicates - the third component of the innovation causes this, but currently I can not visualize this.
First improvement would be to visualize how the third component of the innovation affects the state. Then it would be nice to add covariance data to get a feel how the K matrix is created.
UPDATE #2 Now the plot has vectors in state-space that show how each component of measurement changes the position. From this plot, I can see that the third component of the measurement changes the state most.

","ekf, visualization"
Regarding Kalman filter and estimating heading with magnetic compass,"I have trouble estimating the heading when close to the ""pivot"" point of the compass, and could use some input on how to solve it. I have set up my angles to be from 0-360 degrees during the testing but will be using radians (-pi, pi) from now on.
The setup is a differential robot with IMU, wheel encoders and a magnetic compass. 
A complementary filter is used for fusing gyroZ and odo measurements to estimate the heading, and then correct it with a Kalman filter using the magnetic compass.
My problem occurs when the robot heading is close to -pi/pi .
The estimated heading is useless even though the robot is not even moving.
I am thinking this must be a very common problem and probably has a better solution than what I came up with, which was either re-initializing the integrator when crossing zero, adding 180 degrees each time the error is larger, or just ignoring the compass if the error is too large...
It's my first Kalman filter so I may have made a poor implementation if this is not a common issue...
Edit: trudesagen's solution solved my problem.
","mobile-robot, kalman-filter, compass"
I'm looking for hands-on experience with different types of leg and hip designs for walking robots,"I'm looking to find out, How do human-like legs compare to chicken legs and four-leg systems. In terms of cost, performance, speed, strength and accuracy.
I'm interested in things like speed, agility, turning radius, complexity and cost.
For a design large enough for a person to ride, rider fatigue is also important -- how do they compare in terms of achievable ride smoothness, vibration, and so on?
Are there quantitative benefits of 3 DOF hip joints, compared to 2 DOF?
I realize other factors will come into play as well, such as actuators, joint designs and control systems.
However, my interest at the moment is how basic leg designs compare to one another.
Edit: I'm looking for someone who has used these mechanisms first hand.
","kinematics, walking-robot"
VFH (Vector Field Histogram+): Obtaining the Primary Polar Histogram,"Good day
Note: I have found out that my code works. I have placed a minor explanation to be further expounded.
I have been having trouble obtaining the right directional output from my implementation. I noticed that every time I put an obstacle at the right, it gives left, it gives the right steering direction, the problem is with the presence of a left obstacle where it still tends to steer itself towards that obstacle. I have checked the occupancy map generated using matlab and was found to be correct. I couldn't pinpoint what is exactly wrong with my code for I have been debugging this for almost a week now and was hoping if someone can see the error I cannot.
Here is my code implementation:
//1st:: Create Occupancy Grid from Data-------------------------------------------------
// > Cell Size/Grid Resolution  = 5meters/33 cells = 0.15meters each = 15cm each
// > Grid Dimension = 5meters by 5meters / 33x33 cells //Field of view of robot is 54 degrees
//or 31 meters horizontal if subject is 5 meters away
// > Robot Width = 1meter 100cm
// > Because the focal length of the lens is roughly the same as the width of the sensor,
// it is easy to remember the field of view: at x meters away, you can see about x meters horizontally,
// assuming 4x3 stills mode. Horizontal field of view in 1080p video mode is 75%
// of that (75% H x 55% V sensor crop for 1:1 pixels at 1920x1080).

//Converting the Position into an Angle--------------------------------------------
//from:
//  A. https://decibel.ni.com/content/docs/DOC-17771
//  B. ""USING THE SENSOR KINECT FOR LANDMARK"" by ANDRES FELIPE ECHEVERRI GUEVARA
//1. Get angle
// > Each pixel from the image represents an angle
// > angle = ith pixel in row * (field of view in degrees/number of pixels in row)
// > field of view of Pi camera is 54 degrees horizontal
//2. Convert Polar to Cartesian
// > x = z*cos(angle)
// > y = z*sin(angle)

int arrOccupancyGrid[33][33];
float matDepthZ[33][33];
int robotPosX = 0;
int robotPosY = 0;
int xCoor=0; //Coordinates of Occupancy Map
int yCoor=0;
int xPosRobot=0; //Present cordinates of robot
int yPosRobot=0;
float fov = 54; // 54 degrees field of view in degrees must be converted to radians
float nop = 320; //number of pixels in row
int mapDimension = 33; // 33by33 array or 33*15cm = 5mby5m grid
int mapResolution = 15; //cm

//Limit max distance measured
/*
for(i=0; i< nop ;i++){
    if(arrDepthZ.at(i)>500){
        arrDepthZ.at(i) = 500;
    }
}
*/

for (i=0 ; i < nop; i++){
    //Process data/Get coordinates for mapping
        //Get Angle
        int angle = ((float)(i-160.0f) * ((float)fov/(float)nop)); //if robot is centered at zero add -160 to i
        //cout << ""arrDepthZ "" << arrDepthZ.at(i) << endl;
            //cout << ""i "" << i << endl;
            //cout << ""fov "" << fov << endl;
            //cout << ""nop "" << nop << endl;
            //cout << ""angle "" << i * (fov/nop) << endl;
        arrAngle.push_back(angle);
        //Get position X and Y use floor() to output nearest integer
        //Get X --------
        xCoor = (arrDepthZ.at(i) / mapResolution) * cos(angle*PI/180.0f); //angle must be in radians because cpp
        //cout << ""xCoor "" << xCoor << endl;
        arrCoorX.push_back(xCoor);
        //Get Y --------
        yCoor = (arrDepthZ.at(i) / mapResolution) * sin(angle*PI/180.0f); //angle must be in radians because cpp
                //cout << ""yCoor "" << yCoor << endl;
        arrCoorY.push_back(yCoor);
    //Populate Occupancy Map / Cartesian Histogram Grid

        if((xCoor <= 33) && (yCoor <= 33)){ //Condition Check if obtained X and Y coordinates are within the dimesion of the grid
            arrOccupancyGrid[xCoor][yCoor] = 1; //[increment] equate obstacle certainty value of cell by 1
            matDepthZ[xCoor][yCoor] = arrDepthZ.at(i);
        }

        //cout << ""arrCoorX.size()"" << arrCoorX.size() << endl;
        //cout << ""arrCoorY.size()"" << arrCoorY.size() << endl;

}

for (i=0 ; i < arrCoorX.size(); i++){
  file43 << arrCoorX.at(i) << endl;
}

for (i=0 ; i < arrCoorY.size(); i++){
  file44 << arrCoorY.at(i) << endl;
}

for (i=0 ; i < arrDepthZ.size(); i++){
  file45 << arrDepthZ.at(i) << endl;
}

//------------------------- End Create Occupancy Grid -------------------------


//2nd:: Create 1st/Primary Polar Histogram ------------------------------------------------------
//1. Define angular resolution alpha
// > n = 360degrees/alpha;
// > set alpha to 5 degrees resulting in 72 sectors from 360/5 = 72 ///// change 180/5 = 35
//2. Define number of sectors (k is the sector index for sector array eg kth sector)
// > k=INT(beta/alpha), where beta is the direction from the active cell
//to the Vehicle Center Point (VCP(xPosRobot, yPosRobot)). Note INT asserts k to be an integer

cout << ""2nd:: Create 1st/Primary Polar Histogram"" << endl;

//Put this at the start of the code away from the while loop ----------------
int j=0;
int sectorResolution = 5; //degrees 72 sectors, alpha
int sectorTotal = 36; // 360/5 = 72 //// change 180/5 = 36
int k=0; //sector index (kth)
int maxDistance = 500; //max distance limit in cm
//vector<int>arrAlpha; //already initiated

float matMagnitude[33][33]; //m(i,j)
float matDirection[33][33]; //beta(i,j)
float matAngleEnlarge[33][33]; //gamma(i,j)
int matHconst[33][33]; //h(i,j) either = 1 or 0

float robotRadius = 100; //cm
float robotSafeDist = 50; //cm
float robotSize4Sector = robotRadius + robotSafeDist;

for (i=0; i<sectorTotal; i++){
    arrAlpha.push_back(i*sectorResolution);
}
//---------end initiating sectors----------

//Determine magnitude (m or matMagnitude) and direction (beta or matDirection) of each obstacle vector
//Modify m(i,j) = c(i,j)*(a-bd(i,j)) to m(i,j) = c(i,j)*(dmax-d(i,j)) from sir Lounell Gueta's work (RAL MS)
//Compute beta as is, beta(i,j) = arctan((yi-yo)/(xi-xo))
//Enlarge robot and compute the enlargment angle (gamma or matAngleEnlarge)
int wew =0;
int firstfillPrimaryH = 0; //flag for arrayPrimaryH storage
for (k=0; k<sectorTotal; k++){
    for (i=0; i<mapDimension; i++){
        for (j=0; j<mapDimension; j++){
            //cout << ""i"" << i << ""j"" << j << ""k"" << k << endl;
            //cout << ""mapDimension"" << mapDimension << endl;
            //cout << ""sectorTotal"" << sectorTotal << endl;
            //Compute magnitude m, direction beta, and enlargment angle gamma
            matMagnitude[i][j] = (arrOccupancyGrid[i][j])*( maxDistance-matDepthZ[i][j]); //m(i,j)
            //cout << ""matMagnitude[i][j]"" <<  (arrOccupancyGrid[i][j])*( maxDistance-matDepthZ[i][j]) << endl;
            matDirection[i][j] = ((float)atan2f( (float)(i-yPosRobot), (float)(j-xPosRobot))*180.0f/PI); //beta(i,j)
            //cout << ""matDirection[i][j]"" << ((float)atan2f( (float)(i-yPosRobot), (float)(j-xPosRobot))*180.000/PI) << endl;
            //cout << ""matDepthZ[i][j]"" << matDepthZ[i][j] << endl;
            if(matDepthZ[i][j] == 0){ //if matDepthZ[i][j] == 0; obstable is very far thus path is free, no enlargement angle
                matAngleEnlarge[i][j] = 0; //gamma(i,j)
                //cout << ""matAngleEnlarge[i][j]"" << 0 << endl;
            }
            else{ //if matDepthZ[i][j] > 0 there is an obstacle so compute enlargement angle
                matAngleEnlarge[i][j] = asin( robotSize4Sector / matDepthZ[i][j])*180/PI; //gamma(i,j)
                //cout << ""matAngleEnlarge[i][j]"" << asin( robotSize4Sector / matDepthZ[i][j])*180.0f/PI << endl;
            }

            wew = k*sectorResolution; //k*alpha
            //cout << ""wew"" << k*sectorResolution << endl;
            //Check if magnitude is a part of the sector
            if ( ((matDirection[i][j]-matAngleEnlarge[i][j]) <= wew) && (wew <= (matDirection[i][j]+matAngleEnlarge[i][j])) ){
                matHconst[i][j]=1; //Part of the sector
                //cout << ""Part of the sector ---------------------------------------------------------------"" << endl;
                //cout << ""matHconst[i][j]=1"" << matHconst[i][j] << endl;
            }
            else{
                matHconst[i][j]=0; //Not Part of the sector
                //cout << ""Not Part of the sector"" << endl;
                //cout << ""matHconst[i][j]=0"" << matHconst[i][j] << endl;
            }
            //Compute primary polar histogram Hp(k)
            //cout << ""firstfillPrimaryH"" << firstfillPrimaryH << endl;
            if (firstfillPrimaryH==0){ //If first fill at sector
                //cout << ""matMagnitude[i][j]"" << matMagnitude[i][j] << endl;
                //cout << ""matHconst[i][j]"" << matHconst[i][j] << endl;
                float temp = matMagnitude[i][j]*matHconst[i][j];
                //cout << ""matMagnitude[i][j]*matHconst[i][j]"" << temp << endl;
                arrPrimaryH.push_back(temp);
                firstfillPrimaryH=1; //Trigger flag
                //cout << ""arrPrimaryH kth"" << arrPrimaryH.at(k) << endl;
            }
            else{ //If sector filled previously
                arrPrimaryH.at(k) = arrPrimaryH.at(k)+(matMagnitude[i][j]*matHconst[i][j]);
                //cout << ""arrPrimaryH kth"" << arrPrimaryH.at(k) << endl;
            }

        }
    }
    firstfillPrimaryH=0; //Reset flag
}

","mobile-robot, motion-planning, vfh, path-planning"
Need help in implementing EKF based SLAM,"I just started learning about slam and I have been trying to simulate a robot moving around a set of landmarks for the past 3 days. The landmarks have known correspondences. 
My problem is, if I add motion noise to the covariance matrix in the prediction step, the robot starts to behave very weirdly. If I don't add motion noise in the prediction step, the robot will move around perfectly. I have been trying to figure out why this is happening for 3 days now but cannot find anything wrong with my code.
I have attached a link to github which has all the files pertaining to my project. In the folder named 'octave' the file 'prediction_step' and 'correction_step' contains code for the prediction and correction steps respectively. The ekf_slam file is the main loop which calls the above two functions.
My github repository also contains 3 videos which correspond to robot with no motion noise, robot with motion noise and another video which shows how the robot should ideally go about.
Please help me in figuring out what is wrong with my code in 'prediction_step' and 'correction_step'.
Link to my github repository: please click here 
","slam, ekf"
Why do series elastic actuators have more accurate and stable force control?,"The other day, somebody was telling me about a robot in their lab, and they mentioned that it has ""series elastic"" actuators. But after doing a bit of Googling, I'm still not sure as to what this means, and have been unable to find a simple explanation. It seems that it is something to do with the link between the actuator and the load having a spring-like quality to it, but this is rather vague...
In any case, the what I am really interested in is the advantages and disadvantages of series elastic actuators. Specifically, I have read that one of the advantages is that it allows for ""more accurate and stable force control"". However, this appears counter-intuitive to me. I would have thought that if the link between the actuator and the load was more ""springy"", then this would lower the ability to have accurate control over the force send to the load, because more of this force would be stored and dissipated in the spring, with less directly transferred to the load.
So: Why do series elastic actuators have ""more accurate and stable force control""?
","robotic-arm, actuator, dynamics, torque"
Suggestion for correct battery pack,"I am trying to run 2 12V Geared DC motors which has No-load current = 800 mA(Max), Load current = upto 9.5 A(Max). Runtime to be atleast 3-4 hours.
The motor takes about 10-12 V for operation.
I need a proper battery pack for these, but how can I determine the specs I should go for?
","motor, battery"
"Starting out, dissertation project using computer controlled drone","For my final year in Computer Science university I will be doing a dissertation that includes controlling a drone through computer and communication with an onboard camera for computer vision.
The first step is obtaining a drone that suits my needs, and I have no clue how to go about it. Basically what is needed is a drone that will be able to communicate with a computer both for its movement and to ""stream"" the video to the computer for analysis. 
So, would I go for a store bought drone, a rasperry pi or some other microcontroller based one etc. What do I need to take into consideration etc?
P.S. the project is going to be based indoors, so I don't need crazy range, or very powerf
","quadcopter, control, microcontroller, computer-vision"
"What's the difference between the term ""pose estimation"" and ""visual odometry""?","I'm reading a paper:

Choi C, Trevor A J B, Christensen H I. RGB-D edge detection and
  edge-based registration[C]//Intelligent Robots and Systems (IROS),
  2013 IEEE/RSJ International Conference on. IEEE, 2013: 1568-1575.

which refers: 

Visual features such as corners, keypoints, edges, and color are
  widely used in computer vision and robotic perception for applications
  such as object recognition and pose estimation, visual odometry, and SLAM

I previously assume pose estimation to be roughly equal to visual odometry, yet the text above seems to deny.
So what's their difference? I didn't find much info from google. IMHO, it seems pose estimation is estimating the pose of moving object with the camera static, while visual odometry is estimating the pose of camera in a static(mostly) scene, is that precise enough?
","slam, odometry, pose"
Solving Inverse Kinematics with Gradient Descent,"I am trying to implement my own inverse kinematics solver for a robot arm. My solution is a standard iterative one, where at each step, I compute the Jacobian and the pseudo-inverse Jacobian, then compute the Euclidean distance between the end effector and the target, and from these I then compute the next joint angles by following the gradient with respect to the end effector distance.
This achieves a reasonable, smooth path towards the solution. However, during my reading, I have learned that typically, there are in fact multiple solutions, particularly when there are many degrees of freedom. But the gradient descent solution I have implemented only reaches one solution.
So my questions are as follows:

How can I compute all the solutions? Can I write down the full forward kinematics equation, set it equal to the desired end effector position, and then solve the linear equations? Or is there a better way?
Is there anything of interest about the particular solution that is achieved by using my gradient descent method? For example, is it guaranteed to be the solution that can be reached the fastest by the robot?
Are there cases when the gradient descent method will fail? For example, is it possible that it could fall into a local minimum? Or is the function convex, and hence has a single global minimum?

","robotic-arm, kinematics, inverse-kinematics, jacobian"
Do you have to stop first when switching direction for proper encoding readings?,"Since the encoder is square wave not quadrature, do you have to stop first before changing directions for proper measurements?
In other words, if you are commanding along in one direction at some low speed like 50mm/s or less and want to change direction to -50mm/s, would you first need command it to zero and wait for the encoder to read 0 speed, and then command the reverse direction, in order to get as accurate as possible encoder readings?
","irobot-create, roomba"
Particle filters: How to do resampling?,"I understand the basic principle of a particle filter and tried to implement one. However, I got hung up on the resampling part. 
Theoretically speaking, it is quite simple: From the old (and weighted) set of particles, draw a new set of particles with replacement. While doing so, favor those particles that have high weights. Particles with high weights get drawn more often and particles with low weights less often. Perhaps only once or not at all. After resampling, all weights get assigned the same weight.
My first idea on how to implement this was essentially this:

Normalize the weights
Multiply each weight by the total number of particles
Round those scaled weights to the nearest integer (e.g. with int() in Python)

Now I should know how often to draw each particle, but due to the roundoff errors, I end up having less particles than before the resampling step. 
The Question: How do I ""fill up"" the missing particles in order to get to the same number of particles as before the resampling step? Or, in case I am completely off track here, how do I resample correctly?
","localization, particle-filter"
Change Message Interval ArduPilot,"I am using Mavlink protocol (in c++) to communicate with the ArduPilotMega, I am able to read messages such as ATTITUDE for example.
I am currently getting only 2Hz (message rate) and I would like to increase it. I found out that I should use MESSAGE_INTERVAL in order to change it, and that I probably need to use the command MAV_CMD_SET_MESSAGE_INTERVAL to set it.
So my question is, how do I send that command using mavlink in c++?
I tried doing this with the code below but it did not work, I guess that I should use the command that I mentioned above but I don't know how.
mavlink_message_t command;
mavlink_message_interval_t interval;

interval.interval_us = 100000;
interval.message_id = 30;

mavlink_msg_message_interval_encode(255, 200, &command, &interval);
p_sensorsPort->write_message(command);

Update: I also tried this code below, maybe I am not giving it the right system id or component id.
mavlink_message_t command;
mavlink_command_long_t interval;

interval.param1 = MAVLINK_MSG_ID_ATTITUDE;
interval.param2 = 100000;
interval.command = MAV_CMD_SET_MESSAGE_INTERVAL;
interval.target_system = 0;
interval.target_component = 0;

mavlink_msg_command_long_encode(255, 0, &command, &interval);
p_sensorsPort->write_message(command);

Maybe I am missing something about the difference between target_system, target_component and sysid, compid. I tried few values for each but nothing worked. Is there any ack that will be able to tell me if it even got the command? 
","quadcopter, c++, ardupilot, mavlink"
How to split tasks between interrupts and the main loop on a bare metal controller?,"I'm working on a robotics project where I have 3 services running. I have my sensor DAQ, my logic ISR (motor controller at 20kHz) and my EtherCAT slave controller.
DAQ and EtherCAT run in the idle and the logic runs during an interrupt. The logic does some calculations and controls the motor. The EtherCAT service (kinda like CANbus) runs together with my DAQ in the idle loop. I can not run the DAQ in the interrupt because that leaves me with less than 100ns for the EtherCAT service to run.
I'm not sure whether this is the right way to do this especially considering all the scary things i've read regarding data corruption when using interrupts.
Does anyone have some nice ideas on how to handle these services?
I'm running all my code on a Zynq 7020 (on the ARM Cortex) and it's written in C++.
Here is an example of my code:
/**
 * Get all sensor data
 */
void Supervisor::communication(void) {
    // Get all the sensors data
    dispatchComm.getData(&motorAngle, &motorVelocity, &jointAngle, &springAngle, &tuningParameter);

}

/**
 * Run all the logic
 */
void Supervisor::logic(void) {

    dispatchLogic.calculate(tuningParameter, motorAngle, motorVelocity, jointAngle, springAngle);

    dispatchLogic.getData(&angle, &magnitude);

    // Dispatch values to the motor drive
    dispatchComm.setMotorDriveSetpoint(angle, magnitude);
    dispatchComm.writeToPiggyback((uint32_t) (tuningParameter), motorAngle, motorVelocity);
}

","c++, interrupts"
Calculate the uncertainty of a 6-dof pose for graph-based SLAM,"This question is strongly related to my other question over here.
I am estimating 6-DOF poses $x_{i}$ of a trajectory using a graph-based SLAM approach. The estimation is based on 6-DOF transformation measurements $z_{ij}$ with uncertainty $\Sigma_{ij}$ which connect the poses. 
To avoid singularities I represent both poses and transforms with a 7x1 vector consisting of a 3D-vector and a unit-quaternion:
$$x_{i} = \left( \begin{matrix} t \\ q \end{matrix} \right)$$
The optimization yields 6x1 manifold increment vectors 
$$ \Delta \tilde{x}_i = \left( \begin{matrix} t \\ log(q) \end{matrix} \right)$$
which are applied to the pose estimates after each optimization iteration:
$$ x_i \leftarrow x_i \boxplus \Delta \tilde{x}_i$$
The uncertainty gets involved during the hessian update in the optimization step:
$$ \tilde{H}_{[ii]} += \tilde{A}_{ij}^T \Sigma_{ij}^{-1} \tilde{A}_{ij} $$
where 
$$ \tilde{A}_{ij} \leftarrow A_{ij} M_{i} = \frac{\partial e_{ij}(x)}{\partial x_i} \frac{\partial x_i \boxplus \Delta \tilde{x}_i}{\partial \Delta x_i} |_{\Delta \tilde{x}_i = 0}$$
and
$$ e_{ij} = log \left( (x_{j} \ominus x_{i}) \ominus z_{ij} \right) $$
is the error function between a measurement $z_{ij}$ and its estimate $\hat{z}_{ij} = x_j \ominus x_i$. Since $\tilde{A}_{ij}$ is a 6x6 matrix and we're optimizing for 6-DOF $\Sigma_{ij}$ is also a 6x6 matrix.

Based on IMU measurements of acceleration $a$ and rotational velocity $\omega$ one can build up a 6x6 sensor noise matrix
$$ \Sigma_{sensor} = \left( \begin{matrix} \sigma_{a}^2 & 0 \\ 0 & \sigma_{\omega}^2 \end{matrix} \right) $$
Further we have a process model which integrates acceleration twice and rotational velocity once to obtain a pose measurement.
To properly model the uncertainty both sensor noise and integration noise have to be considered (anything else?). Thus, I want to calculate the uncertainty as
$$ \Sigma_{ij}^{t} = J_{iterate} \Sigma_{ij}^{t-1} J_{iterate}^T + J_{process} \Sigma_{sensor} J_{process}^T$$
where $J_{iterate} = \frac{\partial x_{i}^{t}}{\partial x_{i}^{t-1}}$ and $J_{process} = \frac{\partial x_{i}^{t}}{\partial \xi_{i}^{t}}$ and current measurement $\xi{i}^{t} = [a,\omega]$.
According to this formula $\Sigma_{ij}$ is a 7x7 matrix, but I need a 6x6 matrix instead. I think I have to include a manifold projection somewhere, but how?

For further details take a look at the following publication, especially at their algorithm 2:
G. Grisetti, R. Kümmerle, C. Stachniss, and W. Burgard, “A tutorial on graph-based SLAM,” IEEE Intelligent Transportation Systems Maga- zine, vol. 2, no. 4, pp. 31–43, 2010.

For a similar calculation of the uncertainty take a look at the end of section III A. in:
Corominas Murtra, Andreu, and Josep M. Mirats Tur. ""IMU and cable encoder data fusion for in-pipe mobile robot localization."" Technologies for Practical Robot Applications (TePRA), 2013 IEEE International Conference on. IEEE, 2013.

.. or section III A. and IV A. in:
Ila, Viorela, Josep M. Porta, and Juan Andrade-Cetto. ""Information-based compact Pose SLAM."" Robotics, IEEE Transactions on 26.1 (2010): 78-93.
","slam, ekf, jacobian"
Estimating the displacement of a drone in three dimensions,"Assuming a drone is in two dimension, it has to predict its future position by calculating its future displacement:

For a real quad-rotor, why should we not only estimate the displacement of a robot in three dimensions but also the change of orientation of the robot, its linear velocity and its angular velocity?
","quadcopter, motion-planning, uav"
"Using Accelerometer, Gyroscope and any sensor to track speed, position,","Problem
Currently working on reverse engineering this application zepp.com/baseball. This is a wearable device that can track a users 

speed
positional tracking
when the object makes contact with another one 
3-D Rendering

Currently using an accelerometer and gyroscope to get the yaw, pitch, and roll(orientation) of the device, but do not know how to use that information to calculate speed, or if the device has collided with another object?
","imu, accelerometer, precise-positioning"
Is there an alternative to manifolds when using quaternions for orientation representation in Pose Graph SLAM?,"I want to implement my own pose graph SLAM following [1]. Since my vehicle is moving in 3D-space i represent my pose using a 3D-translation vector and a quaternion for orientation. [1] tells me that it's necessary to adapt their algorithm 1 by using manifolds to project the poses into euclidean space.
I also studied the approach of [2]. In section ""IV.B. Nonlinear Systems"" they write that their approach remains valid for nonlinear systems. I conclude that for their case it's not obligatory to make use of a manifold. But I don't understand how they avoid it. So my questions are:

Is it correct that there is an alternative to manifolds?
If yes, how does this alternative look like?


[1] Grisetti, G., Kummerle, R., Stachniss, C., & Burgard, W. (2010). A tutorial on graph-based SLAM. Intelligent Transportation Systems Magazine, IEEE, 2(4), 31-43.
[2] Kaess, M., Ranganathan, A., & Dellaert, F. (2008). iSAM: Incremental smoothing and mapping. Robotics, IEEE Transactions on, 24(6), 1365-1378.
","slam, pose"
Angular momentum of rimless wheel in Passive Dynamic Walking,"In Tad McGeer's work, Passive Dynamic Walking, in 1990, he mentions the rimless wheel model, which is used to approximate the bipedal locomotion. I can't understand why the angular momentum is written as follows.
$H^-=(\cos 2\alpha_0+r^2_{gyr})ml^2\Omega^-$
I have the following questions:

Isn't the angular momentum be $I*\omega$, $m^2l\Omega$ as the paper's notation?
If $\alpha_0$ is $\frac{\pi}{2}$ and $r_{gyr}$ approaches to 0, shouldn't the angular momentum before impact, $H^-$, be negative? Then how the conservation goes?

","wheel, walking-robot"
Issues with Running Multiple Instructions in Sequence,"I tried to use Microsoft Robotics Dev Studio (sample 4) to write a code that was able for robot to go with a square path by just one clicked. However, there is one problem.
When I try to put DriveDistanceRequest and RotateDegreesRequest in a loop. It would only execute the last request. The problem is that Arbiter.Choice within the DriveDistance is activated immediately as soon as the drive operation starts. Did anyone have this kind of problem before? If so, how do I solve it? If no, how am I able to fix this problem? Thanks your so much.
//-----------------------------------------------------------------------
//  This file is part of Microsoft Robotics Developer Studio Code Samples.
//
//  Copyright (C) Microsoft Corporation.  All rights reserved.
//
//  $File: RoboticsTutorial4.cs $ $Revision: 22 $
//-----------------------------------------------------------------------
using Microsoft.Ccr.Core;
using Microsoft.Ccr.Adapters.WinForms;
using Microsoft.Dss.Core;
using Microsoft.Dss.Core.Attributes;
using Microsoft.Dss.ServiceModel.Dssp;
using Microsoft.Dss.ServiceModel.DsspServiceBase;
using System;
using System.Collections.Generic;
using System.Security.Permissions;
using xml = System.Xml;
using drive = Microsoft.Robotics.Services.Drive.Proxy;
using W3C.Soap;
using Microsoft.Robotics.Services.RoboticsTutorial4.Properties;
using Microsoft.Robotics.Services.Drive.Proxy;
using System.ComponentModel;
namespace Microsoft.Robotics.Services.RoboticsTutorial4
{
[DisplayName(""(User) Robotics Tutorial 4 (C#): Drive-By-Wire"")]
[Description(""This tutorial demonstrates how to create a service that partners with abstract, base definitions of hardware services."")]
[DssServiceDescription(""http://msdn.microsoft.com/library/bb483053.aspx"")]
[Contract(Contract.Identifier)]
public class RoboticsTutorial4 : DsspServiceBase
{
    [ServiceState]
    private RoboticsTutorial4State _state = new RoboticsTutorial4State();

    [ServicePort(""/RoboticsTutorial4"", AllowMultipleInstances=false)]
    private RoboticsTutorial4Operations _mainPort = new RoboticsTutorial4Operations();

    [Partner(""Drive"", Contract = drive.Contract.Identifier, CreationPolicy = PartnerCreationPolicy.UseExisting)]
    private drive.DriveOperations _drivePort = new drive.DriveOperations();
    private drive.DriveOperations _driveNotify = new drive.DriveOperations();

    public RoboticsTutorial4(DsspServiceCreationPort creationPort) :
            base(creationPort)
    {
    }

    #region CODECLIP 02-1
    protected override void Start()
    {
        base.Start();

        WinFormsServicePort.Post(new RunForm(StartForm));

        #region CODECLIP 01-5
        _drivePort.Subscribe(_driveNotify);
        Activate(Arbiter.Receive<drive.Update>(true, _driveNotify, NotifyDriveUpdate));
        #endregion
    }
    #endregion

    #region CODECLIP 02-2
    private System.Windows.Forms.Form StartForm()
    {
        RoboticsTutorial4Form form = new RoboticsTutorial4Form(_mainPort);

        Invoke(delegate()
            {
                PartnerType partner = FindPartner(""Drive"");
                Uri uri = new Uri(partner.Service);
                form.Text = string.Format(
                    Resources.Culture,
                    Resources.Title,
                    uri.AbsolutePath
                );
            }
        );

        return form;
    }
    #endregion

    #region CODECLIP 02-3
    private void Invoke(System.Windows.Forms.MethodInvoker mi)
    {
        WinFormsServicePort.Post(new FormInvoke(mi));
    }
    #endregion


    /// <summary>
    /// Replace Handler
    /// </summary>
    [ServiceHandler(ServiceHandlerBehavior.Exclusive)]
    public virtual IEnumerator<ITask> ReplaceHandler(Replace replace)
    {
        _state = replace.Body;
        replace.ResponsePort.Post(DefaultReplaceResponseType.Instance);
        yield break;
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    //stop
    public virtual IEnumerator<ITask> StopHandler(Stop stop)
    {
        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0;
        request.RightWheelPower = 0;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to stop"", fault);
            }
        );
    }

    //forward
    #region CODECLIP 01-3 
    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    //forward
    public virtual IEnumerator<ITask> ForwardHandler(Forward forward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }
        // movement speed
        // This sample sets the power to 75%.
        // Depending on your robotic hardware,
        // you may wish to change these values.
        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0.5;
        request.RightWheelPower = 0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to drive forwards"", fault);
            }
        );
    }
    #endregion

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // backup speed
    public virtual IEnumerator<ITask> BackwardHandler(Backward backward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();          
        request.LeftWheelPower = -0.6;
        request.RightWheelPower = -0.6;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to drive backwards"", fault);
            }
        );
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // left turn speed
    public virtual IEnumerator<ITask> TurnLeftHandler(TurnLeft turnLeft)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = -0.5;
        request.RightWheelPower = 0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to turn left"", fault);
            }
        );
    }

    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    // right turn speed
    public virtual IEnumerator<ITask> TurnRightHandler(TurnRight forward)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        drive.SetDrivePowerRequest request = new drive.SetDrivePowerRequest();
        request.LeftWheelPower = 0.5;
        request.RightWheelPower = -0.5;

        yield return Arbiter.Choice(
            _drivePort.SetDrivePower(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to turn right"", fault);
            }
        );
    }

    #region CODECLIP 01-4
    private Choice EnableMotor()
    {
        drive.EnableDriveRequest request = new drive.EnableDriveRequest();
        request.Enable = true;

        return Arbiter.Choice(
            _drivePort.EnableDrive(request),
            delegate(DefaultUpdateResponseType response) { },
            delegate(Fault fault)
            {
                LogError(null, ""Unable to enable motor"", fault);
            }
        );
    }
    #endregion

    #region CODECLIP 01-6
    private void NotifyDriveUpdate(drive.Update update)
    {
        RoboticsTutorial4State state = new RoboticsTutorial4State();
        state.MotorEnabled = update.Body.IsEnabled;

        _mainPort.Post(new Replace(state));
    }
    #endregion


    // Here is where I had change the code.
    #region Test Code (Creating Path)
    [ServiceHandler(ServiceHandlerBehavior.Concurrent)]
    public virtual IEnumerator<ITask> PathHandler(StartPath path)
    {
        if (!_state.MotorEnabled)
        {
            yield return EnableMotor();
        }

        for(int i=1; i<3; i++)
        {
                if(i == 1)
                {
                    drive.DriveDistanceRequest distance = new drive.DriveDistanceRequest();
                    distance.Power = 1;
                    distance.Distance = 1;

                    yield return Arbiter.Choice(
                        _drivePort.DriveDistance(distance),
                        delegate(DefaultUpdateResponseType response) { },
                        delegate(Fault fault)
                        {
                            LogError(null, ""Unable to turn left"", fault);
                        }
                    );

                }
                else if(i == 2)
                {
                    drive.RotateDegreesRequest rotate = new drive.RotateDegreesRequest();
                    rotate.Power = 1;
                    rotate.Degrees = 90;

                    yield return Arbiter.Choice(
                        _drivePort.RotateDegrees(rotate),
                        delegate(DefaultUpdateResponseType response) { },
                        delegate(Fault fault)
                        {
                            LogError(null, ""Unable to turn left"", fault);
                        }
                    );
                }

        }

    }
    #endregion
}

}
","mobile-robot, control, irobot-create, mrds"
Computing the Jacobian Matrix -- chain rule?,"I am learning about robot kinematics and the Jacobian matrix, and I'm trying to understand how to compute the Jacobian matrix given a kinematic chain, such as a robot arm. I understand the theory behind the Jacobian matrix, but I'm not sure actually how it would be calculated in practice.
So, let's say that I have a 7 DoF robot arm, with 7 joints and 6 links between the joints. I know how to compute the transformation matrix between each joint, and by applying forward kinematics, I know the pose of the end effector for any configuration of joint angles. To calculate this, I have written some code which stores each transformation matrix, and then multiplies them in series to create the transformation matrix between the first joint and the end effector.
However, how do I now go about computing the Jacobian matrix? My solution so far, is to write down each transformation matrix by hand, then multiply them all by hand, to yield the overall transformation matrix, with respect to the joint angles. I could then differentiate this to create the Jacobian matrix. The problem with this though, is that the maths becomes very, very complicated as I move along the kinematic chain. By the end, there are so many terms as a result of the multiple matrix multiplications, that it just becomes so tedious doing this by hand.
Is there a better way to do this? In the case of calculating the forward kinematics, I didn't have to do it by hand, I just wrote some code to multiply the individual matrices. But when I want the Jacobian matrix, it seems like I need to compute the derivative of the overall transformation matrix after it has been computed, and so I need to do this by hand. What's the standard solution to this? Is it something to do with the chain rule for differentiation...? I'm not sure exactly how this applies here though...
Thank you!
","robotic-arm, kinematics, inverse-kinematics, jacobian, manipulator"
convert toolframe coordinate to world frame coordinates?,"I am not sure how i should explain this, I am looking for a way to plot the trajectory an robot arm.  An object is seen from the toolFrame frame, but how do I plot the position of each joint, such that the frame they uses are the same.
One way would be to use the world frame as reference, but how would i plot the position of the object related to the world frame?
","kinematics, matlab, visualization"
"How to make an ""invisible line following robot""?","I would like to build a robot which follows a virtual path (Not a visible path like a 'black line on a white surface', etc).
I'm just enthusiastic by seeing some sci-fi videos which show robots carry goods and materials in a crowded place. And they really don't follow a physical line. They sense obstacles, depth, etc.  
I would like to build one such robot which follows a specific (virtual)  path from point A to B.
I have tried a couple of things:

Using a magnetic ""Hall effect"" sensor on the robot and wire carrying current (beneath the table). The problem here was that the vicinity of the hall effect sensor is so small (< 2cms) that it is very difficult to judge whether robot is on the line or off the line. Even using series of magnets couldn't solve this issue, as my table is 1 inch thick. So this idea flopped :P
Using an ultraviolet paint (on a line) and using UV leds on the robot as sensors. This will give more Zig-Zag motion for the robot. And due to potential threats of using UV light source, even this idea flopped :P

I finally thought of having a camera on top and using image processing algorithms to see whether robot is on the line or diverging.
Is there any better solution than this? Really looking for some creative and simple solutions. :)
","mobile-robot, localization, wheeled-robot, industrial-robot, line-following"
Type of servo and torque calculation required for a 2axis robot arm,"I am trying to build a 2-axis robot arm with pan and tilt mechanism. The gripper/holder will hold an object weighing 300 grams. The total weight of the arm including the motors will be around 2 kg. I have decided to use 180 degree servo motors. The maximum arm reach will be 340 mm. 
what I want to ask is:

What kind of servos (analog/digital) will be suitable to support the
total weight (2 kg) and the object weight (300 g)?
How do I calculate the required torque?
How many servos should I use to make sure that my arm doesn't flip
over?

Please suggest me if there is a better approach to designing the robot. I am fairly new to electronics and this is the first time I am building a robot.
Thanks in advance. 
","robotic-arm, design, servos"
How do I evaluate the minimum requirements of the processor and camera for a visual SLAM robot?,"I would like to build a visual SLAM robot (just for self-learning purpose) but I get frustrated how I know which processor and camera should be used for visual SLAM. 
First, for the processor, I have seen three articles, which shows different systems are used for implementing their SLAM algorithm:

Implementing SLAM algorithm (however it uses ultrasonic sensor rather than visual sensor) in Raspberry Pi (processing power is only 700 MHz) in Implementing Odometry and SLAM Algorithms on a Raspberry Pi to Drive a Rover 
I have also seen that Boston Dynamics use Pentium CPU, PC104 stack and QNX OS for their Big Dog project, BigDog Overview
November 22, 2008
Then, I also found a project uses a modern XILINX Zynq-7020 System-on-Chip (a device that combines FPGA resources with a dual ARM Cortex-A9 on
a single chip), for a Synchronized Visual-Inertial Sensor System, in A synchronized visual-inertial sensor system with FPGA pre-processing for accurate real-time SLAM

But after reading those, I have no clue how they end up with those decisions to use those kinds of processors, stacks or even OSes for their project. Is there a mathematical way, or a general practice, to evaluate the minimum requirement of the system (as cheap and as power efficient as possible) for an algorithm to run? 
If not, how could I know what processor or system I have to prepare for a visual SLAM robot? If there is no simple answer, it is also cool if you can recommend something I could read to have a good start.
Secondly, I also cannot find clear information which camera I should use for a visual SLAM robot. I also have no idea how they evaluate the minimum requirement of the camera. I found a lot of papers saying they use RGB-D camera but when I Google to find one, there are very few commercially available. The one I found is Xtion Pro Live from ASUS Global (for $170 which is quite affordable for me), but they are out of stock. Are there any practice I can choose a suitable camera system for visual SLAM too? 
Sorry if my question is too long. I feel that choosing the system and camera looks like a thing that requires a lot of experience and background knowledge. So rather than direct suggestions, it is cool if you have some ideas/recommended resources for me to learn the general ways people make such decisions in general or in similar projects, if any.
",slam
Choosing a battery: is a harbor freight solar battery OK for a R/C Lawnmower?,"I have built an R/C Lawnmower.  I call it the Honey Badger, because it tears stuff up (that's a good thing).  Well, I used used batteries to get the project going and now it's long past time to get the Honey Badger going again.
The Honey Badger is built on an electric wheelchair frame, and originally used wheelchair deepcycle batteries.  U1 if I recall.  There are 4 of them wired in 2 banks in series and parallel to give 24V for the 24V motors.
Going down to the used wheelchair parts place is about an hour drive and requires a weekend visit and will get me used batteries of unknown condition.  
Contrast that with Harbor Freight, which is 20 minutes away and has solar batteries the same physical dimensions and comparable (?) electrical characteristics.  I think with coupons, tax, and after playing the game, I can get a battery for ~$50, about the same price as a used U1.
I found that Amazon also has U1 batteries, and they can be had for ~\$120 for 2 with shipping.
Batteries plus will sell me some deepcycle auto batteries of greater Ah capacity for ~$100 each.
Gross for each solution winds up being around the same: ~\$240 - ~\$300.
Is there a difference in technology between a ""solar battery"" and a ""wheelchair battery""?  Is that difference substantial?  Given that I'm pretty rough with this thing, is any particular technology any better suited to these tasks?  Is there a benefit or drawback to using an automotive battery?
I have the charger from the original wheelchair and if I recall, it's good for the capacity and has room to spare.  I think it can put out 5 amps.

",battery
Configuration space obstacle - calculating collision,"I need to calculate the configuration space obstacle to planning a path with a mobile robot. The idea is to divide the obstacles and the robot in triangles and test whether is there any triangle from the robot colliding with any triangle from the obstacles.
The approach to solve this is to test this between two triangles each time so I need to look if any of the 6 edges (3 for each triangle) divide the triangles so 3 vertex from one of them lie in one side and the other 3 vertex lie on the other side of the line.
I wrote some code to calculate the line equation (y = m*x + b) and I think it is correct, but I am having problems when the line is vertical (this means that m = -Inf) because MATLAB gives me a NaN when I calculate the equation for it. I am not sure how to handle this.
Here you can see a snippet from the code where I test the 3 edges from the 
robot triangle:
for i = 1:1:3

    vertex_x = P1(edge(i,:),1);
    vertex_y = P1(edge(i,:),2);
    m = (vertex_y(2) - vertex_y(1))/(vertex_x(2) - vertex_x(1));
    b = -m*vertex_x(1) + vertex_y(1);

    for j = 1:1:6   % For each vertex...
        pto = listaVertices(j,:);
        if (m*pto(1) + b > pto(2))
            % Vertex lies below the edge...
            cont1 = cont1 + 1;
        elseif (m*pto(1) + b < pto(2))
            % Vertex lies above the edge...
            cont2 = cont2 + 1;
        else
            % Vertex lie inside the edge...
            % Do nothing
        end
    end 

    % 3 vertex on one side and 1 on the others side means they do not
    % collide. Two of the vertex always lie inside the line (the two vertex 
    % of each edge).

    if (cont1 == 1 && cont2 == 3) || (cont1 == 3 && cont2 == 1)
        flag_aux = false;   % Do not collide...
    end
    % Reset the counters for the 3 next edges...
    cont1 = 0;
    cont2 = 0;

end

Anyone could help with this issue?
","mobile-robot, motion-planning, matlab"
Brushless motor from RC car won't spin with even small resistance,"I recently bought a RC car kit and after 10 minutes it stopped going. 
When I throttle, I can see the motor trying to spin but it will just grind and get hot quite fast.
The motor does move if I disconnect it from the big gear, but not as fast as it did when new and it will still get very hot. Also, I can stop it with my fingers with a very slight touch.
I don't know anything about motors or ESCs, so I'm not sure if my problem is the motor or the ESC.  Did I burn it out?
","brushless-motor, esc, radio-control"
How to make a stepper motor to rotate and come to a position of certain degress (say for 90 degrees) from any initial position?,"I tried this coding and its working. 
void loop()
{
    int y = 104;
    int x2 = vertical2.currentPosition();
    int z2 = y-x2;
    int x1 = horizontal2.currentPosition();
    int z1 = y-x1;

    horizontal2.moveTo(z1);
    horizontal2.run();

    vertical2.moveTo(z2);
    vertical2.run();
}

But the problem is that if the above coding is placed inside a loop such as 'if loop', its not working. Can anyone help me in figuring out this problem. I am using accelStepper library for the above coding.
void loop()
{
    int dummy=1;
    if(dummy==1)
    {
        int y = 104;
        int x2 = vertical2.currentPosition();
        int z2 = y-x2;
        int x1 = horizontal2.currentPosition();
        int z1 = y-x1;

        horizontal2.moveTo(z1);
        horizontal2.run();

        vertical2.moveTo(z2);
        vertical2.run();
    }
}

","arduino, stepper-motor"
Maximum distance for robotic arm throwing,"I have a 6DOF robotic arm which I am using to throw a ball. Each joint can achieve a maximum velocity of 30 RPM (180 deg/s). I have been trying to generate joint angles manually and feeding them to see how far I can throw the ball until now. This has shown me that it's like less than 2 meters. 
But I feel that I may not be combining the motions of the various motors in order get better throwing distance. I wanted to know if there is a simple way of theoretically determining the maximum distance I can throw. I read a few papers that appear very complicated, I do not need a very accurate value, just an estimate so that I decide whether I should move to a different arm. 
",robotic-arm
Which mechanical device could repeatedly present an ID tag to a card-reader,"I'm trying to build a test-automation robotic arm which can repeatedly present an ID-tag (such as RFID or NFC card or fob) to a card reader.
I suspect our reader fails either (a) after hundreds of presentations or due to fast presentations or (b) at a specific moment in the reader duty cycle.
The tag needs to move in a well-controlled manner:

Quickly present the card, 
Pause (mark)
Quickly remove the card,
Pause (space)
Repeat at 1.

I'm calling the present/remove sequence the mark-space ratio for simplicity.
The tests I want to perform involve varying (a) the frequency and (b) the mark-space ratio, to (a) stress-test and  (b) boundary-test the re-presentation guard times built into the reader to debounce presentations.
The guard times are around 400ms, response around 100ms, so I need something that can move in and out of a 5-10cm range quickly and repeat within those sorts of timescales. 
The distance the card needs to move depends on the reader model, as they have different field ranges. I want to get through the edge of the field quickly to avoid any inconsistencies in testing.
I'm able to do any programming (professional) and simple electromechanical design and build (ex-professional, now hobbyist). I only need to build one, it doesn't have to be particularly robust, but it does need to be fairly accurate with regard to the timings to do the second test.
What I've done so far:
I've built one version already using a Raspberry Pi, GPIO, a stepper motor with an aluminium arm screwed to a wheel.  It works, but it's a bit jerky and too slow, even with a 30cm arm to amplify the motion. It will probably do for the repeat test, but it's not time-accurate enough to do the timing tests.
My other design ideas were: 

Servo (are these also slow?)
Solenoid (fast, but too limited range? and might cause EM?)
Motor (too uncontrollable, and will require too much mechanical work for me)
Rotating drum (fast, stable, but cannot control mark-space ratio)

I'm not a electro-mechanical design expert, so I'm wondering if I'm missing an electrical device or mechanical design which can do this more easily.
","robotic-arm, raspberry-pi, stepper-motor, industrial-robot, automation"
Cosine interpolation between two transformation matrices?,"Is it possible to perform cosine interpolation between two transformation matrices?
It make sense for the translation part, but how about the rotational part?
","robotic-arm, stereo-vision"
Holonomic and Non-holonomic UAV's: Gliders vs Quadcopters,"Good day I would just like to ask if a fixed wing aircraft such as a glider(without thrust capability therefore needs external forces such as air flow to move constraining its movement) can be considered a non-holonomic system considering the fact that it cannot move freely compared to a quadcopter that is holonomic.
I found this information from: What's the difference between a holonomic and a nonholonomic system?
Mathematically:
Holonomic system are systems for which all constraints are integrable into positional constraints.
Nonholonomic systems are systems which have constraints that are nonintegrable into positional constraints.
Intuitively:
Holonomic system where a robot can move in any direction in the configuration space.
Nonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint.
","quadcopter, control, uav"
Bayesian filter for 2-D grid localizaton,"I have some data obtained from an experiment in terms of movements and observations with odometry and sensor data. My task is to find the probability mass on each of the grid cells after each set of motion and observation. I'm a bit lost in figuring out how to compute probability mass for each of the grid cell.
My odometry information is in terms of rotation, translation and rotation and my sensor information is in terms of range and bearing angle. 
How do I calculate the probability of robot present in each of the grid cell?
I have the formula for belief after motion as summation(P(x|u, x')xBel(x'))
How do I compute the motion model with noise?
","localization, filter"
Path planning vs. linear interpolation?,"I at moment trying to convince myself that what I need is a simple path planning algorithm, instead of linearly interpolating between a current and a desired state. 
I am at moment working with an robot arm (UR) with an camera mounted on to its TCP.  The application i am trying to create is a simple ball tracking application which tracks the movement of the ball, while always keeping the object within sight. 
which meant that I needed some form of path planning algorithm which plans the path between my current state and the desired state. The path should should be such that the ball is always kept in focus while the arm moves to the desired state.
But then I began question myself whether it was a bit overkill, and whether a simple straight line interpolation wouldn't suffice?.. I am actual not sure what form of benefit i would have by choosing a pathplanner, than a simple interpolation..
Interpolation would also generate the path I desire, so why choose a pathPlanner at all?
would someone elaborate?
It should be noted that obstacle avoidance is also a part of the task, which could cause trouble for a straight line interpolating. 
",robotic-arm
"How do the PID parameters (Kp, Ki, and Kd) affect the heading of a differential driving robot when they are increased individually?","
Question: A PID controller has three parameters Kp, Ki and Kd which could affect the output performance. A differential driving robot is controlled by a PID controller. The heading information is sensed by a compass sensor. The moving forward speed is kept constant. The PID controller is able to control the heading information to follow a given direction. Explain the outcome on the differential driving robot performance when the three parameters are increased individually. 

This is a question that has come up in a past paper but most likely won't show up this year but it still worries me. It's the only question that has me thinking for quite some time. 
I'd love an answer in simple terms. Most stuff i've read on the internet don't make much sense to me as it goes heavy into the detail and off topic for my case.  
My take on this:
I know that the proportional term, Kp, is entirely based on the error and that, let's say, double the error would mean doubling Kp (applying proportional force). This therefore implies that increasing Kp is a result of the robot heading in the wrong direction so Kp is increased to ensure the robot goes on the right direction or at least tries to reduce the error as time passes so an increase in Kp would affect the robot in such a way to adjust the heading of the robot so it stays on the right path.
The derivative term, Kd, is based on the rate of change of the error so an increase in Kd implies that the rate of change of error has increased over time so double the error would result in double the force. An increase by double the change in the robot's heading would take place if the robot's heading is doubled in error from the previous feedback result. Kd causes the robot to react faster as the error increases. 
An increase in the integral term, Ki, means that the error is increased over time. The integral accounts for the sum of error over time. Even a small increase in the error would increase the integral so the robot would have to head in the right direction for an equal amount of time for the integral to balance to zero. 
I would appreciate a much better answer and it would be great to be confident for a similar upcoming question in the finals. 
","pid, wheeled-robot, differential-drive"
CC3D OpenPilot - Communication port,"I am building a quadrotor with a CC3D (OpenPilot) and a RPi. 
My first idea is trying to communicate the CC3D with the RPi using the main port of the first one but I can't find any information about how to communicate the board with any device (or other information about the commands or serial configuration).
Anybody knows whether this is possible and where to find that info??
","raspberry-pi, quadcopter"
What are the biggest challenges to build an highly performant robotic hand?,"When looking at the robotic hands made by researchers that are said to be rather close to a real human hand, they can easily cost tens of thousands of dollars.  
What makes them so much expensive? Sure there are lots of joints where parts must move, but it's still hard to figure out how it can cost so much even with highly precise servomotors.  
What is so much expensive when trying to build a humanoid hand? How can we make it less expensive? What do these expensive hands can do, that a diy cheap hand project can't?  
Thank you.
",humanoid
iRobot Create 2: Can I load the code in instead of connecting cable? (new learner),"I am a new learner of iRobot. I am trying to program it to control the movement of the Create 2. After glancing through the existing project, I find most of them are based on sending commands to Roomba through a cable. 
Is there anyway to embed the code in and let the Roomba behave accordingly? If there is not such method, which kind of API tool do you think is easiest for beginner? 
",irobot-create
Tracking Landspeed Underwater,"I am hoping someone might be able to nudge me in the right direction (apologies for the long post but wanted to get all the information I have gained so far down.
Basically I am after a solution to record the path my vessel took under water for later analysis…like a bread crumb trail.
Requirements:
Ideally have a range of at least 30meters however if there were no other options I would accept down to 10meters.
Working fresh and salt water.
The vessel is (25cm x 8cm) so size and power consumption are a factors.
It would be traveling roughly parallel to the sea bed at variable distances from the sea bed (range of 0-30 meters)
Does not need to be super accurate, anything less than 5 meters would be fine.
Measurement speed range of 0 – 4 mph.
Measure direction the object was moving in (i.e. forwards, sideways, backwards)…I am planning to use a compass to ascertain N, S, E, W heading.
Options I have discounted:

Accelerometers:

This was my initial thinking but in doing some reading it seems they are not suited to my needs (unless you spend loads of money, and then the solution would end up being too heavy anyway).

Optical Flow:

Looks too new (from a consumer perspective) / complicated. I don’t know what its range would be like. Also requires additional sonar sensor.
Current favorites:

Sonar:
http://www.alibaba.com/product-detail/1mhz-waterproof-transducer-underwater-ultrasonic-sensor_1911722468.html

Simplest use is distance from object, however can use doppler effect to analyse speed of a moving object.
40m range, nice!
Presumptions:
If I fired this at an angle to the seabed I could deduce the speed the floor was ‘moving’ below which would give me the speed of my vessel?
I am also presuming that I could interpret direction of movement from the data?
I presume that the sensor would need to be aimed at an angle of around 45 degrees down to the seabed?

Laser Rangefinder:

Although it works differently to the Sonar the premise of use looks the same, and thus I have the same queries with this as I do with the Sonar above.
Presume if I mounted the sensor behind high quality glass (to waterproof it) then performance would not be impacted that much.
This is a lot more costly so if it does not give me any advantage over sonar I guess there is no point.

Water Flow Meter:
http://www.robotshop.com/en/adafruit-water-flow-sensor.html

Super low cost and simple compared with the other options, I would potentially use a funnel to increase water pressure if it needs more sensitivity at low speed.
Would then just need to calibrate the pulses from the sensor to a speed reading.
Significant limitations of this is it would be registering a speed of 0 if the vessel was simply drifting with the current….its speed over the seabed that I am interested in.
Current favorite option is sonar (with the option of using water flow meter as second data source)…however are my sonar presumptions correct, have I missed anything?
Any better ideas?
","sonar, laser, underwater, rangefinder, acoustic-rangefinder"
How to apply A bang-bang signal of amplitude 1 N and 1 s width as an input force to reproduce certain results in Matlab?,"I working on dynamic modeling and simulation of a mechanical system (overhead crane), after I obtained the equation of motion, in the form: $$ M(q)\ddot{q}+C(q,\dot{q})\dot{q}+G(q)=Q $$
All the matrices are know inertia, $ M(q)$, Coriolis-Centrifugal matrix $ C(q,\dot{q})$, and gravity $ G(q)$ as functions of the generalized coordinates $q$, and their derivatives $\dot{q}$.
I want to solve for $q$, using Matlab ODE (in m-file), I got the response for some initial conditions and zero input, but, I want to find the response, for the aforementioned control signal (A bang-bang signal of amplitude 1 N and 1 s width), I'm trying to regenerate some results from the literature, and what the authors of that work said, regrading the input signal is the following: ""A bang-bang signal of amplitude 1 N and 1 s width is used as an input force, applied at the cart of the gantry crane. A bang-bang force has a positive (acceleration) and negative (deceleration) period allowing the cart to, initially, accelerate and then decelerate and eventually stop at a target location."" I didn't grasp what do they mean by bang-bang signal, I know in Matlab we could have step input, impulse, ...etc. But bang-bang signal, I'm not familiar with. According to this site and this bang bang is a controller rather.
Could anyone suggest to me how to figure out this issue and implement this input signal? preferably in m-file.
The code I'm using is given bellows, two parts:
function xdot = AlFagera(t,x,spec)
% xdot = zeros(8,1);
xdot = zeros(12,1); % to include the input torque

% % Crane Specifications
mp = spec(1);
mc = spec(2);
mr = spec(3);
L = spec(4);
J = spec(5);

g = 9.80;               % accelatrion of gravity (m/s^)

% % matix equations 
M11 = mr+mc+mp; M12 = 0; M13 = mp*L*cos(x(3))*sin(x(4)); M14 = mp*L*sin(x(3))*cos(x(4));
M21 = 0; M22 = mp+mc; M23 = mp*L*cos(x(3))*cos(x(4)); M24 = -mp*L*sin(x(3))*sin(x(4));
M31  = M13; M32 = M23; M33 = mp*L^2+J; M34 = 0;
M41 = M14; M42 = M24; M43 = 0; M44 = mp*L^2*(sin(x(3)))^2+J;
M = [M11 M12 M13 M14; M21 M22 M23 M24; M31 M32 M33 M34; M41 M42 M43 M44];

C11 = 0; C12 = 0; C13 = -mp*L*sin(x(3))*sin(x(4))*x(7)+mp*L*cos(x(3))*cos(x(4))*x(8);
C14 = mp*L*cos(x(3))*cos(x(4))*x(7)-mp*L*sin(x(3))*sin(x(4))*x(8);
C21 = 0; C22 = 0; C23 = -mp*L*sin(x(3))*cos(x(4))*x(7)-mp*L*cos(x(3))*sin(x(4))*x(8);
C24 = -mp*L*cos(x(3))*sin(x(4))*x(7)-mp*L*sin(x(3))*cos(x(4))*x(8); 
C31 = 0; C32 = 0; C33 = 0; C34 = -mp*L^2*sin(x(3))*cos(x(3))*x(8);
C41 = 0; C42 = 0; C43 = -C34; C44 = mp*L^2*sin(x(3))*cos(x(4))*x(7);

C = [C11 C12 C13 C14; C21 C22 C23 C24; C31 C32 C33 C34; C41 C42 C43 C44];
Cf = C*[x(5); x(6); x(7); x(8)];

G = [0; 0; mp*g*L*sin(x(3)); 0];

fx = 0; 

if t >=1 && t<=2
fy = 1.*square(t*pi*2);
else fy = 0;
end

F =[fx; fy; 0; 0];     % input torque vector, 

xdot(1:4,1)= x(5:8);
xdot(5:8,1)= M\(F-G-Cf);
xdot(9:12,1) = F;

And:
clear all; close all; clc;

t0 = 0;tf = 20;

x0 = [0.12 0.5 0 0, 0 0 0 0,0 0 0 0];  % initional conditions

% % spectifications
Mp = [0.1 0.5 1];      % variable mass for the payload
figure
plotStyle = {'b-','k','r'};
for i = 1:3
mp = Mp(i);
mc = 1.06; mr = 6.4;           % each mass in kg
L = 0.7; J = 0.005;            % m, kg-m^2 respe.
spec = [mp mc mr L J];
% % Call the the function
[t,x] = ode45(@(t,x)AlFagera(t,x,spec),[t0 :0.001: tf],x0);

legendInfo{i} = ['mp=',num2str(Mp(i)),'kg'];


fx = diff(x(:,9))./diff(t);
fy = diff(x(:,10))./diff(t);
tt=0:(t(end)/(length(fx)-1)):t(end); % this time vector

% to plot the cart positions in x and y direcitons
subplot(1,2,1)
plot(t,x(:,1),plotStyle{i})
axis([0 20 0 0.18]);
grid
xlabel('time (s)');
ylabel('cart position in x direction (m)');
hold on
legend(legendInfo,'Location','northeast')

subplot(1,2,2)
plot(t,x(:,2),plotStyle{i})
axis([0 20 0 1.1]);
grid
xlabel('time (s)');
ylabel('cart position in y direction (m)');
hold on
legend(legendInfo,'Location','northeast')

end

% to plot the input torque, (bagn-bang signal), just one sample
figure
plot(tt,fy)
grid
set(gca,'XTick',[0:20])
xlabel('time (s)');
ylabel('input signal, f_y (N)');

Furthermore, the results I'm getting and what I supposed to get are shown:


Major difficulties, initial conditions are not clearly stated in the paper, the input force direction, is only in y (which it should be), or it has different direction. I appreciate any help.

the paper I'm trying to recreate is:
  R. M. T. Raja Ismail, M. A. Ahmad, M. S. Ramli, and F. R. M. Rashidi, “Nonlinear Dynamic Modelling and Analysis of a 3-D Overhead Gantry Crane System with System Parameters Variation.,” International Journal of Simulation–Systems, Science & Technology, vol. 11, no. 2, 2010.
  http://ijssst.info/Vol-11/No-2/paper2.pdf

","control, robotic-arm, dynamics, matlab, input"
All-in-one GNSS localization solution (hardware+software),"Is there something like an all-in-one satellite based localization solution that would contain both hardware and software to do GNSS localization for robotics? I mean a package that would also contain an IMU, would fuse it with GPS and filter the result accordingly and then provide a software API to query for location/speed etc.
I am interested rather in some affordable solution but is there some professional hardware too?
I am trying to implement this for my mobile robot and I realize that a smartphone-grade GPS (Samsung J5) gives me better preliminary results than an u-blox eval board (this NEO-M8T with integrated antenna and ground plane) - I wonder why, I guess Android may fuse the IMU and have better readings even with worse antenna?
","localization, software, gps, gnss, hardware"
Low latency control from a laptop,"Lets say that I needed to send sensor readings in increments of 100 bytes from a micro controller to a laptop with sub 2 ms latencies in real time (the data needs to be processed and acted upon immediately (to control a robot)). What interfaces would one use? 
FTDI usb-serial converters aren't an option because they introduce 5-10 ms latencies both ways. PCI cards are an option though.
",untagged
How do I compute the inverse kinematics given a desired transformation matrix?,"I am at the moment trying to implement an inverse kinematics function which function is to take a desired transformation matrix, and the current transformation matrix, and compute the Q states that is needed to move my robot arm from current state to end state. 
I have already written the code, but since my simulation isn't showing the right path, or what I would expect it to be, this makes me unsure as to whether my implementation is correct.  Could someone comment on my implementation and maybe spot an error?
std::vector<Q> pathPlanning::invKin_largeDisplacement(std::vector<Transform3D<>> t_tool_base_desired_i)
{

    for(unsigned int i = 0; i<t_tool_base_desired_i.size();  ++i)
    {
        Transform3D<> T_tool_base_current_i = device_backup->baseTframe(this->toolFrame,state_backup);
        Eigen::MatrixXd jq(device_backup->baseJframe(this->toolFrame,state_backup).e().cols(), this->device.get()->baseJframe(this->toolFrame,state_backup).e().rows());
        jq =  this->device.get()->baseJframe(this->toolFrame,state_backup).e();


        //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  <=> dq = A*du
        Eigen::MatrixXd A (6,6);
        //A = jq.transpose()*(jq*jq.transpose()).inverse();
        A = (jq*jq.transpose()).inverse()*jq.transpose();

        Vector3D<> dif_p = t_tool_base_desired_i[i].P()-T_tool_base_current_i.P(); // Difference in position between current_i and desired_i

        Eigen::Matrix3d dif = t_tool_base_desired_i[i].R().e()- T_tool_base_current_i.R().e(); // Difference in rotation between current_i and desired_i

        Rotation3D<> dif_r(dif); //Construct rotation matrix
        RPY<> dif_rot(dif_r); // compute RPY from rotation matrix

        //Jq*dq = du
        Eigen::VectorXd du(6);
        du(0) = dif_p[0];
        du(1) = dif_p[1];
        du(2) = dif_p[2];

        du(3) = dif_rot[0];
        du(4) = dif_rot[1];
        du(5) = dif_rot[2];

        Eigen::VectorXd q(6);
        q = A*du; // Compute change dq

        Q q_current;
        q_current = this->device->getQ(this->state); // Get Current Q
        Q dq(q);
        Q q_new = q_current+ dq; // compute new Q by adding dq
        output.push_back(q_new); // Pushback to output vector
        device_backup->setQ(q_new,state_backup); //set current state to newly calculated Q.

    }
    return output;
}

Example of output: 
Q{-1.994910, -94.421754, -123.448429, 15.218864, 6.602184, -13.742988}
Q{2627.867315, -2048.863588, -51.340574, 287.654959, 270.187026, 258.581800}
Q{12941.812459, -536.870516, -294.362593, -2145.963577, -31133.660814, -4742.343433}
Q{32.044799, -14.220020, -14.312226, -12.444921, 12.269179, -24.393637}
Q{125.537278, 28.626924, -55.646716, -20.945348, 17.536762, -2.656717}
Q{9.514525, -107.455064, -17.009190, -15.245588, -0.960273, -2.010570}
Q{8.255582, -3.010934, -4.882207, -1.369533, 0.848644, 1.175172}
Q{208.655993, -28.443465, -64.413952, -3.129896, 13.063806, -6.042187}
Q{-73.706483, -20.381540, -5.306434, -1.204419, -4.035149, 21.806934}
Q{10.003481, 10.867394, 13.256192, -6.491445, -1.711469, 2.896646}
Q{24.890626, -72.265307, -94.886507, 12.327304, -4.425786, 4.188531}
Q{7.111258, 31.500732, -0.111033, -20.434697, 5.302118, 1.781690}
Q{477.993581, 659.221820, 19.819916, -88.627757, 65.850191, -77.267367}
Q{-30.672145, -53.496243, -18.170871, 83.648574, 48.311796, -28.015005}
Q{-36.677982, -15.908633, 17.751008, 0.995766, -0.500259, 9.409435}
Q{114246.358249, -10664.813432, -75.904830, 462.907904, 7992.514723, -18484.319327}
Q{83.827086, -75.899321, -38.576446, 37.266068, 47.843725, 39.096061}
Q{-119.682661, -774.773093, -251.969174, 23.212110, -42.662580, 53.247454}
Q{98.608881, -28.013383, 132.896921, 17.121488, 36.916894, -14.627180}
Q{-11519.051453, 5761.564318, -364.916044, -1188.567128, -2582.813750, -462.784007}
Q{54802.605226, 40971.776641, 10204.739981, -654.963987, -244.277958, -8618.970216}
Q{-21.334047, -14.314134, 17.714174, 2.463993, 0.963385, 5.304530}

","robotic-arm, inverse-kinematics, c++"
C++ and Create 2,"I am trying to use C++ to talk to the Create 2 robot. Does anyone have basic code to write/read from the Create 2 using C++ or C?
I am having trouble with converting Create 2 commands (like 145) into one char.
","irobot-create, c, c++"
How to compensate the brushless DC motor for voltage drop?,"I am working on a quadcopter project based on Arduino board, my system is powered by a 4S LiPo battery (14.8V) but the motors behave differently as the battery voltage drops, when discharging. 
Is there any way that I can make the motors behave the same until a minimum value of, say, 5 volts?
My current system works fine at the range from 14.8 to 10 volts, but below that I can't even hover.
",quadcopter
What is the wheel base distance of the create2?,"What is the wheel base distance that should be used for the create2 to calculate angle? I have seen 230.8mm in code samples but the manual seems to indicate 235.0 mm.
","irobot-create, roomba"
Mechanical odometer with digital output,"I would like to mechanically measure the distance a kids electric ATV traveled. The ATV will not be used with kids but as a mobile robot instead. It has a common rear axle for both rear wheels which I think could be a good place to put an odometer on (since the chance both wheels will slip should be minimal). Regarding suspension it has a single shock for rear axle.
My plan is to put a bigger gear on the axle itself and then add a smaller gear to it on which some kind of sensor would measure number of its rotations. One rotation of the axle may be something like 20 rotations of the small gear. What kind of sensor can I use for sensing rotation?
Another way of making an odometer may be some kind of optical solution (disc with holes and an optical sensor) but this seems to be rather complicated and also the the direction of travel could not be easily estimated (unless the motor is running in some direction).
I just found a term called Wheel Speed Sensor which looks interesting and seems to employ primarily non-contact sensing (which is definitely better than mechanical gears). Rather then optical solution I like the Hall effect sensor solution which may be simple and mechanically robust. But still, my question is open on how to implement this...
I would like to use the odometer for both speed estimation and distance estimation. I need to read the sensor from C/C++ on a Linux box.
EDIT: The thing I am looking for is probably correctly called a rotary encoder or a wheel encoder.
The ATV may look like one of these:


","mobile-robot, sensors, odometry, encoding, rotation"
Generate transformation matrices for rotating around a object?,"How do i compute all transformation matrices which places a robot endeffector at the shell of this sphere, with the end effector pointing toward the object in the center. 

I know at all time how far the object is relative to the endeffector, and radius of the sphere is the desired distance i want between the object and endeffector.  
I want by using inverse kinematics pan around this object in a sphere shaped trajectory. 
Each transformation matrix should contain different positions on the sphere and the rotation should be oriented such that the arm looks at the object. 
The position should be relative easy to compute, as i already know the distance to to object, and radius of the sphere. 
But the rotation matrix for each position is still a mystery for me.  
","robotic-arm, rotation"
DC motor with encoder,"Can anyone help me out here with this DC motor, especially the encoder part? I tried searching around for its datasheet but its as short as 1 page and the only spec I get are:
Encoder:
1 pulse/revolution
It has 2 connection on the bottom, and I guess they are for driving the motor, but they say nothing about the 3 connections wires below.

","motor, quadrature-encoder"
Counts of Quadrature Encoder,"Simply, I had Rover 5 with 2 DC motors and 2 quadrature encoders, I just want to use encoders to measure the distance of travelling for each wheel.
To start with, I just want to determine the total counts per revolution. I read the article about quadratic encoder from this broken link.
In Rover 5, each encoder has four wires: red (5V or 3.3V), black(Ground), yellow (Signal 1) and white (Signal 2). I connected each wire in its right place on Arduino Uno board, using the circuit:

rotary encoder ChannelA attached to pin 2 
rotary encoder ChannelB attached to pin 3
rotary encoder 5V attached to 5V
rotary encoder ground attached to ground 

For one encoder, I test the code below to determine the total counts or ticks per revolution, the first program by using loop and second by using an interrupt.
Unfortunately while I run each program separately, rotating the wheel 360 degree by hand, the outputs of these two programs was just ""gibberish"" and I don't know where is the problem . Could anyone help?
Arduino programs posted below.
First program:
// Constants
const int  ChanAPin = 2;    // pin for encoder ChannelA
const int  ChanBPin = 3;    // pin for encoder ChannelB

// Variables
int encoderCounter = -1;   // counter for the number of state changes
int ChanAState = 0;         // current state of ChanA
int ChanBState = 0;        // current state of ChanB
int lastChanAState = 0;     // previous state of ChanA
int lastChanBState = 0;    // previous state of ChanB

void setup() {
  // initialize the encoder pins as inputs:
  pinMode(ChanAPin, INPUT);
  pinMode(ChanBPin, INPUT);
  // Set the pullup resistors
  digitalWrite(ChanAPin, HIGH);
  digitalWrite(ChanBPin, HIGH);

  // initialize serial communication:
  Serial.begin(19200);
  Serial.println(""Rotary Encoder Counter"");
}

void loop() {
  // read the encoder input pins:
  ChanAState = digitalRead(ChanAPin);
  ChanBState = digitalRead(ChanBPin); 
  // compare the both channel states to previous states
  if (ChanAState != lastChanAState || ChanBState != lastChanBState) {
    // if the state has changed, increment the counter
      encoderCounter++;
      Serial.print(""Channel A State = "");
      Serial.println(ChanAState);
      Serial.print(""Channel B State = "");
      Serial.println(ChanBState);      
      Serial.print(""State Changes = "");
      Serial.println(encoderCounter, DEC);
    // save the current state as the last state, 
    //for next time through the loop
    lastChanAState = ChanAState;
    lastChanBState = ChanBState;    
  }
}

The second program (with interrupt)
static long s1_counter=0;
static long s2_counter=0;

void setup()
{
  Serial.begin(115200);

  attachInterrupt(0, write_s1, CHANGE); /* attach interrupt to pin 2*/
  attachInterrupt(1, write_s2, CHANGE); /* attach interrupt to pin 3*/
  Serial.println(""Begin test"");
}

void loop()
{
}

void write_s1()
{
  s1_counter++;
  Serial.print(""S1 change:"");
  Serial.println(s1_counter);
}

void write_s2()
{
  s2_counter++;
  Serial.print(""S2 change:"");
  Serial.println(s2_counter);
}

","mobile-robot, quadrature-encoder"
Why can't i use different ESCs together on a multirotor?,"I'm working on a diy quadcopter build from scratch and have bought a 4pack ESC from Castel Creations.While i currently have my quad up and running(sort of), from what i've read on the various sources and forums on the internet, i am not able to/ not recommended to use different ESCs together on the same quad.
As i bought my ESCs together as a 4 pack, and am not able to buy any replacements unless i were to switch out all 4 of them, this has me worried in the eventual case of a spoilt ESC in the future.
From what i can gleam from various posts on the internet, it seems to have something to do with the rate at which ESCs communicate with the flight controller.If so, can i not simply buy a esc programmer and program all of them to communicate at the same rate?
I've asked the dude at my local hobby shop, and he said that i cannot/should not be using different escs from different brands or even the same brand but different models( i.e 35v & 20V ) ESCs together.
I would really appreciate it if someone were to clarify what exactly is the issue with using different ESCs together on the same quadcopter.
P.S If it helps, i'm currently using the APM 2.6 as my flight controller on a WFLY transmitter and a f450 frame.
","quadcopter, microcontroller, electronics, esc, multi-rotor"
Shallow underwater wireless sensor network,"I need to make shallow (max 2m) underwater wireless sensor network. Data payload is about 10kB/s. I know that VLF band (~3-30kHz)could be the best solutions for that, but cause of time-to-market I cannot make hardware and software from the ground.
Maybe someone could share own-self experience in this filed. If band 100-900MHz could be enough to send 10kB/s from one device to another - from 2m underwater to over a dozen cm from water surface? Maybe some IC for ultrasonic communication exist? Another ideas?
","sensors, wireless, communication"
Is this a singularity or incorrect implementation of inverse kinematics?,"I at moment trying to compute the Q configuration that moves my robot from it current state described by this transformation matrix.
with rotation
0.00549713  0.842013  -0.539429  
0.999983 -0.00362229 0.00453632 
0.00186567 -0.539445 -0.842019

and position as this:
-0.0882761
-0.255069
 0.183645

To this rotatation 
    0  0.942755 -0.333487
    1         0         0
    0 -0.333487 -0.942755

and this position
8.66654
19.809
115.771

Due to the drastic change in the Z direction, I thought i could split the path between start and end into small chunks by creating data inbetween by interpolating, and compute the inverse kinematics for each of these small position.  Problem is that the output i am getting is pretty large.. Which making me suspect that some of the output might be wrong. The simulation i am using constrains the rotation to 360 degrees.. I think something goes wrong.. 
The only reason I could think would do this, would be if the jacobian i am was using had singularities... Which why i assumed that i was running into singualarity issue.. 
setQ{22.395444, 319.402231, 90.548314, -228.989836, -295.921218, -336.808799}
setQ{8.209388, 362.472468, 108.618073, -232.346755, -299.935844, -334.929518}
setQ{8.479842, 399.892521, 127.432982, -234.017882, -303.852583, -335.063821}
setQ{8.224516, 362.232497, 108.666778, -232.319554, -299.899932, -334.928688}
setQ{7.718908, 286.832458, 71.150606, -228.913831, -291.982659, -334.658147}
setQ{7.468625, 249.092444, 52.400638, -227.206436, -288.018036, -334.522738}
setQ{7.220023, 211.325766, 33.656081, -225.496018, -284.049424, -334.387237}
setQ{-6.134091, -2538.260148, -1283.375216, -96.331289, 7.920957, -324.531125}
setQ{-6.261661, -2577.946595, -1301.730132, -94.403263, 12.176863, -324.388990}
setQ{-6.634286, -2697.165915, -1356.762411, -88.601053, 24.968521, -323.962029}
setQ{-6.991781, -2816.625206, -1411.745985, -82.771641, 37.796090, -323.534239}
setQ{-7.334148, -2936.324468, -1466.680853, -76.915029, 50.659572, -323.105620}
setQ{-7.661386, -3056.263702, -1521.567017, -71.031215, 63.558965, -322.676171}
setQ{-8.642914, -3457.794271, -1704.169136, -51.222052, 106.816303, -321.238686}
setQ{-8.988457, -3619.153075, -1777.058457, -43.213761, 124.230964, -320.661112}
setQ{-9.382564, -3821.451508, -1868.048346, -33.135395, 146.089069, -319.937071}
setQ{-9.528439, -3902.557525, -1904.406419, -29.082892, 154.860242, -319.646810}
setQ{-9.667591, -3983.770196, -1940.742846, -25.018300, 163.647376, -319.356179}
setQ{-9.734645, -4024.416527, -1958.902942, -22.981471, 168.046928, -319.210726}
setQ{-9.986053, -4187.268484, -2031.489209, -14.803929, 185.685040, -318.627992}
setQ{-10.210564, -4350.547057, -2103.988889, -6.578030, 203.386994, -318.043783}
setQ{-10.312734, -4432.346324, -2140.206259, -2.446947, 212.261912, -317.751125}
setQ{-10.453381, -4555.245201, -2194.491727, 3.772345, 225.604215, -317.311448}
setQ{-10.496902, -4596.264820, -2212.576060, 5.851488, 230.059630, -317.164705}
setQ{-10.538741, -4637.311102, -2230.654980, 7.933652, 234.519035, -317.017869}
setQ{-10.617377, -4719.483658, -2266.796587, 12.107048, 243.449816, -316.723922}
setQ{-10.812941, -4966.641247, -2375.091527, 24.699772, 270.337923, -315.839868}
setQ{-10.839651, -5007.927501, -2393.121742, 26.809138, 274.833240, -315.692203}
setQ{-10.888029, -5090.579998, -2429.165939, 31.036936, 283.835844, -315.396596}

setQ is just a function for my simulation, the numbers are the actual Q values starting from 0 - 5. (I am using a 6 jointed robot (UR5))
Update
I am using a sphere to compute my desired transformation matrix.. The idea is that i want my arm be on this sphere, point inward to the center. 
std::vector<Transform3D<>> pathPlanning::sphere(double dx, double dy, double dz)
{
    double r = 5.0; // Radius of the sphere -  set to 5.0 cm (TODO: has to be checked if that also is accurate)
    cout << ""Create a sphere"" << endl;

    double current_x = this->device->baseTframe(this->toolFrame,this->state).P()[0];
    double current_y = this->device->baseTframe(this->toolFrame,this->state).P()[1];
    double current_z = this->device->baseTframe(this->toolFrame,this->state).P()[2];


    // Formula for sphere (x-x0)²+(y-y0)²+(z-z0)²=r²
    // x: x = x_0 + rcos(theta)sin(phi)
    // y: y = y_0 + rsin(theta)sin(phi)
    // z: z = z_0 + rcos(phi)
    // Angle range: 0 <= theta <= 2M_PI ; 0 <= phi <= M_PI

    double obj_x = current_x + dx;
    double obj_y = current_y + dy;
    double obj_z = current_z + dz;

    std::vector<Transform3D<>> out;

    int count = 32;

    for(double azimuthal = 0; azimuthal <= M_PI ; azimuthal+=0.01 )
    {

        for(double polar = 0.35; polar <= M_PI-0.35 ; polar+=0.01 )
        {

            double sphere_x = obj_x + r*cos(azimuthal)*sin(polar);
            double sphere_y = obj_y + r*sin(azimuthal)*sin(polar);
            double sphere_z = obj_z + + r*cos(polar);

            //string text = to_string(sphere_x) + "" , "" + to_string(sphere_y)+ "" , "" + to_string(sphere_z);
            //positions << text << endl;

            Transform3D<> transformation_matrix = transform(obj_x,obj_y,obj_z,sphere_x,sphere_y,sphere_z);

            if(0.1<(transformation_matrix.P()[0] - current_x) || 0.1<(transformation_matrix.P()[1] - current_y) || 0.1<(transformation_matrix.P()[2] - current_z))
            {
                cout << ""Interpolate: "" << endl;

                std::vector<Transform3D<>> transformation_i = invKin_LargeDisplacement(transformation_matrix);
                out.insert(out.end(),transformation_i.begin(),transformation_i.end());
                cout << out.size() << endl;
                cout << ""only returning one interpolation onto the sphere!"" << endl;

                return transformation_i;
            }
            else
            {
                cout << ""OK"" << endl;
                out.push_back(transformation_matrix);

            }


            if(count == 32) //TODO: Why...... is this occuring?
            {
                //cout << ""Theta: "" << theta << "" Phi: "" << phi << endl;
                //cout << sphere_x << "" , "" << sphere_y <<"" , ""<< sphere_z << endl;
                count = 0;
            }
            else
            {
                count++;
            }
        }
    }

    return out;
}

This function provides me with the point on the sphere, which is use to create my rotation matrix using transform.
Transform3D<> pathPlanning::transform(double obj_x, double obj_y, double obj_z, double sphere_x, double sphere_y ,double sphere_z)
{
    // Z-axis should be oriented towards the object.
    // Rot consist of 3 direction vector [x,y,z] which describes how the axis should be oriented in the world space.
    // Looking at the simulation the z-axis is the camera out. X, and Y describes the orientation of the camera.
    // The vector are only for direction purposes, so they have to be normalized....
    // TODO: case [0  0 -1]... Why is it happening at what can be done to undo it?
    cout << ""inside Transform"" << endl;
    cout << obj_x << "","" << sphere_x << "" ; ""  << obj_y << "" , "" << sphere_y  <<"" ; ""<< obj_z << "" , "" << sphere_z  << endl;
    Vector3D<> dir_z((obj_x - sphere_x), (obj_y - sphere_y), (obj_z - sphere_z));
    //Vector3D<> dir_z((sphere_x-obj_x), (sphere_y - obj_y), (sphere_z-obj_z));
    dir_z = normalize(dir_z);
    Vector3D<> downPlane(0.0,0.0,-1.0);
    Vector3D<> dir_x = cross(downPlane,dir_z);
    dir_x = normalize(dir_x);
    Vector3D<> dir_y = cross(dir_z,dir_x);
    dir_y = normalize(dir_y);
    Rotation3D<> rot_out (dir_x,dir_y,dir_z);  // [x y z]

    Vector3D<> pos_out(sphere_x,sphere_y,sphere_z);

    Transform3D<> out(pos_out,rot_out);
    cout << ""desired: "" << out << endl;

    return out;
}

The transform basically computes the rotation matrix. The math is based on the on this post by @Ben, which is an answer to a similar problem i am having..      
Update
Error with the rotation matrix was due to the polar coordinate being 0 => sin(0) = 0. 
I made this plot displaying the determinant of the jacobian, while i compute the inverse kinematics for the large displacement. For each inverse kinematics iteration, I set the robot to the new q_i and use that as current and continue computing until i reach the end configuration. 

It seems that alot of them goes toward a singularity or in general a pretty low number..
Update
Again i think the singularities might be the culprit here.. 
determinant: 0.0424284
Q{13.0099, -46.6613, -18.9411, 2.38865, 5.39454, -4.53456}
determinant: -0.0150253
Q{47.1089, -0.790356, 6.89939, -2.725, -1.66168, 11.2271}
determinant: -0.0368926
Q{15.7475, 8.89658, 7.78122, -2.74134, -5.32446, 1.11023}
determinant: -0.0596228
Q{180.884, 66.3786, 17.5729, 9.21228, -14.9721, -12.9577}
determinant: -0.000910399
Q{5426.74, 5568.04, -524.078, 283.581, -316.499, -67.3459}
determinant: -0.0897656
Q{16.6649, -37.4239, -34.0747, -16.5337, -3.95636, -7.31064}
determinant: -0.00719097
Q{-1377.14, 167.281, -125.883, -10.4689, 179.78, 56.3877}
determinant: 0.0432689
Q{22.2983, -10.1491, -15.0894, -4.41318, -2.07675, -3.48763}
determinant: -0.0430843
Q{82.6984, -39.02, -24.5518, 13.6317, 4.17851, -14.0956}
determinant: -0.0137243
Q{425.189, -9.65443, 20.9752, 7.63067, 25.4944, -52.4964}

Everytime i compute a new Q I set the robot in that state, and perform inverse kinematics from that state.. Q is the joint angles for the 6 joints. 
Update
Interpolation is done by lineary dividing the path from start to end into a specified amount of of data points.  

This plot shows  each tranformation matrices generated from the interpolation and with their the position part plotted. The red dots is the path (every 1000th position). The blue ball is the object in want to track, and green dots represents the sphere.. As I am only doing this for the first point on the sphere, it only hits one point on the sphere, which is the top point, which the plot also shows. 
Rotation doesn't show that much change, which also makes sense based difference between the current and desired rotations. 
Update
My InvKin Implementation for LargeDisplacements:
std::vector<Q> pathPlanning::invKin_largeDisplacement(std::vector<Transform3D<>> t_tool_base_desired_i)
{

     Device::Ptr device_backup = this->device;  //Read in device parameter
     WorkCell::Ptr workcell_backup = this->workcell; //Read in workcell parameter
     State state_backup = this->state;
     std::vector<Q> output;

     for(unsigned int i = 0; i<t_tool_base_desired_i.size();  ++i)
     {
         Transform3D<> T_tool_base_current_i = device_backup->baseTframe(this->toolFrame,state_backup); //Read in Current transformation matrix

         Eigen::MatrixXd jq(device_backup->baseJframe(this->toolFrame,state_backup).e().cols(), this->device.get()->baseJframe(this->toolFrame,state_backup).e().rows());

         jq =  this->device.get()->baseJframe(this->toolFrame,state_backup).e(); // Get the jacobian for current_configuration

         //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  <=> dq = A*du
         Eigen::MatrixXd A (6,6);

         //A = jq.transpose()*(jq*jq.transpose()).inverse();
         A = (jq*jq.transpose()).inverse()*jq.transpose();

         Vector3D<> dif_p = t_tool_base_desired_i[i].P()-T_tool_base_current_i.P();  //Difference in position

         Eigen::Matrix3d dif = t_tool_base_desired_i[i].R().e()- T_tool_base_current_i.R().e(); //Differene in rotation
         Rotation3D<> dif_r(dif); //Making a rotation matrix the the difference of rotation
         RPY<> dif_rot(dif_r);    //RPY of the rotation matrix. 

         Eigen::VectorXd du(6); //Creating du
         du(0) = dif_p[0];
         du(1) = dif_p[1];
         du(2) = dif_p[2];

         du(3) = dif_rot[0];
         du(4) = dif_rot[1];
         du(5) = dif_rot[2];

         Eigen::VectorXd q(6);
         q = A*du; // computing dq

         Q q_current;
         q_current = this->device->getQ(this->state);
         Q dq(q); 
         Q q_new = q_current+ dq; // computing the new Q angles
         output.push_back(q_new); store it in the output vector
         device_backup->setQ(q_new,state_backup); //Set the robot to the calculated state. 
     }
     return output;
}

I am pretty sure that my interpolation works, as the plot shows.  My inverse kinematics on the other hand not so sure..
Update
@Chuck mentions in his answer that it would be a good idea to check the core functionality, which might shed some light on what could be going wrong. 
I tried it with an inv.kin function i know would work, which didn't return any result, which make me doubt whether my transformation function i create is accurate?
The robot simulation is the one shown above..  The  Transform function shown above, is the function which i use to compute my desired, and provide my inverse kinematics..  Is something incorrectly setup?

Update
@Chuck came up with an different approach to my problem, which only has 3 DOF, being the position.  I choose change track, and peform a simple inverse kinematics given a distance dx,dy,dz.. Which for some reason isn't working quite good for me? even for small differences... 
Here is my code:
    std::vector<Q>pathPlanning::invKin(double dx, double dy , double dz)
{

    kinematics::State state =  this->state;
    Transform3D<> t_tool_base_current =  this->device.get()->baseTframe(this->toolFrame,state);

    cout <<""Current: ""<< t_tool_base_current.P().e()<< endl;

    Vector3D<> P_desired(0.000001+t_tool_base_current.P().e()[0],t_tool_base_current.P().e()[1],t_tool_base_current.P().e()[2]);
    cout <<""Desired: "" <<P_desired << endl;

    Transform3D<> t_tool_base_desired(P_desired,t_tool_base_current.R());
    Eigen::MatrixXd jq(this->device.get()->baseJframe(this->toolFrame,state).e().cols(), this->device.get()->baseJframe(this->toolFrame,state).e().rows());
    jq =  this->device.get()->baseJframe(this->toolFrame,state).e();


    //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  <=> dq = A*du
    Eigen::MatrixXd A (6,6);

    //A = jq.transpose()*(jq*jq.transpose()).inverse();
    A = (jq*jq.transpose()).inverse()*jq.transpose();

    Vector3D<> dif_p = t_tool_base_desired.P()-t_tool_base_current.P();
    cout <<""difference: "" <<dif_p << endl;

    Eigen::VectorXd du(6);
    du(0) = dif_p[0];
    du(1) = dif_p[1];
    du(2) = dif_p[2];

    du(3) = 0;
    du(4) = 0;
    du(5) = 0;

    Eigen::VectorXd q(6);
    q = A*du;

    Q q_current;
    q_current = this->device->getQ(this->state);
    Q dq(q);
    Q q_new = q_current+ dq;
    std::vector<rw::math::Q> output;
    if(!collision(q_new))
    {
        output.push_back(q_new);
    }
    else
    {
        cout << endl;      
        cout << q_new << endl;
    }

    return output;
}

which outputs this
Current: -0.000799058
-0.282
0.99963
Desired: Vector3D(-0.000789058, -0.282, 0.99963)
difference: Vector3D(1e-05, 0, 0)
setQ{1.559142, 110474925659325248.000000, -1834.776226, 55426871347211368.000000, 0.068436, 88275880260745.328125}

setQ is the state which moves the robot to the desires state.. 
Either is something wrong with my implementation, or it is a singularity..
Especially because i am not moving it that much (0.00001)!!!
Updates
I think I have solved the mystery.. It must be the sphere function which creates points that outside the reach of the robot.!! 
","robotic-arm, inverse-kinematics"
Quadrature Encoder Counts,"Actually , I have been since two weeks looking for convinced and final solution for my problem , actually I am completely lost , I am working on mobile robot (Rover 5) with 2 motors , 2 encoders . the controller that designed to the robot needs to know the odometery of mobile robot (X ,Y, Heading Angle ) , actually I am trying to function the encoders for this purpose , getting X ,Y, Heading Angle by measuring the traveled distance by each wheel , so to get the X ,Y, Heading Angle values , I should compute a accurate readings without missing any counts or ticks as could as possible .
The problem now is :
In the code in the attachment , while I am testing the encoders counts , I noticed that there is a difference between counts of encoders even when they spin in the same constant speed (PMW) , the difference increases as the two motors continue . so I thought that is the main cause of inaccurate odometery results .
In the output of the code (in the attachment also) the first two columns are right and left motors speed , the third & forth columns are right and left encoder counts , the fifth column is the difference between two encoders count , as you could see ,that even when the speed of two motors are approximately the same (each motor feed up with 100 PWM) there is a difference in the encoder counts and as you could see that the difference become big and big as the motors continuing spin .
One thing I thought that sending the same PWM value to two different motors will almost never produce the exact same speed , so I think that I should detect the absolute motion of the motors and adjust the power to get the speed/distance , but when I test the speed of motors after feed them with 100 PWM at same time , the two speeds were almost identical , but I noticed that there is a difference between counts of two encoders even when the motors spin in the same constant speed .
Actually , I don't know where is the problem , Is it in the code ? Is it in the hardware ? or what ? I am completely lost , I need for patient someone to help.
/* Encoder-ino.ino
*/
#define encoder0PinA 2
#define encoder0PinB 4
#define encoder1PinA 3
#define encoder1PinB 5

volatile int encoder0Pos = 0;
volatile int encoder1Pos = 0;
int WR=100;  // angular velocity of right wheel  
int WL=100;  // angular velocity of right wheel                       



long newposition;
long oldposition = 0;
unsigned long newtime;
unsigned long oldtime = 0;
long vel;


long newposition1;
long oldposition1 = 0;
unsigned long newtime1;
unsigned long oldtime1 = 0;
long vel1;



int ENA=8;    // SpeedPinA connected to Arduino's port 8  
int ENB=9;    // SpeedPinB connected to Arduino's port 9 

int IN1=48;    // RightMotorWire1 connected to Arduino's port 48
int IN2=49;    // RightMotorWire2 connected to Arduino's port 49

int IN3=50;    // RightMotorWire1 connected to Arduino's port 48
int IN4=51;    // RightMotorWire2 connected to Arduino's port 49


void setup() {
 pinMode(ENA,OUTPUT);
 pinMode(ENB,OUTPUT);
 pinMode(IN1,OUTPUT);
 pinMode(IN2,OUTPUT);
 pinMode(IN3,OUTPUT);
 pinMode(IN4,OUTPUT);

 digitalWrite(ENA,HIGH);    //enable motorA
 digitalWrite(ENB,HIGH);    //enable motorB
  pinMode(encoder0PinA, INPUT); 
  pinMode(encoder0PinB, INPUT);
  pinMode(encoder1PinA, INPUT); 
  pinMode(encoder1PinB, INPUT);  
// encoder pin on interrupt 0 (pin 2)
attachInterrupt(0, doEncoderA, CHANGE);  

// encoder pin on interrupt 1 (pin 3)

attachInterrupt(1, doEncoderB, CHANGE);  
  Serial.begin (9600);
}

void loop(){ 

    int rightPWM;
  if (WR > 0) {
    //forward
  digitalWrite(IN1,LOW);
  digitalWrite(IN2,HIGH);

  }  else if (WR < 0){
    //reverse
  digitalWrite(IN1,HIGH);
  digitalWrite(IN2,LOW);
  }

  if (WR == 0) {
   rightPWM = 0;
   analogWrite(ENA, rightPWM);
  } else {
    rightPWM = map(abs(WR), 1, 100, 1, 255);
    analogWrite(ENA, rightPWM);
  }

 int leftPWM;

  if (WL > 0) {
     //forward
  digitalWrite(IN3,LOW);
  digitalWrite(IN4,HIGH);
  }  else if (WL < 0) {
     //reverse
  digitalWrite(IN3,HIGH);
  digitalWrite(IN4,LOW);}

  if (WL == 0) {
    leftPWM = 0;
    analogWrite(ENB, leftPWM);
  } else {
    leftPWM = map(abs(WL), 1, 100, 1, 255);
    analogWrite(ENB, leftPWM);
  }


// to determine the speed of motors by encoders 

 newposition = encoder0Pos;
 newtime = millis();
 vel = (newposition-oldposition) * 1000 /(long)(newtime-oldtime);
 oldposition = newposition;
 oldtime = newtime;

 newposition1 = encoder1Pos;
 newtime1 = millis();
 vel1 = (newposition1-oldposition1) * 1000 /(long)(newtime1-oldtime1);
 oldposition1 = newposition1;
 oldtime1 = newtime1;

Serial.print (vel);
Serial.print (""\t"");
Serial.print (vel1);
Serial.print (""\t"");
Serial.print (encoder0Pos*-1); 
Serial.print(""\t""); 
Serial.print (encoder1Pos*-1); 
Serial.print(""\t""); 
Serial.println ((encoder0Pos*-1) -( encoder1Pos*-1)); 
}



// 1 encoder counts

void doEncoderA(){
  // look for a low-to-high on channel A
  if (digitalRead(encoder0PinA) == HIGH) { 
    // check channel B to see which way encoder is turning
    if (digitalRead(encoder0PinB) == LOW) {  
      encoder0Pos = encoder0Pos + 1;         // CW
    } 
    else {
      encoder0Pos = encoder0Pos - 1;         // CCW
    }
  }
  else   // must be a high-to-low edge on channel A                                       
  { 
    // check channel B to see which way encoder is turning  
    if (digitalRead(encoder0PinB) == HIGH) {   
      encoder0Pos = encoder0Pos + 1;          // CW
    } 
    else {
      encoder0Pos = encoder0Pos - 1;          // CCW
    }
  }
}



// 2 encoder counts

void doEncoderB(){
  // look for a low-to-high on channel B
  if (digitalRead(encoder1PinB) == HIGH) {   
   // check channel A to see which way encoder is turning
    if (digitalRead(encoder1PinA) == HIGH) {  
      encoder1Pos = encoder1Pos + 1;         // CW
    } 
    else {
      encoder1Pos = encoder1Pos - 1;         // CCW
    }
  }
  // Look for a high-to-low on channel B
  else { 
    // check channel B to see which way encoder is turning  
    if (digitalRead(encoder1PinA) == LOW) {   
      encoder1Pos = encoder1Pos + 1;          // CW
    } 
    else {
      encoder1Pos = encoder1Pos - 1;          // CCW
    }
  }
}

the result:
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   0
0   0   0   0   2
-181    -90 3   2   1
-111    -55 5   4   1
-187    -187    9   8   2
-176    -235    12  12  1
-200    -200    16  16  1
-250    -250    21  21  1
-250    -250    26  26  1
-210    -210    31  31  1
-238    -285    36  36  1
-315    -263    41  41  1
-300    -200    47  46  2
...
-227    -272    184 182 3
-285    -285    190 187 4
-260    -217    195 193 3
-238    -285    201 199 3
...
-250    -250    1474    1473    2
-250    -250    1480    1479    0
-208    -291    1485    1485    1
-304    -260    1491    1492    1
-240    -240    1498    1498    1
-260    -260    1504    1505    0
-250    -291    1510    1511    1
-280    -240    1516    1517    1
-260    -260    1523    1523    1
...
-250    -250    2953    2948    5
-250    -291    2959    2955    6
-250    -250    2965    2961    6
-291    -250    2971    2967    5
-250    -291    2978    2973    5
-304    -250    2985    2980    8
-320    -250    2992    2986    8
...
-320    -240    3085    3075    10
-291    -291    3092    3082    12
-269    -230    3099    3089    11
-250    -291    3105    3095    11
-280    -280    3112    3102    11
-269    -230    3118    3108    12
-250    -291    3125    3115    11
...
-291    -250    3607    3587    19
-115    -269    3610    3594    17
-240    -240    3617    3601    18
-375    -291    3625    3607    19
-269    -269    3632    3614    20
-291    -250    3638    3620    20
-240    -280    3645    3627    20
-280    -240    3652    3633    18
-200    -280    3657    3640    19
-269    -230    3664    3647    19
-333    -291    3674    3653    23
-400    -280    3682    3659    23
-280    -240    3688    3666    24
-240    -280    3695    3673    24
...
-230    -269    4677    4644    32
-208    -291    4681    4651    32
-280    -240    4690    4657    35
-320    -280    4696    4664    34
-240    -240    4703    4670    34
-291    -291    4710    4677    34
-269    -230    4716    4683    34
-240    -280    4723    4690    34
-280    -240    4727    4697    32
-160    -280    4736    4703    35
-416    -291    4745    4709    38
-346    -230    4753    4716    39
...
-360    -240    6240    6190    51
-375    -291    6247    6197    51
-269    -269    6253    6203    52
-291    -250    6261    6210    53
...
-192    -269    6428    6374    56
-240    -280    6436    6380    57
-291    -250    6443    6387    57
-269    -269    6449    6394    57
...
-269    -269    7763    7687    78
-240    -280    7770    7694    78
-291    -250    7776    7700    76
-192    -269    7781    7707    76
...
-269    -230    8263    8179    84
-250    -291    8269    8186    85
-240    -240    8276    8192    88
-384    -269    8286    8199    88
-250    -291    8292    8206    88
-269    -230    8299    8212    87
-291    -291    8305    8219    88
-240    -240    8310    8225    85
...
-160    -120    8359    8276    83
-125    -166    8362    8280    82
-115    -115    8365    8283    83
-80 -120    8367    8285    82
-125    -83 8370    8288    82
-83 -125    8371    8290    82
-43 -43 8373    8291    81
-83 -83 8374    8293    82
-45 -90 8375    8294    81
-43 -43 8376    8296    81
-43 -43 8377    8296    81
-43 -43 8378    8297    81

","mobile-robot, wheeled-robot, quadrature-encoder"
Inverse Kinematics of DLR/HIT II Hand,"I am trying to find the inverse kinematics formulation of DLR/HIT II hand. Till now I had success in finding an analytical method described in the thesis of Mavrogiannis, Christoforos I. named Grasp Synthesis Algorithms for Multifingered Robot Hands, given in appendix B. 
My question is regarding the A.28 where the author calculates q3 but has mentioned previously in the text that q3 is equal to q2. 
Note: q here denotes joint angles
",inverse-kinematics
Dead Reckoning: Obtaining Position Estimation from Accelerometer Acceleration Integration,"Good day,
I have been reading papers about position integration from accelerometer readings.
I have consulted this paper from freescale on how that is achievable and this article regarding leaky integrators to help in preventing accumulation of errors from integration.
I was testing this algorithm by moving the imu by approximately 0.1 meter. The algorithm does get it right at the instant it arrives at approx 0.1 meter however when left still at that position, the integrated position goes to zero.
It turns out the velocity readings become negative at a certain period after reaching 0.1 meters.

Does anyone have any suggestions in dealing with this error?

Plots (Red is the position, Blue is the velocity.)
The imu(accelerometer) was moved alternating positions 0 meters and 0.1 meters with a stop of approximately 3-5 seconds in between before moving to the next position

Actual Data


Desired Data output (Green - Desired position integration)


Code:
// Get acceleration per axis
float AccX = accelmagAngleArray.AccX;
float AccY = accelmagAngleArray.AccY;
float AccZ = accelmagAngleArray.AccZ;

AccX -= dc_offsetX;
AccY -= dc_offsetY;
AccZ -= dc_offsetZ;

//Calculate Current Velocity (m/s)
float leakRateAcc = 0.99000;
velCurrX = velCurrX*leakRateAcc + ( prevAccX + (AccX-prevAccX)/2 ) * deltaTime2;
velCurrY = velCurrY*leakRateAcc + ( prevAccY + (AccY-prevAccY)/2 ) * deltaTime2;
velCurrZ = velCurrZ*0.99000 + ( prevAccZ + (AccZ-prevAccZ)/2 ) * deltaTime2;
prevAccX = AccX;
prevAccY = AccY;
prevAccZ = AccZ;

//Discrimination window for Acceleration
if ((0.12 > AccX) && (AccX > -0.12)){
  AccX = 0;
}

if ((0.12 > AccY) && (AccY > -0.12)){
  AccY = 0;
}

//Count number of times acceleration is equal to zero to drive velocity to zero when acceleration is ""zero""
//X-axis---------------
if (AccX == 0){ //Increment no of times AccX is = to 0
    counterAccX++;    
}
else{ //Reset counter
    counterAccX = 0;
}

if (counterAccX>25){ //Drive Velocity to Zero
velCurrX = 0;
    prevVelX = 0;
    counterAccX = 0;
}

//Y-axis--------------
if (AccY == 0){ //Increment no of times AccY is = to 0
    counterAccY++;    
}
else{ //Reset counter
    counterAccY = 0;
}

if (counterAccY>25){ //Drive Velocity to Zero
    velCurrY = 0;
    prevVelY = 0;
    counterAccY = 0;
}

//Print Acceleration and Velocity
cout << "" AccX = "" << AccX ;// << endl;
cout << "" AccY = "" << AccY ;// << endl;
cout << "" AccZ = "" << AccZ << endl;

cout << "" velCurrX = "" << velCurrX ;// << endl;
cout << "" velCurrY = "" << velCurrY ;// << endl;
cout << "" velCurrZ = "" << velCurrZ << endl;

//Calculate Current Position in Meters
float leakRateVel = 0.99000;
posCurrX = posCurrX*leakRateVel + ( prevVelX + (velCurrX-prevVelX)/2 ) * deltaTime2;
posCurrY = posCurrY*leakRateVel + ( prevVelY + (velCurrY-prevVelY)/2 ) * deltaTime2;
posCurrZ = posCurrZ*0.99000 + ( prevVelZ + (velCurrZ-prevVelZ)/2 ) * deltaTime2;
prevVelX = velCurrX;
prevVelY = velCurrY;
prevVelZ = velCurrZ;

//Print X and Y position in meters
cout << "" posCurrX = "" << posCurrX ;// << endl;
cout << "" posCurrY = "" << posCurrY ;// << endl;
cout << "" posCurrZ = "" << posCurrZ << endl;

","quadcopter, sensors, localization, integration, dead-reckoning"
Are all Flight Controllers and Remote Controls using the same protocol?,"I'm about to start a project, where I'm sniffing data between remote controls and flight controllers on RC copters and doing stuff with that information.  Do all (or most) flight controllers use the same protocol to communicate with the remote controls, or does it vary based on which one you buy?  I would be testing on drones (DJI phantom and the like).  
So, my real question is:
If I want to write something to read the data, will I need to buy a different flight controller for each protocol used, or do they all use the same protocol, and I can just buy one flight controller, and the info I can get out will be the same for all types of flight controllers?
Also, are the protocols only spoken by the ground remote control and the flight controller?  Does the receiver care what protocol is being used, or is it just a middle man?
","quadcopter, radio-control, research"
Principle of virtual force - General help in understanding / explanation,"I'm an Electronics student taking a module in Robotics. 
From the example,

I understand line 1 as the Jacobian is found from the time derivative of the kinematics equation and such relates joint angles to velocity.
I do not understand why the transpose has been taken on line 3 and how line 4 is produced.
","kinematics, jacobian"
Step size in numerical differentiation,"I get position information and a corresponding timestamp from a motion tracking system (for a rigid body) at 120 Hz. The position is in sub-millimeter precision, but I'm not too sure about the time stamp, I can get it as floating point number in seconds from the motion tracking software. To get the velocity, I use the difference between two samples divided by the $\Delta t$ of the two samples:
$\dot{\mathbf{x}} = \dfrac{\mathbf{x}[k] - \mathbf{x}[k-1]}{t[k]-t[k-1]}$.
The result looks fine, but a bit noisy at times. A realized that I get much smoother results when I choose the differentiation step $h$ larger, e.g. $h=10$:
$\dot{\mathbf{x}} = \dfrac{\mathbf{x}[k] - \mathbf{x}[k-h]}{t[k]-t[k-h]}$.
On the other hand, peaks in the velocity signal begin to fade if I choose $h$ too large. Unfortunately, I didn't figure out why I get a smoother signal with a bigger step $h$. Does someone have a hint? Is there a general rule which differentiation step size is optimal with respect to smoothness vs. ""accuracy""?
This is a sample plot of one velocity component (blue: step size 1, red: step size 10):

","motion, pose"
Implementation of inverse kinematics solution in c++,"I am having some issue with implementing a least square solution of the inverse kinematics problem. 
The q configuration I get are rather large, or makes no sense, so I was hoping someone here could help me find my error in my program. 
rw::math::Q pathPlanning::invKin(double dx, double dy , double dz)
{

    rw::kinematics::State state =  this->state;
    rw::math::Transform3D<> t_tool_base =  this->device.get()->baseTend(state);


    cout << t_tool_base.R().e() << endl;
    cout << endl;
    cout << t_tool_base.P().e() << endl;
    cout << endl;

    Eigen::MatrixXd jq(this->device.get()->baseJend(state).e().cols(), this->device.get()->baseJend(state).e().rows());
    jq =  this->device.get()->baseJend(state).e();


    //Least square solver - dq = [j(q)]T (j(q)[j(q)]T)⁻1 du  <=> dq = A*du
    Eigen::MatrixXd A (6,6);
    //A = jq.transpose()*(jq*jq.transpose()).inverse();
    A = (jq*jq.transpose()).inverse()*jq.transpose();

    std::vector<rw::math::Transform3D<> > out = sphere(dx,dy,dz);

    std::ofstream outfile;
    outfile.open(""q_conf.txt"", std::ios_base::app);

    for(unsigned int i = 0; i <= out.size() ; ++i )
    {
        rw::math::Vector3D<> dif_p = out[i].P()-t_tool_base.P();

        Eigen::Matrix3d dif = out[i].R().e()- t_tool_base.R().e();
        rw::math::Rotation3D<> dif_r(dif);
        rw::math::RPY<> dif_rot(dif_r);

        Eigen::VectorXd du(6);
        du(0) = dif_p[0];
        du(1) = dif_p[1];
        du(2) = dif_p[2];

        du(3) = dif_rot[0];
        du(4) = dif_rot[1];
        du(5) = dif_rot[2];

        Eigen::VectorXd q(6);
        q = A*du;

        rw::math::Q q_current;
        q_current = this->device->getQ(this->state);
        rw::math::Q dq(q);
        rw::math::Q q_new = q_current+ dq;

        //cout << jq << endl;
        //cout << endl;
        //std::string text = ""setQ{""  + to_string(q_new[0])  + "", "" + to_string(q_new[1]) + "", "" + to_string(q_new[2]) + "", "" + to_string(q_new[3]) + "", "" + to_string(q_new[4]) + "", "" + to_string(q_new[5]) + ""}"";
        //cout << text << endl;
        //outfile << text << endl;

    }

    rw::math::Q bla(6); //Just used the text file for debugging purposes,  Which why I just return a random Q config.
    return bla;
}


rw::math::Transform3D<> pathPlanning::transform(double obj_x, double obj_y, double obj_z, double sphere_x, double sphere_y ,double sphere_z)
{
    // Z-axis should be oriented towards the object.
    // Rot consist of 3 direction vector [x,y,z] which describes how the axis should be oriented in the world space.
    // Looking at the simulation the z-axis is the camera out. X, and Y describes the orientation of the camera.
    // The vector are only for direction purposes, so they have to be normalized....
    // TODO: case [0  0 -1]... Why is it happening at what can be done to undo it?

    rw::math::Vector3D<> dir_z((obj_x - sphere_x), (obj_y - sphere_y), (obj_z - sphere_z));
    dir_z = normalize(dir_z);
    rw::math::Vector3D<> downPlane(0.0,0.0,-1.0);
    rw::math::Vector3D<> dir_x = cross(downPlane,dir_z);
    dir_x = normalize(dir_x);
    rw::math::Vector3D<> dir_y = cross(dir_z,dir_x);
    dir_y = normalize(dir_y);

    rw::math::Rotation3D<> rot_out (dir_x,dir_y,dir_z);

    rw::math::Vector3D<> pos_out(sphere_x,sphere_y,sphere_z);

    rw::math::Transform3D<> out(pos_out,rot_out);
    return out;
}

std::vector<rw::math::Transform3D<>> pathPlanning::sphere(double dx, double dy, double dz)
{
    double r = 0.50; // Radius of the sphere -  set to 0.50 cm (TODO: has to be checked if that also is accurate)
    cout << ""Create a sphere"" << endl;

    double current_x = this->device->baseTend(this->state).P()[0];
    double current_y = this->device->baseTend(this->state).P()[1];
    double current_z = this->device->baseTend(this->state).P()[2];

    rw::math::Vector3D<> center(current_x + dx, current_y + dy , current_z + dz);

    // Formula for sphere (x-x0)²+(y-y0)²+(z-z0)²=r²
    // x: x = x_0 + rcos(theta)sin(phi)
    // y: y = y_0 + rsin(theta)sin(phi)
    // z: z = z_0 + rcos(phi)
    // Angle range: 0 <= theta <= 2M_PI ; 0 <= phi <= M_PI

    double obj_x = current_x + dx;
    double obj_y = current_y + dy;
    double obj_z = current_z + dz;

    ofstream positions;
    ofstream rotations_z;
    ofstream rotations_y;
    ofstream rotations_x;
    positions.open (""sphere_positions.csv"");
    rotations_z.open(""z_dir.csv"");
    rotations_y.open(""y_dir.csv"");
    rotations_x.open(""x_dir.csv"");
    std::vector<rw::math::Transform3D<>> out;

    int count = 32;
    for(double theta = 0; theta <= 2*M_PI ; theta+=0.1 )
    {

        for(double phi = 0; phi <= M_PI ; phi+=0.1)
        {

            double sphere_x = obj_x + r*cos(theta)*sin(phi);
            double sphere_y = obj_y + r*sin(theta)*sin(phi);
            double sphere_z = obj_z + + r*cos(phi);

            string text = to_string(sphere_x) + "" , "" + to_string(sphere_y)+ "" , "" + to_string(sphere_z);
            positions << text << endl;

            rw::math::Transform3D<> transformation_matrix = transform(obj_x,obj_y,obj_z,sphere_x,sphere_y,sphere_z);

            string text2 = to_string(transformation_matrix.R().e()(0,2)) + "" , ""  + to_string(transformation_matrix.R().e()(1,2)) + "" , "" + to_string(transformation_matrix.R().e()(2,2));
            string text1 = to_string(transformation_matrix.R().e()(0,1)) + "" , ""  + to_string(transformation_matrix.R().e()(1,1)) + "" , "" + to_string(transformation_matrix.R().e()(2,1));
            string text0 = to_string(transformation_matrix.R().e()(0,0)) + "" , ""  + to_string(transformation_matrix.R().e()(1,0)) + "" , "" + to_string(transformation_matrix.R().e()(2,0));

            rotations_z << text2 << endl;
            rotations_y << text1 << endl;
            rotations_x << text0 << endl;

            if(count == 32) //TODO: Why...... is this occuring?
            {
                //cout << ""Theta: "" << theta << "" Phi: "" << phi << endl;
                //cout << sphere_x << "" , "" << sphere_y <<"" , ""<< sphere_z << endl;
                count = 0;
            }
            else
            {
                count++;
            }

            out.push_back(transformation_matrix);
        }
    }

    positions.close();
    rotations_z.close();
    rotations_y.close();
    rotations_x.close();
    cout << endl;
    cout <<""Object at: "" << obj_x << "","" << obj_y << "","" << obj_z << endl;
    cout << ""done "" << endl;
    return out;
}        

What am I trying to do, I am trying to orbit a robot endeffector  around an object in the center. The trajectory of the endeffector is an sphere where the endeffector should always point in to the object.  The sphere function should compute all transformation matrices which move the robot arm to the different position on the sphere with a given rotation, and the inverse kinematics should compute all the different Q-states, given an x,y,z which is the actual displacement to the object itself.
I am not quite sure where my error could be at, but I think it might either be at transform function where I generate my desired transformation matrix, or in invKin where I create du, I think I might have made an mistake in creating du(3),  du(4), du(5)
The libraries I've been using is Eigen, robwork (basically all rw::) if anyone want to look syntax through. 
Update
Based on @ghanimmukhtar I began checking for singularities for the jacobian.. Which seems in general supringsly low. I computed it for a list of random Q configurations which resulted into this...
Determinant: -0.0577779
Determinant: -0.0582286
Determinant: 0.0051402
Determinant: -0.0498886
Determinant: 0.0209685
Determinant: 0.00372222
Determinant: 0.047645
Determinant: 0.0442362
Determinant: -0.0799746
Determinant: 0.00194714
Determinant: 0.0228195
Determinant: 0.096449
Determinant: -0.0339612
Determinant: -0.00365521
Determinant: -0.030022
Determinant: 0.021347
Determinant: 0.0413364
Determinant: 0.0041136
Determinant: -0.0151192
Determinant: 0.0682926
Determinant: -0.0657176
Determinant: 0.0915473
Determinant: -0.00516008
Determinant: -0.0394664
Determinant: -0.00469664
Determinant: 0.0494431
Determinant: -0.00156804
Determinant: -0.0402393
Determinant: -0.0141511
Determinant: 0.0203508
Determinant: -0.0368337
Determinant: -0.0313431
Determinant: -0.0566811
Determinant: -0.00766113
Determinant: -0.051767
Determinant: -0.00815555
Determinant: 0.0564639
Determinant: 0.0764514
Determinant: -0.0501299
Determinant: -0.00056537
Determinant: -0.0308103
Determinant: -0.0091592
Determinant: 0.0602148
Determinant: -0.0051255
Determinant: 0.0426342
Determinant: -0.0850566
Determinant: -0.0353419
Determinant: 0.0448761
Determinant: -0.0103023
Determinant: -0.0123843
Determinant: -0.00160566
Determinant: 0.00558663
Determinant: 0.0173488
Determinant: 0.0170783
Determinant: 0.0588363
Determinant: -0.000788464
Determinant: 0.052941
Determinant: 0.064341
Determinant: 0.00084967
Determinant: 0.00716674
Determinant: -0.0978426
Determinant: -0.0585773
Determinant: 0.038732
Determinant: -0.00489957
Determinant: -0.0460029
Determinant: 0.00269656
Determinant: 0.000600619
Determinant: -0.0408527
Determinant: -0.00115296
Determinant: 0.013114
Determinant: 0.0366423
Determinant: 0.0495209
Determinant: -0.042201
Determinant: -0.036663
Determinant: -0.103452
Determinant: -0.0119054
Determinant: 0.0692284
Determinant: -0.00717832
Determinant: 0.00729104
Determinant: 0.0126415
Determinant: -0.00515246
Determinant: -0.0556505
Determinant: 0.000670701
Determinant: -0.0545629
Determinant: 0.00251946
Determinant: 0.0405189
Determinant: 0.010928
Determinant: -0.00101032
Determinant: 0.0308612
Determinant: 0.0536183
Determinant: -0.0439223
Determinant: -0.0113453
Determinant: -0.0193872
Determinant: 0.0660165
Determinant: -0.00184695
Determinant: -0.106904
Determinant: 0.01246
Determinant: -0.00883772
Determinant: 0.0601036
Determinant: 0.0468602
Determinant: 0.0513812
Determinant: -0.000663089
Determinant: -0.00392395
Determinant: 0.0710837
Determinant: 0.0629583
Determinant: -0.0464579
Determinant: 0.0257618
Determinant: -0.0193227
Determinant: 0.00388693
Determinant: -0.02003
Determinant: 0.0191158
Determinant: -0.00159198
Determinant: -0.0702308
Determinant: -0.0242876
Determinant: -0.00934638
Determinant: -0.00221986
Determinant: -0.0268925
Determinant: 0.0596055
Determinant: -0.00925273
Determinant: -0.0167357
Determinant: 0.0596476
Determinant: -0.00515798
Determinant: -0.00324081
Determinant: -0.00321565
Determinant: 0.0669645
Determinant: -0.0342913
Determinant: -0.000342155
Determinant: -0.0104422
Determinant: -0.0410489
Determinant: -0.0246036
Determinant: 0.0208562
Determinant: -0.0692963
Determinant: 0.000839091
Determinant: -0.049308
Determinant: -0.0349338
Determinant: 0.0016057
Determinant: -0.00214381
Determinant: -0.0332965
Determinant: 0.0168007
Determinant: -0.0748581
Determinant: -0.00864737
Determinant: -0.0638044
Determinant: -0.00103911
Determinant: -0.00690918
Determinant: 0.000285789
Determinant: 0.0215414
Determinant: 0.0560827
Determinant: -0.0063201
Determinant: -0.00677609
Determinant: -0.00686829
Determinant: 0.0591599
Determinant: 0.0112705
Determinant: 0.0874784
Determinant: -0.0146124
Determinant: -0.0133718
Determinant: -0.0203801
Determinant: -0.0150386
Determinant: -0.102603
Determinant: -0.077111
Determinant: 0.021146
Determinant: 0.089761
Determinant: -0.0532867
Determinant: -0.0620632
Determinant: -0.0165414
Determinant: -0.0461426
Determinant: 0.00144256
Determinant: 0.00844777
Determinant: 0.0893306
Determinant: -0.0814478
Determinant: -0.0890507
Determinant: -0.0472091
Determinant: 0.0186799
Determinant: -0.00224087
Determinant: -0.0242662
Determinant: -0.00195303
Determinant: 0.014432
Determinant: 0.00185717
Determinant: -0.0354357
Determinant: -0.0427957
Determinant: -0.0380409
Determinant: 0.0627548
Determinant: 0.0397546
Determinant: 0.0570439
Determinant: 0.106265
Determinant: 0.0382001
Determinant: -0.0240826
Determinant: -0.0866264
Determinant: 0.024184
Determinant: 0.0841286
Determinant: -0.0303611
Determinant: -0.0337029
Determinant: -0.0202875
Determinant: 0.0643731
Determinant: -0.0475265
Determinant: -0.00928736
Determinant: -0.00373402
Determinant: 0.0636828
Determinant: 0.0122532
Determinant: 0.0398141
Determinant: -0.0563998
Determinant: -0.0778303
Determinant: 0.0164747
Determinant: 0.0314815
Determinant: 0.0744507
Determinant: -0.0897675
Determinant: 0.0260324
Determinant: -0.0734512
Determinant: 0.000234548
Determinant: -0.0238522
Determinant: -0.0849523
Determinant: 0.0204877
Determinant: -0.0715147
Determinant: 0.0703858
Determinant: -0.0142186
Determinant: -0.101503
Determinant: 0.03966
Determinant: 4.69111e-05
Determinant: 0.0394428
Determinant: 0.0409131
Determinant: 8.90995e-05
Determinant: -0.00841189
Determinant: -0.0671323
Determinant: 0.00805167
Determinant: -0.00292435
Determinant: 0.0507716
Determinant: 0.0493995
Determinant: 0.00629414
Determinant: -0.0428982
Determinant: -0.0446924
Determinant: 0.0776236
Determinant: 0.00440478
Determinant: -0.0463321
Determinant: -0.00247224
Determinant: -0.0199861
Determinant: 0.0267022
Determinant: 0.0184179
Determinant: 0.0104588
Determinant: 0.116535
Determinant: -0.0857382
Determinant: -0.0477216
Determinant: 0.0286968
Determinant: 0.0387932
Determinant: 0.042856
Determinant: -0.0964
Determinant: 0.0320456
Determinant: -0.0676327
Determinant: 0.0156632
Determinant: 0.0548582
Determinant: 0.0394791
Determinant: 0.0863353
Determinant: -0.0568753
Determinant: -0.00953039
Determinant: -0.0534666
Determinant: 0.0506779
Determinant: 0.00521034
Determinant: 0.0353338
Determinant: 0.0845463
Determinant: -0.00847695
Determinant: 0.015726
Determinant: -0.0648035
Determinant: 0.0170917
Determinant: 0.0045193
Determinant: -0.0195397
Determinant: 0.00630076
Determinant: -0.0137401
Determinant: 0.0209229
Determinant: 0.00382077
Determinant: -0.0588661
Determinant: -0.0923883
Determinant: -0.00726003
Determinant: -0.0411533
Determinant: 0.00544489
Determinant: 0.0101791
Determinant: 0.0903306
Determinant: -0.0590416
Determinant: -0.0377112
Determinant: -0.0150455
Determinant: 0.0793066
Determinant: 0.0425759
Determinant: -0.040728
Determinant: -0.0376792
Determinant: -0.0387703
Determinant: -0.0232208
Determinant: 0.0506747
Determinant: -0.0284409
Determinant: 0.000536999
Determinant: -0.0289103
Determinant: -0.00586449
Determinant: -0.0805586
Determinant: 0.0133906
Determinant: -0.00311773
Determinant: 0.0184798
Determinant: -0.00981978
Determinant: -0.0491601
Determinant: 0.0452526
Determinant: 0.00411708
Determinant: -0.0515142
Determinant: 0.0121114
Determinant: 0.00636972
Determinant: -0.0126048
Determinant: -0.0412662
Determinant: 0.00195264
Determinant: -0.0726478
Determinant: 0.0692254
Determinant: -0.0256477
Determinant: 0.0702529
Determinant: -0.0052493
Determinant: 0.0625172
Determinant: 0.00282606
Determinant: 0.0229033
Determinant: 0.0558893
Determinant: 0.0766217
Determinant: -0.00388679
Determinant: -0.0193821
Determinant: -0.00718189
Determinant: -0.0864566
Determinant: 0.0809026
Determinant: -0.0398232
Determinant: -0.00224801
Determinant: 0.0333072
Determinant: -0.0212002
Determinant: 0.00371396
Determinant: 0.0162035
Determinant: -0.0811845
Determinant: 0.0148128
Determinant: 0.0372953
Determinant: 0.00351286
Determinant: -0.00103575
Determinant: 0.0384813
Determinant: 0.00752738
Determinant: -0.0248252
Determinant: -0.106768
Determinant: -0.0192333
Determinant: -0.026543
Determinant: -0.0222608
Determinant: -0.0487862
Determinant: 0.00376402
Determinant: -0.0329469
Determinant: 0.00266775
Determinant: 0.0762491
Determinant: 0.0159609
Determinant: -0.0190175
Determinant: -0.0338969
Determinant: -0.0631867
Determinant: -0.0238901
Determinant: 0.107709
Determinant: -7.74935e-05
Determinant: -0.0468996
Determinant: 0.0462787
Determinant: 0.0387825
Determinant: 0.0753388
Determinant: -0.000279933
Determinant: 0.00638663
Determinant: -0.00458034
Determinant: 0.0185849
Determinant: -0.00543503
Determinant: -0.0520309
Determinant: -0.0234638
Determinant: 0.0593986
Determinant: -0.00036774
Determinant: 0.00960819
Determinant: -0.00685314
Determinant: -0.000176925
Determinant: 0.0207583
Determinant: -0.0337003
Determinant: -0.0534818
Determinant: 0.0142158
Determinant: -0.0728077
Determinant: 0.0246877
Determinant: -0.0660952
Determinant: -0.0466
Determinant: 0.0915457
Determinant: -0.00340539
Determinant: 0.00815076
Determinant: -0.0751806
Determinant: -0.00617677
Determinant: 0.0019761
Determinant: -0.0016673
Determinant: 0.0310364
Determinant: 0.0483121
Determinant: -0.00664964
Determinant: 0.0659273
Determinant: -0.019015
Determinant: 0.0087627
Determinant: 0.0267279
Determinant: 0.0253497
Determinant: 0.00246292
Determinant: -0.0684746
Determinant: -0.0234524
Determinant: -0.0197933
Determinant: 0.0120796
Determinant: -0.0192703
Determinant: 0.0853956
Determinant: 0.0388196
Determinant: -0.0599305
Determinant: -0.0626148
Determinant: 0.0258541
Determinant: -0.0341273
Determinant: 0.0972889
Determinant: -0.0306585
Determinant: 0.0188553
Determinant: 0.00247702
Determinant: -0.00368989
Determinant: -0.0951982
Determinant: 0.0113578
Determinant: 0.000762509
Determinant: -0.0225219
Determinant: 0.0414059
Determinant: -0.0244409
Determinant: -0.0425728
Determinant: 0.04275
Determinant: -0.0413427
Determinant: -0.00556264
Determinant: -0.0894398
Determinant: -0.0193197
Determinant: -0.00788038
Determinant: -0.00455421
Determinant: -0.0788177
Determinant: 0.0415381
Determinant: -0.0346766
Determinant: -0.0748027
Determinant: 0.0087688
Determinant: -0.0968796
Determinant: 0.0683526
Determinant: -0.00996678
Determinant: 0.00955922
Determinant: -0.0914706
Determinant: 0.0728304
Determinant: 0.0541784
Determinant: 0.0457072
Determinant: -0.0299529
Determinant: -0.0096473
Determinant: -0.0142643
Determinant: -0.0684794
Determinant: 0.00281004
Determinant: -0.03252
Determinant: -0.0144637
Determinant: 0.0294154
Determinant: 0.00574353
Determinant: -0.019569
Determinant: 0.00492446
Determinant: -0.0526394
Determinant: -0.000870143
Determinant: -0.0180984
Determinant: -0.0144104
Determinant: 0.0456077
Determinant: -0.0113433
Determinant: 0.00377549
Determinant: -0.0775854
Determinant: -0.0336789
Determinant: -0.0744995
Determinant: -0.0427397
Determinant: 0.0300061
Determinant: -0.0326518
Determinant: -0.0333735
Determinant: -0.0284057
Determinant: -0.00999835
Determinant: -0.0380404
Determinant: 0.00648521
Determinant: 0.0449298
Determinant: 0.0120318
Determinant: -0.0230653
Determinant: -0.00934067
Determinant: -0.0175326
Determinant: -0.0799447
Determinant: 0.0679027
Determinant: -0.00670324
Determinant: -0.0841748
Determinant: 0.0236213
Determinant: 0.0386624
Determinant: -0.0239495
Determinant: 0.076976
Determinant: -0.00997484
Determinant: 0.025157
Determinant: -0.0654046
Determinant: 0.0090564
Determinant: 0.00129045
Determinant: -0.105119
Determinant: 0.0976925
Determinant: -0.105149
Determinant: -0.0465851
Determinant: 0.00237453
Determinant: -0.0456927
Determinant: 0.0328236
Determinant: -0.0914691
Determinant: -0.0157904
Determinant: -0.00170804
Determinant: -0.014797
Determinant: 0.00464912
Determinant: -0.035118
Determinant: -0.0242306
Determinant: 0.0081405
Determinant: 0.0733502
Determinant: -0.0860252
Determinant: -0.0511219
Determinant: -0.0925647
Determinant: 0.0495087
Determinant: -0.0515914
Determinant: -0.044318
Determinant: 0.000900043
Determinant: 0.0632521
Determinant: 0.00957955
Determinant: 0.00598059
Determinant: 0.0179513
Determinant: 0.0952263

dx,dy,dz is a is the distance between tcp and an object i want to keep in sight. The sphere is like a safety zone, but is mainly used to compute the orientation of the tool. 
","inverse-kinematics, c++"
Designing Ackerman's Steering Principle for an autonomous robot,"I am working on a high speed autonomous robot (about 6-7 m/s), which does obstacle detection as well as senses traffic lights (I have used Raspberry Pi 3 and Arduino Uno).  
For the steering mechanism, I wanted to implement an Ackerman's steering. I've read about the principle and have understood its basics. Now to actually make the design, I am currently using switchboards, sold here in India, they are surprisingly strong, lightweight, waterproof(they are switchboards) and cheap. Now I got 1 big axle and the small axle cut out already, along with the two L-shaped pieces that join the 2 axles together... I'm just now confused as to how to connect the wheels to the axle and how to make them rotate along side it. The site won't let me upload any pics right now, I'll try again ASAP.
I have the switchboard, an electric drill and will to do anything to make this happen ( ;P ). I don't have access to a 3D Printer.
Any help would be greatly appreciated...  
P.S- And if you have any suggestions of your own, which might be better for my robot, feel free to share them, I'm just looking for a good steering method for my robot.
","arduino, raspberry-pi, navigation, steering"
E: Unable to locate package ros-jade-desktop-full,"I want to install ROS on my Xubuntu 16.04, Xenial Xerus. I have followed the ROS's site instruction: http://wiki.ros.org/jade/Installation/Ubuntu, and did the following: First, setup my sources.list:
sudo sh -c 'echo ""deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main"" > /etc/apt/sources.list.d/ros-latest.list'
Second, set up keys:
sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116
Then, make sure my package is up-to-date: 
sudo apt-get update
Last, try to install ROS jade:
sudo apt-get install ros-jade-desktop-full
And get this error:
E: Unable to locate package ros-jade-desktop-full
Where did I go wrong, and how can I get ROS (any version is ok) running on my Xubuntu 16.04?
",ros
Quadcopter Flight Controller:Why does Using gyroscope data give better results?,"I have succeeded in making my first quadcopter from scratch with a readymade frame. I designed the flight controller myself with help from YMFC-3D youtube series of videos. https://www.youtube.com/watch?v=2pHdO8m6T7c
But in the process, I discovered that using the euler angles or the 'ypr' values from the MPU6050 as the feeback to the PID loop makes it super difficult to tune the quadcopter and even then it doesn't fly great. 
Whereas although not intuitive to me, using the gyroscope values with a complementary filter instantly made the quad respond much better and the tuning also was not too difficult.
Let me clearly define the response in both cases.
Using ypr values:-
+Always keeps overshooting or 'underreaching'
+Very small range of values that can let the quad fly stable
+Drastic Reactions to extreme values of P (Kp)values
Using gyro values:-
+Reaction is much more stable
+Tuning the PID was also simple
+ Even under high values of P(Kp) the quad might crash due to oscillations but not flip or react extremely
Below is a portion of the PID loop:
//gyrox_temp is the current gyroscope output

gyro_x_input=(gyro_x_input*.8)+(gyrox_temp*0.2);//complementary filter

pidrate_error_temp =gyro_x_input - setpoint;//error value for PID loop

pidrate_i_mem_roll += pidrate_i_gain_roll * pidrate_error_temp;
//integral portion

pidrate_output_roll = pidrate_p_gain_roll * pidrate_error_temp + pidrate_i_mem_roll + pidrate_d_gain_roll * (pidrate_error_temp - pidrate_last_roll_d_error);
//output of the pid loop
/pidrate_p_gain_roll-Kp
//pidrate_i_gain_roll-Ki
//pidrate_d_gain_roll-Kd
//this output is given as the pwm signal to the quad plus throttle

","quadcopter, pid, gyroscope"
How can a quadcopter be made to hover perfectly still?,"I need to get my drone flying still enough that I can rest a glass of water on it.
I've tried a few KK boards and APM 2.6 (3.1 software). I've balanced props, set PID settings, auto-trim / autotune and the drone still tends to inconsistently drift a little one way or another.
What is a plausible way to completely isolate drift?
",quadcopter
Quadcopter PID Control: Is it possible to stabilize a quadcopter considering only angle measurements?,"Good day,
I am a student currently working on an autonomous quadcopter project, specifically the stabilization part as of now. I am using a tuned propeller system and I also already considered the balancing of the quadcopter during component placements. I had been tuning the PID's of my quadcopter for the past 3 1/2 weeks now and the best I've achieved is a constant angle oscillation of the quadcopter by +-10 degrees with 0 degrees as the setpoint/desired angle. I also tried a conservative 7 degrees setpoint with the same results on the pitch axis.
As of now my PID code takes in the difference of the angle measurement from the complementary filter ( FilteredAngle=(0.98)*(FilteredAngle + GyroAngleVel*dt) + (0.02)*(AccelAngle) ) and the desired angle.

I have read somewhere that it is IMPOSSIBLE to stabilize the quadcopter utilizing only angle measurements, adding that the angular rate must be also taken into consideration. But I have read a lot of works using only a single pid loop with angle differences (Pitch Yaw and Roll) as the input. 
In contrast to what was stated above, I have read a comment from this article (https://www.quora.com/What-is-rate-and-stabilize-PID-in-quadcopter-control) by Edouard Leurent that a Single PID control loop only angle errors and a Cascaded PID loop (Angle and Rate) that utilizes both angle errors and angular velocity errors are equivalent Mathematically.
If I were to continue using only the Single PID loop (Angle) method, I would only have to tune 3 parameters (Kp, Ki & Kd).
But if I were to change my code to utilize the Cascaded Loop (Angle and Angular Velocity),

Would I have to tune two sets of the 3 parameters (Kp, Ki & Kd for angle and Kp, Ki & Kd for the angular velocity)?
Would the cascaded PID control loop give better performance than the single PID control loop?
In the Cascaded Loop, is the set point for the angular velocity for stabilized flight also 0 in deg/sec? What if the quadcopter is not yet at its desired angle?

Thank you :)
","control, quadcopter, pid, raspberry-pi, stability"
8 wheeled vehicle model,"I want the dynamic model for 8 wheeled robot. I expected to find it easily like the 4 wheeled bicycle model, but I couldn't.
Here is my effort only for rotation not for feedback.

From that I can calculate steering angle, but it was very messy to manage them all.
I need the model for controlling.
","wheeled-robot, dynamics, motion, robotc"
motor inertia tensor?,"In modeling dynamics of a robot ,in which servo motor is adjusted inside the link, there is a need to find inertia tensor of the motor itself,Right?

So if it is needed how can i get the inertia tensor of motor since i couldn't find its solid works model having internal components,i mean gears and other stuff(with related specified materials)?
","servomotor, dynamics"
"How can my robot find its position in any given map without GPS, including when the initial point is not given?","Consider this map

The Contest arena shown in figure 1 consists of two sub arenas, both the sides are identical to each other and their scientists and safe zone locations are similar.
Each sub arena has 3 different colored rooms and a fourth shared room. Each robot will be placed at identical start locations, respective to their arena. These locations will be random and anywhere on the map.
Each room (other than the shared room) will have two entry and exit gates. Both of these gates will be open at all times. The robot can enter and exit from any gate it chooses.
",localization
Single Touch Based Sensor and Odometry SLAM in Noisy Rectilinear Environment,"A Co-worker said he remembered a 2011(ish) ICRA (ish)  paper which used just a touch/bump sensor and odometry to do SLAM. I can't find it - I've paged thru the ICRA and IROS agendas and paper abstracts for 2010-2012, no joy. I found a couple of  interesting papers but my coworker says these aren't the ones he remembered.
Foxe 2012 http://staffwww.dcs.shef.ac.uk/people/C.Fox/fox_icra12.pdf
Tribelhorn a & Dodds 2007:  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.5398&rep=rep1&type=pdf
Background - I'm trying to make a Lego Mindstorms bot navigate and map our house. I also have a IR sensor, but my experience with these is that they are really noisy, and at best can be considered an extended touch sensor.
Regards, 
 Winton
",slam
What is the maximum payload weight for create 2/can I use old create accessories with the Create 2?,"I'm attempting to build a heavy platform on the Create 2 but am worried about weight on the platform.  What is the maximum weight for the platform and is there an optimum?
I have an old create and want to know if any of my existing cables and accessories can be used with the new Create 2? 
","mobile-robot, irobot-create"
How can I improve ZED Camera precision?,"I'm using Stereolabs ZED camera for my computer vision project. I did a small research about several sensors on the market and ultimately we decided to go with the ZED Camera. 
However I'm finding that the precision of the camera isn't that great. And the Point Cloud takes too much storage space. Anyone found the same problems? And if so, how did you managed them?
Thank you!
","computer-vision, stereo-vision"
Testbed for testing navigation algorithms,"I'm looking for a testbed (simulator or web-based interface that lets me to have control on a robot) for testing different routing and navigation algorithms. Is there such a system on the web?
","navigation, routing"
Quality check robot,"How to develop a robot based system to continuously monitor and check products for defeat which are moving on a conveyer belt using sensors and kick out the defect product from the queue?
",microcontroller
KUKA robot - update coordinates,"I need to develop something in order to update some coordinates in a KUKA KR C4 robot predefined program.
After some research I've found some ways to do it, but all of them non free.
I had several options, like developing a HMI in the console with 3 buttons, to touch up the 3 coordinates that I have to update for example.
Sending a XML file would work too but I need a RSI connection, and I can't do it without proper software (I guess).
Do you know about something like this? Or a C++ library that allows me to have access the .src/.dat files or to create a new one with the same ""body"" but with different coordinates?
Summing up, imagine that I have a conveyor that carries boxes and I need to develop a pick and place program. So far so good. But every 100 boxes, the size changes (and I can't predict it). So the operator goes and updates the coordinates, but I want to make sure that he won't change anything else in the program. Any ideas?
","robotic-arm, industrial-robot, kuka"
Is my servo fried?,"I got a new servo a few days back (RC Servo, Futaba FP-S148). I first tested it out with the Sweep sketch on Arduino, powering it with the Arduino 5v and GND pins only. It was working, just fine.
Today I was trying to use it in my robot and I tried powering it with 2 LiPo batteries (Samsung ICR16850 2200mAh, from an old laptop battery) connected in series, giving 8.32v. As soon as I connected my servo, it started rotating randomly, I had not connected it to my Arduino yet. I quickly took it out.
Next, I used a L7805 to get 5.13v regulated supply out of my batteries that I used earlier. When I connected my batteries to the servo, and the servo to the Arduino, uploaded the sketch, the servo started behaving rather strangely, it first did a complete turn and then stopped. Only a humming sound came from the servo. Strange thing is, whenever I connect one of my Multimeter leads to the power cables, the servo immediately turned in the opposite direction only as long as only lead was in contact with either the positive or negative wire.
Otherwise, the servo just gives a humming sound.
Have I fried my servo? Or is it some other issue?
UPDATE 1
I stripped down the servo and checked the motor. It is working fine, seems like this is a gear problem.
","arduino, battery, rcservo"
Implementing an analytic version of an inverse kinematic,"People have recommended me implement an analytic version of inverse Jacobian solver, such that I won't be forced only the least square solution, but would have an local area of solution near to the one I desire. 
I can't seem to implement it correctly, I mean how much does it differ from the least square inverse kinematics which I have implemented here?
Eigen::MatrixXd jq(device_.get()->baseJend(state).e().cols(),device_.get()->baseJend(state).e().rows());
      jq = device_.get()->baseJend(state).e(); //Extract J(q) directly from robot


      //Least square solver - [AtA]⁻1AtB

      Eigen::MatrixXd A (6,6);
      A = jq.transpose()*(jq*jq.transpose()).inverse();



      Eigen::VectorXd du(6);
      du(0) = 0.1 - t_tool_base.P().e()[0];
      du(1) = 0 - t_tool_base.P().e()[1];
      du(2) = 0 - t_tool_base.P().e()[2];
      du(3) = 0;  // Should these be set to something if i don't want the tool position to rotate?
      du(4) = 0;
      du(5) = 0;

      ROS_ERROR(""What you want!"");
      Eigen::VectorXd q(6);
      q = A*du;


      cout << q << endl; // Least square solution - want a vector of solutions. 

I want a vector of solution - how do I get that?
the Q is related to this https://robotics.stackexchange.com/questions/9672/how-do-i-construct-i-a-transformation-matrix-given-only-x-y-z-of-tool-position
The robot being used is a UR5  - https://smartech.gatech.edu/bitstream/handle/1853/50782/ur_kin_tech_report_1.pdf
","robotic-arm, inverse-kinematics, industrial-robot, c++"
Getting “rospack package not found error” in ROS,"I created a package in catkin workspace and put a publisher.py node inside the src directory of package which worked fine. Then i added another node subscriber.py node and used catkin_make to build. Now when I try to run any of the nodes or find package i am getting above error. Am I missing any step ?
Thanks.
",ros
"Is there a ""follow me"" Roomba/Create that works like a BEAMBot?","The diagram below shows an old BEAMBot strategy:
 
Is there code or an example using this method? I would rather avoid OpenCV, ultrasonic, GPS etc. I just want the Roomba wheels to react as I go straight, turn left or right. Finally, I could add a front wheel on a servo and try having the Roomba turn with me. 
Also has anybody added big, all terrain wheels to a Roomba to replace the originals?
","control, irobot-create, roomba"
How to determine the angles between a UAV and a sphere,"I have an UAV modeled in three dimensions with let's say position coordinates $p_{uav} = (x_1,y_1,z_1)$ that is moving in a direction $d = (d_x,d_y,d_z)$ and a moving obstacle modeled as a sphere with known centre coordinates $p_{sph}=(x_2,y_2,z_2)$ and radius $ r_{sph}$. 
If I have a plane $p$  in the direction of movement of the UAV that intersects the sphere, I want to be able to calculate the angles with respect to the vehicle's movement formed by the tangents to the sphere in the plane $ p$. In the figure, I would like to know how to calculate the angles $α_1$ and $α_2$.

If it helps, what I am looking is an extension in three dimensions for this:

Which is a vehicle in two dimensions ;it is obviously an easier problem which requires only the centre of the circle. However I am not really sure how to make it work in 3D, as supposedly the plane can intersect the sphere at any two points, not necessarily the centre. 
Thanks in advance for your help.
","localization, kinematics, geometry"
What frames are supported by the Dynamixel XL-320 OLLO?,"I was recently looking into purchasing either a Dynamixel AX-12A or XL-320. The XL seems to use OLLO frames, which only seem to be available in a toy-like set. 
I was wondering if there are any other frames available or if I should just get an AX-12?
","servos, walking-robot, dynamixel"
"Best sensor to determine ""up"" versus ""down""","I want to start designing an Arduino project and have telemetry readings that indicate tilt or angle of placement. 
Would an accelerometer be the best for determining tilt? Are there good tutorials? 
","mobile-robot, arduino"
Choosing the right Mecanum wheel,"I am part of my college robotics team which is preparing for Robocon 2017.
We have used Mecanum wheels in last Robocon competition, but we have faced huge slip and vibration. I have looked for all kinematic and dynamic formulas and all stuff about Mecanum wheels, but still can't get to a conclusion for my problem.
Video of the problem
The robot is around 25 kg and the Mecanum wheel diameter is about 16 cm with 15 rollers (single type). Please help me why it happened like that!?
Also suggest me what to do now - Should I design a new Mecanum wheel or bring it from market? 
If I should design, what parameters should I consider, and please help me how to design in CAD software like SolidWorks? And then, shall I give it to 3D printing?
If I should buy directly from market, where should I buy?
","design, wheel"
Most accurate rotation representation for small angles,"Assume that I have a rigid body for which I know that it can rotate with respect to a global reference frame (which is considered fixed and already given) for only a few degrees of angle, so I can describe its rotation by using the small angle approximation. For this system, I would like to know if there is a rotation representation that offers more accuracy when compared with other representation methods.
The main representation methods that I considered are the euler angles and the pitch-yaw-roll transformation. To my perception, I think that pitch-yaw-roll representation is expected to be more accurate, since all the angles are expressed with respect to the initial coordinate frame. On the other hand, euler angles are defined on different frames, so I am not sure if the resulting angles will be really small.
To sum up, I know that the body can rotate for only a few degrees and I would like to know which coordinate representation is much probable to deliver the smallest angles, such that the small angle approximation is more valid.
It could also be the case that there is not a general answer (so it depends on the specific configuration) but still I haven't found anything about this topic on the related literature!
Example (no small angle approx used): Assume I have a coordinate frame which describes a point in space by the following vector
$P2=\begin{bmatrix} 4 \\ 1 \\ 0.05 \end{bmatrix}$.
Given another coordinate frame which is rotated with respect to the previous one, the description of the same point is given by     
$P1=\begin{bmatrix} 3.8933 \\
    1.3566 \\
   -0.0630 \end{bmatrix}$.
Using Euler angles, I can find that the rotation matrix $R_{euler}$ is characterized by the angles $0.1,0.2,0.1$ rads, which correspond to the angle of rotation around z axis, the rotation around the resulting y axis and the rotation around the resulting z axis, respectively (these are basic stuff, it is explained in many books.). So I have that $P1=R_{euler} P2$.
Now I want to find the corresponding rotation matrix if I use the pitch-yaw-roll representation. Here I have to solve an optimization problem and the solution that I get (maximum error between P1 and the estimated P1 is $3 \times 10^{-8}$) delivers me the following angles
$\begin{bmatrix}  -0.0103   \\ 0.0257  \\  0.0902\end{bmatrix}$,
which correspond to the rotation around the x,y and z axis of the initial coordinate frame. 
","inverse-kinematics, geometry, rotation"
Performing inverse kinematics based on a displacement of the end effector?,"I think i have an simple problem, but can't my head around how i should resolve it...
My setup looks like this: 


The grey box on end effector is supposed to be an camera, which measures a dx,dy,dz between the object and the camera. These are used to  position the camera such that dz between the object and the camera is equal to 0.5, and dx = dy = 0. 
I know that I using inverse kinematics can determine the Q which positions it according the given rotation and position, but what if I only provide it a position only?
How do extract all Q that make dx = dy = 0, and dz = 0.5, while keeping the object in sight at all time?
An example could be if an object was placed just above the base (see second image), it should then find all possible configurations which in this case would consist of the arm rotating around the object, while the camera keeps the object in sight...
Update
I just realized a possible solution would be to create a sphere with the object in centrum  an radius of dz, and then use this sphere to extract all pairs of rotations and position... But how would one come by with such an solution?
","robotic-arm, inverse-kinematics, stereo-vision"
Suitable uC for atonomous robot,"I am going to build an autonomous robot with Kalman-filter for localization integrated from Lidar, encoder, IMU and GPS. I will also add obstacle avoidance while moving to the required position.
Is the ATmega32 8-bit suitable for that (or Arduino Mega) or do I have to use avr32, ARM, or PIC32 and which is better?
","mobile-robot, arduino, localization, microcontroller"
What type of actuator should I use?,"I need to find out if There is a way to get at least 60 Hz of linear Motion with at least 5 mm of stroke that I intend to make linear persistence of vision device(not rotating one)It must be small and light as possible. ( maybe 50 mm long and 10-15 mm diameter or around these) (less than 500 grams) The Load will be around 50 grams.  There are voice coils that is very expensive, can I use solenoids for instance or what do you recommend? 
Thanks 
","actuator, motion"
How can I calculate processing speed of microcontroller,"I need a microcontroller that can process minimum 2mb data per second.
How do I determine what processors will be able to do this?
Also how can I calculate the processing speed in per second of any microcontroller?
I am very much scared with my college project and I need help.
",microcontroller
How to know the payload of the chassis from its motors?,"I'm doing a mobile robot project with robotic arms, I wanted to buy a chassis for my robot that can carry enough weight, but many websites don't give definitive answers about maximum payload.
Is there is a way to figure this out just by knowing details about the motors?
",mobile-robot
How do we write a STOP to a continuous Servo?,"I'm using processing to send strings to Arduino, using functions like 
else {
    int u=90;
    port.write(u+""z"");
}

on the processing side and in the Arduino side I'm using calls like 
  case 'z':
    z.write(v);
    v = 0;
    break;
  case 'L':
    z.write(0);
    //v = 0;
    break;
}

yet I can't get the servo to stop at all. How do I make it shut off?
If it was a regular servo I wouldn't even ask because that's easy but I write 0 or 90 or LOW and nothing, it just keeps spinning in one direction but when it meets one of the conditions in my statements it switches polarity/direction and that's good - I want that but I made this function to make it stop and it is not doing so, does anyone have any ideas ?
I am using a Parallax Continuous Rotation Servo.
",arduino
Dynamic Model of a Manipulator,"I'm stuck on equation 4.30 of page 176 in
http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf
This equation:
$\frac {\partial M_{ij}} {\partial \theta_k} = \sum_{l=\max(i,j)}^n \Bigl( [A_{ki} \xi_i, \xi_k]^T A_{lk}^T {\cal M}_l' A_{lj} \xi_j + \xi_i^T A_{li}^T {\cal M}_l' A_{lk} [A_{kj} \xi_j, \xi_k] \Bigr)$  
seems impossible to process because it requires adding a 2x1 to a 1x2 matrix.
going by ROWSxCOLUMNS notation. Matrices M and A are 6x6 and $\xi$ is a 6x1, so how does this addition statement fit the rules of matrix addition?  This must be my mistake, I just don't see how.
","dynamics, matlab"
Bluetooth module HC-05 giving ERROR :(0),"I am working right now with Arduino UNO and HC-05 bluetooth module.I followed the instruction given on this link for wiring. So there are 2 mode of working with this HC-05 module

Simple serial communication
Working in AT command mode so as to change the parameters of HC-05 module

As long as I work in simple serial communication mode, everything works fine but when I tried to change the parameters of module, it didn't work out. For working in At command mode, PIN NO 34 of HC-05 module needs to be high which I had taken care of. Lately I found that in mu module they had knowingly not connected the Berg strip to PIN 34, so I connected the PIN directly, even though I am not able to change the parameters of module and when I write any command on COM port of arduino IDE, I get this response
Enter AT commands:
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^
ERROR:(0)
ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿõÿýì¢^

I think that garbage is due to my code
Here is my code:
#include <SoftwareSerial.h>

SoftwareSerial BTSerial(10, 11); // RX | TX

void setup()
{
  pinMode(9, OUTPUT);  
  digitalWrite(9, HIGH);
  Serial.begin(9600);
  Serial.println(""Enter AT commands:"");
  BTSerial.begin(38400);  
}

void loop()
{
  uint8_t x;
  char CommandFromSerial[50]="" "";
  char ResponseFromBluetooth[50]= "" "";

  if ((Serial.available())){
    if(Serial.available()>0){
      for(x=0;x<50;x++)
        CommandFromSerial[x]=Serial.read();
      BTSerial.println(CommandFromSerial);
    }
  }

  if ((BTSerial.available())){
    if(BTSerial.available()>0)
      for(x=0;x<50;x++)
        ResponseFromBluetooth[x]=BTSerial.read();
    Serial.println(ResponseFromBluetooth);
  }
}

I am not able to figure out what I am doing wrong. I used this command on COM port 
AT\r\n and many other commands but every time I get the same response.
Did I mess up with my bluetooth module unknowingly?
","arduino, c, communication, serial"
What rating Li-Po battery should I get for this configuration?,"I will have this configuration: 

A2212 Brushless Motor 1000KV - 4 each
ECS - 30A Electronic Speed Control (ESC) - 4 each 
Propeller - 1045 Propeller CW & CCW Pair 10 inch * 4.5 pitch
Arduino Mega - 2560 board
Raspberry Pi 3 
Open pilot CC3D flight controller 

I want to know what rating Li-Po battery should I get for this configuration.
The reason behind my asking here is because a simple google search is not able to satisfy me with an explanation...
Also, my weight will be 1.5 kg for the quadcopter, so I need a stable current discharge.
This is my first quadcopter, I am a Computer Science guy, so I have little knowledge of electronics, I'm learning, but need help...
","quadcopter, arduino, raspberry-pi, battery"
What's the difference between a holonomic and a nonholonomic system?,"I was wondering if a 1D point mass (a mass which can only move on a line, accelerated by an external time-varying force, see Wikipedia - Double integrator) is a holonomic or a nonholonomic system? Why?
I think that it is nonholonomic since it cannot move in any direction in its configuration space (which is 1D, just the $x$ axis). E.g. if the point mass is moving at $$x=10$$ with a velocity of 100 m/s in positive $x$-direction it cannot immediately go to $$x=9.9$$ due to its inertia. However, I have the feeling that my thoughts are wrong...
The background is the following:
I am trying to understand what holonomic and nonholonomic systems are. What I found so far:
Mathematically:

Holonomic system are systems for which all constraints are integrable into positional constraints.
Nonholonomic systems are systems which have constraints that are nonintegrable into positional constraints. 

Intuitively:

Holonomic system where a robot can move in any direction in the configuration space. 
Nonholonomic systems are systems where the velocities (magnitude and or direction) and other derivatives of the position are constraint. 

","dynamics, movement"
Depth map with Raspberry Pi,"Is it possible to get two images from the Raspberry Pi camera mounted on a remote controlled bot and have them sent to a computer through Wi-Fi and process the images in the computer to generate a depth map?
All this is to be done in a very short time so that the robot can be helped with its locomotion without making it completely autonomous.
",raspberry-pi
Forward kinematics with DH parameters,"I just started learning robotics at school and I have some problems to solve forward kinematics with DH parameters. I don't really understand how I can get them from the image. I would appreciate if somebody could help me with it.
. 
","forward-kinematics, dh-parameters"
Quadcopter refuses to fly when the Yaw PID component is added,"Good day,
I would like to ask why is it that when I add the Yaw control to my PID controller for each motor. The quadcopter refuses to take off or maintain its altitude. I am curently using a Cascaded PID controller for attitude hold using an Accelerometer, a Magnetometer and a Gyroscope, and a 40Hz Ultrasonic Sensor for Altitude Hold. Since the scope is indoor I have done away with the barometer due to its +-12m error. 
Resulting Response
Without Yaw Control, the plot below shows the response of the quadrotor.

With Yaw Control, the plot below shows the response of the quadrotor.

Debugging
I found out that each of the outputs from each PID's give a too high of a value such that when summed together goes way over the PWM limit of 205 or Full Throttle.

Without yawPID contribution
The limiter kicks in without damaging the desired response of the system thus is still able to fly albeit with oscillatory motion along the z axis or height



With yawPID contribution
The added yaw components increases the sum of the PID's way above the limit thus the limiter compesates the excess too much resulting in an over all lower PWM output for all motors thus the quad never leaves the ground.



//Motor Front Left (1)
float motorPwm1 =  pitchPID + rollPID + yawPID + baseThrottle + baseCompensation;
//Motor Front Right (2)
float motorPwm2 =  pitchPID - rollPID - yawPID + baseThrottle + baseCompensation; 
//Motor Back Left (3)
float motorPwm3 = -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation; 
//Motor Back Right (4)
float motorPwm4 = -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation;

Background
The PID parameters for the Pitch, Yaw and Roll were tuned individually meaning, the base throttle was set to a minimum value required for the quadcopter to be able to lift itself.
The PID parameters for the Altitude Sensor is tuned with the other controllers active (Pitch and Roll).
Possible Problem

Limiter algorithm

A possible problem is that the algorithm I used to limit the maximum and the minimum throttle value may have caused the problem. The following code is used to maintain the ratio of the motor values instead of limiting them. The code is used as a two stage limiter. In the 1st stage, if one motorPWM is less than the set baseThrottle, the algorithm increases each motor PWM value until none of them are below that. In the 2nd stage, if one motorPWM is more than the set maxThrottle, the algorithm decreases each motor PWM value until none of them are above that. 
//Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 

float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
float minPWM = motorPWM[0];
int i;
for(i=0; i<4; i++){ // Get minimum PWM for filling
    if(motorPWM[i]<minPWM){
        minPWM=motorPWM[i];
    }
}

cout << "" MinPWM = "" << minPWM << endl;

if(minPWM<baseThrottle){
    float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
    cout << "" Fill = "" << fillPwm << endl;
    motorPwm1=motorPwm1+fillPwm;
    motorPwm2=motorPwm2+fillPwm;
    motorPwm3=motorPwm3+fillPwm;
    motorPwm4=motorPwm4+fillPwm;
}

float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
float maxPWM = motorPWM2[0];
for(i=0; i<4; i++){ // Get max PWM for trimming
    if(motorPWM2[i]>maxPWM){
        maxPWM=motorPWM2[i];
    }
}

cout << "" MaxPWM = "" << maxPWM << endl;

if(maxPWM>maxThrottle){
    float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
    cout << "" Trim = "" << trimPwm << endl;
    motorPwm1=motorPwm1-trimPwm;
    motorPwm2=motorPwm2-trimPwm;
    motorPwm3=motorPwm3-trimPwm;
    motorPwm4=motorPwm4-trimPwm;
}

This was obtained from pixhawk. However the difference is that they employ only upper bound compensation limiting, while mine also performs lower bound compensation limiting which may cause more saturation once it reaches the second stage.

From:https://pixhawk.org/dev/mixing

Gains are set too high.

It is also possible that I've set my P gains too high thus exceeding the max RPM limit of the motors causing the Limiter algorithm to overcompensate.
Current PID Settings:
The minimum motor value for the quad to lift itself is 160 while the maximum limit is 200 from the PWM time high of 2000ms 

Pitch (Cascaded P-PID controller)
Rate P = 0.07
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2
Roll (Cascaded P-PID controller)
Rate P = 0.09
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2
Yaw (Cascaded P-PID controller)
Rate P = 0.09
Rate I = 0.03
Rate D = 0.0001
Stabilize P = 2
Hover (Single loop PD controller)
P = 0.7
D = 35

Possible Solution
I think I have set the PID parameters particularly the P or D gain too high that the computed sum of the outputs of the controller is beyond the limit. Maybe retuning them would help.

I would just like to ask if anyone has encountered this problem or if you have any suggestions. Thank you :) 

EDIT
I have added the plots of the response when the control loop is fast (500Hz) and Slow (300Hz)

500Hz: Does not fly
  
300Hz: Flies
  

","quadcopter, control, pid, raspberry-pi, stability"
Robotic Arm analysis in Matlab/simulink,"I am going through a paper, Kinematic Modelling and Simulation of a 2-R Robot Using SolidWorks and Verification by MATLAB/Simulink, which is about a 2-link revolute joint robotic arm. According to the paper, the trajectory analysis of the robot was done via simulations in MATLAB/Simulink. 
It shows the following picture, Trajectory generation of 2‐R robot with MATLAB/Simulink:

and then, Simulink - Simulation block to calculate the trajectory:

I think this is done in SimMechanics, but I am not sure. Experienced users, can you please tell me what am I looking at and how can I reproduce this?
","robotic-arm, matlab, simulation"
inverse kinematics for 6 jointed robots,"I am a  uncertain about how to compute the right homogeneous transformation matrix to compute an inverse kinematic Q-configuration. 
Looking at robot like this 
Where at the end of this robot I have a camera mounted on to it. 
The purpose of my application is to make the robot follow an object, so basically tracking it.  The camera provide me with an X,Y,Z coordinate, which the position i want place my robot arm. 
First question - How do i set up the desired homogenous transformation matrix?
The way i see it, I have 2 transformation matrices being T_tool_base and T_world_tool which become T_world_base = (T_tool_base) (T_world_tool)
My question is that how do i compute my desired transformation matrix. 
I think i know how i should setup the transformation matrix for the camera which would be like this
T_world_tool = 0 0 0 x
               0 0 0 y
               0 0 0 z
               0 0 0 1
(Second question is regarding the rotation matrix, how do prescribe such that rotation in arbitrary as long the endpoint has the desired position in the world frame?)
but what should t_tool_base entail? should it entail the transformation of its current state or the desired transformation, and if so how do i extract the desired t_tool_base transformation?...
","robotic-arm, kinematics, inverse-kinematics"
Electric vs. internal combustion engine for propulsion,"What are the main differences between electric motor and internal combustion engine for an ATV-sized mobile robot platform in terms of functionality, implementation difficulty (""RC"" conversion, ""electronic"" operation), durability and maintenance when used as an autonomous platform? A full sized ATV/UTV like Polaris Ranger (EV) is in question.
Are the advantages/disadvantages basically the same as the differences between electric and nitro RC cars or does the bigger scale adds something important to the game? I can think of the main differences like bigger range and faster ""refueling"" with IC and less maintenance with electric but I am interested in a detailed comparison.
The transmission for the IC engine is considered to be automatic.
EDIT: The fuel injection for IC is considered to be electronic (EFI) but I do not know whether that also means the ""electronic"" throttle (no mechanical wire as with carburetor?). Whatever the throttle may be I see the lag between its ""actuation"" and the engine running into higher RPM and giving more power/speed as the main disadvantage for IC control - however, it may probably be quite easy dealt with in software (by adding some timeout when checking desired RPM).
","mobile-robot, electronics, engine, electric"
Quaternion Kalman Filter Algorithm,"I have been stuck on this for weeks, I really hope that someone can help me with this,thank you in advance.
I am trying to write an IMU attitude estimation algorithm using quaternion kalman filter. So based on this research paper: https://hal.archives-ouvertes.fr/hal-00968663/document, I have developed the following pseudo code algorithm:
Predict Stage:
Qk+1/k = Ak * Qk;  where Ak contains the gyro measurement. 

Pk+1/k = Ak * Pk *Ak.transpose() + Q; where Q is assumed to be zero.
After prediction, we can use this formula to get the supposed gravity measurement of accelerometer Yg in body frame :
Yg = R * G;  // R is the rotation matrix generated from quaternion Qk+1/k and G = (0,0,0,9.81).
This equation then translates to the following equation which allows me to get measurement model matrix H.
H * Qk+1/k = 0; //where H stores value related to (Yg-G).
Update Stage:
K = P * H * (H * P * H.transpose()+R)^(-1); //R should be adaptively adjusted but right now initialized as identity matrix
Qk+1/k+1 = (I-KH)Qk+1/k;
Qk+1/K+1 = (Qk+1/K+1)/|Qk+1/k+1|; //Normalize quaternion
Pk+1/K+1 = (I - KH)Pk+1/k;
The following is the main part of my code. The complete C++ code is at here https://github.com/lyf44/fcu if you want to test.
Matrix3f skew_symmetric_matrix(float a, float b, float c, float d){
    Matrix3f matrix;
    matrix << a,d*(-1),c,
              d,a,b*(-1),
              c*(-1),b,a;
    return (matrix);
}

void Akf::state_transition_matrix(float dt,float gx,float gy, float gz){
    Vector3f tmp;
    tmp(0) = gx*PI/180;
    tmp(1) = gy*PI/180;
    tmp(2) = gz*PI/180;
    float magnitude = sqrt(pow((float)tmp(0),2)+pow((float)tmp(1),2)+pow((float)tmp(2),2));

    /*q(k+1) = |  cos(|w|*dt/2)       | quaternion_multiply q(k)
               |  w/|w|*sin(|w|*dt/2) |
    */
    //w/|w|*sin(|w|*dt/2)
    tmp = tmp/magnitude*sin(magnitude*dt/2);
    //quaternion multiplication
    A(0,0) = cos(magnitude*dt/2);
    A.block<3,1>(1,0) = tmp;
    A.block<1,3>(0,1) = tmp.transpose()*(-1);

    Matrix3f skew_symmetric;
    skew_symmetric = skew_symmetric_matrix((float)A(0,0),(float)tmp(0),(float)tmp(1),(float)tmp(2));
    A.block<3,3>(1,1) = skew_symmetric;
}

void Akf::observation_model_matrix(Vector3f meas){
    Vector3f G;
    Vector3f tmp;
    G << 0,0,9.81;
    /* H = | 0        -(acc-G).transpose     |
     *     | (acc-G)  -(acc+G).skewsymmetric |
     */
    tmp = meas-G;
    H(0,0) = 0;
    H.block<3,1>(1,0) = tmp;
    H.block<1,3>(0,1) = tmp.transpose()*(-1);
    tmp = tmp+G+G;
    Matrix3f matrix;
    matrix = skew_symmetric_matrix(0,(float)tmp(0),(float)tmp(1),(float)tmp(2));
    H.block<3,3>(1,1) = matrix*(-1);
    //H = H*(0.5);
    cout<<""H""<<endl;
    cout<<H<<endl;
    cout<<""H*X""<<endl;
    std::cout<<H*X<<std::endl;
}

void Akf::setup(){
    X_prev = Vector4f::Zero(4,1);
    X_prev(0) = 1;
    Q = Matrix4f::Zero(4,4);
    Z = Vector4f::Zero(4,1);
    R = Matrix4f::Identity(4,4);
    P_prev = Matrix4f::Identity(4,4);
    P_prev = P_prev*(0.1);
    I = Matrix4f::Identity(4,4);

    sum = Vector4f::Zero(4,1);
    noise_sum = Matrix4f::Zero(4,4);
    counter=1;
}

void Akf::predict_state(){
    cout<<(60*counter%360)<<endl;
    X = A*X_prev;
    A_T = A.transpose();
    P = A*P_prev*A_T+Q;
}

void Akf::update_state(){
  Matrix4f PH_T;
  Matrix4f tmp;

  PH_T = P*H.transpose();
  S = H*PH_T+R;
  if (S.determinant()!= 0 )
  {
      tmp = S.inverse();
      K = P*H*tmp;
      //std::cout<<""K""<<std::endl;
      //std::cout<<K<<std::endl;
      X_updated = (I-K*H)*X;
      X_updated = X_updated /(X_updated.norm());
      P_updated = (I-K*H)*P;
  }
  else{
      X_updated = X;
      std::cout<< ""error-tmp not inversible!""<<std::endl;
  }
  X_prev = X_updated;
  P_prev = P_updated;
}

void rotation_matrix(Vector4f q,Matrix3f &rot_matrix){
    int i;
    for (i=1;i<4;i++){
        q(i) = q(i)*(-1);
    }
    Matrix3f matrix;
    matrix(0,0) = pow((float)q(0),2)+pow((float)q(1),2)-pow((float)q(2),2)-pow((float)q(3),2);
    matrix(0,1) = 2*(q(1)*q(2)-q(0)*q(3));
    matrix(0,2) = 2*(q(0)*q(2)+q(1)*q(3));
    matrix(1,0) = 2*(q(1)*q(2)+q(0)*q(3));
    matrix(1,1) = pow((float)q(0),2)-pow((float)q(1),2)+pow((float)q(2),2)-pow((float)q(3),2);
    matrix(1,2) = 2*(q(2)*q(3)-q(0)*q(1));
    matrix(2,0) = 2*(q(1)*q(3)-q(0)*q(2));
    matrix(2,1) = 2*(q(0)*q(1)+q(2)*q(3));
    matrix(2,2) = pow((float)q(0),2)-pow((float)q(1),2)-pow((float)q(2),2)+pow((float)q(3),2);
    rot_matrix = matrix;
}

Vector3f generate_akf_random_measurement(Vector4f state){
    int i;
    //compute quaternion rotation matrix
    Matrix3f rot_matrix;
    rotation_matrix(state,rot_matrix);
    //rot_matrix*acceleration in NED = acceleration in body-fixed frame
    Vector3f true_value = rot_matrix*G;
    std::cout<<""true value""<<std::endl;
    std::cout<<true_value<<std::endl;
    for (i=0;i<3;i++){
        noisy_value(i) = true_value(i) + (-1) + (float)(rand()/(float)(RAND_MAX/2));
    }
    return (noisy_value);
}

int main(){
      float gx,gy,gz,dt;
      gx =60; gy=0; gz =0; //for testing, let it rotate around x axis by 60 degree  
      myakf.state_transition_matrix(dt,gx,gy,gz); // dt is elapsed time
      myakf.predict_state();
      Vector4f state = myakf.get_predicted_state();
      Vector3f meas = generate_akf_random_measurement(state);
      myakf.observation_model_matrix(meas);
      myakf.measurement_noise();
      myakf.update_state();
      q = myakf.get_updated_state();

The problem that I face is that my code does not work.The prediction stage works fine but the updated quaternion state is only correct for the first few iterations and it starts to drift away from the correct value. I have checked my code against the research paper multiple times and ensured that it is in accordance with the algorithm proposed by the research paper.
In my test, I am rotating around x axis by 60 degree per iterations. The number below the started is the angle of rotation. state and updated state is the predicted and updated quaternion respectivly while true value, meas, result are acceleration due to gravity in body frame.As the test result indicates, everything is way off after rotating 360 degrees.
The following is my test result:  
1
started
60
state
0.866025
0.5
0
0
true value
0
8.49571
4.905
meas
0.314533
7.97407
4.98588
updated state
0.866076
0.499913
-2.36755e-005
1.56256e-005
result
0.000555564
8.49472
4.90671

1
started
120
state
0.500087
0.865975
-2.83164e-005
1.69446e-006
true value
0.000306622
8.4967
-4.90329
meas
-0.532868
8.79841
-4.80453  
updated state
0.485378
0.862257
-0.129439
-0.064549
result
0.140652
8.37531
-5.10594

1 
started
180
state
-0.0107786
0.989425
-0.0798226
-0.12062
true value
-2.35843
-0.0203349
-9.52226
meas
-1.39627
-0.889284
-8.74243
updated state
-0.0195091
0.981985
-0.151695
-0.110965
result
-2.19598
-0.0456112
-9.56095 

1
started
240
state
-0.507888
0.840669
-0.0758893
-0.171946
true value
-3.59229
-8.12105
-4.16894
meas
-4.52356
-7.73113
-4.98735
updated state
-0.53758
0.811101
-0.212643
-0.0889171
result
-3.65783
-8.18397
-3.98485

1
started
300
state
-0.871108
0.433644
-0.139696
-0.183326
true value
-3.94732
-6.909
5.73763
meas
-4.36385
-6.98853
5.39759
updated state
-0.86404
0.436764
-0.102296
-0.228487
result
-3.69216
-6.94565
5.86192
1
started  
0
state
-0.966663
-0.0537713
 0.0256525
 -0.249024 
true value
0.749243
0.894488
9.74036
meas
-0.194541
0.318586
10.1868
updated state
-0.78986
-0.0594022
0.0311688
-0.609607
result
1.1935
0.547764
9.72171 

1
started
60
state
-0.654338
-0.446374
0.331797
-0.512351
true value
8.74674
2.39526
3.74078
meas
9.36079
2.96653
3.57115
updated state
-0.52697
-0.512048
0.221843
-0.64101
result
8.73351
2.50411
3.70018

Can someone help me confirm that my understanding about the theory of this quaternion kalman filter and my pseudo code is correct? Also, if anyone has implemented attitude estimation using maybe a different version of quaternion kalman filter, I would greatly appreciate if you can provide a pseudo code and a little explanation.
Thank you guys very much!
","quadcopter, kalman-filter"
Where I can learn algorithms or and find examples of code for controlling a rover?,"I am a programmer by profession and new to Robotics. I have studied ECE, so know electronics, but not very familiar with mechanical aspects of robotics. I am working on a learning project with Dagu Rover 5 platform.
I am trying to control the 4 DC motors with PWM and want to use the optical encoders for feedback. I am looking for some algorithms, example code in C to effectively control the rover. I know how to control the GPIO, PWM and interrupts from the processor. I am more interested in learning the algorithm that controls the motors based on this. For now, i am working on a manual robot, controlled with up/down/left/right keys. In future, I would like to add sensors, camera etc and work on autonomous aspects. Any pointers would be helpful.
For reference, I am working on the Raspberry Pi platform to control the rover.
","mobile-robot, algorithm, pwm, c"
PID tuning with methods like GA and PSO,"I have recently started reading about PID tuning methods and algorithms, and I encountered the particle swarm optimization algorithm and genetic algorithm.
The problem is, that I don't understand how each particle/chromosome determines his fitness. On real physical system, each particle/chromosome checks his fitness on the system? Wouldn't it take a really long time? I think that I am missing something here... Can those algorithms be implemented on an actual physical system? If so, then how?
","pid, algorithm"
Motor upgrade to higher torque?,"I have assembled a 4WD car using kits I bought on ebay.
I have 4 motors similar to this one: .
The description says:


Operating voltage: 3V~12VDC 
  (recommended operating voltage of about 6 to 8V)
Maximum torque: 800gf cm min (3V)
No-load speed: 1:48 (3V time)
The load current: 70mA (250mA MAX) (3V)
This motor with EMC, anti-interference ability. 
  The microcontroller without interference.
Size: 7x2.2x1.8cm(approx)


I am not too fond of the max speed I can reach, but I would be able to provide more power, because I have a 12V 2A battery onboard.
So far I have used 6V, because that seemed to be the safer voltage choice.
Has anybody tried successfully higher voltages, without wearing down the motor in few hours (I've read this can happen)?
Alternatively, can someone recommend replacement motors that would tolerate reliably a higher power envelope?
I would like to preserve the gearbox and replace only the motor, if possible.
I think I could fit a motor 2-4 mm longer (replacing the transparent strap which bonds it to the gearbox), if that makes any difference.
BTW, I'm making the assumption:
higher_voltage => higher_torque => higher_speed
but I'm not sure it's overall correct.
I expect that it would at least produce higher acceleration during the transients.
",brushless-motor
Quadcopter heading calculation,"I'm working on an autonomous quad copter, I have two GPS co-ordinates (source and destination co-ordinates). I need to move my quad from the source to the destination, for this I need to calculate the heading and set the yaw value of my quad. How can I calculate the heading and make sure the quad is headed in the right direction as the target co-ordinates?
If I use magnetometer the declination angle will vary from place to place and so I will have to keep changing the declination angle. If I'm calculating based on just the GPS co-ordinates, it's not accurate. 
What is the best way to do this?  How do I calculate the above? 
","quadcopter, gps, magnetometer"
ComputeShortestPath() in Dstar lite algorithm,"In optimized D*Lite algorithm as shown in the figure below (page 5, of the paper D*Lite), when the procedure ComputeShortestPath() is called for the first time in line 31, U(list of inconsistent vertices) contains only goal vertex ($s_{goal}$). Thus in the procedure ComputeShotestPath()(line 10-28), $u = s_{goal}$. And as, $k_{old}=k_{new}$ (because $k_m=0$), condition $k_{old}\leq k_{new}$ is satisfied and $u = s_{goal}$ is again inserted in U with same value of $k_{old}=k_{new}$. Thus, it seems that line(11-15) will run forever, and the algorithm will not be able to find the shortest path from goal to start.
I know that this algorithm has been widely used and I am failing to understand it. But where am I going wrong? 

","mobile-robot, control, robotic-arm, motion-planning, algorithm"
Self Powered Quadcoptor,"I have this idea or a very curious question in my mind. I am no where near professional though, but i would like it to be answered. 
We all know how wind turbines can be used to generate electricity. So is it possible to create a quadcoptor that will start with some minimal power by small battery but in time will sustain and keep its system on by self generating electricity and keep on rotating its rotors on its own without other external supply? 
",quadcopter
Hand-eye calibration?,"I am having an issue with some hand-eye calibration. 
So i am using a simple robot which at its tool point has an stereo camera mounted on it. 
I want to perform some visual serving/tracking based stereo images extracted from the camera in the ""hand"". The camera provides me x,y,z coordinates of the object I want to track. 
I can at all time extract an homogenous transformation matrix from base to tool (not cam) as (T_tool_base). 
Firstly... I guess i would need perform some form of robot to (vice versa) camera calibration, My idea was that would consist of something like this 
T_base_world = (T_base_tool) (T_tool_cam) (T_cam_world) 

Where the T_tool_cam would entail the calibration... since the camera is at the tool point, would that entail the T_tool_cam should entail information on how much the camera is displaced from the tool point, and how it is rotated according to the tool point? or is not like that?
secondly... How do i based purely x,y,z coordinate make an homogeneous transformation matrix, which includes an rotation matrix ?
thirdly.. Having a desired Transformation matrix which in theory this     
T_base_world = (T_base_tool) (T_tool_cam) (T_cam_world) 

would provide me, would an inverse kinematics solution provide me with one or multiple solution?... In theory should this only provide me one, or what?
","robotic-arm, inverse-kinematics, rotation"
Meaning of s_last in D star Lite algorithm,"In the D*Lite algorithm, described in line 21 of Figure 3, on page 4, in D* Lite, the main() starts with defining $s_{last}=s_{start}$. But value of $s_{last}$ is never updated in the entire algorithm. 
So what is the purpose of defining this term and what does it mean?

","mobile-robot, control, robotic-arm, motion-planning, algorithm"
Create2 Serial. Canonical versus Number of Bytes interface,"There is a message system which does not appear to be document in the OI spec. This appears to be a a canonical terminal type serial interface in which messages come back such as firmware version and stuff. I am not sure how to determine what the end of this type of message is? It is a fixed number of end lines? or Bytes. One message seems to indicate STR730 which would be a 730 byte string.
The open interface spec seems to indicate a non canonical interface spec in which you read a fixed number of bytes with no processing of end lines. Is this correct?
","irobot-create, roomba"
Servo motors for large scale RC car,"I want to convert an electric ATV (quad) for kids (like the HIGHPER ATV-6E) to radio control for a robotics project. These small ATVs are about a meter long and weigh about 40 kg. I need to choose servo motors for steering and braking. What grade servos do I need and how much torque do they need to have? Can I use the strongest RC servo I may find (like this 115kg/cm one or maybe even more, with metal gears of course) or do I need an ""industrial grade"" servo?
I plan to use one servo for steering and one for braking. For braking the ATV has mechanical disc brakes - two discs in the front and one common disc in the rear (there are two brake levers - front/rear). I plan to use only one servo and use it either for front or for rear. The plan is to mount the brake wire to the servo which would ""simulate"" the lever movement.
I guess I could also make a ""weak"" servo stronger by adding a proper gear, but I am not really into mechanical engineering much and would prefer an off-the-shelf component.

","servomotor, rcservo, steering"
Ultrasonic sensor range and shape,"I have been looking for a cheap ultrasonic sensor that is not blind under +/-30 cm but the only sensors I could find use the following shape, which is not suitable for my project (because of the robot design that only has 1 hole, and not 2..) : 
Is there any chance to find a sensor with that other shape with a range starting around 5cm ?

Actually I am wondering if that 2nd shape makes this constraint mandatory or if I just did not found the appropriate product.
",ultrasonic-sensors
What is the millisecond rate that robot-create can respond to two different drive commands?,"I want to issue two slightly different drive commands what is the smallest loop rate that the robot-create accepts new commands?
I know from reading the documentation that it appears the sensors are read every 15ms. 
Not sure what the command rate is?
",irobot-create
What is a CIM motor?,"I'm trying to make decisions for motors on a robot build. I keep running across CIM Motors. What is a CIM Motor? Where does the designation CIM come from? What does CIM mean?
",motor
Quadcopter PID Algorithm,"I'm trying to implement a PID control on my quadcopter using the Tiva C series microcontroller but I have trouble making the PID stabilize the system. 
While I was testing the PID, I noticed slow or weak response from PID controller (the quad shows no response at small angles). In other words, it seems that the quad's angle range has to be relatively large (above 15 degrees) for it to show a any response. Even then, the response always over shoots no matter what I, D gains I choose for my system. At low P, I can prevent overshoot but then it becomes too weak.   
I am not sure if the PID algorithm is the problem or if its some kinda bad hardware configuration (low IMU sample rate or maybe bad PWM configurations), but I have strong doubts about my PID code as I noticed changing some of the gains did not improve the system response.  
I will appreciate If someone can point out whether i'm doing anything wrong in the PID snippet for the pitch component I posted. I also have a roll PID but it is similar to the code I posted so I will leave that one out.
void pitchPID(int16_t pitch_conversion)
{
  float  current_pitch = pitch_conversion;
  //d_temp_pitch is global variable
  //i_temp_pitch is global variable
  float  pid_pitch=0; //pitch pid controller
  float  P_term, I_term, D_term;
  float  error_pitch = desired_pitch - current_pitch;

  //if statement checks for error pitch in negative or positive direction
  if ((error_pitch>error_max)||(error_pitch<error_min))
    {
      if (error_pitch > error_max) //negative pitch- rotor3&4 speed up
        {
          P_term = pitch_kp*error_pitch; //proportional
          i_temp_pitch += error_pitch;//accumulate error
          if (i_temp_pitch > iMax)
            {
            i_temp_pitch = iMax;
            }
          I_term = pitch_ki*i_temp_pitch;
          if(I_term < 0)
            {
            I_term=-1*I_term;
            }
          D_term = pitch_kd*(d_temp_pitch-error_pitch);
          if(D_term>0)
            {
            D_term=-1*D_term;
            }
          d_temp_pitch = error_pitch; //store current error for next iteration
          pid_pitch = P_term+I_term+D_term;
          if(pid_pitch<0)
            {
            pid_pitch=(-1)*pid_pitch;
            }
          //change rotor3&4
          pitchPID_adjustment (pid_pitch, 'n'); //n for negative pitch
        }
      else if (error_pitch < error_min) // positive pitch- rotor 1&2 speed up
        {
        P_term = pitch_kp*error_pitch; //proportional
        i_temp_pitch += error_pitch;
        if (i_temp_pitch < iMin)
          {
          i_temp_pitch = iMin;
          }
        I_term = pitch_ki*i_temp_pitch;
        if(I_term > 0)
          {
          I_term=-1*I_term;
          }
        D_term = pitch_kd*(d_temp_pitch - error_pitch);
        if(D_term < 0)
          {
          D_term=-1*D_term;
          }
        d_temp_pitch = error_pitch;
        pid_pitch = P_term+I_term+D_term;
        if(pid_pitch<0)
          {
          pid_pitch=(-1)*pid_pitch;
          }
        print(pid_pitch);//pitch
        printString(""\r\n"");
        //change rotor1&2
        pitchPID_adjustment(pid_pitch,'p'); //p for positive pitch
      }
    }
  }

 
void pitchPID_adjustment(float pitchPIDcontrol, unsigned char pitch_attitude)
  {
  if (pitchPIDcontrol>(maximum_dutyCycle-set_dutyCycle))
    {
    pitchPIDcontrol=maximum_dutyCycle-set_dutyCycle;
    }
  switch (pitch_attitude){
  //change rotor1&2
  case 'p': //positive status
    PWM0_2_CMPA_R += (pitchPIDcontrol);//(RED)//motor1
    PWM0_0_CMPA_R += (pitchPIDcontrol);//(Yellow)//motor2
    break;
  //change rotor 3&4
  case 'n': //negative status
    PWM0_1_CMPA_R += pitchPIDcontrol;//(ORANGE)//motor3
    PWM1_1_CMPA_R += pitchPIDcontrol;//(green)//motor4
    break;
  }

Also, can someone please tell me how this motor mixing works?: 
Front =Throttle + PitchPID 
Back =Throttle - PitchPID 
Left =Throttle + RollPID 
Right =Throttle - RollPID

vs what I did in the function:
void pitchPID_adjustment(float pitchPIDcontrol, unsigned char pitch_attitude)

","quadcopter, control, pid, imu, pwm"
Reverse engineering commercial drone control algorithms,"I'm wondering if there is a way to figure out the actual controllers used in the commercial drones such as AR drone and Phantom. According to AR drone SDK, users are not allowed to access the actual hardware of the platform yet they are only capable of sending and receiving commands from/to the drone. 

Edit:
I'm hoping to to check the actual controller utilized in the software. When I fly AR drone, it seems the platform can't stabilize itself when I perform aggressive maneuvers, therefore, I can guess that they use linearized model which is applicable for using simple controllers such as PD or PID
","quadcopter, control"
How is Topology-based representation invariant to certain change in environment,"The article of Topology-based representation (page no. 13, line 5) says that, topology-based representation is invariant to certain changes in the environment. That means the trajectory generated in topology-based space will remain valid even if there are certain changes in the environment. But how is this possible? Is there any simple example to understand this concept?
","mobile-robot, control, robotic-arm, motion-planning"
Counting number of people entering a room,"We are making a project in which we want to count the no. of people entering and leaving a room with one single entrance. We are using IR sensors and detectors for this ,along with an Aurdino. We have a problem in this system,  i.e when two or more persons are entering or leaving the room at a time we are getting a wrong count. Thanks in advance for your valuable time and solution.....If there is any other better way,please state that.
","arduino, sensors, microcontroller"
iRobot Create 2: Angle Measurement,"I have been working on trying to get the angle of the Create 2. I am trying to use this angle as a heading, which I will eventually use to control the robot. I will explain my procedure to highlight my problem.
I have the Create tethered to my computer. 

I reset the Create by sending Op code [7] using RealTerm.
The output is:

bl-start
  STR730
  bootloader id: #x47175347 4C636FFF
  bootloader info rev: #xF000
  bootloader rev: #x0001
  2007-05-14-1715-L
  Roomba by iRobot!
  str730
  2012-03-22-1549-L
  battery-current-zero 252

  (The firmware version is somewhere in here, but I have no clue what to look for--let me know if you see it!) 


I mark the robot so that I will know what the true angle change has been.
I then send the following codes [128 131 145 0x00 0x0B 0xFF 0xF5 142 6]. This code starts the robot spinning slowly in a circle and request the sensor data from the sensors in the group with Packet ID 2. The output from the Create seen in RealTerm is 0x000000000000, which makes sense.
I wait until the robot has rotated a known 360 degrees, then I send [142 2] to request the angle difference. The output is now 0x00000000005B.

The OI specs say that the angle measurement is in degrees turned since the last time the angle was sent; converting 0x5B to decimal is 91, which is certainly not 360 as expected. 
What am I doing wrong here? Is the iRobot Create 2 angle measurement that atrocious, or is there some scaling factor that I am unaware of? are there any better ways to get an angle measurement?
","irobot-create, roomba"
Meaning of the equation of graphical model,"The paper Topology-based Representations for Motion Planning
and Generalisation in Dynamic Environments with
Interactions by Ivan
et.al., says on page 10 that the Approximate Inference Control (AICO) framework translates the robot dynamics to the graphical model by the following equation:

What does p(x0:T,u0:T) mean? I feel that p means 'prior of' some uncertain quantity, but I'm not sure about this. 
","mobile-robot, control, robotic-arm, motion-planning"
MEMS accelerometer calibration,"I am trying to calibrate a MEMS accelerometer. I was able to calibrate it for the current axis which is parallel to gravity and shows correctly, 1g. But the other two axes which should be 0.00g are showing +-0.02g instead. 
So, e.g., when the accelerometer's x axis is parallel to gravity, it should show (1g, 0g, 0g) and not (1g, 0.02g, -0.01g) like now.
How could I eliminate those values, e.g. further calibrate accelerometer? 
EDIT: The acelerometer's datasheet says nothing about calibrating except that The IC interface is factory calibrated for sensitivity (So) and Zero-g level (Off) (page 20).
","design, electronics, accelerometer, calibration"
Difference between g-value and rhs-value in Lifelong Planning A*,"What is the difference between g-value and rhs-value of Lifelong Planning A* algorithm? 
According to this link, D* Lite, g(s) directly correspond to the
g-values of an A* search, i.e. g(s) = g(s') + c(s',s), and rhs(s) is given as 
$$
rhs(s) = \begin{cases}0 & s = s_{start}  \\ \min_{s'\in Pred(s)}(g(s') + c(s', s)) & \text{otherwise} \end{cases}
$$
where, Pred(s) denotes the set of predecessors of node 's'. 
Thus, unless node 's' has more than one predecessor, its g-value and rhs-value will remain same. 
So, my question is, in which case will the rhs-value and g-value of a node be different?
","mobile-robot, robotic-arm, wheeled-robot, motion-planning, algorithm"
"Robotic arm [""FAiL""] error display. - Festo / Mitsubishi Melfa RV-2AJ (Controller CR1-571)","To avoid wasting your time on this question, you might only want to react on this if you have knowledge of industrial robotic arms specific.
Common troubleshooting is unlikely to fix this problem or could take too much time.
We've started a project with the Mitsubishi Melfa RV-2AJ robotic arm.
Everything went fine until the moment we replaced the batteries.
The controller displays: ""FAiL"" and does not respond to any buttons or commands sent through serial connection.
We did replace the batteries of both the robot and the controller. As it took some time to get the batteries delivered, we've left the robot (and controller) withouth power for the weekend. (Which might have caused this problem)
Is there anyone with knowledge of Mitsubishi Robotic arms around here?
I'm kinda hoping it would be a common problem/mistake and anyone with experience on this subject would know about it?
",robotic-arm
Anthropomorphic Arm,"I developed an anthropomorphic arm (structure in aluminium) with 6 DOF (3 plus spherical wrist) for direct kinematic. 
I chose magnetic rotary encoders to measure angles but I am not satisfied, due to them causing noise on angle measurements. 
What do you advise me? 

To add another sensor and perform a sensor fusion? 
To replace magnetic encoders with optical ones? 
or... what else?

","arm, manipulator"
Quadcopter propeller physics,"In propellers as the airspeed increases thrust decreases. Is the air speed component taken as a vector quantity perpendicular to the propeller? If thats true the its quiet easy to visualize in case of airplanes but for quadcopters will it be  ""copter_airspeed * sin(copter tilt)""? 
",quadcopter
"Meaning of symbol, 'curly N' in the equation of Linear Gaussian system dynamics","In the article of Topological Based Representation(Page no. 12), the equation of the Linear Gaussian system dynamics is given as 

In above equation what is the meaning of 'curly N'? 
","mobile-robot, control, robotic-arm, wheeled-robot, motion-planning"
Meaning of 'sign' in Writhe Matrix,"Following is the equation of Writhe matrix from the article Topology based Representation(page no. 6).

What is the meaning of 'sign' in the second part of this equation? I am not sure if this is some typo in that article as the other article of Hierarchical Motion Planning(page no. 3), compleletely neglects the term 'sign[...]'

","mobile-robot, control, robotic-arm, wheeled-robot, motion-planning"
"Considerations to design actuators, and loop feedback systems, for a robotic arm","Let's say I have an industrial sized 6DOF robotic arm. I want to control each one of the six joints despite the non-linearity produced by the chain structure, the gravity and the weight of the loads it could lift. 
I don't focus here on the speed nor the power limitations, I just want the arm to respond well. Moreover, I would like to avoid the use of any prior knowledge such as inertial computation. Then I had these considerations, considering that I can play with both the actuator design, and the loop feedback control system:

Limit the maximum speed of each actuator to smooth their error variation.
Increase the damping of the actuators to avoid high frequency instability.     
Find a good control system, such as a PID, to make sure the targets are reached without oscillations. 

Do you have any other considerations in mind? Do you know what process(es) industrial designers follow?
EDIT: As it is said in the comments, my question concern the design of an adaptive controller for a robot arm, which is, how to design a joint control system (actuator + loop control) that don't need inertia and masses to be computed (the controller could adapt to its own structure, or to the loads it lifts). 
I'll be very much interested if you know some paper about adaptive control in the field of robotic arms.    
","control, pid, robotic-arm, design, actuator"
What iRobot products support the open interface besides the iRobot Create?,"I have read that certain iRobot products support or can be hacked to support something close to the open interace. There is even a book about hacking Roomba. What Robots have this capability?
",irobot-create
Choose and connect a camera to a robot,"There are tons of cameras in devices around us these days. There are used photo cameras, smartphones, tablets at my home gathering dust.
I wonder, what the easiest way could be to get a camera module from some device, connect it to my robot project with a soldering iron and make it work from the software point of view. I am planning to use something like Arduino, an STM32 platform, or probably Intel Edison.
May be some camera modules are easier to solder and program for a custom  project? Or shouldn't I look this way and better find a camera module that is specially designed for custom projects?
",cameras
Use data from gyroscope to calculate orientation,"From a gyroscope I'm getting angular velocities [dRoll, dPitch and dYaw] as rad/s, sampled at intervals dt = 10ms.
How do I calculate the short term global orientation (drift ignored) of the gyroscope?
Pseudo code would be helpful.
",gyroscope
Neural Nework code or library for MSP430G2553 microcontroller,"I am new here and i am new to neural network also. :P
I have gone through the concepts of Neural Networks but i want to implement it in my project including microcontroller MSP430G2553 on LaunchPad Series.
I am using some sensors and i want to use some neural network code to manipulate the data from sensors to get some threshold.
I went through this post and tried to implement the codes from the link given but it is giving some error on less ram, i guess it is due to my mcu.  
So, i wanted some help regarding the neural network code or library for Energia which i should use.
Thanks in Advance.
","microcontroller, electronics, machine-learning, embedded-systems"
Is it possible to run a neural network on a microcontroller,"Could you implement a simple neural network on a microprocessor such as the Arduino Uno to be used in machine learning?
","microcontroller, machine-learning"
APM Mission Planner 2.0.18 Install Firmware Failure Mac OS X 10.11,"I installed Multiple versions of APM (2.0.7, 2.0.17, 2.0.18) on Windows 7, Ubuntu 14.04, and OSX 10.11. I could connect to my ArduPilot but could not install firmware. Here's the error I would get:
Started downloading http://firmware.diydrones.com/Copter/stable/apm2-hexa/ArduCopter.hex
Finished downloading /var/folders/r4/s_j4c02s3wvcx6wy41__rnwh0000gp/T/APM Planner.uq1800
Opening firmware file...
Unable to open file: /var/folders/r4/s_j4c02s3wvcx6wy41__rnwh0000gp/T/APM Planner.uq1800

",ardupilot
DC Motor Control,"My project requires a DC motor for mobility, very similar to an RC car. If precision isn't critical, can I use a solid state relay instead of a motor driver? If the vehicle moves an extra inch on the ground, I don't really care.
","motor, driver"
"Euler-Lagrange systems, autonomous or nonautonomous?","I was reading an article on Euler-Lagrange systems. It is stated there that since M(q) and C(q,q') depend on q, it is not autonomous. As a result, we cannot use LaSalle's theorem. I have uploaded that page of the article and highlighted the sentence. (ren.pdf)
Then, I read Spong's book on robotics, and he had used LaSalle's theorem. I am confused. (spong.pdf)
I did some research, and found out that non-autonomous means it should not explicitly depend on the independent variable. Isn't independent variable time in these systems? So, shouldn't they be considered autonomous?
","control, dynamics, robotc"
Electronic circuit for heating nylon fishing line muscle,"I'm trying to make artificial muscles using nylon fishing lines (see http://io9.com/scientists-just-created-some-of-the-most-powerful-muscl-1526957560 and http://writerofminds.blogspot.com.ar/2014/03/homemade-artificial-muscles-from.html)
So far, I've produced a nicely coiled piece of nylon fishing line, but I'm a little confused about how to heat it electrically.
I've seen most people say they wrap the muscle in copper wire and the like, pass current through the wire, and the muscle acuates on the dissipated heat given the wire resistance.
I have two questions regarding the heating:
1) isn't copper wire resistance extremely low, and thus generates very little heat? what metal should I use? 
2) what circuit should I build to heat the wire (and to control the heating)? Most examples just ""attach a battery"" to the wire, but afaik that is simply short-circuiting the battery, and heating the wire very inneficiently (and also may damage the battery and it could even be dangerous). So what's a safe and efficient way to produce the heat necessary to make the nylon muscle react? (I've read 150 centigrads, could that be correct?) for example with an arduino? or a simple circuit in a breadboard?
thanks a lot!
","arduino, electronics, actuator"
Quadcopter PID: Controller is Saturating,"Good day,
I am currently creating an autonomous quadcopter using a cascading PID controller specifically a P-PID controller using angle as setpoints for the outer loop and angular velocities for the inner loop. I have just finished tuning the Roll PID last week with only +-5 degrees of error however it is very stable and is able to withstand disturbances by hand. I was able to tune it quickly on two nights however the pitch axis is a different story.
Introduction to the Problem:
The pitch is asymmetrical in weight (front heavy due to the stereo vision cameras placed in front). I have tried to move the battery backwards to compensate however due to the constraints of the DJI F450 frame it is still front heavy.
In a PID controller for an asymmetrical quadcopter, the I-gain is responsible for compensating as it is the one able to ""remember"" the accumulating error.
Problem at Hand
I saw that while tuning the pitch gains, I could not tune it further due to irregular oscillations which made it hard for me to pinpoint whether this is due to too high P, I or D gain. The quadcopter pitch PID settings are currently at Prate=0.0475 Irate=0.03 Drate=0.000180 Pstab=3 giving an error from the angle setpoint of 15degrees of +-10degrees. Here is the data with the corresponding video.
RATE Kp = 0.0475, Ki = 0.03, Kd = 0.000180 STAB Kp=3
Video: https://youtu.be/NmbldHrzp3E
Plot:

Analysis of Results
It can be seen that the controller is saturating.
The motor controller is currently set to limit the pwm pulse used to control the ESC throttle to only 1800ms or 180 in the code (The maximum is 2000ms or 205) with the minimum set at 155 or 1115ms (enough for the quad to lift itselft up and feel weightless). I did this to make room for tuning the altitude/height PID controller while maintaining the throttle ratio of the 4 motors from their PID controllers.  

Is there something wrong on my implementation of limiting the maximum throttle?

Here is the implementation:
 //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i<4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]<minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout << "" MinPWM = "" << minPWM << endl;

    if(minPWM<baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout << "" Fill = "" << fillPwm << endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i<4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]>maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout << "" MaxPWM = "" << maxPWM << endl;

    if(maxPWM>maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout << "" Trim = "" << trimPwm << endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }

Possible solution
I have two possible solutions in mind

I could redesign the camera mount to be lighter by 20-30 grams. to be less front heavy
I could increase the maximum throttle but possibly leaving less room for the altitude/throttle control.


Does anyone know the optimum solution for this problem?

Additional information
The quadcopter weighs about 1.35kg and the motor/esc set from DJI (e310) is rated up to 2.5kgs with the recommended thrust per motor at 350g (1.4kg). Though a real world test here showed that it is capable at 400g per motor with a setup weighing at 1600g take-off weight 
How I tune the roll PID gains
I had set first the Rate PID gains. at a setpoint of zero dps

Set all gains to zero.
Increase P gain until response of the system to disturbances is in steady oscillation.
Increase D gain to remove the oscillations.
Increase I gain to correct long term errors or to bring oscillations to a setpoint (DC gain).
Repeat until desired system response is achieved

When I was using the single loop pid controller. I checked the data plots during testing and make adjustments such as increasing Kd to minimize oscillations and increasing Ki to bring the oscillations to a setpoint. I do a similar process with the cascaded PID controller.
The reason why the rate PID are small because rate Kp set at 0.1 with the other gains at zero already started to oscillate wildy (a characteristic of a too high P gain). https://youtu.be/SCd0HDA0FtY
I had set the Rate pid's such that it would maintain the angle I physically placed it to (setpoint at 0 degrees per second) 
I then used only P gain at the outer loop stabilize PID to translate the angle setpoint to velocity setpoint to be used to control the rate PID controller.
Here is the roll axis at 15 degrees set point https://youtu.be/VOAA4ctC5RU
Rate Kp = 0.07, Ki = 0.035, Kd = 0.0002 and Stabilize Kp = 2

It is very stable however the reaction time/rise time is too slow as evident in the video.
","quadcopter, pid, stability"
Does a controlling system need to be more complex than the system being controlled?,"Is there any theoretical principle, or postulate, that states that the controlling system has to be more complex than the system being controlled, in any formal sense of the notion ""complex""?
","control, theory"
Why do I need I-gain in my outer-loop?,"I'm implementing a set of loops to control pitch-and-roll angular positions.
In an inner-loop, motor speeds are adjusted to achieve desired angular rates of rotation (the ""inner-loop setpoints"").
An outer-loop decides these desired angular rates (the ""inner-loop setpoints"") based on the aircraft's angular positions.

Outer-loop

Frequency = ~400Hz
Outer PV = input angular position (in degrees)
Outer SP = desired angular position - input angular position (in degrees)


Inner-loop

Frequency = ~760Hz
Inner PV = input angular rotation (in degrees-per-second)
Inner SP = constant1 * Outer MV (in degrees-per-second)
PWM = Inner MV / constant2 (as percentile)


I understand what I-gain does and why this is important, but I'm not able to see any practical reason for also having I-gain specified in the outer-loop. Surely the inner-loop would compensate for any accumulated error, leaving no error to compensate for in the outer-loop, or is my thinking flawed?
Any example gain values to elaborate would be greatly appreciated.
","quadcopter, pid"
How to interrupt on a data ready trigger when communications to the sensor are interrupt driven?,"Background: I'm using the L3GD20H MEMS gyroscope with an Arduino through a library (Pololu L3G) that in turn relies on interrupt-driven I2C (Wire.h); I'd like to be able to handle each new reading from the sensor to update the calculated angle in the background using the data ready line (DRDY). Currently, I poll the STATUS register's ZYXDA bit (which is what the DRDY line outputs) as needed.
General question: With some digital output sensors (I2C, SPI, etc.), their datasheets and application notes describe using a separate (out-of-band) hardware line to interrupt the microcontroller and have it handle new sets of data. But on many microcontrollers, retrieving data (let alone clearing the flag raising the interrupt line) requires using the normally interrupt-driven I2C subsystem of a microcontroller. How can new sensor data be retrieved from the ISR for the interrupt line when also using the I2C subsystem in an interrupt-driven manner?
Possible workarounds:

Use nested interrupts (as @hauptmech mentioned): re-enable I2C interrupt inside of ISR. Isn't this approach discouraged?
Use non-interrupt-driven I2C (polling)--supposedly a dangerous approach inside of ISRs. The sensor library used depends on the interrupt-driven Wire library.
[Edit: professors' suggestion] Use a timer to interrupt set to the sample rate of the sensor (which is settable and constant, although we measure it to be e.g. 183.3Hz rather than 189.4Hz per the datasheet). Handling the I2C transaction still requires re-enabling interrupts, i.e. nested interrupts or performing I2C reads from the main program.

[Edit:] Here's a comment I found elsewhere on a similar issue that led me to believe that the hang encountered was from I2C reads failing inside an interrupt handler: https://www.sparkfun.com/tutorials/326#comment-4f4430c9ce395fc40d000000 

…during the ISR (Interrupt Service Routine) I was trying to read the
  device to determine which bit changed. Bad idea, this chip uses the
  I2C communications which require interrupts, but interrupts are turned
  off during an ISR and everything goes kinda south.

","arduino, microcontroller, gyroscope, i2c, interrupts"
Pick and place robot,"I have to simulate a pick and place robot (3 DOF). I tried with MATLAB. It should pick and place different objects according to their geometry. 
Where can I find similar m-codes and algorithms?
","robotic-arm, matlab"
KUKA delimiter .NET,"I have a chance to develop a user interface program that lets the user control a KUKA robot from a computer. I know how to program stuff with the KUKA utilities, like OrangeEdit, but I don't know how to do what I want to do. I don't even know what's the ""best"" language to talk to the robot.
My idea is to control the robot with the arrow buttons, like up/down controls the Z axis and left/right controls the X/Y axes.
Can someone help me here? I know there's a lot of libraries to control the robot even with an Xbox controller, but if I limit the robot to 3 axes I might be able to control with simple buttons. 
Edit: Now imagine that i have a routine that consists on going from P1 to P2 then to P3. I know i can ""touch up"" the points to refresh its coordinates using the console, but can i do it in a .net application? like modifying the src/srcdat files?
","robotic-arm, kuka"
iRobot Create 2 stuck in Clean mode?,"I'm using the Delphi example to command my Create 2, I just adapted the demo code to Unicode (DelphiXE). I use the original iRobot USB to serial cable. 
My Create 2 seemed to be responding fine to all the commands send via serial yesterday and correctly received all sensor data back this morning, until I recharged the battery. Now when I send ""7"" ""Soft reset"" the robot attempts every time to start a clean cycle. It also attempts to start the clean cycle when I press the clean button. It tells me to move the Roomba to a new location, which is normal in cleaning mode because my wheels are not touching my desk. Communication via serial seem to be fine because I still get the Soft Reset response texts in the log memo of my app when I use the 2 buttons method to soft reset my Create 2, so there is still communication both ways.
I must say I had the same yesterday after charging but after a while unexpectedly, don't know why, the robot responded again fine to my commands.
It really seems to me the Create 2 is stuck in the Cleaning mode, or am I missing something?
BTW, I also tried to fix the problem by removing the battery.
",irobot-create
Create 2 light red/green,"I am working on a project with the Create 2. Just recently I have run into a problem with the battery state. The Create 2 has been charging all night so its clean light shows green. However, when I unplug it and press the clean button, it shows red and will not consistently run commands from my Arduino that I have hooked up to it. 
What could be the problem?
","arduino, irobot-create, battery"
Can you interface to a Braava Jet?,"Is there any open interface access to the new Braava jet just to drive it around?
",irobot-create
Fixed Wing UAV: Do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?,"As we all know fixed wing vehicles are designed to have inherent instability which is what enables all fixed wing vehicles to fly.
However does this apply to all cases?

Do inherently unstable systems desire to be stable for all cases when a closed loop control is implemented on them?

","control, pid, microcontroller, uav, stability"
Fastening sheet steel on nylon,"I'm trying to attach a small piece of sheet steel (30mm x 50mm x 1mm) to a small piece of nylon (50mm x 50mm x 4mm). Does anyone know how they could be fastened using small screws (

Any thought appreciated.
",mechanism
Remaking an RC transmitter for controlling aircraft,"I am thinking about working on alternative drone controllers. I am looking into making it more easy to use and a natural feel (debating between sensor bracelets, rings, etc.).
The main issue I have is, I've been looking over all the standard RC transmitters that are used to control RC aircraft, but I am not sure what technology is inside of them, what kind of ICs they use for the actual RC signals. 
I want more information on how to make an RC transmitter myself, mainly the protocol that's used to send messages, and what circuitry is needed to actually transmit that, what kind of components do I need and how should I implement the software?
I was aiming at doing this as a side project (hobby), but now I have the chance to use it as a uni project as well, so I'd like to give it a shot now, but I lack the proper information before getting started. 
I'd rather not take apart my current RC controller and use an oscilloscope to decode the protocol. 
Any answers (short or long) and reading material is appreciated.
Other questions, can the protocol be implemented in software on an embedded system (Raspberry Pi, Arduino, Intel Galileo, etc.)?
I am asking this because the frequency for these are 2.4 GHz.
This is part of a bigger project, drone related currently, and I could use alternative methods of sending the information, through other wireless means, as the first prototype, suggestions are welcomed.
Need: aircraft RC transmitter protocol info, RC transmitter components & schematics, anything else that might help with the transmission side
","microcontroller, radio-control"
What algorithm should I implement to program a room cleaning robot?,"For this question assume that the following things are unknown:

The size and shape of the room
The location of the robot
The presence of any obstacles

Also assume that the following things are constant:

The size and shape of the room
The number, shape and location of all (if any) obstacles

And assume that the robot has the following properties:

It can only move forward in increments of absolute units and turn in degrees. Also the operation that moves will return true if it succeeded or false if it failed to move due to an obstruction
A reasonably unlimited source of power (let's say it is a solar powered robot placed on a space station that faces the sun at all times with no ceiling)
Every movement and rotation is carried out with absolute precision every time (don't worry about unreliable data)

Finally please consider the following properties of the robot's environment:

Being on a ceiling-less space station the room is a safe but frustratingly close distance to passing comets, so the dust (and ice) are constantly littering the environment.

I was asked a much simpler version of this question (room is a rectangle and there are no obstacles, how would you move over it guaranteeing you could over every part at least once) and after I started wondering how you would approach this if you couldn't guarantee the shape or the presence of obstacles. I've started looking at this with Dijkstra's algorithm, but I'm fascinated to hear how others approach this (or if there is a well accepted answer to this? (How does Roomba do it?)
","mobile-robot, artificial-intelligence, algorithm, coverage, theory"
How can we use the accelerometer for altitude estimation?,"I am currently implementing an autonomous quadcopter which I recently got flying and which was stable, but is unable to correct itself in the presence of significant external disturbances. I assume this is because of insufficiently tuned PID gains which have to be further tweaked inflight.
Current progress:

I ruled out a barometer since the scope of my research is only indoor flight and the barometer has a deviation of +-5 meters according to my colleague.
I am currently using an ultrasonic sensor (HC-SR04) for the altitude estimation which has a resolution of 0.3cm.  However I found that the ultrasonic sensor's refresh rate of 20Hz is too slow to get a fast enough response for altitude correction.
I tried to use the accelerations on the Z axis from the accelerometer to get height data by integrating the acceleration to get velocity to be used for the rate PID in a cascaded pid controller scheme. The current implementation for the altitude PID controller is a single loop pid controller using a P controller with the position input from the ultrasonic sensor.
I had taken into account the negative acceleration measurements due to gravity but no matter how much I compute the offset, there is still the existence of a negative acceleration (eg. -0.0034). I computed the gravitational offset by setting the quadcopter to be still on a flat surface then collecting 20,000 samples from the accelerometer z axis to be averaged to get the ""offset"" which is stored as a constant variable. This variable is then subtracted from the accelerometer z-axis output to remove the offset and get it to ""zero"" if it is not accelerating. As said in the question, there is still the existence of a negative acceleration (eg. -0.0034). My quad then proceeds to just constantly climb in altitude. With only the ultrasonic sensor P controller, my quad oscillates by 50 cm.


How can this consistent negative acceleration reading be effectively dealt with?

Possible Solution:
I am planning to do a cascading PID contoller for the altitude hold with the innerloop (PID controller) using the accelerometer and the outer loop (P controller) using the sonar sensor. My adviser said that even a single loop P controller is enough to make the quadcopter hold its altitude even with a slow sensor. Is this enough? I noticed that with only the P gain, the quadcopter would overshoot its altitude.


Leaky Integrator: I found this article explaining how he dealt with the negative accelerations using a leaky integrator however I have a bit of trouble understanding why would it work since I think the negative error would just turn to a positive error not solving the problem. I'm not quite sure. http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it
Single loop PD controller with the ultrasonic sensor only:
Is this feasible using feedback from a slow sensor?

Sources:

LSM303DLHC Datasheet: http://www.st.com/web/en/resource/technical/document/datasheet/DM00027543.pdf
Leaky integrator: http://diydrones.com/forum/topics/multi-rotors-the-altitude-yoyo-effect-and-how-to-deal-with-it
ArduPilot PID Loop: http://copter.ardupilot.com/wp-content/uploads/sites/2/2012/12/Alt-Hold-PID-version-3.0.1.jpg

","quadcopter, control, pid, raspberry-pi, sensor-fusion"
Building a stationary robot which can talk,"I am a Computer Science major and I only have basic ideas on Robotics. I am planning to build a stationary cubical AI. 
The main purpose of this bot will be that, it will have a sensor to check if the door has been opened and immediately asks a question ""who has opened the door?"" I also want it to recognize the correct words to interact the word, I am not talking about voice recognition but word recognition so that who ever speaks the correct words(words in bot's memory) can interact with it. Depending on who opens the door(prolly my family) I want it to speak out different things. I want it to respond to simple questions like, ""what is the date and time?"" , "" a random qoute or a fact or a joke"". 
Is this too hard to achieve? Could anyone give me a basic idea on how to approach this project? 
","sensors, communication, first-robotics, speech-processing"
"In order to integrate MCL and Occupancy Grid to implement Grid-based FastSLAM, do you have to record all data?","It's unclear as to how one goes about integrating Occupancy Grid mapping and Monte Carlo localization to implement SLAM.
Assuming Mapping is one process, Localization is another process, and some motion generating process called Exploration exist. Is it necessary to record all data as sequenced or with time stamps for coherence? 
There's Motion: $U_t$, Map: $M_t$, Estimated State: $X_t$, Measurement: $Z_t$
so..

each Estimated state, $X_t$, is a function of the current motion, $U_t$, current measurement, $Z_t$, and previous map, $M_{t-1}$;
each confidence weight, $w_t$, of estimated state is a function of current measurement, $Z_t$,  current estimate state, $X_t$, and previous map, $M_{t-1}$;
then each current map, $M_t$ is a function of current measurement, $Z_t$, current estimated state, $X_t$,  and previous map, $M_{t-1}$.

So the question is, is there a proper way of integrating mapping and localization processes? Is it something you record with timestamp or sequences? Are you suppose to record all data, like FullSLAM, and maintain full history. 
How can we verify they are sequenced at the same time to be referred to as current (i.e. measurement) and previous (measurement).
","slam, occupancygrid"
Real-time object classification for an indoor autonomous quad-rotor,"I am designing an indoor autonomous drone. I am currently writing an object classification program in OpenCV for this purpose. My objects of interests for classification are: ceiling fans; AC units; wall and ceiling lamps, and; wall corners. I am using BoW clustering algorithm along with SVM classifier to achieve this (I'm still in the process of developing the code, and I might try other algorithms when testing).
The primary task of the drone is to successfully scan (what I mean by scanning is moving or hovering over the entire ceiling space) a ceiling space of a given closed region while successfully avoiding any obstacles (like ceiling fans, AC units, ceiling and wall lamps). The drone's navigation, or the scanning process over the ceiling space, should be in an organised pattern, preferably moving in tight zig-zag paths over the entire ceiling space.
Having said that, in order to achieve this goal, I'm trying to implement the following to achieve this:

On take off, fly around the given closed ceiling space and use SLAM to localise and map its environment.
While running SLAM, run the object classier algorithm to classify the objects of interests and track them in real time.
Once obtained a detail map of the environment and classified all objects of interest in the local environment, integrate both data together to form an unified map. Meaning on the SLAM output, label the classified objects obtained from the classifier algorithm. Now we a have fun comprehensive map of the environment with labeled objects of interest and real-time tracking of them (localization).
Now pick random corner on the map and plan a navigation pattern in order to scan the entire ceiling space. 

So the question now here is, is using object classification in real-time will yield successful results in multiple environments (the quad should be able to achieve the above mentioned tasks in any given environment)?. I'm using a lot of train image sets to train my classifier and BoW dictionary but I still feel this won't be a robust method since in real-time it will be harder to isolate an object of interest. Or, in order to overcome this, should I use real-situation like train images (currently my train images only contain isolated objects of interests)?
Or in my is using computer vision is redundant? Is my goal completely achievable only using SLAM? If, yes, how can I classify the objects of interest (I don't want my drone to fly into a running ceiling fan mistaking it for a wall corner or edge). Furthermore, is there any kind of other methods or sensors, of any type, to detect objects in motion? (using optical flow computer vision method here is useless because it's not robust enough in real-time).
Any help and advice is much appreciated.
","mobile-robot, quadcopter, slam, opencv"
Are Artificial Intelligence and Robotics Different?,"I need help in differentiating between AI and Robotics. Are AI and Robotics two different fields or is robotics a subject in AI?
I want to pursue a career in AI and Robotics. So I need your valuable suggestion. I searched the web and also some universities that I want to apply and I cannot find any such thing that I am searching for.
",artificial-intelligence
Quadrocopter PID,"I am building a quadcopter for my school project. I am trying to program my own flight controller using PID algorithm.
I'll try to make my question simple using as an example below only two motors
                       1-----------2

Let's say I am trying to stabilize my two motor system using gyro from the diagram below to one above
                       1--
                          -----
                               ----2

Using the formula Output = (gyro - 0) * Pgain
Do I need to increase the output only on the motor 2 or would I have to do both:
increase the output on the 2nd motor while decreasing the output on the first motor? Thank you
","quadcopter, pid, ardupilot, logic-control"
"Name of large robotic arms (two finger) with wrist, arm, hands and spinning shoulder axis","I've been looking for large robotic arms (with two fingers) and the arm so they are able to pick up and drop things in a space around the arm (and even spin around the 'wrist').
I'm not sure what the terminology is for such an arm. I've seen this, OWI-535 Robotic Arm Edge, and it looks close. Is there something larger that can be hooked up to a Raspberry Pi instead of the remote controller?
Is there a particular term for this in a generic context? Or is there a way to build such an arm using off the shelf parts?
","robotic-arm, raspberry-pi"
What is the common process to place a robotic arm gripper,"I implemented a simulation for a robotic arm that has to grab things. This arm has a 6DOF structure and a simple gripper on the top. I made a simple CCD IK algorithm to control the arm. I can use it in two ways:

Compute the position of the last joint of the arm before the hand
part (which means 1 end-effector). Then use an analytical method to
place the hand in a good orientation.
Compute directly the arm, and the hand position by giving the CCD IK algorithm 2 end-effectors that are the 2 finger of the hand.

What is the most used method for a grabbing arm robot ? I'm not willing to find a solution, just to know what people usually do. 
","robotic-arm, inverse-kinematics"
OpenRAVE ChechCollison command in C++,"What is the equivalent code of ""env.CheckCollision(robot)"" in C++? Even though it is said that conversion of commands from python to c++ is easy and intuitive, where can I find a proper documentation for this conversion?
","motion-planning, algorithm"
RRT algorithm in C++,"I want to implement RRT for motion planning of a robotic arm. I searched a lot on the internet to get some sample code of RRT for motion planning, but I didn't get any. Can someone please suggest a good source where I can find RRT implemented in C++ for any type of motion planning.
","robotic-arm, motion-planning, algorithm"
Programming A Rover,"I am part of my College team which is planning to enter a Mars Rover Challenge. In the point of view of a programmer, where should I start? I know C is the main language NASA used for their Rover and I have a basic understanding of it. Plus, how much should I look into the RTOS part for making a rover?
Any books/links to this topic would be greatly appreciated. 
","programming-languages, c"
Quadcopter Props? Wood vs Plastic vs Carbon Fiber,"All the pro FPV builds and the more expensive quads don't seem to be using plastic props. Any reason for this?
",quadcopter
Why would a drone need a magnetometer? Are an accelerometer and a gyroscope not sufficient?,"Why would a drone need a magnetometer? What would the drone do with this information? I think it would be to tell direction, but why would it need this if it has an accelerometer and a gyroscope?
","mobile-robot, magnetometer"
Pose-Graph-SLAM: How to create edges if only IMU-odometry is given?,"I want to estimate the poses of a vehicle at certain key frames. The only sensor information I can use is from an IMU which yields translational acceleration and orientation measurments. I obtain a 7D pose, i.e. 3D position vector + unit quaternion orientation, if I integrate the translational acceleration twice and propagate the orientation measurements.
If I want to add a new edge to the graph I need a constraint for that edge. In general, for pose graphs this constraint represents a relational transformation $z_{ij}$ between the vertex positions $x_i$ and $x_j$ that are connected by the edge. 
Comparing my case to the literature the following questions arised:

How do I calculate a prediction $\hat{z}_{ij}$ which I can compare to a measurement $z_{ij}$ when computing the edge error? Initially, I understood that graph slam models the vertex poses as gaussian distributed variables and thus a prediction is simply calculated by $\hat{z}_{ij}=x_i^{-1} x_j$. 
How do I calculate the information (preferred) or covariance matrix? 
How and when do I update the information matrices? During optimization? Or only at edge creation? At loop closure?
I read about the chi-square distribution and how it relates to the Mahalanobis distance. But how is it involved in the above steps? 
Studying current implementations (e.g. mrpt-graph-slam or g2o) I didn't really discover how predictions (or any probability density function) is involved. In contrast, I was even more confused when reading the mrpt-graph-slam example where one can choose between raw poses and poses which are treated as means of a probability distribution.

","slam, imu, data-association"
Any books or web resources for robotics mechanical design?,"I plan to build a mechanism with multiple axis, which is similar to a robot. To start, I need to define some specifications such as repeatable precision, speed, acceleration, and payload. Then the motor and structure is selected and designed based on these parameters. After that, I need to choose methods to manufacture these components. I would like to consult experienced experts in this forum that is there any suggested books, textbooks, or website resources I can learn these knowledge? 
","mechanism, manufacturing, books"
Slam Odometer Requirement,"How accurate must my odometer reading be for SLAM ?
I am writing this extra section because it says my question body does not meet the quality standard. 
",slam
Why does the Pixhawk have 2 IMUs,"I was looking at the Pixhawk specs and noticed that it has 2 different IMUs- Invensense and STM. Is it for redundancy or does it have any other higher purpose?
",uav
VEX Cortex Motor Speeds up under load,"I am trying to get my robot to drive straight and am having trouble.  I find that when running the motors with no load they run fine.  If I put a load on one motor it accelerates.  The other performs as expected, it tries to maintain speed.  I am running 393 motors with encoders and PID selected.  I am running robot C.
See the following video: https://youtu.be/u3P0Wectwco
program is as follows;
#pragma config(I2C_Usage, I2C1, i2cSensors)
#pragma config(Sensor, dgtl12, killB,          sensorTouch)
#pragma config(Sensor, I2C_1,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_2,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Sensor, I2C_3,  ,               sensorQuadEncoderOnI2CPort,    , AutoAssign )
#pragma config(Motor,  port2,           rmotor,        tmotorVex393_MC29, PIDControl, reversed, driveRight, encoderPort, I2C_1)
#pragma config(Motor,  port3,           lmotor,        tmotorVex393_MC29, PIDControl, driveLeft, encoderPort, I2C_2)
#pragma config(Motor,  port4,           topmotor,      tmotorVex393_MC29, openLoop, encoderPort, I2C_3)
#pragma config(Motor,  port5,           pmotor,        tmotorVex393_MC29, openLoop)
//*!!Code automatically generated by 'ROBOTC' configuration wizard               !!*//

void StopAll(){
    motor[rmotor] = 0;
    motor[lmotor] = 0;
    motor[topmotor] = 0;
    motor[pmotor] = 0;
}

//Stops the program at the push of a button
task eStop(){
    while (SensorValue(killB) == 0){
        wait1Msec(10);
    }
    StopAll();
    stopAllTasks();
}


task main()
{
    startTask(eStop);

    nMotorEncoder[rmotor] = 0;
    nMotorEncoder[lmotor] = 0;

    motor[rmotor] = 15;
    motor[lmotor] = 15;
    wait1Msec(20000);
    motor[rmotor] = 0;
    motor[lmotor] = 0;
    StopAll();


}

Thank you,
Mark
","pid, robotc, vex"
Why can two Series 1 XBees only talk in X-CTU?,"I have two Series 1 XBees that won't be in transparent mode because they are in AT command mode when I'm not in X-CTU.  I had asked for help elsewhere and no one had the answer except telling me about flow control.
The XBees had been configured properly with the MY and DL settings.  I'm thinking maybe I should shorten the timeout so they supposedly get out of AT command mode but they both stay in AT command mode.  The only time I can get the two Series 1 XBees to talk is under X-CTU.  I need the two Series 1 XBees to automatically be in transparent mode when powered on.  
",wireless
"If I must fly my drone in bad weather, how can I maintain control of it in strong winds?","If I need to fly a drone in strong winds, how can I stabilize it? Should I use accelerometers and gyroscopes to keep it steady? Or should I just use some flight technique under such circumstances?
","quadcopter, navigation, accelerometer"
Choosing motor type for high reliability for many cycles,"I am designing a multi modal stent testing machine which will bend, twist, and compress stents (very thin, light, and fragile cylindrical meshes for in arteries) in a tube. The machine will operate at maximum 3.6 Hz for months at a time (> 40 million cycles). As the machine will be in a lab with people, the noise should be minimal. I am choosing actuators for my design but was overwhelmed by the range of products available.
For rotating the stents around their axis, I will need a rotary actuator with the following specs:

torque: negligible max angle: 20 deg
angular velocity needed: max 70 deg/s
hollow shafts are a plus

For compressing the stents, I will need a linear actuator with the following specs:

force: low (<1N)
max stroke: 20mm but if possible 70mm for allowing different stent lengths 
stroke velocity needed: max 120mm/s

Price of these motors is not the driving factor.
I looked into stepper motors, servo motors, and piezoelectric motors. There seems to be s huge selection that fits my requirements. If all motor types have a reliability that suits my needs, which characteristics/advantages/disadvantages should I consider that determine the selection of suitable actuators? I do know what the difference is between the motor types, but there is a lot of overlap. Concrete suggestions are welcome.
","actuator, reliability"
SLAM starting point / quardrant issue,"I have a question that is puzzling me. I have a simple rectangle enclosure and i have extracted the Lidar and odometer data from a test run. If i put my starting position at [0,0,90] it gives bad map. However if i shift it away from [0,0] to something like [50,50,90] the map seems fine. How can this be ? 
",slam
Reliably establishing communication and OI mode with Create 2,"I've started tinkering with a Create 2, but I'm having issues reliably getting it to accept my commands.  I can occasionally get it right, but sometimes, it just seems to ignore me.  I'm guessing my cleanup code isn't getting the state fully reset or something.  Is there a good pattern to follow for fail-safe initialization code?
Here's what I'm doing right now:

Pulse BRC low for 1 second
Wait 1 second
Send 16x 0 bytes (to make sure if it's waiting for the rest of a command, this completes it - seemed to help a bit when I added this)
Send 7 (reset)
Wait 10 seconds
Send 128 (start)
Wait 2 seconds
Send 149 35 (ask for the current OI state)
Wait 1 second
Send 131 (safe mode)

Sometimes I'm then able to issue 137 (drive) commands and have it work.  Most times it doesn't.  The times when it doesn't, I'm seeing a lot of data coming from the Create 2 that I'm not expecting, that looks something like this (hex bytes):
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 3f 2a ff 73 21 09 cc 0a 88 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 3f 2a ff 73 21 09 cc 0a

There's more, but my logging cut it off.  I get the same pattern a couple of times, and it seems to be at least partially repeating.  I thought maybe it's the 16 0-bytes I sent followed by 003f 2aff 7321 09cc 0a88, but I still don't know how to interpret that.
Sometimes it will make some noise for the reset command, but usually ignores the start/safe mode commands completely (I can tell because the green light stays on).
",irobot-create
Canny's Roadmap Algorithm,"Where can I find a general implementation of Canny's Roadmap Algorithm(or Silhouette Method) for Robot Motion Planning?
",motion-planning
Relationship between motor torque and acceleration,"I am working on designing and building a small (1 1/2 lbs), 2-wheeled, differential drive Arduino-controlled autonomous robot. I have most of the electronics figured out, but I am having trouble understanding how much torque the motors will actually need to move the robot. I am trying to use the calculations shown here and the related calculator tool to determine what speed and torque I will need. I will be using wheels 32mm in diameter and one of Pololu's High-Power Micro Metal Gearmotors. I performed the calculations for a robot weight of 2 lbs to be safe and found that the 50:1 HP Micro Metal Gearmotors (625 RPM, 15 oz-in) should theoretically work fine, moving the robot at 3.43 ft/s with an acceleration of around 29 ft/s^2 up a 5-degree incline. 
However, I have not found an explanation for several things that I think would be very important to know when choosing drive motors. When the robot is not moving and the motors are turned on at full power, they should need to deliver their full stall torque. Based on the calculations, it seems that any amount of torque can get the robot moving, but the more torque, the faster the robot's acceleration. Is this true? Also, if the power source cannot supply the full stall current of the motors, will the robot not be able to start moving? In my case, I am powering the robot through a 7.2V (6S) 2200mAh NiMH battery pack that can provide around 2.6A continuously, and when it does that the voltage drops to less than 1V. Will this be able to power my motors? Once the robot reaches full speed and is no longer accelerating, theoretically the motors will not be providing any torque, but I do not think this is the case. Is it, and if so, how will I know how much torque they will be providing? Will the motors I chose have enough torque to move my robot?
","mobile-robot, motor"
PID tuning for Quadcopter,"I have been stabilizing my quadcopter. I tuned my angle PIDies and my quadcopter tries to stabilize itself, but there is some overshooting. Which is, I think, due to gyro rates. I have read that we have to use two PIDies on an axis. I'm having problems to attach these two PIDies. 
Can anyone help me in cascading angle PID and rate PID? Will I have to tune rate PID after tuning angle PID?
","quadcopter, pid"
"In the SLAM for Dummies, why are there extra variables in the Jacobian Matricies?","I am reading SLAM for Dummies, which you can find on Google, or at this link: SLAM for Dummies - A Tutorial Approach to Simultaneous Localization and Mapping.
They do some differentiation of matrices on page 33 and I am getting different answers for the resulting Jacobian matrices. 
The paper derived 
$$
\left[ {\begin{array}{c}
\sqrt{(\lambda_x - x)^2 + (\lambda_y - y)^2} + v_r \\ \tan^{-1}\left(\frac{\lambda_y - y}{\lambda_y - x}\right) - \theta + v_\theta \end{array}} \right]
$$
and got
$$
\left[ {\begin{array}{ccc}
\frac{x - \lambda_y}{r},& \frac{y - \lambda_y}{r},& 0\\ \frac{\lambda_y - y}{r^2},& \frac{\lambda_y - x}{r^2},& -1 \end{array}} \right]
$$
I don't get where the $r$ came from. I got completely different answers. Does anybody know what the $r$ stands for? If not, is there a different way to represent the Jacobian of this matrix?
",slam
Building a robotic clamp,"If I had a single stepper motor how could I use it to create a robotic clamp that could simply grab hold of something like a plank of wood and release it?
Are there any standard parts that I could use for this? I'm having trouble finding out what the names of the parts would be.
",robotic-arm
How to make a robot arm follow a shape/path,"First of all hope this is not a stupid question but I couldn't find any ware a solution.
I have constructed a 3 DOF robot arm. I want it to follow a trajectory on a 2D plane (XY). Tha shapes I want to follow are lines, cycles and splines. I now the math behind these 3 shaped (how they are defined). I have the kinematics, the inverse kinematics the jacobian and the whole control system (with the PID controller). The system receives as inputs, Xd(position), Xd'(velocity) and Xd''(acceleration) over time.
I found the following image that shows (more or less) my system.

So here is were I am stuck. How do I translate the shape to the position, velocity and acceleration that each joint needs to make so the end effector moves in the Cartesian space according to that shape?
","pid, robotic-arm, line-following"
Quadcopter: Stabilization along the z-axis (for holding altitude),"I recently spent some work on my quadcopter firmware.
The model is stabilizing its attitude relatively well now.
However I noticed, that it is changing its altitude sometimes (maybe pressure changes, wind or turbulence).
Now I want to get rid of these altitude drops and found not much literature.
My approach is using the accelerometer:

Calculates the current g-force of the z-axis
if the g-force is > 0.25 g and longer than 25 ms, then I feed the accelerometer term (cm per s²) into the pid
the output is sent to the motors

The model now reacts when it is falling down with an up-regulation of the motors.
However, I am not sure, whether it is smart to feed the current acceleration into the regulator and I currently wonder, whether there is a smarter method to deal with sudden and smaller changes in altitude.
Current code: 
# define HLD_ALTITUDE_ZGBIAS 0.25f
# define HLD_ALTITUDE_ZTBIAS 25

const float fScaleF_g2cmss = 100.f * INERT_G_CONST;
int_fast16_t iAccZOutput = 0; // Accelerometer

// Calc current g-force
bool bOK_G;
float fAccel_g = Device::get_accel_z_g(m_pHalBoard, bOK_G); // Get the acceleration in g

// Small & fast stabilization using the accelerometer
static short iLAccSign = 0; 
if(fabs(fAccel_g) >= HLD_ALTITUDE_ZGBIAS) {
  if(iLAccSign == 0) {
    iLAccSign = sign_f(fAccel_g);
  }

  // The g-force must act for a minimum time interval before the PID can be used
  uint_fast32_t iAccZTime = m_pHalBoard->m_pHAL->scheduler->millis() - m_iAccZTimer;
  if(iAccZTime < HLD_ALTITUDE_ZTBIAS) {
     return; 
  }

  // Check whether the direction of acceleration changed suddenly
  // If so: reset the timer
  short iCAccSign = sign_f(fAccel_g);
  if(iCAccSign != iLAccSign) {
    // Reset the switch if acceleration becomes normal again
    m_iAccZTimer = m_pHalBoard->m_pHAL->scheduler->millis();
    // Reset the PID integrator
    m_pHalBoard->get_pid(PID_ACC_RATE).reset_I();
    // Save last sign
    iLAccSign = iCAccSign;
    return;
  }

  // Feed the current acceleration into the PID regulator
  float fAccZ_cmss = sign_f(fAccel_g) * (fabs(fAccel_g) - HLD_ALTITUDE_ZGBIAS) * fScaleF_g2cmss;
  iAccZOutput = static_cast<int_fast16_t>(constrain_float(m_pHalBoard->get_pid(PID_ACC_RATE).get_pid(-fAccZ_cmss, 1), -250, 250) );
} else {
  // Reset the switch if acceleration becomes normal again
  m_iAccZTimer = m_pHalBoard->m_pHAL->scheduler->millis();
  // Reset the PID integrator
  m_pHalBoard->get_pid(PID_ACC_RATE).reset_I();
}

","quadcopter, multi-rotor"
SLAM Map Quardrant,"Transformation of frame from global to local is crucial for measurement update but how do we keep track of the direction ? eg: robot location [10,2] and [-10,2] requires different sign. Is there a way to not have to set a if else case and have a general expression ?
",slam
Can I control iRobot Create 2 with NI myRIO and LabVIEW codes?,"I need to know if the iRobot Create 2 can be controlled with a NI myRIO that has been programmed through LabVIEW. 
The goal is to program an autonomous robot for real-time tracking using a Kinect sensor. 
","mobile-robot, irobot-create, labview"
Arduino compatible sensor for motion detection and positioning,"I am working on a project that requires motion detection and positioning. I've worked substantially with a camera but the issue with this is that I need something sleek, small and not heavy at all. Cameras also tend to rely on luminosity and they don't work well in poorly lit spaces.
I need someone who's worked on something like this or who knows the best sensor for this purpose.
",arduino
I need the specifications for iRobot Create 2,"I need the specifications for the Create 2. I need it for research purposes. So I think I'm going to need a high computational computer on board.
Please suggest some nice configuration. 
",irobot-create
Measure weight of an object using a servo,"Assuming a quality industrial servo, would it possible to calculate the weight/resistance of a load? Maybe by comparing current draw in a holding position, or by the time it takes to lift/lower an object. Could it accurately measure grams, kilograms? What kind of tolerance could be achieved?
I'm trying to eliminate the need for a dedicated weight measurement sensor.
","sensors, robotic-arm, servomotor"
Dynamic model of a robot lifted by a balloon (Multibody system),"I'm having a hard time trying to understand how to obtain the dynamic model for a system similar to the image.  
The balloon is a simple helium balloon, however the box is actually an aerial differential drive platform (using rotors). Now there's basically a model for the balloon and another for the actuated box. However I'm lost to how to combine both. 
The connection between both is not rigid since it is a string.
How should I do it? Is there any documentation you could point me to, in order to help me develop the dynamics model for this system? 
Since I'm so lost, any help will be useful. Thanks in advance!  

","mobile-robot, kinematics, dynamics"
Why does the ESC stop?,"I've built a quadcopter with four brushless motors and ESCs (30A). I'm using an Arduino to control them. I haven't written any complex code; just enough to get them running. Everything is fine until I send a number over 920 to the serial. Then, for some reason, all the motors stop spinning. I'm using three freshly bought and charged LiPo cells (V = 11.1V). Here is the link for the site that I bought them from (I cannot seem to find any other resource about them) : 4x A2212 1000KV Outrunner Motor + 4x HP 30A ESC + 4x 1045 prop (B) Quad-Rotor. 
When I tried turning on only one motor, I could write up to about 1800 microseconds, while both with 4 and with 1 motor, the minimum that it works is 800. 
Can somebody explain why this happens and I how I can fix it? 
Here is my code: 
#include <Servo.h>

int value = 0;

Servo first,second,third,fourth;

void setup() {

  Serial.begin(9600);    // start serial at 9600 baud
  first.attach(6);
  second.attach(9);
  third.attach(10);
  fourth.attach(11);

}

void loop() {

    first.writeMicroseconds(value);
    second.writeMicroseconds(value);
    third.writeMicroseconds(value);
    fourth.writeMicroseconds(value);

    if(Serial.available() > 0){
      value = Serial.parseInt();
    }

}

","arduino, quadcopter, brushless-motor, esc"
Applying MoCap data to real life robot,"I have a Kinect Sensor, and iPi software I use to create motion capture data to use in film editing. I am looking at creating a small, Raspberry Pi driven bipedal robot just for fun, and I was wondering if it was possible to use the MoCap to control the robot? It will only be 20-30 cm tall, with six servos (hips, knees, ankles). Is it possible to apply the movement from these six joints on the human body to my robot, like having a string directly from my left knee joint to its left knee servo? It could either be in real-time, like following my actions, or using pre-recorded data.
(NOTE: If needed, I can plug it directly to my Apple/Windows computer, if the Pi could not support this. Also, it will have no upper torso at the moment.)
","mobile-robot, motion-planning, servos"
my quadcopter settling time is very large,"my quadcopter's settling time is very large, that is it sets its setpoint in very large amount of time, during which it has covered a large distance. But at settle point, when i gives it a jerk or push its returns to settle in normal duration. doesnt over shoots(little). The problem is with the settling time that is when i move the stick front or back it takes huge amount of time. what could be wrong. i have tried giving more P value and I value to PID but then it overshoots and get unstable. This is my PID program. the PID values are given. I read 6 channels from remote using the command pulsein(). which i guess is taking upto 20ms per command.
kp = 1.32;
ki= 0.025;
kd= 0.307;
void PID() {
  error = atan2(lx,ly);
  error *= 1260/22;   
  error = setpoint1 - error;   
  now = millis(); 
  dt = now - ptime;
  ptime = now;
  dt /= 1000;  
  integ = integ + (error * dt);
  der = (error - prerror) / dt;
  pidy = (kp * error); 
  pidy += (ki * integ);
  pidy += (kd * der); 
  //Serial.println(error);
  prerror = error;
}

pidy is added and subtracted to esc speeds respectively.
","arduino, quadcopter, pid"
Which kind of valve is used for dispensing food grains?,"I am doing a project on an automated grain dispenser system using a PLC control. I need a valve for dispensing grain from hopper to packet. I should be able to control the flow of the grain. 
So what kind of valve should I use for flow control of the grain? There are different types of grains like rice, wheat, etc., and the valve should be controlled by the PLC (opening and closing of valve).
",automation
Automatic sliding window shutter,"I want to build an automatic sliding window shutter and need help
with part selection and dimensioning.
Some assumptions: 

window width 1.4 m
sliding shutter weight 25 kg
max speed 0.07 m/s
max acceleration 0.035 m/s^2
pulley diameter 0.04m.

Leaving out friction I need a motor with about 0.02 Nm of torque and a rated speed of 33 rpm.
What I would like to use:

motor controller with soft-start and jam protection,
dc motor 24V,
Pulleys and timing belts.

Would you suggest other components or a different setup?
How do I connect motor and pulleys (clamping set?)?
Do I need additional bearings because of the radial load?
M=P
M=B=P
M=P=B
M=B=P=B

(M motor, P pulley, B bearing, = shaft)
If so I have to extend the motor shaft. What would I use for that (clamp collars, couplings?)?
What width do I need for the belts? Which belt profile (T, AT, HDT) should I use?
Update
The construction I am aiming for resembles the one which can be seen on page 6 (pdf numbering) here.
","motor, design, microcontroller, automation"
Computer stereo vision simulator,"In my research project I deal with a mobile robot that perceives through stereo vision. As the stereo input data I currently use several datasets taken from a passenger vehicle that contain real world photos. The datasets are good to get started but have a limited content so I would need to model my own traffic situations to further work on the stereo vision system.
I am thinking about using some kind of synthetic graphics simulation as the input for the stereo system. What are my options? I can imagine a 3D graphics rendering engine whose output would be fed as the input for the stereo vision could probably be used.
I found there are general robotic simulators available like Gazebo but since I am all new to robotic simulation I do not really know where to begin.
EDIT:
I forgot to write that all my code is a pure C++. I use OpenCV and LIBELAS for stereo vision and Point Cloud Library (PCL) for visualization. All glued together into a single C++ project and compiles into single binary.
","mobile-robot, simulator, stereo-vision"
Which geo-projection to use for odometry,"I would like to make a little survey regarding the (geo)spatial projection that you use when elaborating your GPS and movement data for the spatial awareness of your robots. 
Moving all GPS coordinates to a planar projection seems to be the more reasonable choice since not only distances (for which several formulas and approximations exist), but bearings must be computed.
Generally, although scales are pretty small here, avoiding equirectangular approximation seems a good idea in order to keep a more consistent system.
Avoiding working in the 3D world (Haversine and other great-circle stuff) is probably a good idea too to keep computations low-cost.
Moving the world to a 2D projection is hence what seems to be the best solution, despite reprojection of all input GPS coordinates is needed. 
I would like to get opinions and ideas on the subject 
(...if ever anyone is interested in doing it U_U).
","odometry, geometry"
Quadcopter multiple ESC angles glitch,"I'm developing my fligth controller board on Tiva Launchpad for quadcoper and while calibrating PID I discovered an unexpected behaviour: sometimes quadcopter seems to experience random angle errors. While trying to investigate it, I've figured out that my code if fairly trying to compensate tham, as soon as they appear - but do not cause them. Even more - i've discovered that such behaviour appears only when two (or more) motors are adjusted, while one motor system shows pretty good stabilisation.
Here is code for PMW output for different motors:
torque[0] = (int16_t)(+ angles_correction.pitch - angles_correction.roll) + torque_set;
torque[1] = (int16_t)(+ angles_correction.pitch + angles_correction.roll) + torque_set;
torque[2] = (int16_t)(- angles_correction.pitch + angles_correction.roll) + torque_set;
torque[3] = (int16_t)(- angles_correction.pitch - angles_correction.roll) + torque_set;

and here is recorded angles for system with one motor and two motors:

To be sure that it's not the algorithm problem, while recording this angles only Integral part of PID was non-zero, so angles were not even stabilised.
My question is - could esc noise each other (in my quad they are quite close to each other - just few sentimeters away) to cause such behaviour?
Thanks a lot!
","quadcopter, pid, esc"
Suitable D star variant is for non-holonomic motion planning of mobile robots,"I am working on a non-holonomic motion planning problem of a mobile robot in a completely unknown environment. After going through some research papers, I found that D-star algorithm is widely used in such conditions. But there are many D-star variants like Focused D*, D*-Lite, Field D* etc... So which of these variants is suitable in this case? Also please suggest any other better approach for this problem?
","mobile-robot, motion-planning, algorithm"
Ultrasonic Sensor's Lag (20Hz) effect on PID contol loop rate (150Hz),"Good day, I would like to ask how is it possible to use an ultrasonic sensor for altitude hold in a quadcopter if the sampling rate of the Ultrasonic sensor (HC-SR04) is only 20Hz before incurring any errors through polling when I had tested it. I have seen this sensor being implemented on other projects however I could not find any papers that explain the use of this sensor in better detail. I have seen possible solutions on the raspberry pi one using interrupts and the other using Linux's multithreading.
If my understanding is right, to use interrupts, I need a some sort of data ready signal from the ultrasonic sensor. However this is not available in this particular sensor. Is it possible to use the echo pin as the positive edge trigger for the interrupt service routine (read sonar/calculate distance function). But would this not introduce inconsistent loop execution times which is bad for a consistent PID loop.
Another approach is to use multithreading using the wiring-pi library which enables me to run a function, let's say a function that triggers the sonar and calculates the distance along side the pid control loop. How would this affect the PID control loop rate?

Which is the best way to implement sonar sensor based altitude hold?

","quadcopter, pid, stability, real-time, sonar"
Jacobian for Inverse Kinematics with quaternion of end effector,"Quaternion has four parameters. Calculating Jacobian for inverse-kinematics, 3 positions and four quaternion parameters make Jacobian $7\times7$ instead of $6\times6$. How to reduce Jacobian to $6\times6$ when using quaternion?
","robotic-arm, inverse-kinematics, jacobian"
Why does my RoboClaw seem to be ignoring the PID gain settings?,"I'm seeing a behavior in my RoboClaw 2x7 that I can't explain.  I've been trying to manually tune the velocity PID settings (I don't have a windows box so I can't use Ionmc's tuning tool) by using command 28 to set the velocity PID gains, then command 55 to verify that they're set correctly, then 35 to spin the wheel at half of its maximum speed.  The problem is that no combination of PID gains seems to make any difference at all.  I've set it to 0,0,0 and the motor still spins at roughly the set point.
I must be doing something wrong, but I'm pouring over the datasheet and I just don't see what it is.  By all rights the motor shouldn't spin when I use 0,0,0!  Any ideas?
","control, pid"
Vision-based Position Estimation for a quadrotor,"As a subtask inside a main project I need to compute the position (x,y,z) of a quadrotor using an homography. 
To do this I will use a camera (attached to the quadrotor) pointing down to an artificial landmark on the floor. Basically I need to compute the extrinsic parameters of the camera to know the pose with respect to the landmark. I know the projective points of the landmark in the camera, and the intrinsic matrix of the camera but I also need the real landmark position [X, Y, Z].
I suppose that Z coordinate is equal to 0 because the landmark is plane, but I am not sure how to compute the real [X,Y] coordinates.
Any idea how to do that?
I am also interested in put the (x,y,z) position of the quadrotor into a control path, anybody knows where I can find info about the most common controllers for do this kind of task?
","control, computer-vision, quadcopter, uav, visual-servoing"
Lateral load on a servo motor,"Looking at pictures of existing designs for quadropod robots, the servos in the legs seem to usually be mounted inside the chassis, with a second attachment at back of the servo as well, such as this:

rather than putting what looks like an asymmetrical load, like the knees here:

Is this for aesthetics or are there real structural reasons to minimize the lateral load on the axle on a robot of this size?
",joint
Does anyone have any walking patterns for a Biped Scout? (LYNXmotion),"I recently got a LYNX Biped Scout and found that it is really hard to actually come up with a working ""Gait"" or walking pattern. 
Making a servo move is easy, that's not the problem, I previously built a robotic arm from scratch (I have pictures if anyone is interested) and that one can be controlled via Arduino and a few potentiometers as it only has 4 degrees of freedom so it's not too hard to keep track of the different limbs.
However the Scout is a different beast entirely. It's a purpose built kit with 12 servos and to control them I'm using the LYNX SSC-32 Sequencer which is distributed freely on their website. The only problem is that making them all move in sequence to produce a convincing walking motion is actually really hard.
Has anyone got any patterns for this robot they would be happy to share? 
",walk
How to charge a LiFePO4 battery?,"From what I've seen, LiFePO4 batteries seem like one of the top battery choices for robotics applications. However, I've seen people mentioning that you can't use a charger for a different battery to charge these, but I haven't seen why. If I were to build my own setup to charge LiFePO4 batteries what would it specifically need to do? What kind of voltages or current rates does it need to supply to charge these?
More specifically, I was think about setting up a solar charger for these batteries. Is there any immediate reason why this is a bad solution? Such as, the battery needs to charge with a current above some amount for it to work properly?
If you're ambitious enough to provide an example along with your explanation, I'm specifically thinking of having 4 of these batteries with 2 pairs of 2 in series in parallel. 
",battery
"Usage of Multibeam 2D Imaging Sonar for AUVs, testing them in the pool environment","I belong to an AUV team at my university. We are planning to have a Multibeam 2D Imaging Sonar (the Blueview P900) for our AUV to detect obstacles underwater.
I have the following questions to ask on the feasibility of testing/implementing such sonars on AUVs.

As we know that these multibeam sonars have multiple reflections arriving at different times from various surfaces while testing in a pool, is there any recommended way to filter these noises in the image obtained from the sonar pings?
Are such sonars in use/test by any other team/organization anywhere else who do pool testing other than a ocean/reservoir testing where multiple reflections are almost zero except the reflections from the obstacle(s)?
Also i would like to know the recommended image processing algorithms that can be implemented/used to detect obstacles from the sonar images.

",sonar
Finding high torque servo for robotic arm,"I am new working with robotic arms but I am having trouble finding the correct servo for the base of the arm. 
It is a 2 link robot - each link weighs 1.2 kg and is 40 cm long. I have a gripper of 10 centimeters. The servo in the gripper can hold a max of 4kg. The whole robotic arm, including the maximum load it will carry and the servos and other accessories, is 8.3 kg. The maximum load it needs to carry is 4 kg at the end of the arm at 90 cm. 
What servo could I use to move the rotary base and what servo could I use to lift the arm in the base? The last one is to move the link so it would be preferable to have a 2 axis servo.
The only specification I need right now is what servo to use my energy supply are two 12 volts DC batteries connected in series with 18Ah. I need the servo to be DC. The other things can be worked around the servo that can best do the work.
","robotic-arm, servomotor"
Understanding Drift in Simultaneous Localization and Mapping (SLAM),"I am trying to understand the effect of drift in Simultaneous Localization and Mapping (SLAM). My understanding is that drift occurs because the robot tracks its position relative to a set of landmarks it is storing, but each landmark has a small error in its location. Therefore, an accumulation of these small errors over a long trajectory causes a large error by the end of the trajectory.
However, what I am confused about is what would happen when the robot tracks its way back to its starting positions. Suppose the robot starts in position A, and then starts to move along a path, mapping the environment as it does so, until it reaches position B. Now, the robot will have some error in its stored position of B, due to the drift during tracking. But then suppose the robot makes its way back to A, by tracking relative to all the landmarks it created during the first path. When it reaches A, will it be back at the true position of A, i.e. where it started the first path? Or will it have drifted away from A?
My intuition is that it will end up at the true position of A, because even though the landmarks have errors in them, as long as the error is not too large then the robot will eventually get back to the position where it stored the landmarks for A. And once it is there, those landmarks are definitely correct, without error, because they were initialized before any drift errors had started to accumulate.
Any help? Thanks!
","mobile-robot, localization, slam, navigation, mapping"
Topics of object perception for pr2,"I started to use ROS hydro (Robot Operating System) on ubuntu, using the simulator ""Gazebo"" and roscpp library, in order to program some robots. 
In case of pick up and place known objects by robots, what are the topics of object perception for pr2 in ROS??
","ros, gazebo"
"How to detect wall-corners, fans, lights in an indoor using CV?","Im currently working on an autonomous indoor quad-rotor. For this purpose I'm using OpenCV to enable computer vision in my drone. I need to be able to detect wall corners, fans (both stationary and rotating), lights and lamps, wall paintings and any other object associated with the walls and ceiling of an indoor environment. Until now I have come up with two ideas to achieve this. 
1) Establish ML (machine learning). Use feature descriptors like SIFT, SURF to collect a set of feature descriptors from a training set and try detect the objects of interest. The main issue with this is the access to SIFT and SURF algorithms as they are not available in OpenCV 3. 
2) Implement SLAM algorithm and map the environment and then use the information returned to identify the wall-corners. Of course this way I will be not able to detect fans and lights. 
So the question is, is there are any other methods I could use other than ones listed above in order to achieve my goal. Am I missing something on image segmentation, clustering or image transforms (hough line/circle) which could be utilised in my situation? 
Thanks
","computer-vision, machine-learning, opencv"
"2 wheeled, 2 motor robot control","I decided to work on a 2 wheeled robot position mapping problem. For that I have 2 DC motors with encoders from pololu.
I have the following two questions:

Do I need to know the model of each motor in order to tune a controller?
What are the steps/approach used to get a good mapping (let's say 1 mm maximum error)?

","pid, wheeled-robot, odometry"
Changing behaviour Roomba 880,"I've seen that it is possible to use some micro controller to send commands to the Roomba through the SCI but i was more interested in changing the behavior of the roomba operation (e.g: change the priority of the behaviours)
Is there some IDE for roomba?
Regards
",roomba
Micro Powder Dosing,"I am in process of designing a micro powder doser for metallic powder into plastic capsules (the capsule volume would be bigger than what we require, so traditional capsule filling wont work). The quantity I need to dose is 400 mg .. What in your opinion would be the best approach for this? 
As per my research, I have found out that there can be 3 approaches

Auger based solution, where an auger is used to control the powder drop gravitationally. 



Volumetric
As powder quantities would be small, a metal rods with groove of proper volume can be designed to dispense powder. For keep powder flowing we can use a vibration motor to pour powder in the groove. A stepper motor can rotate the rod for dispensing the powder. I have made the rod bigger to show the concept, and then how it would be mounted in a cap so when we rotate it that much volume of powder is dropped from the cap.



Weight based measurement.
Dropping powder on to a weighing balance, and control it via feedback .. I think it would be a difficult thing to do, plus time consuming, provided we'd have to fill thousands of capsules.

Prio # 1 is accuracy, with an error margin of 1 -2 % .. secondly cost .. I'd like it not to be too costly .. 300 - 500 $. 
The metal powder is not magnetic .. is very fine .. and doesn't clump .. I have read articles and it appears by continues tapping the powder flow can be improved a lot.
","arduino, motor"
Mobile phone power packs,"For my robotics project I would like to utilise readily available mobile phone 'power banks' to simplify the power system for my robot. However, such power banks output 5V, great for the logic systems but not for the motors. 
I was wondering if I could wire the outputs of two power banks in series and get 10V or is this a very bad idea? Should I wire them in parallel and use a boost converter? Is a custom solution using 'ordinary' Li-Po batteries and associated charging circuit the best answer?
Additional Information:

This will be a two wheeled robot.
5V Logic
7+V Motor driver
Power Banks: 5V 2.1Amp 2100mAh

","mobile-robot, power, battery"
Blade 180QX :What does this red wire do?,"I have a Blade 180QX quadrotor / quadcopter and I had to move a red wire shoved under the circuit board when I fixed a broken power wire. Now the red wire shown out straight from the circuit board is not in the right configuration - or at least as it was. If I understood what it was for I might know how to place it. Is this a horizon sensor (temperature)?
Ever since I had to move this wire, the quadrotor goes unstable when flying. The only appreciable change is the wire POSITION.
The red wire was not attached anywhere else on the board. It was shoved under the circuit board inside the battery holder.

",quadcopter
Why cannot we find EtherCAT shields?,"I have a riddle about EtherCAT in mind and I'd like to have your point of view about it...
With the rise of open platforms and hardware, and easily accessible embedded machines, it is now rather straightforward to install a RT system such as Xenomai on a raspberry PI, or a beagleboard black, or whatever cheap platform you prefer...
Now to connect these a RT bus would be really cool (e.g. EtherCAT...).
Hence my question: every hobbyist face the same problems with RT communication, so is there any good reason why there does not exist any open EtherCAT shield for raspberry PI or beagleboards? It would solve so many problems...
Any thoughts on why? Any idea?
",communication
Suggestions on object types (features) to track from ARDrone 2 camera,"
UPDATE
  I have aded 50 bounty for this question on the StackOverflow

I am trying to implement object tracking from the camera(just one camera, no Z info). Camera has 720*1280 resolution, but I usually rescale it to 360*640 for faster processing.
This tracking is done from the robots camera and I want a system which would be as robust as possible. 
I will list what I did so far and what were the results.

I tried to do colour tracking, I would convert image to hsv colour space, do thresholding, some morphological transformations and then find the object with the biggest area. This approach made a fair tracking of the object, unless there are no other object with the same colour. As I was looking for the max and if there are any other objects bigger than the one I need, robot would go towards the bigger one
Then, I decided to track circled objects of the specific colour. However, it was difficult to find under different angles
Then, I decided to track square objects of specific colour. I used this


       // Approximate contour with accuracy proportional
        // to the contour perimeter
        cv::approxPolyDP(
                cv::Mat(contours[i]),
                approx,
                cv::arcLength(cv::Mat(contours[i]), true) * 0.02,
                true
        );


and then I checked this condition

if (approx.size() >= 4 && approx.size() <= 6)

and afterwards I checked for

solidity > 0.85 and aspect ratio between 0.85 and 1.15

But still result is not as robust as I would expect, especially the size. If there are several squares it would not find the needed one.

So, now I need some suggestions on what other features of the object could I use to improve tracking and how? As I mentioned above several times, one of the main problems is size. And I know the size of the object. However, I am not sure how I can make use of it, because I do not know the distance of the object from the camera and that is why I am not sure how to represent its size in pixel representation so that I can eliminate any other blobs that do not fall into that range.
UPDATE
In the third step, I described how I am going to detect squares with specific colour. Below are the examples of what I am getting. 
I used this HSV range for the red colour:

Scalar(250, 129, 0), Scalar(255, 255, 255), params to OpenCV's inRange function
HMIN = 250, HMAX = 255; SMIN = 129, SMAX = 255; VMIN = 0, VMAX = 255;
  (Would like to see your suggestions on tweaking this values as well)

So, in this picture you can see the processing; gaussian blurring (5*5),
morphological closing two times (5*5). And the image with the label ""result"" shows the tracked object (please look at the green square).

On the second frame, you can see that it cannot detect the ""red square"". The only main difference between these two pics is that I bended down the lid of the laptop (please look closer if you cannot notice). I suppose this happens because of the illumination, and this causes the thresholding to give not desired results. 

The only way, I can think of is doing two separate processing on the image. First, to do thresholding based on the colour as I was doing above. Then if I find the object to move to the next frame. If not to use this opencv's find squares method.
However, this method will involve doing too much of processing of the image. 
","quadcopter, cameras, opencv"
ROS tutorials no longer working,"Has anyone ever run into a case where a fresh install of ROS cannot run its tutorial packages?
I am running ROS Indigo on an nVidia Jetson TK1, using the nVidia-supplied Ubuntu image. I just did a fresh install, Ubuntu and ROS, just to keep things clean for this project. I am building a kind of 'demo-bot' for some students I will be teaching; it will use both the demo files and some of my own code. Now, after setting things up, I try to run the talker tutorial just to check to make sure that everything is running, and rospack is pretty sure that the tutorials don't exist.
For example, inputting this into the terminal
rosrun rospy_tutorials talker

Outputs
[rospack] Error: package 'rospy_tutorials' not found

This is the case for every tutorial file; python and C++. Now, I am sure the tutorials are installed. I am looking right at them in the file system, installed from the latest versions on github. So I think it is something on ROS' side of things.
Has anyone ever bumped into something similar before, where a ROS package that supposedly was installed correctly isn't found by ROS itself? I would rather not have to reinstall again if I can avoid it.
EDIT #1
After playing with it some more, I discovered that multiple packages were not running. All of them - some turtlebot code, and some of my own packages - returned the same error as above. So I suspect something got messed up during the install of ROS.
roswtf was able to run, but it did not detect any problems. However, going forward.
EDIT #2
I double checked the bashrc file. One export was missing, for ROS directory I was trying to work within. Adding it did not solve the problem. 
I am still looking for a solution, that hopefully does not involve reflashing the TK1.
EDIT #3
Alright, so I've been poking at this for a few days now and pretty much gave up trying to get ROS to work correctly, and decided a re-flash was necessary. But I think I found something when I booted up my host machine. In my downloads folder, I have the v2.0 and the v1.2 JetPack. I know I used the v2.0 for this latest install, and it has been the only time I have used it (it provides some useful updates for OpenCV and bug fixes, among other things). I'm going to re-flash using the v1.2 JetPack this time, see if things behave better with ROS under that version. Its a long shot, but it is all I have to work with at the moment, and it shouldn't lose any ROS capabilities (aside from some of the stuff I wanted to do with OpenCV). I'll update everyone if that seems to work.
EDIT #4
Ok, everything seems to be working now. The problem does seem to be an issue with Jetpack v2.0. I suspect that some change, somewhere between v1.2 and v2.0 (made to accommodate the new TX1 board), messes with running ROS indigo on a TK1. I'm going to be a more detailed explanation in an answer to this question.
",ros
How can I send a jpeg image to a microcontroller via USART?,"How can I send a jpeg image to a microcontroller via USART?
","electronics, embedded-systems"
How to guide a camera through a circular tube?,"Let's say I have a 6-DOF flying camera and I want to make it move through a circular tube autonomously and let's suppose that the camera and the system that makes it fly are considered to be just a point in space. Which feature of the image I get from the camera can I use to move the camera appropriately, that is to get in one end of the tube and get out from the other? 
For example, I thought I could use edge detection. As the camera moves forward through the tube, due to the fact that its far plane is not infinitely away, there is a dark circle forming where the camera sees nothing surrounded by the walls of the tube. I think that ""preserving"" this circle might be the way to go (for example if it becomes an ellipse I have to move the camera accordingly for it to become a circle again), but what are the features that will help me ""preserve"" the circle?
I would like to use image-based visual servoing to do that. However, what troubles me is the following. In most visual-servoing applications I have seen, the control objective is to make some features ""look"" in a certain way from the camera point of view. For example, we have the projections of 4 points and we want the camera to move accordingly so that the projections' coordinates have some specific values. But the features are actually the same. 
In my case I thought that for example I could say that I want the projections of the 4 ""edge points"" of the circle/ellipse to take specific values so that they define a circle centered at the fov of the camera. But if the camera moves to achive this setup of features, then the 4 new ""edge points"" will correspond to the projections of 4 different real points of the pipe and the theory collapses. Am I right to think that? Any way to get past it?
Any oher ideas or relevant literature?
","localization, cameras, motion-planning, visual-servoing, exploration"
Papers on Algorithms in Robotics,"I'm a CS student and I need to give a 30-minute lecture about 1-2 papers describing 1-2 algorithms for any of the main problems in Robotics (navigation, coverage, patrolling, etc.).
I have no background in Robotics specifically, but I did take classes such as Algorithms and AI (including some basic AI algorithms such as A*, IDS, UCS, and subjects such as decision trees, game theory, etc.).
The difference between simply describing one of the above is that I need the paper to refer to actual physical robots and their algorithms, with real problems in the physical world, as opposed to AI ""agents"" with more theoretical algorithms.
I am required to lecture on 1-2 academic papers, published from 2012 onward, with a ""respectable"" amount of citations. Any suggestions of such papers would be greatly appreciated!
","navigation, algorithm, theory, coverage"
Inverse kinematics with singularity in MATLAB,"I want to find the general coordinates q=[alpha,beta,gamma] (3 revolute joints) that minimizes the norm ||rGoal - r||_2 with rGoal not included in the manipulator workspace. 
The problem is already solved for coordinates rGoal inside the manipulator workspace, but I really dont know how to do that for Singularities.
% given are the functions 
%   r_BF_inB(alpha,beta,gamma) and
%   J_BF_inB(alpha,beta,gamma) 
% for the foot positon respectively Jacobian

r_BF_inB = @(alpha,beta,gamma)[...
    -sin(beta + gamma) - sin(beta);...
  sin(alpha)*(cos(beta + gamma) + cos(beta) + 1) + 1;...
  -cos(alpha)*(cos(beta + gamma) + cos(beta) + 1)];

J_BF_inB = @(alpha,beta,gamma)[...
                                              0,             - cos(beta + gamma) - cos(beta),            -cos(beta + gamma);...
 cos(alpha)*(cos(beta + gamma) + cos(beta) + 1), -sin(alpha)*(sin(beta + gamma) + sin(beta)), -sin(beta + gamma)*sin(alpha);...
 sin(alpha)*(cos(beta + gamma) + cos(beta) + 1),  cos(alpha)*(sin(beta + gamma) + sin(beta)),  sin(beta + gamma)*cos(alpha)];

% write an algorithm for the inverse kinematics problem to
% find the generalized coordinates q that gives the endeffector position rGoal =
% [0.2,0.5,-2]' and store it in qGoal
q0 = pi/180*([0,-30,60])';
rGoal = [0.2,0.5,-2]';

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% enter here your algorithm
q=q0;
b=true;
while b 
    q_old=q;
    dr=rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3));
    dq=J_BF_inB(q_old(1),q_old(2),q_old(3))\dr;
    q=q_old + dq;
    if norm(rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3)))< 1E-3
        b=false
    end
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

qGoal = q; 

A stopping criteria I can use is:
norm(rGoal-r_BF_inB(q_old(1),q_old(2),q_old(3)))< norm(rGoal-r_BF_inB(q(1),q(2),q(3)))

But that does not really give me the correct answer.
How can this be solved in general for a rGoal outside of the workspace?
",inverse-kinematics
6D localization with 6 lasers,"I have to know where a multi-rotor is, in a rectangular room, via 6 lasers, 2 on each axis.
The problem is like this: 
Inputs :

Room : square => 10 meters by 10 meters
6 positions of the lasers : Fixed on the frame
6 orientations of the lasers : Fixed on the frame
The 6 measurements of the lasers
The quaternion from the IMU of my flight controller (PixHawk).
The origin is centered on the gravity center of the multi-rotor and defined as if the walls are perpendicular to each axes (the normal of the wall in X is (-1,0,0)) 

Output :

Position in 3D (X,Y,Z)
Angular position (quaternion)

Since I got the angular position of the multi-rotor, I rotated the laser positions and orientations via the quaternion, then extrapolate via the 6 measurements and I got the 3 walls. (orientations of the walls are trivial, then only one point is enough to determine its position.
Badly, I noticed that the yaw (rotation about z) measurement from the PixHawk is unreliable. Then I should measure the yaw from the lasers, but I do not success to do it. Event if the 2D problem is easy, I am lost in 3D.
Does someone know if it [Algorithm to know XYZ position and quaternion from 6 measurments] exists somewhere ? Or what is the right way to go on this problem ? 
The question : How could I get the yaw from 2 measurements from 2 lasers which I know the original position, orientation and the pitch and roll. 
NOTE : Green pointers are the origin position, Red pointers are the ""final"" position, but could be rotated around the red circle (due to yaw).

","algorithm, geometry"
Action cost to get smooth path,"What action cost should be used to get a smooth path? Like we use distance traversed to get the shortest path. Will the cost to get a smooth path will be something related to rate of change of slope of the path?
",motion-planning
How difficult it is to build simple robots (for example Line follower) using raspberry pi and ROS?,"I want to build a low cost robot, running ROS for educational purposes. It can be a simple line follower using raspberry pi and an IR sensor. Is it overambitious as a beginner project? How difficult is it to make ROS run on custom hardware?
P.S. I am newbie in both robotics and programming and I am more interested in building actual robots than running simulations. Also, I cant afford to buy ROS compatible robots.
","ros, raspberry-pi, electronics"
Waiting for the r_gripper_sensor_controller/gripper_action action server to come up,"I started to use ROS hydro (Robot Operating System) and roscpp.
I tested some examples to move the gripper of pr2 in Gazebo (especially the code in :  http://wiki.ros.org/pr2_gripper_sensor_action/Tutorials/Grab%20and%20Release%20an%20Object%20Using%20pr2_gripper_sensor_action ) with catkin package.
I launch : roslaunch pr2_gazebo pr2_empty_world.launch 
and when I run the node of code with : rosrun pack_name node_name, I get :: Waiting for the r_gripper_sensor_controller/gripper_action action server to come up ... Waiting for the r_gripper_sensor_controller/gripper_action action server to come up ...
I want to know the cause of those lines in order to see the results. what should I do??
it is notable that when I launch : roslaunch pr2_gripper_sensor_action pr2_gripper_sensor_actions.launch
      in the previous link, I get : 
[pr2_gripper_sensor_actions.launch] is neither a launch file in package [pr2_gripper_sensor_action] nor is [pr2_gripper_sensor_action] a launch file name
","ros, gazebo"
Are operational space and joint space dependent on each other?,"Some questions about this, my friends and I argued with this problem.
Are operational space and joint space dependent on each other?
I know that $x_e$ (end effector's pos.) and $q$ (joint var.) can be expressed by an equation with non-linear function $k$:
$x_e = k(q)$
But I don't think that it tells us operational space and joint space are dependent. 
",kinematics
Establishing Data Transfer between two Raspberry Pi's using GPIO,"Good day
I am currently implementing an autonomous quadcopter with stereo vision using raspberry Pi. One (Let's call this Pi_1) is responsible for stereo vision the other is responsible for motor control and trajectory planning (Pi_2). I was looking for a way to transfer a 480 element float vector via GPIO from Pi_1 to Pi_2. Pi_1 stereovision program runs at 2Hz while Pi_2 motor control runs at 210Hz. 

Is there any protocol fast enough to deliver this amount of information to the second raspberry pi via GPIO? 

I am currently looking at SPI but I saw that the Raspberry Pi cannot be turned to a Slave making it not an option. I also looked at UART however it is too slow for my needs. All the I2c ports on the Pi are currently being used by the stereo vision cameras and the IMU's. If the gpio option is not feasible, I am also open for other suggestions such as using other hardware (middle man) or wireless options.   
","quadcopter, raspberry-pi, communication"
Atlas Robot Reference,"Boston Dynamics keeps making great robots, however, I dont see any papers that they publish.  Although now I can find papers on people using the ATLAS robot, I can not find an original paper detailing the robot or its mechanics designs.  Is there a reference for the robot, should I use youtube videos?
","design, mechanism, humanoid"
3 way check valve,"I am working on a micro dispensing system, using syringe pump. The design involves a syringe on top to be moved by stepper motor. There would be one liquid reservoir form which the syringe would pull liquid from, and push it to eject liquid from other end. 
When we pull the syringe, the liquid is sucked into the syringe, while the other opening is shut. When the syringe is pushed, the liquid is ejected from the other end.
The quantity of liquid to be dispensed would be very small (400mg) so i am using small syringe of 1 or 2 ml .. as per my measurement, after every 100 dispensing operations, 1 ml syringe would be empty and we would need to pull liquid from the reservoir into the syringe, and do the dispensing again. 
My question is, I am unsure about the check valve here. Is there a 'Single' check valve available which would allow this kind of flow to happen ?
","motor, design"
Pure Arduino Quadcopter,"I recently bought a set of escs, brushless outrunner motors and propellers. I'm trying to perform a calibration on the esc, but I can't find how I can do that without using components other than the arduino uno itself. The setup I've managed to make is the one shown in the picture. The escs are a mystery, as there is no manual to be found. If it helps, the buy link is this : http://www.ebay.co.uk/itm/4x-A2212-1000KV-Outrunner-Motor-4x-HP-30A-ESC-4x-1045-prop-B-Quad-Rotor-/111282436897
There might also be a problem with the battery (LiPo 3.7V, 2500mAh).

Can andybody figure out what I'm doing wrong?
The sample arduino code I found was this:
#include <Servo.h>

#define MAX_SIGNAL 2000
#define MIN_SIGNAL 700
#define MOTOR_PIN 9

Servo motor;

void setup() {
  Serial.begin(9600);
  Serial.println(""Program begin..."");
  Serial.println(""This program will calibrate the ESC."");

  motor.attach(MOTOR_PIN);

  Serial.println(""Now writing maximum output."");
  Serial.println(""Turn on power source, then wait 2 seconds and press any key."");
  motor.writeMicroseconds(MAX_SIGNAL);

  // Wait for input
  while (!Serial.available());
  Serial.read();

  // Send min output
  Serial.println(""Sending minimum output"");
  motor.writeMicroseconds(MIN_SIGNAL);

}

void loop() {  
}

","arduino, quadcopter, esc"
SLAM : Why is marginalization the same as schur's complement?,"Consider the system 
$$
\tag 1
H\delta x=-g
$$
 where $H$ and $g$ are the Hessian and gradient of some cost function $f$ of the form $f(x)=e(x)^Te(x)$. The function $e(x)=z-\hat{z}(x)$ is an error function, $z$ is an observation (measurement) and  $\hat{z}$ maps the estimated parameters to a measurement prediction. 
This minimization is encountered in each iteration of many SLAM algorithms, e.g.one could think of $H$ as a bundle adjustment Hessian. Suppose $x=(x_1,x_2)^T$, and let $x_2$ be some variables that we seek to marginalize. Many authors claim that this marginalization is equivalent to solving a smaller liner system $M\delta x_1=-b$ where $M$ and $g$ are computed by applying Schur's complement to (1), i.e. if
$$H=
\begin{pmatrix}
H_{11} & H_{12}\\
H_{21} & H_{22}
\end{pmatrix}
$$
then
$$
M=H_{11}-H_{12}H_{22}^{-1}H_{21}
$$ 
and 
$$
b=g_1-H_{12}H_{22}^{-1}g_2
$$
I fail to understand why that is equivalent to marginalization... I understand the concept of marginalization for a Gaussian, and I know that schur's complement appears in the marginalization if we use the canonical representation (using an information matrix), but I don't see the link with the linear system. 
Edit: I understand how Schur's complement appears in the process of marginalizing or conditioning $p(a,b)$ with $a,b$ Gaussian variables, as in the link supplied by Josh Vander Hook. I had come to the same conclusions, but using the canonical notation: If we express the Gaussian $p(a,b)$ in canonical form, then $p(a)$ is gaussian and its information matrix is the Schur complement of the information matrix of $p(a,b)$, etc. Now the problem is that I don't understand how Schur's complement appears in marginalization in bundle adjustment (for reference, in these recent papers: c-klam (page 3 if you want to look) and in this (part titled marginalization). In these papers, a single bundle adjustment (BA) iteration is performed in a manner similar to what I initially described in the question. I feel like there is a simple connection between marginalizing a Gaussian and the marginalization in BA that I am missing. For example, one could say that optimizing $f$ (one iteration) is equivalent to drawing a random variable following a denstiy $$e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$  where $\Sigma$ is the inverse of the Hessian $H$ of $f$, and $\mu$ is the true value for $x$ (or an approximation of that value), and that marginalizing this density is equivalent to using Schur's compelement in the bundle? I am really confused...  
","slam, computer-vision"
simulation of robots,"I am designing a new mechanism similar to robot arm. It would be a 6 or 7 axis with arms but not the same with traditional articulated arms. As a result, new DH matrix,and inverse kinematics involve. I would like to consult the robot professionals in this forum that do you suggest any simulation tool of this mechanism?
I plan to start with start and end points. Then I will do a trapezoid velocity plan and take sample points with sampling time along the path. After that, I would like to transfer these sampling points to motor joints by DH matrix and inverse kinematics. Finally I would do some basic 3D animation to visualize the movement temporally. I do not plan to simulate controller behavior because in my application motor drivers deal with it. I only need to focus on sending reasonable commands to motor drivers. 
In my opinion, Matlab, octave, VC++, and some third-party tools are candidates. Starting from ground zero would be a time-consuming work. I would appreciate if any experts can share a tool or open source code from his or her experience. I did some search on Matlab robotics toolbox but I am not sure if it fits my need because it is expensive and optimized for ROS. In Octave there are also some robotics toolbox but I am not sure about what it can do and what it cannot. 
","robotic-arm, matlab, simulation, c"
Cartesian space velocity profile to minimize jerk,"I am working with a 6 DOF manipulator. Currently I have implemented a simple velocity controler along a fixed direction on xyz space. I control the xyz space velocity (xdot) by using a predefined velocity profile against time. 
Joint values are updated based on the defined velocity profile.
Assume I want to move robot along a direction parallel to z axis, I define a trapezoidal velocity profile (z dot) over time as following, 
In the robot controller program, I convert this (z dot) to velocity at a time in joint space by multiplying by inverse of jacobian. In this way I can move robot as needed.
My question is how we can define the above velocity profile over time, so that the total jerk in joints over time is minimized ?
Your help is really appreciated.
","control, robotic-arm, manipulator"
Evaluating the similarity of two 7 Degree of Freedom Arms,"I am working on the Baxter robot where I have a first arm configuration and a bunch of other arm configurations, where I want to find the closest arm configuration to the first among the many other arm configurations. The trick here is that the end effector location/orientation is the exact same for all the arm configurations, they are just different ik solutions. Can anyone point me towards the right direction towards this? Thank you.
","robotic-arm, inverse-kinematics"
Use MATLAB Compiler SDK generated package in KUKA Sunrise.Workbench,"The MATLAB Compiler SDK allows to create a wrapper for a MATLAB function which can be accessed by Java software. Based on my understanding, KUKA's Sunrise.Workbench IDE uses most of the standard Java functions. 
I was trying to read the package generated using the MATLAB Compiler SDK (the new version of MATLAB Builder JA) into the workbench platform. I could successfully read the package into Eclipse IDE, but not into Workbench.
The reason for using the Compiler SDK is that, I have some functions is MATLAB, and I want to use the same in the workbench programming.
Does anyone have experience with the same? Appreciate any help.
","matlab, robotc"
Uncented Kalman Filter for Dummies,"I need some help here because I can't figure how the Unscented Kalman Filter works.
I've searched for examples but all of them are too hard to understand.
Please someone can explain how it works step by step with a trivial example like position estimation, sensor fusion or something else?
",kalman-filter
Syncing camera with other signals,"I am not sure if this is the best place to ask this question, but hopefully someone here can give me some advice. I have a device hooked up to a data acquisition system that can provide sync out signal and record sync in signals. I need to synchronize my recordings with this device to a video feed. I am having trouble finding a camera that can provide a sync signal or any other good way to accomplish this. Thanks for your help.
","sensors, cameras"
Solar Cells Charging a Li-Po Battery,"Is there a way to charge a Li-Po battery using solar panels to increase the flight time of a quadcopter during its flight?
","quadcopter, power"
Static equilibrium for 7 dof manipulator,"I have a 7 dof manipulator (Kuka LBR4+) and would like to calculate the joint torques needed to keep the arm in a static equilibrium. In most books the transposed jacobian is used to map the forces applying on the end effector to the joint torques.
$\tau = J^T\cdot F$
That however doesn't take the mass of the links into account. Is there a way to calculate the needed torques for a given configuration so that, assuming an ideal case, by setting these torques the arm will be in a static equilibrium?
cheers
EDIT:
For everybody interested, i found a solution to this problem in Introduction to Robotics - Third Edition by John J. Craig on Page 175-176. It is done with the aid of the iterative Newton-Euler dynamics algorithm. The actual trick is, to set all velocities and accelerations to zero except for the base acceleration. The base acceleration will be $^0 \dot v_0 = G$, where G has the magnitude of the gravity vector but points in opposite direction. This is equivalent to saying that the base of the robot is accelerating upwards with 1 g and this upward acceleration causes exactly the same effect on the link as gravity would.
","torque, manipulator"
How to communication with old robotic arms from NakkaNippon Electric?,"I have a few robotic manipulators from NakkaNippon Electric and I'm trying to communicate with them using RS232 without success. The robot model are microrobots 88-4 or 88-5.
I'm sending commands via COM port, but I can't received anything from the box. I'm using a USB-to-DB9 converter (FTDI) with a DB9-DB25 cable.
On the net, the only reference I have for the robot is from an old post from year 2000 that was from user @peterkneale.
If it can help, here is the link to the scanned PDF manual.
You can see the commands on page 23-24 of the pdf (page 21-22 in document). 
Any advice would be grateful
",robotic-arm
Check collision between robot and environment in OpenRAVE,"I have a robot arm in an environment. How can I check for collision between this robot arm and the environment?
","robotic-arm, motion-planning, python"
Programming 4-digit seven segment display using interrupts only,"I have to program an autonomous bot (using an ATmega2560). It has a 4-digit seven segment display attached to it. I have to make the bot traverse through arena while continuously displaying the time in seconds on the seven segment display.
I can't use the code to display on seven segment display in my main() function.
Any help? 
",interrupts
Wearable Accelerometor,"I've worked with wiimote accelerometer, but I think now I want to move past that. Mostly because I want to have a wider range of available gestures and I think that using only one accelerometer has too many limitations for what I want to do. I'm looking for something compatible with arduino or RPi. Does anyone have recommendations on how I should do this?
","control, sensors, accelerometer"
"What is the difference between planning for kinematic car, dynamic car, blimp and quadrotor","I am working with a sampling based planning library. When I looked into the implementation, I found for kinematic car a SE2 state space(x, y, yaw), for dynamic car a SE2 compound state space (space allowing composition of state spaces), for blimp and quadrotor a SE3 compound state space was used. I could understand the design of SE2 and SE3 state spaces but the compound state spaces of dynamic car, blimp and quadrotor I could not comprehend or differentiate.
what is the difference in terms of state space for motion planning for kinematic car, dynamic car, blimp and quadrotor?
","mobile-robot, quadcopter, motion-planning"
using the brush assembly and Aerovac bin space?,"A lot of the Create 2's interior space is taken up by the brush assembly and the Aerovac bin.  I'd like to take these out and put in my own stuff, but I'm concerned that the Roomba might get confused by the fact that I've unplugged these items.  Is there anything special I need to do, aside from adding an appropriate amount of weight in that area?
",irobot-create
How to use quaternions to feed a PID quadcopter stabilization loop?,"I'm making a quadcopter. I have set up a PID loop to stabilize it to a given Euler angle (pitch and roll). The problem arises when the roll approaches 90 degrees (45 degrees and up). The values don't make sense anymore, as it approaches the gimbal lock. I intend to make it do complex maneuvers like looping etc., which exceeds the 45 degree roll limit.
How can I use quaternions to overcome this problem? (I get quaternions from the MPU-9150.) I have read many articles on the matter of quaternions, but they all talk about rotations in 3D software, and tweening between two rotation points. This makes little sense as I do not know imaginary numbers and matrices.
","quadcopter, pid, stability"
iRobot Create: Making Noise and Flashing Red Light While Charging,"My iRobot Create is playing a tune about every 30 seconds and continuously flashing a red light when I attempt to charge it. What is the issue?
",irobot-create
What is the best way to attach a 3D printed part to a servo for robotics use?,"I am trying to make custom parts that fit directly onto a servo. Doing this has proved more difficult than I've expected so far.
I was hoping to avoid incorporating the provided servo horns into the 3D printed part, so I've been trying this method out. Below are images of my current test - a 3D printed attachment to the servo, with an indentation for an M3 nut (the servo accepts an M3 bolt) for attachment to the servo. The plastic ring doesn't have the spline (I can't print that level of detail I think) but is tight around it. The top piece attaches to a 3/8"" nut for use with the 3/8"" threaded rod I had laying around.


So far, I'm having difficulty of this setup working with any level for torque and not just spinning in place.
So... is this the correct approach? Am I going to have to design a piece with the servo horn inside of it to get the servo to connect? Are there better approaches I haven't considered?
","servos, 3d-printing"
Quadcopter PID output,"I'm trying to develop a control system to a quadcopter and one of my options is to use a PID controller (which I think is the most used method).
From what I've read, the commom strategy is to run a PID algorithm to each axis. My question is: how the PID output is converted to PWM signals?
I'm asking that because the three axes and the four rotors depend on each other. I mean, if I slow down a couple of rotors which are opposite to each other then the quadcopter will move down in a vertical axis. But if I speed one of them and slow down the other, the quadcopter will rotate in a different axis.
So we cannot isolate each axis and associate them with a single rotor or a pair of those. And because of that, how can a PID output (which is associated to an axis) can be converted to PWM signals to the rotors?
Is that a MIMO system?
","quadcopter, pid"
Using genetic algorithm for tuning controllers,"I've read some papers for controlling nonlinear systems (e.g. nonlinear pendulum). There are several approaches for targeting nonlinear systems. The most common ones are feedback linearizaing, backstepping, and sliding mode controllers. 
In my case, I've done the theoretical and practical parts of controlling nonlinear model of a simple pendulum plus other manipulators problems in C++. For the pendulum, I've utilized a backstepping controller for solving the tracking task for the angular displacement and velocity. The results are 
$$
\ddot{\theta} + (k/m) \dot{\theta} + (g/L) \sin\theta= u 
$$
where $m=0.5, k=0.0001, L=.2$ and $g=9.81$.


The results are good. However, tuning the controller is time consuming. The majority of papers use genetic algorithms for tuning their controllers such as PD, PID, and backstepping controllers. I'm clueless in this field and I hope someone sheds some light on this concept, preferable if there is a MATLAB sample for at least controlling a simple pendulum.
So far I've designed a simple GUI in C++/Qt in order to tune the controller manually. In the below picture, the response of the controller for step function. 
 
",control
Helicopter Stabilization Algorithm,"I've hacked a rc helicopter, and I am able to control it by running a program on my computer. I am interested in writing algorithms that will stabilize the helicopter. For instance, the helicopter is hovering, and then if it is shoved off balance it can return to its previous position in a stable state. Any help on an algorithm would be awesome.
","algorithm, stability"
A Vector Field Histogram implementation in Python 2.7,"I am trying to implement the Vector Field Histogram as described by Borenstein, Koren, 1991 in Python 2.7 using the SciPy stack.
I have already been able to calculate the polar histogram, as described in the paper, as well as the smoothing function to eliminate noise. This variable is stored in a numpy array, named self.Hist.
However, the function computeTheta, pasted below, which computes the steering direction, is only able to compute the proper direction if the valleys (i.e. consecutive sectors in the polar histogram whose obstacle density is below a certain threshold) do not contain the section where a full circle is completed, i.e. the sector corresponding to 360º.
To make things clearer, consider these two examples: 

If the histogram contains a peak in the angles between, say, 330º and 30º, with the rest of the histogram being a valley, then the steering direction will be computed correctly.
If, however, the peak is contained between, say, 30º and 60º, then the valley will start at 60º, go all the way past 360º and end in 30º, and the steering direction will be computed incorrectly, since this single valley will be considered two valleys, one between 0º and 30º, and another between 60º and 360º.
def computeTheta(self, goal):
thrs = 2.
s_max = 18


#We start by calculating the sector corresponding to the direction of the target.
target_sector = int((180./np.pi)*np.arctan2(goal[1] - self.VCP[1], goal[0] - self.VCP[0]))
if target_sector < 0:
    target_sector += 360
target_sector /= 5

#Next, we assume there is no best sector.
best_sector = -1
dist_best_and_target = abs(target_sector - best_sector)

#Then,  we find the sector within a valley that is closest to the target sector.
for k in range(self.Hist.shape[0]):
    if self.Hist[k] < thrs and abs(target_sector - k) < dist_best_and_target:
        best_sector = k
        dist_best_and_target = abs(target_sector - k)

#If the sector is still -1, we return it as an error.
print (target_sector, best_sector)
if best_sector == -1:
    return -1

#If not, we can proceed...
elif best_sector > -1:
    #... by deciding whether the valley to which the best sector belongs is a ""wide"" or a ""narrow"" one.
    #Assume it's wide.
    type_of_valley = ""Wide""

    #If we find a sector that contradicts our assumption, we change our minds.
    for sector in range(best_sector, best_sector + s_max + 1):
        if sector < self.Hist.shape[0]:
            if self.Hist[sector] > thrs:
                type_of_valley = ""Narrow"" 

    #If it is indeed a wide valley, we return the angle corresponding to the sector (k_n + s_max)/2.
    if type_of_valley == ""Wide"":

        theta = 5*(best_sector + s_max)/2
        return theta

    #Otherwise, we find the far border of the valley and return the angle corresponding to the mean value between the best sector and the far border.
    elif type_of_valley == ""Narrow"":
        for sector in range(best_sector, best_sector + s_max):
            if self.Hist[sector] < thrs:
                far_border = sector

        theta = 5*(best_sector + far_border)/2
        return theta


How can I address this issue? Is there a way to treat the histogram as circular? Is there maybe a better way to write this function?
Thank you for your time.
","motion-planning, python, planning"
"Do ""nano bots"" (that can fit inside the human body) actually exist?","I was wondering, do we have real nano bots, like the ones in the movies? 
I think we have bots which can move through the blood vessels, am I right?
",mobile-robot
How to align solidworks global origin with assembly origin while exporting in solidworks to urdf,"I have created a robot model in solidworks and exported in solidworks to urdf plug-in. When exporting the co-ordinates of the model is misaligned which is causing problem while using in ROS.

As you could see in picture the Z-axis is horizontal in assembly whereas vertical in solidworks. How to align these co-ordinates. The generated co-ordinate system must be similar to solidworks' co-ordinates
PS: I have mated the assembly origin and base_link origin
","mobile-robot, ros, navigation, odometry, gazebo"
what determines the speed of quadrotor,"How to design a quadrotor which travels at particular maximum speed? and how to determine the power required for a quadrotor to hover?
","mobile-robot, quadcopter, power"
EKF SLAM (prediction of new landmarks),"Prediction of new landmarks are commonly expressed as:
        Xm = Xr + r*cos(phi + theta_r),
        Ym = Yr + r*sin(phi + theta_r)

However this is only true for point landmarks. What if I am extracting line feature?
","slam, ekf"
Ultrasonic flow sensor,"The goal is to have a non-invasive flow meter that I can clamp over hydraulic lines.
As a student of hydraulics, I ended up looking and poking around for a good way to make an ultrasonic flow sensor with Arduino and possibly the hc-sr04. Not married to either idea. 
So, I admit, I know nothing, but is it possible to do this?
Is there an easier way?
",ultrasonic-sensors
What kind of sensor I can use to identify which fruit it is (like mango or apple)?,"What kind of sensor I can use to identify which fruit it is (like mango or apple). Moreover, is there any sensor to identify different varieties of apples or mangoes. 
",sensors
How does information gain based exploration differ from frontier based?,"I've recently come across the concept of using information gain (or mutual information criteria) as a metric for minimizing entropy on a map to aid in robotic exploration. I have somewhat of a basic question about it. 
A lot of papers that talk about minimizing entropy consider an example case of something like a laser scanner and try to compute the 'next best pose' so that the maximum entropy reduction is achieved. Usually this is mentioned like ""information gain based approaches help finding the best spot to move the robot such that the most entropy is minimized using raycasting techniques, as opposed to frontier based exploration which is greedy"" etc. But I don't understand what the underlying reason is for information gain/entropy based exploration being better. 
Let's say a robot in a room with three walls and open space in front. Because of range limitations, it can only see two walls: so in frontier based exploration, the robot has two choices; move towards the third wall and realize it's an obstacle, or move towards the open space and keep going. How does an information gain based method magically pick the open space frontier over the wall frontier? When we have no idea what's beyond our frontiers, how can raycasting even help?
","mapping, exploration"
Plotting location using wheel encoder data,"Context: I am working with the SFU Mountain Dataset [http://autonomylab.org/sfu-mountain-dataset/]
The UGV image - via the SFU Mountain Dataset website:

I have used the following state update equations (Husky A200 - differential drive)
State Update - from Prob. Robotics, Thrun et. al [x' y' theta'] represent the state at the next time step


After plotting the x and y positions based on just the wheel encoder data (v_fwd and w -> the dataset provides these directly, instead on the vr and vl), the curve seems to be quite weird and unexpected.
Wheel Odometry Data - http://autolab.cmpt.sfu.ca/files/datasets/sfu-mountain-workshop-version/sfu-mountain-torrent/encoder-dry-a.tgz
Blue - Wheel Odom  |  Red - GPS

Actual path!

Question: Is the above curve expected (considering the inaccuracy of wheel odometry) or is there something I'm missing? If the wheel encoder data is that bad, will an EKF (odom + imu) even work?
PS: I'm not worried about the EKF (update step) just as yet. What concerns me more is the horrible wheel odometry data.
","localization, wheeled-robot, ekf, odometry, differential-drive"
PID Control: Integral error does not converge to zero,"Good day,
I had been recently reading up more on PID controllers and stumbled upon something called integral wind up. I am currently working on an autonomous quadcopter concentrating at the moment on PID tuning. I noticed that even with the setpoint of zero degrees reached in this video, the quadcopter would still occasionally overshoot a bit:  https://youtu.be/XD8WgVFfEsM
Here is the corresponding data testing the roll axis:

I noticed that the I-error does not converge to zero and continues to increase:


Is this the integral wind-up?
What is the most effective way to resolve this?

I have seen many implementations mainly focusing on limiting the output of the system by means of saturation. However I do not see this bringing the integral error eventually back to zero once the system is stable.
Here is my current code implementation with the setpoint of 0 degrees:
cout << ""Starting Quadcopter"" << endl;

float baseThrottle = 155; //1510ms
float maxThrottle = 180; //This is the current set max throttle for the PITCH YAW and ROLL PID to give allowance to the altitude PWM. 205 is the maximum which is equivalent to 2000ms time high PWM 
float baseCompensation = 0; //For the Altitude PID to be implemented later


delay(3000);

float startTime=(float)getTickCount();
deltaTimeInit=(float)getTickCount(); //Starting value for first pass

while(1){
//Read Sensor Data
readGyro(&gyroAngleArray);
readAccelMag(&accelmagAngleArray);

//Time Stamp
//The while loop is used to get a consistent dt for the proper integration to obtain the correct gyroscope angles. I found that with a variable dt, it is impossible to obtain correct angles from the gyroscope.

while( ( ((float)getTickCount()-deltaTimeInit) / ( ((float)getTickFrequency()) ) ) < 0.005){ //0.00209715|0.00419
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
       cout << "" DT endx = "" << deltaTime2 << endl;
}
//deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
deltaTimeInit=(float)getTickCount(); //Start counting time elapsed
cout << "" DT end = "" << deltaTime2 << endl;


//Complementary Filter
float pitchAngleCF=(alpha)*(pitchAngleCF+gyroAngleArray.Pitch*deltaTime2)+(1-alpha)*(accelmagAngleArray.Pitch);
float rollAngleCF=(alpha)*(rollAngleCF+gyroAngleArray.Roll*deltaTime2)+(1-alpha)*(accelmagAngleArray.Roll);
float yawAngleCF=(alpha)*(yawAngleCF+gyroAngleArray.Yaw*deltaTime2)+(1-alpha)*(accelmagAngleArray.Yaw);


//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

//Motor Front Right (2)
float motorPwm2 =  -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Left (3)
float motorPwm3 = pitchPID + rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Right (4)
float motorPwm4 = pitchPID - rollPID - yawPID + baseThrottle + baseCompensation;


 //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i<4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]<minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout << "" MinPWM = "" << minPWM << endl;

    if(minPWM<baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout << "" Fill = "" << fillPwm << endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i<4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]>maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout << "" MaxPWM = "" << maxPWM << endl;

    if(maxPWM>maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout << "" Trim = "" << trimPwm << endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }

//PWM Output
    gpioPWM(24,motorPwm1);  //1
    gpioPWM(17,motorPwm2);  //2
    gpioPWM(22,motorPwm3);  //3
    gpioPWM(18,motorPwm4);  //4

","control, quadcopter, pid, stability, tuning"
memory as a benchmarking criteria for motion planning algorithm,"Are there still applications where memory is still a criteria with respect to motion planning algorithms. Are memory efficient motion planning algorithms still relevant?
","mobile-robot, quadcopter, motion-planning"
Finding Center of Mass for Humanoid Robot,"I've been working on Humanoid Robot, and I face the problem of finding the Center of Mass of the Robot which will help in balancing the biped. Although COM has a very simple definition, I'm unable to find a simple solution to my problem.
My view: I have already solved the Forward and Inverse Kinematics of the Robot with Torso as the base frame. So, if I can find the position(and orientation) of each joint in the base frame, I can average all of them to get the COM. Is this approach reasonable? Will it produce the correct COM?
Can anyone offer any series of steps that I can follow to find the COM of the biped? Any help would be appreciated. 
Cheers! 
","mobile-robot, inverse-kinematics, humanoid, balance"
Inverse kinematics after calibration,"I am working on a 6DOF robot arm project and I have one big question. When I first derived the inverse kinematics (IK) algorithm after decoupling (spherical wrist), I could easily get the equations based on nominal DH values, where alpha are either 0 or 90 degrees and there are many zeros in $a_i$ and $d_i$. However, after kinematics calibration, the identified DH parameters are no longer ideal ones with a certain small, but non-zero, bias added to the nominal values. 
So my question is, can the IK algorithm still be used with the actual DH parameters? If yes, definitely there will be end-effector errors in actual operation. If not, how should I change the IK algorithm? 
P.S. I am working on a modular robot arm which means the DH bias could be bigger than those of traditional robot arms. 
","inverse-kinematics, calibration"
Multi-Rate Sensor Fusion using EKF,"Context: I have an IMU(a/g/m) + Wheel Odometry measurement data that I'm trying to fuse in order to localize a 2D (ackermann drive) robot.
The state vector X = [x y yaw].
I'm using the odometry data to propagate the state through time (no control input).
The update step includes the measurement vector Z = [x_odo y_odo yaw_imu].
I have two questions:
1.Does it make sense to use the odometry data(v_linear, omega) in both the prediction as well as update steps?
2.How do I account for the frequency difference between the odometry data(10Hz) and the imu data(40Hz)? Do I run the filter at the lower frequency, do I dynamically change the matrix sizes or is there any other way?
Thanks!
","localization, wheeled-robot, imu, ekf, odometry"
How to import catia assembly to Matlab. Simmechanics,"In catia .stl format is available only for part file not for assembly file. Please help how to import asembly in simmechanics
.CATProduct to .stl
Or Is there any other way to do?
",matlab
How is homotopy used in planning algorithms?,"What is an intuitive understanding for homotopy?  At what stage is homotopy (I understand it as stretching or bending of path) in a planning algorithm?  Is homotopy involved, for example, while implementing an algorithm like RRT?
","motion-planning, rrt"
Find orientation through Transformation matrix,"I have a robot with 3 rotational joints that I am trying to simulate in a program I am creating. So I have 4 frames, one base frame, and each joint has a frame. I have 3 transformation functions to go from frame 1 or 2 or 3 to frame 0.
By using the transformation matrix, I want to know how much each frame has been rotated (by the X,Y and Z axis) compared with the base frame. Any suggestions?
The reason I want this is because I have made some simple 3D shapes that represent each joint. By using the DH parameters I made my transformation matrices. When ever I change my θ (it does not mater how the θ changes, it just does), I want the whole structure to update. I take the translation from the last column. Now I want to get the rotations.
","robotic-arm, dh-parameters"
iRobot Create 2 to Vacuum?,"I just got a Create 2 for Christmas, and while I'm planning to create with it (obviously), I'd like to use it around the house as a vacuum if at all possible. I've heard that you can buy parts for the Roomba and throw them on to this chassis, but I wanted to confirm/refute that before I bought anything. Is that possible or am I crazy?
",irobot-create
What charger to use with my ZIPPY Compact 6200mAh 4s 40c Lipo MultiRotor Battery,"Hello I am trying to build a Quadcopter for a school project and I need to finish quick before our mission trip because I was asked to finish it before then so that I could take it. But I am having a problem figuring out which charger would work with my ZIPPY Compact 6200mAh 4s 40c Lipo Pack 

Here are the specs on the battery

Capacity: 6200mAh
Voltage: 4S1P / 4 Cell / 14.8V
Discharge: 40C Constant / 50C Burst
Weight: 589g (including wire, plug & case)
Dimensions: 158x46x41mm
Balance Plug: JST-XH
Discharge Plug: HXT4mm


Also I will be running the Tarot T4-3D Brushless Gimbal for GoPro (3-Axis)
and if anyone can tell me a good battery to run it off of or maybe it would be better to run it off my main battery.
Thanks in Advance
","quadcopter, battery, lithium-polymer"
List of books similar to Thrun's Probabilistic Robotics for robot mechanics and manipulation,"What?
Put together here a list of books (like the one for C/C++ on StackOverflow) that are spiritually similar to Sebastian Thrun's Probabilistic Robotics for robotic manipulation and mechanics.
Why?
Thrun's book is a wonderful resource for implementable algorithms while also dealing with the mathematics/theory behind them. In somewhat similar vein for robotic mechanics there is ""A Mathematical Introduction to Robotic Manipulation - S.Sastry, Z.Li and R.Murray"" which has a lot of mathematical/theoretical content. What is missing however in this book are the algorithms concerned with how should/would one go about implementing the theoretical stuff.
Requirements

Ideally list books dealing with diverse areas of robotics.
The books have to present algorithms like what Thrun does in his book.
Algorithms presented have to be language agnostic and as much as possible not be based on packages like MATLAB in which case they should be categorized appropriately.

","kinematics, inverse-kinematics, algorithm, dynamics, books"
How does the strategy changes in a RobCup soccer competition without connection to outside?,"How is a new team strategy during a robocup competition sent to each player of a robot team? Robots in the Standard Platform League (i.e. SPL), for example, are fully autonomous and there is no connection with non-team members (except pulling from the GameControl).
",soccer
Making a car go straight,"I'm trying to work a car that's being controlled by an Arduino. I'm using the following chassis: New 2WD car chassis DC gear motor, wheels easy assembly and expansion and an L298N motor driver.
The problem is that it's hard to make the car go straight. Giving the same PWM value to the motors still makes them spin in different speeds, trying to calibrate the value is hard and every time I recharge my batteries the value changes.
What are my options on making the car go straight when I want (well, sometimes I'll want to turn it around of course)?
I've thought about using an encoder but I wish to avoid that since it will complicate the whole project, is there any other viable option? and even when using an encoder, Does it means I will need to keep track all the time and always adjust the motors value continuously? is there some built-in library for that?
","arduino, wheeled-robot, calibration, two-wheeled"
Detecting the presence of a person in a room,"I am working on my first hobby project and I'm not very familiar with sensors yet. I am trying to build a system which detects the presence of a person in a small room with a single entrance/exit door.
The idea is when the first person enters the room, the lights turn on and any following person doesn't affect state of the lights. After the last person leaves, the lights should turn off. In a programmatic sense, the lights should turn on when present person count is greater than 0.
I have explored my options and found out that infrared sensors are usually used for this type of problem. What I am not sure is how to detect whether person has entered or left, so I would like to ask for some help with this.
",sensors
How can I make a quadcopter avoid obstacles using infrared?,"I have a quadcopter built, and I need to be able to make it to autonomously follow a route and avoid obstacles where possible.
My general plan is to have an array of sensors on a pre-defined ""front"". The quadcopter will only go forward. Generally I'd like to make  it so that if the sensors pointing at a higher angle detect something getting closer as the bot moves forward, the quadcopter will stop, descend until the distance to that detected object decreases, and then continues forward. Similarly, I'd like the opposite event to happen if the sensors pointing at a lower angle detect something getting closer to the quadcopter.
I'm thinking of having something like 9 small infrared distance detectors (pointing up, forward, down || left, forward, right), basically a 3x3 matrix.
Would anyone have any ideas of the feasibility of this? I'd like to use a raspberry pi, but it will probably also need an additional board to read in the values from its sensors. In addition, I have no idea which sensors to use, or if infrared can even work. Any suggestions are more than welcome.
I was also thinking about ultrasonic sensors, but having 9 of them could get cluttered, and I'd worry about their short range when a crash means death for the quadcopter. I also fear they would cause interference with each other.
","mobile-robot, quadcopter, sensors"
"Sphero's logic, how does it work","I'm willing to make my first robot, and I'd like to make one similar to the Sphero.
I know I have to add 2 motors in it, and make it work as a hamster ball, but I don't understand how I can make it rotate on the x axis aswell and not only on the y axis, if we assume that the y one is in front of the robot and the x one on its sides.
Any ideas?
","design, two-wheeled"
How to compute the error function in graph SLAM for 3D poses?,"Given a pose $x_i = (t_i, q_i)$ with translation vector $t_i$ and rotation quaternion $q_i$ and a transform between poses $x_i$ and $x_j$ as $z_{ij} = (t_{ij}, q_{ij})$ I want to compute the error function $e(x_i, x_j) = e_{ij}$, which has to be minimized like this to yield the optimal poses $X^* = \{ x_i \}$:
$$X^* = argmin_X \sum_{ij} e_{ij}^T \Sigma^{-1}_{ij} e_{ij}$$
A naive approach would look like this:
$$ e_{ij} = z_{ij} - f(x_i,x_j) $$
where $z_{ij}$ is the current measurement of the transform between $x_i$ and $x_j$ and $f$ calculates an estimate for the same transform. Thus, $e_{ij}$ simply computes the difference of translations and difference of turning angles:
$$ e_{ij} = \begin{pmatrix} t_{ij} - t_j - t_i \\\ q_{ij} (q_j q_i^{-1})^{-1} \end{pmatrix} $$
Is there anything wrong with this naive approach? Am I missing something?
","slam, errors"
How do I work out the specifications of motors and propellers for a quadcopter?,"What will be the specifications of motors and propellers that can approx produce a thrust of 100kg in a quadcopter?
We are planning to lift a total weight of 50 Kg along with the 20 Kg weight of the quadcopter itself. So at 50% throttle the total thrust produced by it should be 150 Kg with a per motor total thrust of 37.5 kg.
I have looked at this answer to How to calculate quadcopter lift capabilities? but don't understand how to use this information to work out the specifications of motor and propeller required for my application.
The answer given in previous question is limited for small quad & I require the specifications of BLDC motor such as Kv,torque,Imax,V,Power,etc and of Propeller suitable for such motor.
",quadcopter
ROS on Raspberry Pi Model 2: UbuntuARM vs ROSBerryPi,"Before I ask my question, I'd better confirm that I've read the most prominent post about running ROS on Raspberry Pi devices.
That post contains some valuable information, but it's a bit dated, and ROS support for ARM devices is much better these days. In fact, ROS 2.0 is evidently going to have excellent support for running on embedded devices like the Raspberry Pi.
I just got a Pi model 2 for my birthday, and I'm really eager to get ROS running on it so I can build a robot I've been working on, which is based on the Wild Thumper 6WD platform.
From my perspective, here are a few pros & cons regarding UbuntuARM and ROSBerryPi:
UbuntuARM
Pros:

Ubuntu is the official ROS distro and is the most well-supported ROS OS 
The best documentation on the ROS wiki for running on ARM devices is written for UbuntuARM

Cons:

Raspbian (on which ROSBerryPi is based) is the official distro for Rasbperry Pi and thus has the best support for the board.


ROSBerryPi
Pros:

Raspbian (on which ROSBerryPi is based) is the official distro for Rasbperry Pi and thus has the best support for the board.
Cons:
ROS is not well supported on OS's other than Ubuntu
To use the ROSBerryPi distro, you must build ROS from source.

My question is: can anyone provide any further insight into this dilemma? If you've been running ROS on your Raspberry Pi 2 (model 2 only please; the model B+ has completely different issues, like not being well-supported by Ubuntu), what's your experience been? 
Which distro would/did you choose, and why?
","ros, raspberry-pi, arm"
Freewheel diode / capacitor with this board?,"With DC motors, it is common to put a freewheel diode and/or a capacitor in order to protect the equipment as the motor can induce current into the system.
I plan to use this board to control a 24V DC motor with a Arduino-like microcontroler. In an example in their documentation, they don't put such protection, so I wanted to know if it's unsafe, or is it that the board already protects the system?
The example in question:

","arduino, motor, protection"
Choosing the state vector for an EKF,"Could someone help me understand the logic behind choosing a particular state space vector for an EKF?
Context: Say there is a 4 wheeled robot that operates only in 2D. It is equipped with an inertial unit (a/g/m) and wheel encoders (I understand that these alone might not satisfy accuracy constraints, but consider this as a hypothetical case).
Now, some literature has the state as [q, x, y, vx, vy]' while a few others as [q, q_dot, x, y, vx, vy]'. My question is, what is the advantage with having certain 'rate terms' as opposed to only the normal parameters? Also, what about including bias terms in there?
How do I go about selecting an appropriate state space vector for any use-case (in general)? Is there a set of intuitive/mathematical steps to consider/follow?
Thanks!
","wheeled-robot, kalman-filter, ekf, pose"
Choosing stepper motor for hand,"I'm developing a robotic hand, and decided to place motors inside joints (as in picture) and I'm stuck with finding a stepper motor that can fit there. Approximate size of motor body is radius - 10mm, length - 10 mm. 
Any suggestions?

",stepper-motor
How to make a Raspberry Pi?,"I'm currently a robotics hobbyist and am full fledged in Arduino and I have used the Raspberry Pi to make some robots and PCs. Currently, I am thinking of making my own Raspberry Pi, from scratch, on a breadboard or a PCB or something. I surfed the web quite a bit and I did not get the answer I was hoping for. By making a Pi, I mean like instead of buying an Arduino, I can make one myself by buying the Atmega328, Crystal oscillators, etc. I am asking for this because my school requires me to do a project in which I make a computer or a gaming console or something like that and I would hate to look at the disappointed face of the tester all because I just bought a Pi , an connected some devices to it. Thanks in advance!
","arduino, raspberry-pi"
How does the cliff sensors on ROOMBA work through a glass wall?,"I want to use IR sensors to detect whether my dustbin is full but I want to protect it from outside dust. I am planning to use the IR sensors on Roomba.

How are they working despite a plastic wall?
Also, what is the range of the sensor?
Can they detect obstacle at about 25 cm?
Why is there a wall between the IR sensors?
Is there a reason they are positioned at certain angle?

","sensors, roomba"
Connecting a CC3D board with Raspberry Pi to get telemetry data,"I want to get telemetry data in my Raspberry Pi that will be connected to a CC3D board either via USB cable or Serial communication. How can I get the data? I plan to have wifi communication between the Pi and my Laptop. Also OPLink modems will be used both in the Pi and the CC3D for the telemetry. Does anyone have a python example that may help to build an interface or output in the Linux shell to get raw telemetry data in RPi? 
","serial, communication"
Lag in altitude measurements using a barometer and an acclerometer,"I am fusing data from the barometer and accelerometer using a complementary filter. However, there is a considerable lag in the readings which is affecting the Alt-Hold performance. 
Accelerometer : mpu 6050 
Baro : ms 5611. 
Here's the code : https://github.com/cleanflight/cleanflight/blob/master/src/main/flight/altitudehold.c
You will find the filter being implemented in the function calculateEstimatedAltitude()
Does anyone have any suggestions to improve the measurements ?
","quadcopter, sensors, sensor-fusion"
CTL port in a motor controller,"When I look at my 160A motor controller, it has a port that is called ""CTL.""
What does CTL stand for? Is that a sort of protocol like RS232?
",motor
Sensors for identifying stacked books,"I am working on a robotics application that involves moving objects (e.g. books) between several (around 10) stacks. To measure the performance, I'd like to be able to measure which book is located on each of the stacks. The order is not important I just want to know if a book is on one of the stacks. 
The stacks are separated by at least one meter and the height of the stacks is less than 30cm (< 8 Books). 
If have thought of putting an RFID card in every book and fixing RFID readers above (or below) the stack positions. Several readers could be attached via SPI or I2C to some arduinos or RPis. 
What to you think about this approach? Is there a simpler way? Could someone maybe recommend a sensor that could solve this problem? 
// Update:
I can modify the books (e.g. add a QR-Marker) to some extent, but can't guarantee that the orientation on the stack is fixed. 
",untagged
Reference request: Path accuracy algorithm in the joint angle space,"I am currently reviewing a path accuracy algorithm. The measured data are points in the 7 dimensional joint space (the robot under test  is a 7 axes Robot, but this is not of importance for the question). As far as I know path accuracy is measured and assessed in configuration (3 D) space. Therefore I am wondering if a path accuracy definition in joint angle space has any practical value. Sure, if one looks at the joint angle space as a 7 dimensional vector space in the example (with Euclidean distance measure) one can do formally the math. But this seems very odd to me. For instance, an angle discrepancy between measured and expected for the lowest axis is of much more significance than a discrepancy for the axis near the actuator end effector.
So here is my Question: Can anyone point me to references where path accuracy in joint space and/or algorithms for its calculation is discussed ?  
(I am not quite sure what tags to use. Sorry if I misused some.)
","algorithm, industrial-robot, joint"
A simple function plotter project,"To plot any curve or a function on a paper we need points of that curve, so to draw a curve, I will store a set of points in the processor and use motors, markers and other mechanism to draw straight lines attaching these points and these points are so close to each other that the resultant will look an actual curve.
So I am going to draw the curve with a marker or a pen.

Yes to do this project I need motors which would change the position of a marker but which one?

With my knowledge stepper motor and servo motors are appropriate but not sure whether they are appropriate since I have never used them, so will they work?
The dimension of paper on which I will be working on is 30x30 cms.
I have two ideas for this machine 
a. A rectangular one as shown 
I would make my marker holder movable with help of rack and pinion mechanism but I am not sure that this would be precise and I may have to alter to some other mechanism and if you know such then that can really help me.
b. A cylindrical one 
Here I would roll a paper on this cylinder and this paper will get unrolled as the cylinder rotates and even the marker holder is movable but only in X direction and the rolling of paper is nothing but change of Y position.

Which one of the above two methods is good?
I know about microcontrollers and I want to control the motors using them so I decided to go with Atmega 16 microcontroller. But here i might need microstepping of signals how would I be able to do that with microcontrollers?

If you know the answer to atleast one of the questions then those answers are always welcomed. 
If you need any clarifications about these then please leave a comment.
Thankyou for your time.
Your sincerely,
Jasser
Edit : To draw lines of particular slope I would have to know the slope between two points and the depending on the slope I would rotate motors with particular speed so that marker will move in a straight fashion with that slope.
","microcontroller, stepper-motor, servomotor"
Analogue video to digital,"I have FPV camera which outputs analog video (RCA, PAL).
I want to capture video and do image processing, therefore I need some way to convert the analog video to digital.
Can some one recommend me how to do it? Is there advice or a shield which can assist?
Please note:

I want to convert the frames with minimum latency, because it is a real time flying drone.
I don't need to convert the image to some compressed format (which encoding/ decoding may take time), if I can get the RGB matrix straight, it is preferred.
I thought about digital output camera, but I need one which weighs few grams and I haven't found yet.

","quadcopter, cameras"
PID Tuning for an Unbalanced Quadcopter: When do I know if the I-gain I've set is too high?,"Good day,
I am working on an autonomous flight controller for a quadcopter ('X' configuration) using only angles as inputs for the setpoints used in a single loop PID controller running at 200Hz (PID Implementation is Here: Quadcopter PID Controller: Derivative on Measurement / Removing the Derivative Kick). For now I am trying to get the quadcopter to stabilize at a setpoint of 0 degrees. The best I was able to come up with currently is +-5 degrees which is bad for position hold. I first tried using only a PD controller but since the quadcopter is inherently front heavy due to the stereo cameras, no amount of D or P gain is enough to stabilize the system. An example is the image below which I added a very small I gain:

As you can see from the image above (at the second plot), the oscillations occur at a level below zero degrees due to the quadcopter being front heavy. This means that the quad oscillates from the level postion of 0 degrees to and from a negative angle/towards the front. To compensate for this behaviour, I discovered that I can set the DC level at which this oscillations occur using the I gain to reach the setpoint. An image is shown below with [I think] an adequate I gain applied:

I have adjusted the PID gains to reduce the jitters caused by too much P gain and D gain. These are my current settings (Which are two tests with the corresponding footage below):

Test 1: https://youtu.be/8JsraZe6xgM
Test 2: https://youtu.be/ZZTE6VqeRq0
I can't seem to tune the quadcopter to reach the setpoint with at least +-1 degrees of error. I noticed that further increasing the I-gain no longer increases the DC offset. 

When do I know if the I-gain I've set is too high? How does it reflect on the plot?

EDIT:
The Perr in the graphs are just the difference of the setpoint and the CF (Complementary Filter) angle.
The Derr plotted is not yet divided by the deltaTime because the execution time is small ~ 0.0047s which will make the other errors P and I hard to see.
The Ierr plotted is the error integrated with time.
All the errors plotted (Perr, Ierr, Derr) are not yet multiplied by the Kp, Ki, and Kd constants
The 3rd plot for each of the images is the response of the quadcopter. The values on the Y axis correspond to the value placed as the input into the gpioPWM() function of the pigpio library. I had mapped using a scope the values such that 113 to 209 pigpio integer input corresponds to 1020 to 2000ms time high of the PWM at 400Hz to the ESC's
EDIT:
Here is my current code implementation with the setpoint of 0 degrees:
cout << ""Starting Quadcopter"" << endl;

float baseThrottle = 155; //1510ms
float maxThrottle = 180; //This is the current set max throttle for the PITCH YAW and ROLL PID to give allowance to the altitude PWM. 205 is the maximum which is equivalent to 2000ms time high PWM 
float baseCompensation = 0; //For the Altitude PID to be implemented later


delay(3000);

float startTime=(float)getTickCount();
deltaTimeInit=(float)getTickCount(); //Starting value for first pass

while(1){
//Read Sensor Data
readGyro(&gyroAngleArray);
readAccelMag(&accelmagAngleArray);

//Time Stamp
//The while loop is used to get a consistent dt for the proper integration to obtain the correct gyroscope angles. I found that with a variable dt, it is impossible to obtain correct angles from the gyroscope.

while( ( ((float)getTickCount()-deltaTimeInit) / ( ((float)getTickFrequency()) ) ) < 0.005){ //0.00209715|0.00419
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
       cout << "" DT endx = "" << deltaTime2 << endl;
}
//deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())); //Get Time Elapsed
deltaTimeInit=(float)getTickCount(); //Start counting time elapsed
cout << "" DT end = "" << deltaTime2 << endl;


//Complementary Filter
float pitchAngleCF=(alpha)*(pitchAngleCF+gyroAngleArray.Pitch*deltaTime2)+(1-alpha)*(accelmagAngleArray.Pitch);
float rollAngleCF=(alpha)*(rollAngleCF+gyroAngleArray.Roll*deltaTime2)+(1-alpha)*(accelmagAngleArray.Roll);
float yawAngleCF=(alpha)*(yawAngleCF+gyroAngleArray.Yaw*deltaTime2)+(1-alpha)*(accelmagAngleArray.Yaw);


//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

//Motor Front Right (2)
float motorPwm2 =  -pitchPID - rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Left (3)
float motorPwm3 = pitchPID + rollPID + yawPID + baseThrottle + baseCompensation; 

//Motor Back Right (4)
float motorPwm4 = pitchPID - rollPID - yawPID + baseThrottle + baseCompensation;


 //Check if PWM is Saturating - This method is used to fill then trim the outputs of the pwm that gets fed into the gpioPWM() function to avoid exceeding the earlier set maximum throttle while maintaining the ratios of the 4 motor throttles. 
    float motorPWM[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float minPWM = motorPWM[0];
    int i;
    for(i=0; i<4; i++){ // Get minimum PWM for filling
        if(motorPWM[i]<minPWM){
            minPWM=motorPWM[i];
        }
    }

    cout << "" MinPWM = "" << minPWM << endl;

    if(minPWM<baseThrottle){
        float fillPwm=baseThrottle-minPWM; //Get deficiency and use this to fill all 4 motors
        cout << "" Fill = "" << fillPwm << endl;
        motorPwm1=motorPwm1+fillPwm;
        motorPwm2=motorPwm2+fillPwm;
        motorPwm3=motorPwm3+fillPwm;
        motorPwm4=motorPwm4+fillPwm;
    }

    float motorPWM2[4] = {motorPwm1, motorPwm2, motorPwm3, motorPwm4};
    float maxPWM = motorPWM2[0];
    for(i=0; i<4; i++){ // Get max PWM for trimming
        if(motorPWM2[i]>maxPWM){
            maxPWM=motorPWM2[i];
        }
    }

    cout << "" MaxPWM = "" << maxPWM << endl;

    if(maxPWM>maxThrottle){
        float trimPwm=maxPWM-maxThrottle; //Get excess and use this to trim all 4 motors
        cout << "" Trim = "" << trimPwm << endl;
        motorPwm1=motorPwm1-trimPwm;
        motorPwm2=motorPwm2-trimPwm;
        motorPwm3=motorPwm3-trimPwm;
        motorPwm4=motorPwm4-trimPwm;
    }

//PWM Output
    gpioPWM(24,motorPwm1);  //1
    gpioPWM(17,motorPwm2);  //2
    gpioPWM(22,motorPwm3);  //3
    gpioPWM(18,motorPwm4);  //4

","pid, raspberry-pi, quadcopter, tuning"
Software real-time of ROS system,"As far as I know, a hardware real-time robot control system requires a specific computing unit to solve the kinematics and dynamics of a robot such as interval zero RTX, which assigns CPU cores exclusively for the calculation, or a DSP board, which does exactly the same calculation. This configuration makes sure that each calculation is strictly within, maybe, 1 ms. 
My understanding is that ROS, which runs under Ubuntu, doesn't have a exclusive 
computing unit for that. Kinematics and dynamics run under different threads of the same CPU which operates the Ubuntu system, path plan, and everything else. 
My question is that how does ROS achieve software-real time? Does it slow down the sampling time to maybe 100ms and makes sure each calculation can be done in time? Or the sampling time changes at each cycle maybe from 5ms, 18ms, to 39ms each time in order to be as fast as possible and ROS somehow compensates for it at each cycle?
","microcontroller, ros, real-time"
Telemetry with APM 2.6 and XBee,"The transmission of telemetry data between the ground base station and APM 2.x (Arducopter), using XBee, is not well documented. The only documentation is Telemetry-XBee, but it does not specify what XBee version is used. I have been checking and I guess is version 1 (this one has P2P link and the others not), but I am not sure.
I would like to know, what XBee modules people use for flying drones? Do they have problems with the APM connection? How can I control the drone remotely using the XBee link with Mavlink protocol?
","quadcopter, radio-control, mavlink"
Do walking robots use accelerometers?,"My understanding of walking robots (e.g. https://www.youtube.com/watch?v=xJlkBBdyBYI) is that they use a gyroscope to determine the current orientation of the robot, or each joint of the robot. This is because if you just put encoders on each joint, the cumulative error over the entire robot will be too large to maintain stability. Therefore, a gyroscope measures the ""real"" orientation, and this is used for feedback when the robot is walking.
However, I'm also aware that some walking robots use accelerometers to maintain stability. What would be the benefit of using an accelerometer in this case? Would it be used instead of a gyroscope, or together with a gyroscope?
My guess is that gyroscopes do not measure acceleration directly (unless you were to numerically calculate this based on lots of orientation readings), but accelerometers do measure it directly (and more reliably than this numerical method). Know the acceleration as well as the position then enables to robot to more accurately predict its future position, and hence the feedback loop is more robust. Is this correct, or am I missing the point?
","control, sensors, accelerometer, dynamics, walking-robot"
"What does ""6 degrees of freedom"" mean?","I am looking at this page that describes various characteristics of gyroscopes and accelerometers. Close to the end (where they speak about IMUs), the names of the items have something like this:

9 degrees of freedom
6 degrees of freedom

Can anyone explain what does this mean?
","accelerometer, gyroscope"
L293D won't turn motor backwards,"My small robot has two motors controlled by an L293D and that is controlled via a Raspberry Pi. They will both go forwards but only one will go backwards. 
I've tried different motors and tried different sockets in the breadboard, no luck. Either the L293D's chip is broken (but then it wouldn't go forwards) or I've wired it wrong. 
I followed the tutorial, Controlling DC Motors Using Python With a Raspberry Pi, exactly.
Here is a run down of what works. Let the 2 motors be A and B:
When I use a python script (see end of post) both motors go ""forwards"". When I change the values in the Python script, so the pin set to HIGH and the pin set to LOW are swapped, motor A will go ""backwards"", this is expected. However, motor B will not move at all. 
If I then swap both motors' wiring then the original python script will make both go backwards but swapping the pins in the code will make motor A go forwards but motor B won't move.
So basically, motor A will go forwards or backwards depending on the python code but motor B can only be changed by physically changing the wires.
This is forwards.py
import RPi.GPIO as GPIO
from time import sleep

GPIO.setmode(GPIO.BOARD)

Motor2A = 23
Motor2B = 21
Motor2E = 19

Motor1A = 18
Motor1B = 16
Motor1E = 22

GPIO.setup(Motor1A, GPIO.OUT)
GPIO.setup(Motor1B, GPIO.OUT)
GPIO.setup(Motor1E, GPIO.OUT)

GPIO.setup(Motor2A, GPIO.OUT)
GPIO.setup(Motor2B, GPIO.OUT)
GPIO.setup(Motor2E, GPIO.OUT)

print(""ON"")
GPIO.output(Motor1A, GPIO.HIGH)
GPIO.output(Motor1B, GPIO.LOW)
GPIO.output(Motor1E, GPIO.HIGH)

GPIO.output(Motor2A, GPIO.HIGH)
GPIO.output(Motor2B, GPIO.LOW)
GPIO.output(Motor2E, GPIO.HIGH)

And this is backwards.py
import RPi.GPIO as GPIO
from time import sleep

GPIO.setmode(GPIO.BOARD)

Motor2A = 21
Motor2B = 23
Motor2E = 19

Motor1A = 16
Motor1B = 18
Motor1E = 22

GPIO.setup(Motor1A, GPIO.OUT)
GPIO.setup(Motor1B, GPIO.OUT)
GPIO.setup(Motor1E, GPIO.OUT)

GPIO.setup(Motor2A, GPIO.OUT)
GPIO.setup(Motor2B, GPIO.OUT)
GPIO.setup(Motor2E, GPIO.OUT)

print(""ON"")
GPIO.output(Motor1A, GPIO.HIGH)
GPIO.output(Motor1B, GPIO.LOW)
GPIO.output(Motor1E, GPIO.HIGH)

GPIO.output(Motor2A, GPIO.HIGH)
GPIO.output(Motor2B, GPIO.LOW)
GPIO.output(Motor2E, GPIO.HIGH)

If you see this diff https://www.diffchecker.com/skmx6084, you can see the difference:



Below are some pictures. You can use the colour of the cables to link them between pictures

enter image description here


","wheeled-robot, raspberry-pi"
it's worth to make a line follower using a raspberry pi and a web cam?,"I wonder if this would be a competitive robot compared with one made with a traditional approach using a microcontroller and infrared sensors. I suppose that raspberry can perform an edge detection to tell the dynamic of the line far away, much more that the infrared sensor, but how fast can the raspberry do this process? should be a relative simple process in terms of computational requirements , an edge detection in a high contrast arena. Probably the bigger issue would be get the relative position of the robot respect to the line, may be a combination of the camera with some infrared sensors would work better, and what about the size? the robot will be significantly bigger when is used a camera and a raspberry for the control.
","raspberry-pi, line-following"
Determine robot's position in a nearby room,"Scenario
I have 2 roaming robots, each in different rooms of a house, and both robots are connected to the house wifi. Each robot only has access to the equipment on itself.
Question
How can the robots be aware of each other's exact position using only their own equipment and the house wifi?
EDIT: Additional Info
Right now the robots only have:

RGBDSLAM via Kinect
No initial knowledge of the house or their location (no docks, no mappings/markings, nada) 
Can communicate via wifi and that part is open ended

I'm hoping to be able to stitch the scanned rooms together before the robots even meet. Compass + altimeter + gps will get me close but the goal is to be within an inch of accuracy which makes this tough. There IS freedom to add whatever parts to the robots themselves / laptop but the home needs to stay dynamic (robots will be in a different home every time).
","mobile-robot, localization, precise-positioning"
Matlab Control Toolbox root locus,"I'm using the control system toolbox provided by matlab to estimate the gains of my controller: using root locus design I get a graph like this one .
My question is: what is the x on the x-axis? maybe a pole position at a previous iteration of the optimization procedure that I have run to find a gain value that satisfies my requirements? It shouldn't be the open loop pole position, because my system is formed by two integrators multiplied by a constant (1/inertia). Thanks
Edit: I add the requested details: I start from the following simulink diagram:

my trasfer function is $$G_\Theta(s) = \frac{Y(s)}{U(s)} = \frac{\Theta(s)}{\tau_\Theta} =  \frac{1}{I_y s^2}$$ with
Iy = 0.0054 (another little question, the point in which I'm taking out the torque is correct?)
and then I select analysis , control design, compensator design. I select Kp and Kd as the gains to be tuned, and I use the root locus for specifying the constraints. Then I click in SISO Design Task, automated tuning, optimize compensator, which automatically tries to find gain values to satisfy my constraints. The white are is the area that satisfies the constraints, and I think that the pink squares are my poles position after having been completed the optimization procedure. This is correct? But in this case, what is the x(pole) shown? Thanks
","control, matlab"
How to load a PUMA robot in the existing environment in OpenRAVE 0.9,"I have a PR2 robot in an environment, which can be seen on the GUI of OpenRAVE. 
Now, how can I load a PUMA robot arm in the same environment?
","robotic-arm, motion-planning"
Mass Matrix in Lagrange equation,"I want to find the equations of motion of RRRR robot.I have studied about it a bit but I am having some confusion.
Here in one of the lectures I found online it describes as Inertia matrix of a link as Ii which is computed by tilde of I also described in picture below???

So tilde of I is computed wrt to fixed frame attached to the centre of mass.
However in another example below from another source there is no rotation matrix multiplication with Ic1 and Ic2 as shown above.Am I missing something??

What is the significance of multiplying Rotation matrix with Ic1 or tilde of I?
I am using former approach and getting fairly large mass matrix. Is it normal to have such long terms inside Mass matrix?? still need to know though which method is correct?? 



the equation i used for Mass Matrix is
","robotic-arm, dynamics"
Alternative to BeagleBone Black for Node.js based remote control project?,"I am working on a remote control project that involves using Node.js and Socket.io to transmit joystick data from a webpage to my BeagleBone Black.
However, I am somewhat disappointed with the BeagleBone - it seems like what should be such simple tasks such as connecting to Wi-Fi can be quite tricky...
My question is: Are there alternative boards I should be looking at? Boards that also have Node.js libraries with PWM support, could stream video from a webcam, but are easier to set up and have a larger developer community?
","control, microcontroller, pwm, beagle-bone"
iRobot Create 2/Roomba 530 Screw size/thread?,"I've looked everywhere I can think of to find this information, but haven't come across anything. Does anyone know what kind of screws I can use to replace the ones on top of my Roomba 530? 
I realize that the Create 2 is technically a 600 series, but I would expect they were the same.
I'd like to replace the screws on my Roomba with standoffs so I can stick a mounting plate on top of it. (Additional sensors, CPU, etc.)
","irobot-create, roomba"
iRobot Create 2 IR bump light sensor specifications,"Can someone tell me where can I find some specifications about the iRobot Create 2 IR bump light sensors?
We have an SDF model of the Create 2 that uses the Hoyuko laser range finder sensor and we have to simulate the behavior of the IR sensors starting from the laser scan data. 
Hence we would like to have some additional information on the IR sensors which we can't find anywhere on the web - such as their exact position in the robot chassis, their maximum range, the shape of the obstacle detection field and so on.
","sensors, irobot-create"
Create 2 Cable with 700 Series Roomba,"For the last few months I have been playing with ROS on an nVidia Jetson TK1 development board. Up until this point, it has mostly been playing with the GPIO header, an Arduino Uno, a couple physical contact sensors, and a few custom motor and servo boards that I slapped together. But lately I've been eyeing an old 700 series Roomba that has been gathering dust (was replaced by an 800 series).
Does anyone know if the Communication Cable for Create 2 will work with a 700 series Roomba?
I know there are DIY designs out there, but I have always been a fan of using off-the-shelf components if they exist - you rarely save more money than your time is worth if it is something like a cable or similar component. So if the Create 2 cable will work, I'll use that. If not, I'll see what I can do to make my own.
","ros, irobot-create, roomba"
How do I get the Create 2 to communicate with a laptop via the serial to USB cable?,"My computer will not recognize the Serial to USB cable.  I have tried it on a Mac and an HP.  
Is there a driver that I need to install?  If it is supposed to install automatically, it is not.
","irobot-create, serial, usb"
Is this supposed to be a bearing?,"I'm looking at the assembly of a tail rotor that should look like this

original image
I wonder if the ""tail output shaft stopper"" (circled in red) is meant to be a bearing or just a piece of metal stopper:

My reading is that, since it's held by 2 set screws, the whole part should rotate with the rod. While rotating, it'd rub against the bevel gear on the tail drive though. Am I missing something?
","mechanism, torque, gearing"
Is AllJoyn a good ROS alternative regarding message passing cross multiple devices?,"I've used ROS for a while, my environment is Raspberry Pi + Ubuntu + OpenCV + ROS + C/C++. I also use several ROS packages (tf2, usb_camera, slam related, and laser scanner capture.) Also, in my projects, nodes are in multiple devices, and I'm using multiple master package for one project.
I did review some tutorials about AllJoyn, but no handon experience so far.
The questions are:

regarding to message (especially, ROS image) passing cross devices, is AllJoyn a good ROS alternative? (devices are connected by wifi or bluetooth.)
For AllJoyn, does it still need single master (like roscore in ROS) to coordinate the nodes (or similar)? 

Thanks.
","ros, alljoyn"
How many DOFs required to define a 3D pose,"I understand that to be able to define point in 3D space, you need three degrees of freedom (DOFs). To additionally define an orientation in 3D space, you need 6 DOFs. This is intuitive to me when each of these DOFs defines the position or orientation along one axis or an orthogonal X-Y-Z system.
However, consider a robot arm such as this: http://www.robotnik.eu/robotics-arms/kinova-mico-arm/. This too has 6 DOFs, but rather than each DOF defining a position or orientation in an X-Y-Z system, it defines an angular rotation of one joint along the arm. If all the joints were arranged along a single axis, for example, then these 6 DOFs would in fact only define one angular rotation.
So, it is not true that each DOF independently defines a single position or orientation. However, in the case of this robot arm, it can reach most positions and orientations. I'm assuming this is because the geometry of the links between the joints make each DOF define an independent position or orientation, but that is a very vague concept to me and not as intuitive as simply having one DOF per position or orientation.
Can somebody offer some help in understanding these concepts?
",kinematics
How modular arm joints work,"Hello I'm trying to figure out how modular arm joints are designed and what kind of bearings/shafts are used for a modular-type robotic arm. Take ""UR arm"" for example. I believe those 'T-shaped pipes' include both a drive and bearing system. And as you can see from second image, it can be detached easily. So I think it's not just a simple ""motor shaft connecting to the member that we want to rotate"" mechanism. I'm wondering which type of mechanism and bearing system is inside of those T-shaped pipes. How can i transfer rotational motion to a member without using shafts? 


","mechanism, joint, arm"
Joystick Rate Limit Filter For FRC Java Programming,"I am a programmer for my school's FRC robotics team and have received the request from our hardware/driving department to limit the speed at which the robot's motors can accelerate given a joystick input telling it to increase the speed of the motor. For example, when the robot first starts up and the driver decides to move the joystick from the center to the fully up position (0 to full motor power), we don't want it to literally go from 0 to full motor power in an instant - it obviously creates some rather jerky, unstable behavior. How might I receive the target joystick position from the joystick, save it, and build up to it over time (and if any other inputs are sent in this process — like telling it to turn around — stop the current process and enact the new one)?
I am using Java with WPILib's 2016 robotics library: here's the API http://first.wpi.edu/FRC/roborio/release/docs/java/, and here's the tutorials http://wpilib.screenstepslive.com/s/4485/m/13809.
I am using the ""IterativeRobot"" template class, and teleop is being run in the method teleopPeriodic(), which is continuously called every few milliseconds in the program (it's where i'm receiving joystick input and calling the method RobotDrive.tankDrive() with the inputs).
I realize this is more of a programming question than a robotics question, but I figured it would be better to put it here than in stack overflow, etc. If someone could give me some simple pseudocode or just a conceptual idea of how this might be done (not necessarily as it pertains directly to the library or the language I'm using), that would be great.
","software, first-robotics"
Metal shaft design for a 6mm plastic bevel gear,"I have a small POM bevel gear with these dimensions:

It has a 6mm hole for the shaft and a M4 hole for the set screw.
Suppose this bevel gear is meshed with a 45T bevel gear and give a max. output torque of 0.4kg/cm. How should the design of the 6mm shaft be? Should the diameter be precisely 6mm? Should it be flattened into a 'D' shape (so that the set screw can hold the shaft)? I'm planning to use a metal shaft.
Any help will be appreciated.
Thanks
","mechanism, torque, gearing"
Robotic manipulator Jacobian by product of exponentials,"I've taken a class and started a thesis on robotics and my reference for calculating the Jacobian by product of exponentials seems incorrect, see:
http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf
Specifically the resulting Jacobian matrix for the SCARA manipulator on page 118 would have us believe that the end effector translational velocity depends on joints 2 and 3 rather than 1 and 2.
Could someone please explain me why?
","jacobian, manipulator, product-of-exponentials"
The logic of implementing an Auto-level function in a PID flight controller,"So I have multi-rotor with a basic PID controller, that keeps its axis stable through the gyroscope. However, the multi rotor, does not keep its height or position. So I would like to use an accelerometer for keeping its rough position (auto level). I want to use both the gyro and accelerometer, but how would the accelerometer values be used, is it implemented through the PID the same ways the gyro values are (degrees per second, which is the rate I used to calculate PID)? And then adjusting the esc through that?? I am confused at that part (the basic logic for using the accelerometer values) 
","quadcopter, pid, imu, accelerometer"
Attaching a M5 screw shaft to a cog wheel,"Does anyone know of a good (low profile) way to attach the threaded (chamfered) end of a M5 screw shaft to a 30MM (30T 1M) plastic cog wheel? Would it be a good idea to widen the shaft hole and just let the threads grip the hole?
EDIT The torque will be around 6kg-cm and the plastic I'm looking at is POM.
","mechanism, torque, gearing"
Is there a way to measure 3 axis orientation without a magnetometer?,"I have bought an STM iNEMO evaluation board in order to monitor the inclination of a separate magnetic sensor array as it moves in a linear scan outside of a (non-magnetic) stainless steel pipe. I want to measure the inclination of the sensor along the scan and ensure that it does not change. The problem I have found is that the measured magnetic field from the integrated magnetometer varies greatly with position along the pipe, and in turn, causes a large, position dependent error in one axis of the inclination reported by the iNEMO IMU. In fig. 1 below I show the set up of the test, I measured the inclination from the IMU while moving it along the length of the pipe and back again. The board did not change inclination throughout the measurement. In Fig 2 I show the magnetometer and inclination measurements recorded by the ""iNEMO application"" showing the large error in one of the inclinations.
My question is whether you know if there is any way of correcting for the magnetic field variation so that I can still accurately determine the inclination in all three directions? My data suggests to me that the magnetic field variation measured from the magnetometer is much greater than the geomagnetic field, so the inclination measurement will always be inaccurate. A follow up question I then have is: Is there a way to measure 3 axis orientation WITHOUT using a magnetometer?


","imu, gyroscope, magnetometer, orientation"
Cost of material to 3D print,"Can someone please share the typical cost of material to 3D print an object like a raspberry pi case?  Thank you.
",3d-printing
How does one calculate distance and angle using a target with known measurements?,"The target is in the shape of a U where the horizontal segment is 20 inches, and the two vertical segments are 14 inches. We are using a camera to image the target, and then using vision processing to isolate the target from the rest of the image. We know the vertical field of view, and the horizontal field of view of the camera. The resolution of the camera is 640x480 pixels.
The vertical distance between the camera on the robot and the target is constant but as of yet unknown because the robot hasn't been constructed yet. It is known, however, that the target will always have a higher elevation than the camera.
How can we use this data to calculate in real time the robot's distance to the target, and the angle to the target?
","computer-vision, real-time"
How to connect Phone to Robot while charging phone from external battery?,"I am controlling a robot via usb from an Android phone running the robot's code. This phone has a poor battery and I need to extend its life with a USB charger (can't change phones). How can I charge an android phone via usb, while maintaining a USB connection to the robot? I can solder wires together if needed, or can buy adapters as needed.

","mobile-robot, usb"
Understanding the various attitude estimation methods,"I am building a quadcopter using the Arduino Uno with a 6dof accelerometer and gyro. I will be adding a separate 3 axis magnetometer soon for heading. I have successfully implemented the code that reads the data coming in from these sensors and prints them out. 
I have also checked for bias by averaging 100 measurements. My code calculates the pitch from the accel and pitch from the gyro respectively:
pitchAccel = atan2((accelResult[1] - biasAccelY) / 256, (accelResult[2] - biasAccelZ) / 256)*180.0 / (PI); 
pitchGyro +=((gyroResult[0] - biasGyroX) / 14.375)*dt;
I am then using a complementary filter to fuse the two readings together like this:
pitchComp = (0.98*pitchGyro) + (pitchAccel*0.02);
I am stuck on how to proceed from here. I am using the same procedure for roll, so I now have readings for pitch and roll from their respective complementary filter outputs. 
I have read a lot of articles on the DCM algorithm which relates the angles from the body reference frame to the earth reference frame. Should that be my next step here? Taking the pitch and roll readings in the body reference frame and transforming them to the earth reference frame? Repeat the entire procedure for yaw using the magnetometer? If yes, how should I go about doing the transformations? 
I understand the math behind it, but I am having a hard time understanding the actual implementation of the DCM algorithm code-wise. 
Any help is appreciated!
","arduino, quadcopter, accelerometer, gyroscope"
Is the distortion introduced by a lens protector significant in practice?,"I have a computer-vision application I made to localize a robot in a room; the software has been in use for a while and is working fine.
When I calibrated the camera and got the intrinsics and lens distortion coefficients there was a lens protector on the lens, mounted on the robot's lid.
If I take off the robot's lid (and thus the lens protector) the localization solution becomes erratic and inaccurate, so I think the lens protector might be changing the distortion properties significantly.
Today the lens protector became detached and it was replaced shortly after. So now the calibration may no longer be valid and the localization solution is much more noisy.
Can a lens protector can greatly effect the distortion properties of the image, or can someone offer another explanation?
I intend to recalibrate and super-glue the lens protector down to the robot's lid, but I am curious if this is my problem, and if anyone else has encountered this with lens protectors.
","computer-vision, cameras, calibration"
Quadcopter PID Controller: Derivative on Measurement / Removing the Derivative Kick,"Good day,
I am currently implementing a single loop PID controller using angle setpoints as inputs. I was trying out a different approach for the D part of the PID controller. 
What bought this about is that when I was able to reach a 200Hz (0.00419ms) loop rate, when adding a D gain, the quadcopter seems to dampen the movements in a non continous manner. This was not the case when my algorithm was running at around 10Hz. At an angle set point of 0 degrees, I would try to push it to one side by 5 degrees then the quad would try to stay rock solid by resisting the movements but lets go after while enabling me to get it of by 2 degrees (the dampening effect weakens over time) then tries to dampen the motion again.
This is my implementation of the traditional PID:

Derivative on Error:

//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchError - pitchPrevError;
pitchPrevError = pitchError;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollError - rollPrevError;
rollPrevError = rollError;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawError - yawPrevError;
yawPrevError = yawError;


//PID controller list
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum + pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum + rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum + yawKd*yawErrorDiff/deltaTime2;

//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

What I tried to do now is to implement a derivative on measurement method from this article to remove derivative output spikes. However the Derivative part seems to increase the corrective force than dampen it.

Derivative on Measurement:

//Calculate Orientation Error (current - target)
float pitchError = pitchAngleCF - pitchTarget;
pitchErrorSum += (pitchError*deltaTime2);
float pitchErrorDiff = pitchAngleCF - pitchPrevAngleCF; // <----
pitchPrevAngleCF = pitchAngleCF;

float rollError = rollAngleCF - rollTarget;
rollErrorSum += (rollError*deltaTime2);
float rollErrorDiff = rollAngleCF - rollPrevAngleCF; // <----
rollPrevAngleCF = rollAngleCF;

float yawError = yawAngleCF - yawTarget;
yawErrorSum += (yawError*deltaTime2);
float yawErrorDiff = yawAngleCF - yawPrevAngleCF; // <----
yawPrevAngleCF = yawAngleCF;


//PID controller list // <---- The D terms are now negative
float pitchPID = pitchKp*pitchError + pitchKi*pitchErrorSum - pitchKd*pitchErrorDiff/deltaTime2;
float rollPID = rollKp*rollError + rollKi*rollErrorSum - rollKd*rollErrorDiff/deltaTime2;
float yawPID = yawKp*yawError + yawKi*yawErrorSum - yawKd*yawErrorDiff/deltaTime2;


//Motor Control - Mixing    
//Motor Front Left (1)
float motorPwm1 =  -pitchPID + rollPID - yawPID + baseThrottle + baseCompensation;

My question now is:

Is there something wrong with my implementation of the second method? 

Source: http://brettbeauregard.com/blog/2011/04/improving-the-beginner%E2%80%99s-pid-derivative-kick/
The way I've obtained the change in time or DT is by taking the timestamp from the start of the loop then taking the next time stamp at the end of the loop. Their difference is obtained to obtain the DT. getTickCount() is an OpenCV function.
/* Initialize I2c */  
/* Open Files for data logging */ 
while(1){
    deltaTimeInit=(float)getTickCount();

    /* Get IMU data */
    /* Filter using Complementary Filter */
    /* Compute Errors for PID */
    /* Update PWM's */

    //Terminate Program after 40 seconds
    if((((float)getTickCount()-startTime)/(((float)getTickFrequency())))>20){
            float stopTime=((float)getTickCount()-startTime)/((float)getTickFrequency());
    gpioPWM(24,0);  //1
    gpioPWM(17,0);  //2
    gpioPWM(22,0);  //3
    gpioPWM(18,0);  //4
    gpioTerminate(); 
        int i=0;
    for (i=0 ; i < arrPitchCF.size(); i++){
        file8 << arrPitchCF.at(i) << endl;
    }

    for (i=0 ; i < arrYawCF.size(); i++){
        file9 << arrYawCF.at(i) << endl;
    }

    for (i=0 ; i < arrRollCF.size(); i++){
        file10 << arrRollCF.at(i) << endl;
    }

    for (i=0 ; i < arrPitchAccel.size(); i++){
        file2 << arrPitchAccel.at(i) << endl;
    }

    for (i=0 ; i < arrYawAccel.size(); i++){
        file3 << arrYawAccel.at(i) << endl;
    }

    for (i=0 ; i < arrRollAccel.size(); i++){
        file4 << arrRollAccel.at(i) << endl;
    }

    for (i=0 ; i < arrPitchGyro.size(); i++){
        file5 << arrPitchGyro.at(i) << endl;
    }

    for (i=0 ; i < arrYawGyro.size(); i++){
        file6 << arrYawGyro.at(i) << endl;
    }

    for (i=0 ; i < arrRollGyro.size(); i++){
        file7 << arrRollGyro.at(i) << endl;
    }

    for (i=0 ; i < arrPWM1.size(); i++){
        file11 << arrPWM1.at(i) << endl;
    }

    for (i=0 ; i < arrPWM2.size(); i++){
        file12 << arrPWM2.at(i) << endl;
    }

    for (i=0 ; i < arrPWM3.size(); i++){
        file13 << arrPWM3.at(i) << endl;
    }

    for (i=0 ; i < arrPWM4.size(); i++){
        file14 << arrPWM4.at(i) << endl;
    }

    for (i=0 ; i < arrPerr.size(); i++){
        file15 << arrPerr.at(i) << endl;
    }

    for (i=0 ; i < arrDerr.size(); i++){
        file16 << arrDerr.at(i) << endl;
    }


        file2.close();
        file3.close();
        file4.close();
        file5.close();
        file6.close();
        file7.close();
        file8.close();
        file9.close();
        file10.close();
        file11.close();
        file12.close();
        file13.close();
        file14.close();
        file15.close();
        file16.close();
        cout << "" Time Elapsed = "" << stopTime << endl;
        break;
    }

    while((((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency())))<=0.00419){ //0.00209715|0.00419
       cout << "" DT end = "" << deltaTime2 << endl;
       deltaTime2=((float)getTickCount()-deltaTimeInit)/(((float)getTickFrequency()));
    }

    cout << "" DT end = "" << deltaTime2 << endl;
}

Here's my data:



","control, quadcopter, pid, raspberry-pi, stability"
How to implement transmission in tracked chassis with one motor?,"I see that in small robots tracked chassis is implemented with 2 motors, each powering one side of the vehicle, like this: 
(image stolen from here)
But in real scale tanks I assume there is only one motor so there must be some way of applying power to both sides independently.
","tracks, gearing, chassis"
Quadcopter program execution time optimization using Raspberry Pi by increasing i2c baudrate,"
Is it possible to speed up execution time of a c++ program in raspberry pi solely by increasing the i2c baudrate and increasing the sampling frequency of the sensors? 

I have the issue of sudden jerkiness of my quadcopter and found the culprit which is the frequency at which my loop excecutes which is only about 14Hz. The minimum requirement for a quadcopter is 100-200hz. It is similar to the issue he faces here Raspberry Pi quadcopter thrashes at high speeds
He said that he was able to increase his sampling rate from 66hz to 200hz by increasing the i2c baudrate. I am confused on how that is done.
In the wiring pi library, it says that we can set the baudrate using this command:
gpio load i2c 1000

will set the baud rate to 1000Kbps – ie. 1,000,000 bps. (K here is times 1000)

What I am curious about is how to set this baudrate to achieve my desired sampling rate?
I plan on optimizing it further to achieve at least a 100Hz sampling rate
As of now, the execution time of each loop in my quadcopter program is at 0.07ms or 14Hz.
It takes 0.01ms to 0.02ms to obtain data from the complementary filter.
I have already adjusted the registers of my sensors to output readings at 190Hz (Gyroscope L3GD20H) and 200Hz (Accelerometer LSM303) and 220Hz (Magnetometer LSM303).
","quadcopter, pid, raspberry-pi, sensor-fusion, c++"
How to estimate the position of multiple static ground targets captured from a down facing camera?,"An aerial vehicle captures images of the ground using its down facing camera. From the images, multiple targets are converted from their pixel position to the camera reference frame using the pinhole camera model. Since the targets are static and there is information of the vehicle attitude and orientation, each sample is then converted to the world referencial frame. Note that all targets are on a flat, level plane.
The vehicle keeps ""scanning"" for the targets and converting them to the world referencial frame. Due to the quality of the camera and detection algorithm, as well as errors on the altitude information, the position of the ""scanned"" targets is not constant (not accurate). A good representation might be a gaussian distribution around the target true position, however it will also be influenced by the movement of the aerial vehicle.
What's the best approach to estimate the position of the targets from multiple readings?
This basically resumes to a problem of noise removal (as well as outlier removal) and estimation, so I would like to know what algorithms and strategies could solve the problem. In the end I expect to implement and test a collection of different approachs to understand their performance on this specific problem.
Furthermore, this system is implemented using ROS, so if you know of packages that already do what I'm searching for I would be glad to hear. You can also cite papers on the topic that you think might be of my interest.
","localization, ros, computer-vision, algorithm, uav"
"Quadrotor - Control system, where to begin?","I am starting to assemble a quadrotor from scratch.
Currently, I have this:

Structure;
an IMU (accelerometer, gyro, compass);
4 ESCs and DC motors;
4 propellers;
Raspberry Pi to control the system, and;
LiPo battery.

I have calibrated the ESCs and the four motors are already working and ready.
But now I am stuck.
I guess the next step is to dive deeply in the control system, but I am not sure where to begin. I read some articles about the control using PIDs, but I don't know how many should I use, or whether I need to model the quadrotor first to compute kinematic and dynamic of the quadrotor inside the RPi.
Sorry if the question is too basic!
More details
The structure is from a kit. Well, all I have now is the ESCs calibrated, although I do not have documentation of them to adjust the cut off voltage for the LiPo battery. I have been made tests with some Python code I found to have PWM outputs for the motors and to control I2C bus to communicate with IMU. 
One of my problems now is that I need RPIO library for PWM and the quick2wire-python-api to work with the I2C libraries from the MIT to control my IMU but as far as I know RPIO works with Python2 and quick2wire works with Python3 so I don't know how to manage this.
So actually, I have no code yet to control the four motors in parallel, only have testing code to test them separately and also with the IMU.
About the IMU, I am still learning how to work with it and how to use the MIT library. The unit includes those sensors:

ADXL345
HMC5883L-FDS
ITG3205

You can see a picture of the quadrotor below,

So as I said before, I would like to know how to handle the control system and how it is implemented inside the Raspberry Pi, and then start to work with the Python code to assemble the motors, the IMU and the control.
","control, pid, raspberry-pi, quadcopter, beginner"
Complete Quadrotor Tutorial (Text)Book?,"I'm looking for a complete tutorial textbook for how to build and control a quadrotor (dynamics, control, etc.).
I'm an engineer with a broad background in programming, mechanics, and control but it's been several years and I'm rusty. I was just wondering if anyone knew of a great ""from the ground up"" tutorial for quadrotors? I found this book which looks interesting but thought I'd ask here too.
Thanks!
EDIT
So, assume I've taken a formal course on all necessary topics: system modeling, mechanics, control theory, state estimation, programming, etc.
I'm looking for a book that assumes the reader is familiar with the topics but also goes step-by-step. For example, instead of just stating ""here are the system equations"" I'm looking for ""let's derive the system equations"" (but assumes you are familiar with modeling/kinematics). I'd like to start a quadcopter as a side project but have precious spare time so I'd prefer a single good reference instead of jumping from each individual topic textbook; maybe I'm just being greedy :)
",quadcopter
q-state vector used to define the transformation matrix? how?,"How can it be used to determine the transformation matrix?
an example could be at computing the inverse kinematics for small displacements:
J(q)$\Delta$q = $\Delta$u
$\Delta$U is a vector defining the difference between current and desired position. The desires position can always be computed, but if keep solving this in such manner that every time you solve $$J(q)\Delta q = \Delta u$$
you do this 

q:= q + $\Delta$q
Compute $T_{base}^{tool}(q)$
Compute the difference between $[T^{tool}_{base}]_{desired~position} $ and $T_{base}^{tool}(q)$.
If change is less than 10^-5 finish and output Q, if not resolve. 

How would you compute The transformation matrix based on q state vector. 
","inverse-kinematics, jacobian"
Do these motors really have enough torque to lift 130 pounds,"I was looking at these dc motors and I converted the torque to force at a meter distance and I got that two of them should be able to lift over 130 pounds, is this right? http://www.active-robots.com/high-torque-dc-servo-motor-10rpm-with-step-dir-drive There has to be some catch i am not seeing
","motor, torque"
What to do when position control with trajectories is interrupted?,"What are strategies used when trajectories, which are applied to a robotic joint, are interrupted? Say a robotic arm hits an obstacle, the controller just keeps applying the trajectory. Then at the end, the error gets so large, the torque can get quite strong and damage the robot or snap.
","control, robotic-arm, joint"
Software to control an Arduino setup with a timing belt and stepper motors,"I would like to know what software is available to control a timing belt with stepper motors for arduino board.Much like how its done in 3d printing.
But in this case i wont be making a 3d printer.Just one simple setup.
","arduino, robotic-arm, stepper-driver"
Wheel encoder triggers interrupt too many times,"I am building a simple robot with two driving wheel.
I want to control the wheel rotation using a wheel encoder like this one.
Here is a code I have on Arduino to try to understand the problem I'm facing:
int count = 0;
void setup() {
  Serial.begin(9600);
  attachInterrupt(digitalPinToInterrupt(2), upL, RISING);
  pinMode(2, INPUT);
}

void loop() {
  Serial.println(String(digitalRead(2)) + ""__"" + String(count));
}

void upL(){
  count++;
}

What I notice is:
The interrupt is triggered multiple times when the sensor beam is cut once.
But when I digitalRead the pin, then there is only one change.  
I also noticed that the interrupt is also triggered when going from HIGH to LOW.
Here is an example of the ouput I have:
0__0
0__0
0__0
0__0
...
...
0__0
0__0
0__0   <<< change from LOW to HIGH... 
1__9   <<< the interrupt must have incremented only once... 9 instead !
1__9
1__9
1__9
...
...
1__9
1__9   <<< change from HIGH to LOW. the interrupt shouldn't be triggered
0__24  <<< still... we have 15 increments
0__24
0__24
0__24
0__24
...
...
0__24
0__24   <<< same here...
1__51   <<< 26 increments
1__51
...
...
1__51  <<< same here...
0__67  <<< 16 increments
0__67
0__67

The only way I can explain that is that during a change of state, the signal received from the sensor is not really square, but somehow noisy.
Like in this image :

Therefore we would have, indeed, many RISING on one change....
(However reading the output of the sensor on an analog pin shows a direct variation from 880(HIGH) to 22(LOW))
Does anyone have another explanation? Or a solution to this problem ?

EDIT
Thanks to @TobiasK I know that this is called a bouncing effect. By doing further research I came across this solution:
playground.arduino.cc/Main/RotaryEncoders (Ctrl+F for rafbuff).
I'm trying it and i'll let you know.
","arduino, wheel, two-wheeled, interrupts"
mbed dc motor speed control using optocoupler encoder,"i have been playing with mbed since few week ago and would like to create a test program for dcmotor speed control. im using a 24V dc motor with encoder attach.. then test,
at first i test with supplying a 5v and 3.3V from mbed to the motor. the thing work fine giving a 16 and 10rpm respectively.
later i try using 12v it give 40rpm. 
then i try a 24v input, now the speed wont come out. and also the counter that count each on/off tick not counting up.
any possible reason?
my though that maybe when it move so fast that the ISR routine couldn't catch up to the speed..


","control, motor"
Localization with only IMU,"What will be the best approach to get the most localization accuracy out of only an accelerometer and gyroscope?
","localization, imu, accelerometer, gyroscope, algorithm"
How can I send video from my Arduino camera module video to my Android screen?,"I'm trying to connect a camera module to my Arduino Mega, connect my Mega to my Android phone (throught BlueTooth or other), and send the live view of the camera to the mobile phone.
I saw a video online that showed this for still images -- an image captured by the camera module on the Arduino was sent to Android and the output image was viewed after a couple of seconds (the time to send image by BT).
Is this doable with live video instead of image?  If yes, please guide me; if no, please suggest some workarounds.
","arduino, cameras"
Autonomous robots hardware structure planning,"I ask this question to clear my concept about hardware structure of humanoid autonomous fire robot, Here is scenario a fire robot detect humans from fire, there are some vision cameras some temperature and smoke sensors which help to perform this task. Now a days in market there are many processors like ARMV7 and Snapdragon which process tasks in any device and control the system i don't think autonomous fire robot use some kind of processors.
Does autonomous robots like fire robot use processors or micro controllers. Does it require OS or Rams environment? Like any computer system which use these kind of things.
","embedded-systems, humanoid"
Modified DH Parameters?,"Is the notation of the geometry of robots from Khalil and Kleinfinger be considered as one of the probably ""many"" Modified DH Parameters?
",dh-parameters
How to combine odometry information with time-shifted information from imu,"I'm working with a differential-drive robot that has odometry measurements from wheel shaft encoders and heading information from an imu (I'm using BNO055 in imu mode to get Euler angles, primarily the heading angle).
I'd like to use the imu header angle to augment the odometry which is prone to slipping and other errors, but the imu lags the odometry by up to 100ms.
How can I combine these measurements to get the best estimate of the robot pose?
Thanks for any word on this.
","mobile-robot, imu, odometry"
Gazebo laser plug-in fails to publish scan results,"I have added hokuyo laser plug-in to mu urdf file. I successfully launched the robot in gazebo and done required changes to visualize it in rviz. I didn't get any error but the laser scan results are not published.
Fine my results below 
Terminal while launching gazebo model in world

Result for rostopic echo /scan

I have also verified the tf of the model in rqt and its fine
Aslo find my urdf file below, can someone help me to fix this?
Link & joint:
<link name=""hokuyo_link"">
    <collision>
      <origin xyz=""0 0 0"" rpy=""0 0 0""/>
      <geometry>
        <box size=""0.1 0.1 0.1""/>
      </geometry>
    </collision>
    <visual>
      <origin xyz=""0 0 0"" rpy=""0 0 0""/>
      <geometry>
        <box size=""0.1 0.1 0.1""/>
      </geometry>
    </visual>
    <inertial>
      <mass value=""1e-5"" />
      <origin xyz=""0 0 0"" rpy=""0 0 0""/>
      <inertia ixx=""1e-6"" ixy=""0"" ixz=""0"" iyy=""1e-6"" iyz=""0"" izz=""1e-6"" />
    </inertial>
  </link>

  <joint name=""hokuyo_joint"" type=""fixed"">
    <axis xyz=""0 1 0"" />
    <origin xyz=""0 -0.055 0.2"" rpy=""0 0 0""/>
    <parent link=""Base_plate""/>
    <child link=""hokuyo_link""/>
  </joint>

Controller:
<gazebo reference=""hokuyo_link"">
        <sensor type=""gpu_ray"" name=""hokuyo"">
          <pose>0 0 0 0 0 0</pose>
          <visualize>false</visualize>
          <update_rate>40</update_rate>
          <ray>
            <scan>
              <horizontal>
                <samples>100</samples>
                <resolution>1</resolution>
                <min_angle>-1.570796</min_angle>
                <max_angle>1.570796</max_angle>
              </horizontal>
            </scan>
            <range>
              <min>0.10</min>
              <max>30.0</max>
              <resolution>0.01</resolution>
            </range>
          </ray>
          <plugin name=""gpu_laser"" filename=""libgazebo_ros_gpu_laser.so"">
            <topicName>/scan</topicName>
            <frameName>hokuyo_link</frameName>
          </plugin>
        </sensor>
      </gazebo>

","mobile-robot, ros, navigation, simulation, gazebo"
How to interpret the result of image rectification?,"I've been trying to understand image rectification. Most of the time the result of image rectification is illustrated by the original image (i.e the image before rectification) and the rectified image, like this:
The original image:

The rectified image:

To me the original image makes more sense, and is more 'rectified' than the second one. I mean, why does the result of rectification looks like that? and how are we supposed to interpret it? what information does it contain? 
An idea has just dawned on me : could it be that this bizarre shape of the rectified image is dependent on the method used for rectification, because  here polar rectification was used (around the epipole)?
","mobile-robot, stereo-vision, 3d-reconstruction"
Crock Pot Knob Turner,"I have a Crock Pot with an analog knob and would like to find a way to turn the knob by using and appliance timer. I have no idea where to begin. I need help.
Thanks
","control, sensors, stepper-motor"
Typical Problem in Simple Line Follower Using 3 sensors,"I am working on building a line follower robot using ATmega2560 and I want its movement to be more precise. I am facing a very typical problem.
It consists of three(3) IR sensors. The thickness of the line to be followed is 1.2cm and the gap between the sensors is more than that, around 1.8cm. 
So if the black line comes between the center and any of the side sensors, all the three sensors are on white and it stops.
And I need the robot to stop over white, due to my application. 
So please can anyone suggest me a good algorithm to tackle this situation.
I think PID control can be of good use, as i searched on Google. But I don't understand how to implement it with three sensors.
Please Help
","microcontroller, line-following, avr"
how to actuate pneumatic muscle by the signals received by emg sensors interfaced with raspberry pi?,"My aim is to actuate a pneumatic muscle based on signals received by EMG sensors placed on the biceps. Is there any Matlab code which can process the received EMG signals and convert them into any form which can be useful for muscle actuation?
The linked video gives better insight to my question: exoskeleton arm pneumatic muscle
","raspberry-pi, matlab"
Figure out PID values from drone specs,"I have all the specs from a quadcopter, everthing, would it be possible to figure out the pid from those specs?
","quadcopter, pid"
Proportional controller error doesn't approach zero,"I'm reading this pdf. The dynamic equation of one arm is provided which is 
$$
l \ddot{\theta} + d \dot{\theta} + mgL sin(\theta) = \tau
$$
where 
$\theta$ : joint variable. 
$\tau$ : joint torque
$m$ : mass
$L$ : distance between centre mass and joint. 
$d$ : viscous friction coefficient
$l$ : inertia seen at the rotation axis. 

I would like to use P (proportional) controller for now. 
$$
\tau = -K_{p} (\theta - \theta_{d})
$$
My Matlab code is 
clear all
clc

t = 0:0.1:5;

x0 = [0; 0];

[t, x] = ode45('ODESolver', t, x0);

e = x(:,1) - (pi/2); % Error theta1

plot(t, e);
title('Error of \theta');
xlabel('time');
ylabel('\theta(t)');
grid on

For solving the differential equation 
function dx = ODESolver(t, x)

dx = zeros(2,1);
%Parameters:

m  = 2;
d  = 0.001;
L  = 1;
I = 0.0023;
g  = 9.81;

T = x(1) - (pi/2);


dx(1) = x(2);

q2dot = 1/I*T - 1/I*d*x(2) - 1/I*m*g*L*sin(x(1));

dx(2) = q2dot;

The error is 

My question is why the error is not approaching zero as time goes? The problem is a regulation track, so the error must approach zero. 
","control, dynamics, manipulator"
Forward kinematic computing the transformation matrix,"I am the moment trying to compute the transformation matrix for robot arm, that is made of 2 joints (serial robot arm), with which I am having some issues. L = 3, L1 = L2 = 2, and q = ($q_1$,$q_2$,$q_3$) = $(0 , \frac{-\Pi}{6},\frac{\Pi}{6})$
Based on this information I have to compute the forward kinematic, and calculate the position of each joint. 
Problem here is though, how do I compute the angle around x,y,z.. for the transformation matrix.  Using sin,cos,tan is of course possible, but what do their angle corresponds? which axis do they correspond to?

I tried using @SteveO answer to compute the $P_0^{tool}$ using the method he provided in his answer, but I somehow mess up something, as the value doesn't resemble the answer given in the example.. 
 
","kinematics, forward-kinematics, orientation"
"How to find the body jacobain, for each link in a robot manipulator?","The links twist could be obtained, and thus The spatial manipulator Jacobian could be done, but when it comes to the body Jacobian, it is becomes difficult. Moreover, the adjoint transformation relates both Jacobain, but however that is 4*4 while the Jacobian is 6*n; how does it works? as in the picture, he is getting a body jacobian for each link, not one jacobian matrix for the whole robot, I don't know.
Any help is highly regarded.
Like this example or here for
full details
","robotic-arm, kinematics, inverse-kinematics, manipulator, jacobian"
How to find kinematics of differential drive caster robot?,"I'm working on a little project where I have to do some simulations on a small robot.
I my case I'm using a differential-drive robot as one of the wheels of a bigger robot platform (which has two differential-drive casters), and I really do not understand how to find its kinematics in order to describe it in a model for finding the speed V_tot of the platform.

This is my robot and I know the following parameters

d is the distance between a joint where my robot is costrained
blue point is the joint where the robot is linked to the robot platform
L is distance between the wheels
r the radius of the wheel
the robot can spin around the blue point and with and THETA angle

As I know all this dimensions, I would like to apply two velocities V_left and V_right in order to move the robot.
Let's assume that V_left = - V_right how do I find analitically the ICR (Istantaneous Center of Rotation) in this costrained robot?
I mean that I cannot understand how to introduce d in the formula.
","mobile-robot, kinematics, wheeled-robot, differential-drive, two-wheeled"
Powering 6 servomotors requiring 6V and 2A each,"The title pretty much says it all.  I'm on a team that is currently building a robotic arm for the capstone project of my engineering degree, our design is similar to the Dobot (5 degrees of freedom). We purchased our 6 servomotors, and each one requires 2A at 6V.  
From my preliminary research, I haven't been able to find a power source that could satisfy this.  We'd rather not purchase six individual AC/DC power source for each servo, and we've heard that these can introduce problems, as they aren't necessarily voltage-regulated.  Another suggestion we've received is to buy a computer power source, and modify it to output our the voltage and amperage we need. This raises some concerns, since our professor running the course might find this dangerous.
We'd like some input into how we can power our servos effectively, without going overboard on costs (we are students, after all).
Thanks!
","power, servos, servomotor, arm"
How do I create a portable solar panel lipo charger?,"We're working on a quadcopter that will carry a solar panel on top that will continually charging the lipo battery of the quad. What's the smallest and easiest way to recreate a charger that will allow safe charging for the lipo battery?
","quadcopter, battery"
inverse kinematics osciliations..,"I am the moment having some issues with an Jacobian going towards a singularity (i think)as some of its values becomes close to zero, and my robot oscillates, and therefore thought that some form of optimization of the linear least square solution is needed. 
I have heard about the interior point method, but I am not sure on how i can apply it here. 
My equation is as this.. 
J(q)dq = du 
How would i have to implement the optimization, and would be needed?
","inverse-kinematics, c++"
How a 3d printer moves the header vertically in a MakerBot printer,"I would like to know how the header in this MakerBot printer moves in the vertical up /down direction.  Is there a detailed explanation of it including the parts involved?

","mechanism, 3d-printing"
Real-time GY-85 IMU sensor interfacing with Simulink,"How do I read real time values from the GY-85 IMU sensor at Simulink connected via Arduino? Also, I intend to interact with the Virtual Reality environment at Simulink using this GY-85 IMU sensor.
Is this possible?
How do I make MATLAB read real time values from this GY-85 IMU sensor connected to Arduino via I2C communication ?
Please help!
","imu, matlab"
Is it possible to make DIY clone of MakerBeam,"Is it possible to make clone of http://www.makerbeam.eu/ of some easy accessible material like:

wood
plywood
OSB
MDF
HDF
others

Using any type of CNC machine to mill some holes and rails in those materials may give sufficient results e.g. to make a prototype of 3D printer of such ""beams"".
Of course it won't be as rigid and durable but for making prototypes it may be good idea.
Just for reference: when reading this http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html I've found this http://bitbeam.org/, this https://www.google.com/search?q=grid+beam&client=ubuntu&hs=zAr&channel=fs&source=lnms&tbm=isch&sa=X&ved=0CAcQ_AUoAWoVChMIiLHdqf2MyQIVQZoUCh1cHwV1&biw=1215&bih=927 and this http://www.gridbeam.com/, and this https://www.tetrixrobotics.com/.
",mechanism
Best strategy for area scanning using little sensing bots,"I'm currently working on a school project about simulating robots scanning an area, but it has been a struggle to find what strategy the robots should use.  Here are the details:
I am given a certain amount of robots, each with a sensing range of $r$.  They spawn one after another.  Their task to scan a rectangular area.  They can only communicate with each other when they are within communication range. 
I am looking for the best strategy, (i.e. time efficient solution) for this. 
Any reply or clue to the strategy will be appreciated. 
","mobile-robot, sensors, coverage"
Dynamic simulation of compliant elements in quadruped robot,"I have a preliminary design for a legged robot that uses compliant elements in the legs and in parallel with the motors for energy recovery during impact as well as a pair of flywheels on the front and back that will oscillate back and forth to generate angular momentum. I'd like to create a dynamic simulation of this robot in order to be able to test a few control strategies before I build a real model. What simulation package should I be using and why? 
I have heard good things about MSC Adams, namely that it is slow to learn, but has a lot of capability, including integration with matlab and simulink. I have also heard about the simmechanics toolbox in matlab, which would be nice to use since I already am decent with CAD and know the matlab language. I am not yet familiar with simulink, but have used Labview before.
","mobile-robot, design, dynamics, matlab, simulation"
Need the uav to perform a circle turn,"I've got a project that will require my drone to perform a circle turn while the drone is always facing the tangent of the turning curve. Similiar to a car that is performing a frictionless banked turn. 
Just wondering which method should i use to achieve it, the throttle control can be ignored since i already have a pid on height control.
Any suggestions would be appreciated. 
","quadcopter, pid, multi-rotor"
How to find center of a disk using robotic arm,"Hello I am new to the field of robotics but have some knowledge of raspberry pi, arduino, python.
I want to make Robotic arm which can be used to find the centre of any disk.
There may be disk of different diameter coming one after another on conveyor.
I need to make hole at the center of disk using robotic arm. How can I do this ?
What techniques and sensors I should use to implement the mechanical and electronic part.
(I don't want to use camera and openCV). Thanks in advance.

","robotic-arm, mechanism, electronics, movement"
Can i use DC Brushed motors for building a drone?,"I want to make a drone. But my budget is very low. Brushless motors are very expensive. I want use the Brushed CHEAP ones. can i us them ?
","quadcopter, motor, brushless-motor"
How can this mobile robot rotate so perfectly?,"Please have a look at this youtube video.
When this mobile robot operates, the rack (which is said to weight up to 450 kg) can have its Center Of Mass (COM) distributed at any location. For example, this COM can be located 5 or 10 cm off from the center of the robot. Because of this, when the robot revolves, the center of rotation will not be at the center of the robot anymore. However as you see in the video, it can still rotate many circles perfectly around its up-right axis.
So what do you think about this? Is this possible by mechanical design only? Or did they use some kind of advanced feedback control system to counter the effect of the off-center COM?
","mobile-robot, control, mechanism"
Quadcopter Charging,"Our thesis is about Multicopter using two batteries wherein the first battery is used and when the power goes out, a switch is placed use to charge the first battery while the second battery is powering the multicopter, the process repeats again and again until all the power of the battery is drained. Is the process possible to achieved?
Edit 1:
Just to clear everything up again. The thesis is about a Quadcopter that uses two batteries as its power source. These two batteries are attached to a ""switching circuit system"" that will allow one battery to be drained then the quadcopter will be transferred to the other battery. The solar panels serve as the ""charger"" of whatever battery is in its standby mode (the discharged battery). So while one battery is being used by the quadcopter, one will be connected to a solar panel which will be charging that battery. Once the other battery is drained, the quadcopter will switch to the charged battery. This will continue to go on until both batteries are drained. Is this achievable?
","battery, multi-rotor"
Tf frame origin is offset from the actual base_link,"I have built my differential drive mobile robot in solidworks and converted that to URDF file using soliworks2urdf converter. I successfully launched and robot and simulated with tele-operation node. Since i am intended to use navigation stack in i viewed the transform of the robot in rviz which resulted as below.

As you can see the base plate is the one which supports the wheels and castors but the tf of base plate is shown away from the actual link and even odom is away from the model. Where have i gone wrong and how to fix this. Refer the URDF of model below.
    <?xml   version=""1.0""?>
<robot
  name=""JMbot"">
  <link
    name=""Base_plate"">
    <inertial>
      <origin
        xyz=""-0.3317 0.71959 -0.39019""
        rpy=""0 0 0"" />
      <mass
        value=""0.55378"" />
      <inertia
        ixx=""0.0061249""
        ixy=""0.00016086""
        ixz=""-8.6651E-18""
        iyy=""0.0041631""
        iyz=""-1.4656E-17""
        izz=""0.010283"" />
    </inertial>
    <visual>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Base_plate.STL"" />
      </geometry>
      <material
        name="""">
        <color
          rgba=""0.74902 0.74902 0.74902 1"" />
      </material>
    </visual>
    <collision>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Base_plate.STL"" />
      </geometry>
    </collision>
  </link>
  <link
    name=""Wheel_R"">
    <inertial>
      <origin
        xyz=""0.010951 1.1102E-16 -1.1102E-16""
        rpy=""0 0 0"" />
      <mass
        value=""0.45064"" />
      <inertia
        ixx=""0.00091608""
        ixy=""-1.2355E-19""
        ixz=""1.0715E-18""
        iyy=""0.00053395""
        iyz=""-6.7763E-20""
        izz=""0.00053395"" />
    </inertial>
    <visual>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Wheel_R.STL"" />
      </geometry>
      <material
        name="""">
        <color
          rgba=""0.74902 0.74902 0.74902 1"" />
      </material>
    </visual>
    <collision>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Wheel_R.STL"" />
      </geometry>
    </collision>
  </link>
  <joint
    name=""Wheel_R""
    type=""continuous"">
    <origin
      xyz=""-0.14688 0.40756 -0.73464""
      rpy=""-2.7127 -0.081268 -3.1416"" />
    <parent
      link=""Base_plate"" />
    <child
      link=""Wheel_R"" />
    <axis
      xyz=""1 0 0"" />
  </joint>
  <link
    name=""Wheel_L"">
    <inertial>
      <origin
        xyz=""-0.039049 2.2204E-16 2.498E-15""
        rpy=""0 0 0"" />
      <mass
        value=""0.45064"" />
      <inertia
        ixx=""0.00091608""
        ixy=""-9.6693E-19""
        ixz=""-1.7816E-18""
        iyy=""0.00053395""
        iyz=""1.3553E-19""
        izz=""0.00053395"" />
    </inertial>
    <visual>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Wheel_L.STL"" />
      </geometry>
      <material
        name="""">
        <color
          rgba=""0.74902 0.74902 0.74902 1"" />
      </material>
    </visual>
    <collision>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Wheel_L.STL"" />
      </geometry>
    </collision>
  </link>
  <joint
    name=""Wheel_L""
    type=""continuous"">
    <origin
      xyz=""-0.46668 0.40756 -0.70859""
      rpy=""2.512 0.081268 3.4272E-15"" />
    <parent
      link=""Base_plate"" />
    <child
      link=""Wheel_L"" />
    <axis
      xyz=""-1 0 0"" />
  </joint>
  <link
    name=""Castor_F"">
    <inertial>
      <origin
        xyz=""2.2204E-16 0 0.031164""
        rpy=""0 0 0"" />
      <mass
        value=""0.056555"" />
      <inertia
        ixx=""2.4476E-05""
        ixy=""-2.8588E-35""
        ixz=""1.0281E-20""
        iyy=""2.4476E-05""
        iyz=""-1.2617E-20""
        izz=""7.4341E-06"" />
    </inertial>
    <visual>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Castor_F.STL"" />
      </geometry>
      <material
        name="""">
        <color
          rgba=""0.75294 0.75294 0.75294 1"" />
      </material>
    </visual>
    <collision>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Castor_F.STL"" />
      </geometry>
    </collision>
  </link>
  <joint
    name=""Castor_F""
    type=""continuous"">
    <origin
      xyz=""-0.31952 0.39256 -0.57008""
      rpy=""-1.5708 1.1481 -1.3614E-16"" />
    <parent
      link=""Base_plate"" />
    <child
      link=""Castor_F"" />
    <axis
      xyz=""0 0 1"" />
  </joint>
  <link
    name=""Castor_R"">
    <inertial>
      <origin
        xyz=""-1.1102E-16 0 0.031164""
        rpy=""0 0 0"" />
      <mass
        value=""0.056555"" />
      <inertia
        ixx=""2.4476E-05""
        ixy=""0""
        ixz=""-3.9352E-20""
        iyy=""2.4476E-05""
        iyz=""-1.951E-20""
        izz=""7.4341E-06"" />
    </inertial>
    <visual>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Castor_R.STL"" />
      </geometry>
      <material
        name="""">
        <color
          rgba=""0.75294 0.75294 0.75294 1"" />
      </material>
    </visual>
    <collision>
      <origin
        xyz=""0 0 0""
        rpy=""0 0 0"" />
      <geometry>
        <mesh
          filename=""package://jmbot_description/meshes/Castor_R.STL"" />
      </geometry>
    </collision>
  </link>
  <joint
    name=""Castor_R""
    type=""continuous"">
    <origin
      xyz=""-0.34387 0.39256 -0.86909""
      rpy=""1.5708 -0.93144 3.1416"" />
    <parent
      link=""Base_plate"" />
    <child
      link=""Castor_R"" />
    <axis
      xyz=""0 0 1"" />
  </joint>

<gazebo>
<plugin name=""differential_drive_controller""
    filename=""libgazebo_ros_diff_drive.so"">
    <leftJoint>Wheel_L</leftJoint>  
    <rightJoint>Wheel_R</rightJoint>
    <robotBaseFrame>Base_plate</robotBaseFrame>
    <wheelSeparation>0.235</wheelSeparation>
    <wheelDiameter>0.12</wheelDiameter>
    <publishWheelJointState>true</publishWheelJointState>
    </plugin>
<plugin name=""joint_state_publisher""
    filename=""libgazebo_ros_joint_state_publisher.so"">
    <jointName>Castor_F, Castor_R</jointName>
    </plugin>

</gazebo>

</robot>

","mobile-robot, ros, navigation, odometry, gazebo"
imu position without GPS or camera,"I have a IMU that has 3-axis accelerator, 3-axis magnetometer, 3-axis gyroscope and row, yaw, pitch value. I want to get the location of the IMU coordinate(the beginning point is (0,0,0)) but I know just using double integration will has dead reckoning problem. And I found a lot of paper just talking about combining IMU with GPS or camera by using Kalman filter. Is it possible that I just use a IMU to get a slightly precise position data? Because in the future work I will use multiple IMUs bounded on human arms to increase the accuracy. 
","imu, gps"
Angle Random Walk vs. Rate Noise Density (MPU6050),"I’ve made a datalog from a MPU6050 (IMU: gyroscope and accelerometer) at 500Hz sample rate. Now I want to calculate the characteristics from the gyro to evaluate the sensor. 
For the gyro I’ve found following values in the datasheet:
Total RMS Noise = 0.05 °/s
Low-frequency RMS noise = 0.033 °/s 
Rate Noise Spectral Density = 0.005 °/s/sqrt(Hz)
Now I want to ask how I can calculate these values from my dataset?
At the moment I’ve the following values from the dataset:
Standard deviation = 0.0331 °/s
Variance = 0.0011
Angular Random Walk (ARW) = 0.003 °/sqrt(s) (From Allan deviation plot)
Bias Instability = 0.0012 °/s
Is the ARW equal to the Rate Noise Spectral Density mentioned in the datasheet? And also is the RMS Noise from the datasheet equal to the standard deviation? 
edit:
I found following website: http://www.sensorsmag.com/sensors/acceleration-vibration/noise-measurement-8166
There is the statement:  ""...Because the noise is approximately Gaussian, the standard deviation of the histogram is the RMS noise"" So I guess the standard deviation is the RMS noise from the datasheet. But how about the ARW?
","sensors, imu, gyroscope, sensor-fusion, statistics"
Tracking objects from camera; PID controlling; Parrot AR Drone 2,"I am working on a project where I should perform object tracking using the camera of Parrot AR Drone 2.0. So the main idea is, a drone should be able to identify a specified colour and then follow it by keeping some distance.
I am using the cvdrone API to establish communication with the drone. This API provides function:
ARDrone::move3D(double vx, double vy, double vz, double vr)

which moves the AR.Drone in 3D space and where 

vx: X velocity [m/s]
vy: Y velocity [m/s]
vz: Z velocity [m/s]
vr: Rotational speed [rad/s]

I have written an application which does simple image processing on the images obtained from the camera of the drone using OpenCV and finds needed contours of the object to be tracked. See the example below:

Now the part I am struggling is finding the technique using which I should find the velocities to be sent to the move3D function. I have read that common way of doing controlling is by using PID controlling. However, I have read about that and could not get how it could be related to this problem.
To summarise, my question is how to move a robot towards an object detected in its camera? How to find coordinates of certain objects from the camera? 
","control, pid, quadcopter, tuning, opencv"
"I am an entrepreneur and I want to start building robots for businesses, where do I start?","Over the last couple of years I've had good success with my technology startups and now looking to enter into robotics. I was interested in robotics and automation ever since I was a kid (yes, that sounds nerdy). So my question is: Where to get started, what to build? and how to sell? And lastly, how difficult it is to sell in this industry?
",industrial-robot
"How can I detect ground collision, for an hexapod robot?","I'm planning to build a small (probably around 30 centimeters in diameter, at rest) hexapod robot, but, currently, it would only be able to walk on even ground. To improve this, I would have to, somehow, detect when each leg collides with the ground. 
Ideally, I would be able to know how much weight each leg is supporting, so that I could both balance the weight and adapt to moving (up or down) terrain (so, if you put a finger below one leg and lifted it, the leg would go up); however, if that's not possible, a simple binary signal would do.
Is there a simple and compact method to do this? 
","sensors, hexapod"
DC Motor to open a door,"I want to open a door using a DC motor. I've estimated that the required power in the worst case would be around 35-40W (considering a ~80% efficiency). The whole is controlled by a Particle Photon.
I was thinking to use a L298N to control the output of current to the motor. However, when I looked for powerful enough motors, they would all consume too much current when stalling (> 4A part of the L298N datasheet).
Do you have ideas of how to overcome this? Maybe there's another dual-bridge that can handle more current, maybe there exists a DC motor that is ok for a L298N, or maybe I need to have simultaneous DC motors?
Edit: this part should be a question by itself. I'll keep it here so that future visitors know what the sub-question was about, but please ignore it from now on if answering.

As a sub-question, would it be better to use a brushed or a brushless DC motor?

","motor, h-bridge"
Path following with precise positioning system (RTK),"Is there any general problem in using precise positioning (centimeter-accurate GNSS RTK system meant) for autonomous car guidance given I have a predefined path the car should follow? I mean, autonomous cars were the topic #1 at CES 2016 yet no such simple system seems to have been introduced to date... Of course the ""path planning"" is only a part of the ""autonomous package"" and other problems need to be solved (collision prevention etc.) but I really wonder whether something simple like RTK guidance could be used.
An RTK system relies on very little amount of live correction data (about 1 kB/s) and mobile networks are really ubiquitous today so I can not really see a technical problem in such solution given there are enough RTK base stations around.
EDIT:
This question is only about using precise positioning to follow a predefined track in obstacle-free environment. I am not asking about other systems that need to be implemented in an autonomous car like collision prevention etc. (such systems may employ LIDAR or a stereo camera). Surely a collision prevention is a must for an autonomous system but I consider a theoretical case only.
An updated question may be: Is precise satellite positioning accurate enough to guide/navigate a full-scale passenger car in an obstacle-free outdoor environment in the speed of about 100 km/h given I have a precise-enough path prerecorded that is to follow?
Some answers below already say yes, this is a solved problem. It would be nice to have the answers elaborated in more detail regarding the existing solutions (accuracy, possible problems etc.). One of the solutions may probably be the open source APM autopilot which work for rovers too (example by Emlid) but that does not seem to use RTK so the accuracy may be rather low.
","navigation, gps, precise-positioning"
Exchange air and maintain thermal insulation,"My application is composting with worms outdoors inside an a styrofoam cooler.  I use a heat lamp and a thermo-electric cooler to maintain the temperature in the bin when the temperature outside is out of bounds for healthy worms.  When the temperature outside is in bounds, I'd like to exchange the air in the bin with fresh air from outside, but I don't want to permanently compromise the insulating properties of my bin with lots of air holes.  So I'm looking for actuator solutions that would allow me to open/close a window of sorts.  I'm considering a solenoid air valve but I don't necessarily need/want an air compressor - a simple fan is sufficient to circulate the air. Any suggestions?
","actuator, valve"
P gain tuning for quadcopter (Is my perception for a P-gain too high correct?),"Good day,
I am currently working on a project using Complementary filter for Sensor fusion and PID algorithm for motor control. I viewed a lot of videos in youtube as well as consulted various blogs and papers with what to expect with setting the P gain to high or too low.
P Gain too low

easy over correction and easy to turn by hand

P Gain too high

oscillates rapidly

I have a sample video of what I think a high P gain (3 in my case) looks like. Do this look like the P gain is too high?  https://youtu.be/8rBqkcmVS1k 
From the video:
I noticed that the quad sometimes corrects its orientation immediately after turning few degrees (4-5 deg). However, it does not do so in a consitent manner.
It also overcorrects.
The reason behind my doubt is because the quadcopter doesn't react immediately to changes. I checked the complementary filter. It updates (fast) the filtered angle reading from sudden angular acceleration from the gyro as well as updates the long term filtered angle changes from the accelerometer (albeit slowly). If I am right, is the the P gain is responsible for compensating the ""delay""?
The formula I used in the complementary filter is the following:
float alpha = 0.98;
float pitchAngleCF=(alpha)*pitchAngleCF+gyroAngleVelocityArray.Pitch*deltaTime)+(1-alpha)*(accelAngleArray.Pitch);

Here is a video for a P gain of 1: https://youtu.be/rSBrwULKun4
Your help would be very appreciated :)
","quadcopter, pid, sensor-fusion, tuning, filter"
RC car circuit: what does it do?,"I have just disassembled a RC car (a BBR 1:18 Scale Ferrari Enzo from 2005). Attached to the stepper motor that controls the steering of the car was a small circuit board with a cog wheel attached to it. I am trying to figure out what this circuit does. My idea is that it is responsible of keeping track of the wheels position but I am not certain. Here is a picture of it:


As you can see there are 5 wires coming out of it and I know that the green one is GND. Does anyone have an idea of what its function might be?
","stepper-motor, radio-control, circuit"
Are joint state vectors limited?,"Are joint-state vectors $q$, which define the position and orientation of a set of joints, limited somehow? 
I know they are used for the rotation part of the transformation matrix => therefore I would think they were limited within 0 and 2$\pi$
",joint
Create 2: Wheel interface board replacement,"http://www.irobotweb.com/~/media/MainSite/PDFs/About/STEM/Create/Create_2_Wheel_Hack.pdf?la=en
Scroll to page 3.
I'm trying to interface the Roomba's preloaded navigation system with a pair of motors not attached to the roomba itself- however, to do this I need an interface board of the same dimensions as the one pictured in the above document. It has 0.050"" (1.27) contact centers, which don't seem to be commercially available. Can anyone provide any help locating PCBs of this size?
","irobot-create, electronics, navigation, roomba"
Erratic motor behavior. Is it due to the faulty remote control or Grounding or something else?,"I am using arduino mega to run 4 motors via 4 motor controllers. I am using a ps2 controller as the remote control. When the joystick is at rest, the motor should stop.
The problem is the motor still moves randomly when the joystick is at rest. Sometimes, the joystick can only produce forward motion on the motor but not backward. Is this the grounding issue or the PS2 remote control issue ir others.. Does the GND from the arduino board have to be connected to the GND from the external battery? 
How can I troubleshoot this? 
Thanks.
","mobile-robot, control, power"
What exactly are PPM controlled ESCs? Are most ESCs available to build quadcopters PPM Controlled?,"First, I don't see any manufacturer or online store telling whether an ESC is PPM controlled or not. Second, I have also been Googling and asking comments in all about ESCs youtube videos for long, but I couldn't find anything useful.

Why do I need PPM controlled ESCs?

I'm doing a project based on AndroCopter and its clearly mentioned that it specifically requires the use of PPM controlled ESCs.

Can I use any ESCs available in the market for this project?

It's also mentioned in the Github repo that PPM controlled ESCs are the most common ones. However from some who explained ESCs in Youtube video has commented back for my doubt telling that most common ESCs are PWM controlled which is contradicting the previous statement.
PS: I need to use Arduino Mega to control the four ESCs. And Arduino Mega is programmed to send out PPM signals which is exactly why I need PPM controlled ESCs. Correct me if I made any mistakes.
","arduino, quadcopter, esc"
Orienting rectangular plastic bricks,"As part of a sorting machine, I need to orient a pile of plastic brick-shaped objects (which are all identical in size - about 3cm x 2cm x 1.5cm) so that they always end up with the white side facing up:

These will then be fed into a bowl feeder type of machine for further re-orienting. 
How can I accomplish this, preferably without optical sensors? I was thinking about cutting into the bricks and putting magnets inside, but is there a more elegant solution?
","mechanism, orientation"
Does the mAh of a battery mean longer power or more power?,"If a Lipo battery has more mAh will it be slower to run out of energy or will it have larger power output?
Thanks in Advance
","power, battery"
Arduino project: Turning with a fixed front wheel axis,"I am working on an Arduino robot project. This project requires a base with 4 wheels where the back wheels are attached to two DC motors that can be controlled independently of each other. I am thinking that the robot will turn by giving power to just one of the motors but I am having trouble with how the front axis should look like. Would it be possible to have a solid front axis with 2 wheels and still possible for the robot to turn or would the friction be too great?
","arduino, wheeled-robot, brushless-motor"
Are there any Lithium Ion battery monitors designed for hobbyists (quadcopters)?,"I have a friend who is getting into quadcopters and being the good techie buddy, I'm trying to find the right technology for battery monitoring so his expensive machine does not fall out of the sky unexpectedly. 
So far the only technology for hobbyists that I am seeing is voltage monitors, which aren't really useful for this battery chemistry. With the flat voltage curve LiIon has I'd expect a voltage monitor to falsely report a low battery when you draw extra current and indeed I'm seeing exactly this when my buddy does fast maneuvers mid flight.
In my day job we use charge counting battery monitors (BMS) for this battery chemistry. Usually custom designed for the battery pack (just like for laptop batteries, etc). Sometimes built into the battery pack, or sold by cell suppliers.
Have I missed a product for electric aircraft? Are hobbyists in the battery dark ages?
","battery, lithium-polymer"
Accurate technique to locate position?,"What is the most accurate way to locate the position and orientaion of the body during some motion (rotation and translation)?
I need to track the body very precisely, the required accuracy is 100-200 microns, and with rather high frequency - at least 1kHz. The body has one rotation axis. This axis can translate along the path. Normally the track has ellipse like shape, but translation path can change, that's why I need to track the body. The limit of motion is 50 cm on any direction. Maximum velocity is 5 m/s. 
Requirments about sensors: it's possible to place any sensor on the surface, but it's impossible to change the construction. So it's impossible to use encoders at the rotation axis to measure the angle. 
I tried to do it with MEMS 9DOF sensors, but because of the noise it's very difficult to understand when there is a motion and when it's a noise.
Another idea is to use magnet and magnetomemter, but how is it possible to measure the resolution in this way? 
","kinematics, navigation, tracks, orientation"
iRobot Create 2 sensors,"Can someone please provide me with a list of sensors on the create 2?  I am hoping to get one soon, but want to be sure it has ultrasonic sensors and not just bump sensors before I do.
","irobot-create, roomba"
How do I Program the Create 2,"I just un-boxed and set the Create 2 to charge over night.
How do I program it? Where is the software?
Daniel
","irobot-create, programming-languages"
iRobot Create 2 serial battery power,"I don's seem to be able to get any battery power from Create 2. I spliced the original cable it came with, and tried to use the power from red/purple(+) and yellow/orange(-) to power a Raspberry Pi2, with no luck. While the serial-to-USB cable still works, and I am able to command the robot via Python, there seems to be no power coming on the red/purple cables. I tried with a multimeter with no luck, even as I moved the device from passive/safe/full modes. There is no power even when Create 2 is charging/docked.
","raspberry-pi, irobot-create, serial, roomba"
Arduino-Create 2: Reading Sensor Values,"Over the past few weeks, I have been attempting to interface the iRobot Create 2 with an Arduino Uno. As of yet, I have been unable to read sensor values back to the Arduino. I will describe by hardware setup and my Arduino code, then ask several questions; hopefully, answers to these questions will be helpful for future work with the Create 2.
Hardware:
The iRobot Create 2 is connected to the Arduino Uno according to the suggestions given by iRobot. Instead of the diodes, a DC buck converter is used, and the transistor is not used because a software serial port is used instead of the UART port.
Software:
The following is the code that I am implementing on the Arduino. The overall function is to stop spinning the robot once the angle of the robot exceeds some threshold. A software serial port is used, which runs at the default Create 2 Baud rate.
#include <SoftwareSerial.h>
int rxPin=3;
int txPin=4;
int ddPin=5; //device detect
int sensorbytes[2]; //array to store encoder counts
int angle;
const float pi=3.1415926;
#define left_encoder (sensorbytes[0])
#define right_encoder (sensorbytes[1])
SoftwareSerial Roomba(rxPin,txPin);

void setup() {
  pinMode(3, INPUT);
  pinMode(4, OUTPUT);
  pinMode(5, OUTPUT);
  pinMode(ledPin, OUTPUT);
  Roomba.begin(19200);

  // wake up the robot
  digitalWrite(ddPin, HIGH);
  delay(100);
  digitalWrite(ddPin, LOW);
  delay(500);
  digitalWrite(ddPin, HIGH);
  delay(2000);

  Roomba.write(byte(128));  //Start
  Roomba.write(byte(131));  //Safe mode
  updateSensors();

  // Spin slowly
  Roomba.write(byte(145));
  Roomba.write(byte(0x00));
  Roomba.write(byte(0x0B));
  Roomba.write(byte(0xFF));
  Roomba.write(byte(0xF5));  
}

void loop() {
    updateSensors();
    // stop if angle is greater than 360 degrees
    if(abs(angle)>2*pi){
      Roomba.write(173);
      delay(100);
    }
}

void updateSensors() {
  // call for the left and right encoder counts
  Roomba.write(byte(148));
  Roomba.write(byte(2));
  Roomba.write(byte(43));
  Roomba.write(byte(44));
  delay(100);

  // load encoder counts into an array
  int i = 0;
  while(Roomba.available()) {
    int c = Roomba.read();
    sensorbytes[i] = c;
    i++;
  }
  angle=((right_encoder*72*pi/508.8)-(left_encoder*72*pi/508.8))/235;
}

Questions:

Am I loading the sensor values into the array correctly? This same code works when a bump and run program is implemented, but that requires knowing only one bit rather than two bytes.
How many bytes can be read over the serial connection at a time? A previous post (Help sending serial command to Roomba) highlights that one byte can be sent at a time. Does this imply that the reverse is true? If so, would a solution be to use a char array to read the values instead and then to append two chars to form an signed int?
Is serial communication synchronization a problem? I am assuming that synchronization is not a problem, but is it possible for the bytes to be split on the nibble boundaries? This would present a problem because there is not a nibble datatype. 

","arduino, irobot-create, roomba"
Best power solution for my robot,"I've built quadruped robot which is using 12 servos (TowerPro SG90 Servo) and Raspberry Pi (model B 1). Right now I'm ""feeding"" it with 5V 2.5A charger. 
How can I make it un-tethered? What should I look out for when selecting batteries? Also, I think that I need to separate powering of RPi and servos because power is ""jumping"" when it moves and that isn't good for RPi.  
A little video - Testing Walking Algorithm
","mobile-robot, raspberry-pi, power, battery, walking-robot"
Will a 5200mAh 30C 22.2V 6S Lipo battery work with,"I am building a Quadcopter and I was wondering if a 5200mAh 30C 22.2V 6S Lipo battery will work with a 40Amp Esc's, MT4108 370 KV Motors, and GEMFAN 1470 Carbon Fiber Props. The over all payload will be about 5-6 pounds.
",quadcopter
iRobot Create 2: Encoder Counts,"This post is a follows from an earlier post (iRobot Create 2: Angle Measurement). I have been trying to use the wheel encoders to calculate the angle of the Create 2. I am using an Arduino Uno to interface with the robot.
I use the following code to obtain the encoder values. A serial monitor is used to view the encoder counts.
void updateSensors() {
  Roomba.write(byte(149)); // request encoder counts
  Roomba.write(byte(2));
  Roomba.write(byte(43));
  Roomba.write(byte(44));
  delay(100); // wait for sensors 
  int i=0;
  while(Roomba.available()) {
    sensorbytes[i++] = Roomba.read();  //read values into signed char array
  }

  //merge upper and lower bytes
  right_encoder=(int)(sensorbytes[2] << 8)|(int)(sensorbytes[3]&0xFF);
  left_encoder=int((sensorbytes[0] << 8))|(int(sensorbytes[1])&0xFF);

  angle=((right_encoder*72*3.14/508.8)-(left_encoder*72*3.14/508.8))/235;
}

The code above prints out the encoder counts; however, when the wheels are spun backwards, the count increases and will never decrement. Tethered connection to the Create 2 using RealTerm exhibits the same behavior; this suggests that the encoders do not keep track of the direction of the spin. Is this true? 
","irobot-create, roomba"
Determine current roomba state / operating mode,"Using the SCI messages, I would like to determine the current operating mode or state of a iRobot Roomba 780. Finally, I would like to detect and separate four states: 

Cleaning
In docking station
Returning to docking station
Error (e.g. trapped on obstacle)

What is a fast and reliable way to detect those states using SCI data?
The Roomba SCI Sensor packets ""Remote Control Command"" and ""Buttons"" seem to return the currently called commands and not the currently executed ones.
",roomba
Gazebo or SimMechanics,"I'm now considering to choose Gazebo or SimMechanics for simulating my quadruped robot.
I set some standards for the simulation:

Support Real-time application with ROS
Simulate contact impact well(with ground) (deformable if possible)
Good rendering Quality.

I have learned Gazebo for months, and see it has some limits to meet my requirements, especially the contact and friction problems. 
I didn't use SimMechanics, but when i'm very impressed on it when i see this video.
Anyone who has experiences on Quadruped Simulation can share me some advice?
Thank you so much.
","simulator, gazebo"
2D Robot Motion,"Good day,
I have a robot with an IMU that tells Yaw Rate, and Magnetic heading. It also tells Xvelocity and YVelocity at that instance of the vehicle, on the vehicle frame. (So irrespective of heading, if the robot moved forward, yvelocity would change for example)
Assuming my robot starts at position (0,0) and Heading based on the Magnetic heading, I need to calculate the next position of the robot based on some world frame. How can I do this?
",mobile-robot
POM gears and metal fittings,"I'm looking at this setup: 

where the POM bevel gears are fitted with some kind of metal (bronze?) tube inside which fits over the shaft. What benefits does this method provide? Is it to allow the shaft to free-spin? The metal fitting wouldn't be able to grip on the shaft - right?
Is it supposed to be a canonical approach to fitting a POM gear for (relatively) high-load applications?
","motor, mechanism, gearing"
Moving a small plate back and forth,"I'm hoping to move a plate (3MM x 45MM) back and forth using a DC motor. Here's my idea so far:

The motor drives a threaded shaft which is attached on one side of the plate. To help alignment, a rod is added to the other side of the plate (red). My guess is that if it's just a rod in a through hole, it could potentially jam.
AFAIK, usually, in bigger setups, a linear bearing would come in handy. However, given that the plate is just 3MM thick, are there better ways to help alignment? Could making the edge around the through hole like the inside of a donut help? Something like

Is it easy to make? In fact, is my concern actually valid?
Thanks
EDIT The centre area of the plate needs to be kept clear. This is intended to be part of a (~10MM thick) pole climber, where several guide rollers are fitted on the left side of the plate and a motor driven roller is on the left of the part (not depicted). So the idea is the press the guide roller against the pole until the two rollers have a good grip on the pole. The whole car is fairly light, so the force expected is around 30N.
Here's a more complete depiction:

The rollers are spring loaded, but they need to be released and retracted - and adjusted for different pole widths.
","motor, linear-bearing, tracks"
Suitable gear construction for a robotic extender - plastic?,"I have a rather simple setup for my robotic extender. The DC motor turns a shaft with a worm on it. The worm connects to a small worm gear (green) which itself has a small gear (red) on the same shaft connecting to a large gear (blue):

The DC motor's gearbox gives around 50-100rpm and has a stall torque of around 2kg-cm. The small gear should be around 15MM tall and the large gear should be around 45MM tall.
If the load (on the large gear) has a maximum torque of 5kg-cm and a typical torque of 3kg-cm, could the two gears be made from plastic and be of module (metric form for pitch, as @Chuck pointed out) 0.5? Is a higher module needed? How about the worm, could it be made of plastic (nylon?)?
Any help will be appreciated.
EDIT Fixed typos and updated diagram.
","motor, torque, gearing"
Ensemble Kalman Filter SLAM,"I know that there is an extended kalman filter approach to simultaneous localization and mapping.  I'm curious if there is a SLAM algorithm that exploits the ensemble kalman filter.  A citation would be great, if at all possible.
","kalman-filter, slam, reference-request"
What is the difference between path planning and motion planning?,"What are the main differences between motion planning and path planning?
Imagine that the objective of the algorithm is to find a path between the humanoid soccer playing robot and the ball which should be as short as possible and yet satisfying the specified safety in the path in terms of the distance from the obstacles. 
Which is the better terminology? motion planning or path planning?
","mobile-robot, motion-planning, humanoid"
Sending commands from Ubuntu,"I have a iRobot Create model 4400 and I need to send commands to the open interface through Ubuntu. I'm using gtkterm at 57600 baud but when I press play button, it only drives around itself. I have tried to send commands as raw data and as hexadecimal data but it doesn't work.
What am I doing wrong?
","irobot-create, roomba, linux"
What is the difference between Task-Level and Joint-Level Control Systems?,"While doing a literature review of mobile robots in general and mobile hexapods in particular I came across a control system defined as ""Task level open loop"" and ""Joint level closed loop"" system.

The present prototype robot has no external sensors by
  which its body state may be estimated. Thus, in our simulations and experiments, we have used joint space closed
  loop (“proprioceptive”) but task space open loop control
  strategies.

The relevant paper is A simple and highly mobile hexapod
What is the meaning of the terms ""joint-level"" and ""task-level"" in the context of the Rhex hexapod?
","mobile-robot, control, walking-robot, hexapod"
PID Conundrums for Legged Robots,"I am currently working on a legged hexapod which moves around using a tripod gait. I have two sets of code to control the tripod. 
Set 1: Time based control
In this code set, I set the tripod motor set to move at their rated rpm for a required amount of time before shifting to the other tripod motor set.
PID control would be based on counting the number of transitions using an optical speed encoder, Calculating the error based on difference between actual speed and required speed and then adjusting the error with fixed Kd and Ki values.
Set 2: Transitions based control
In this code set I count to the number of transitions required to complete one rotation of the leg(tripod motor set) before starting the other leg(tripod motor set).
PID control would be time based. Calculation of error would be the difference in time taken for individual motors of the motor set.
Query:
The set 2 shows promising results even without PID control, but the first set does not.Why so? The motors are basically set to move 1 rotation before the other set moves. 
Would the speed differences between the motors cause it to destabilize?
How often do I update the PID loop?
My robot seems to drag a little bit. How do I solve this?
","mobile-robot, pid, legged, walking-robot, hexapod"
Servo motor considerations for a quadruped,"I'm building a quadruped and I'm not sure of the features I should be looking for in a servo motor. e.g. digital vs analog, signal vs dual bearings. Some of the ones I'm considering are here
","rcservo, walking-robot"
Servos power supply in Quadruped Robot,"I'm facing a problem while building my quadruped robot which is figuring out the efficient power supply needed for the 12 servos. I'm using 12 MG995 tower pro servos powered by 2 lithium batteries 2x3.7v (about 8 volts) with 2200 mA . I really don't know if that enough for the servos or something else is needed to be added(i hardly fitted the 2 batteries into the robot's body) 
any suggestions please?
","power, battery, servomotor, walking-robot"
Learning Algorithms for Walking Quadruped,"I'm building a 4 legged robot (quadruped) with 3 Degrees of freedom per leg.
The goal of my project is to make this robot able to learn how to walk.
What learning algorithms will I need to implement for it to work?
I'm using an Arduino Uno for the microcontroller.
","arduino, microcontroller, machine-learning, walking-robot"
"Roomba schedule opcode: 167, byte 1","Just a short question: The iRobot Create 2 Open Interface spec says:
Serial sequence: [167] [Days] [Sun Hour] [Sun Minute] [Mon Hour] etc.
Can somebody explain to me, what ""Days"" stands for?
",irobot-create
What are the reasons for not having autonomous robots in our daily activities?,"The fact is that the more I search the less I find autonomous (real) robots in use. The companion robots are all toys with limited useless functionality. Whenever there is a natural disaster you don’t see operational search and rescue robots in the news. Even military robots in service are all remotely controlled machines. They are not intelligent machines. Industrial robotic arms are deterministic machines. The only robots with some levels of autonomous functionality are cleaning bots, warehouse operations bots and farming robots.
On the other hand, today:

the artificial intelligence algorithms are very good in making decisions 
the sensing technologies are very sophisticated
the communication technologies are very fast
we can manufacture cheap parts
people are extremely gadget savvy

So, why there is no real robot in our day to day life? No investment in the domain? No market yet? Not enough knowledge in the domain? A missing technology? Any idea?
",mobile-robot
I want to control a sewing machine motor; need help with choices,"I've got an industrial sewing machine (think ""can sew with thread that looks like string, and has no trouble pounding through 20 layers of Sunbrella fabric""); it's got a 1 HP motor to power it. (I've got a smaller machine as well, w/ a 1/2 or 3/4 HP motor, which I might work on first.) The motor is a ""clutch motor"" which is always on, and a foot-pedal engages a clutch, so unless you ""slip the clutch"", you're basically always sewing fast or stopped. I'd like better control. In particular, I'd like to 

Be able to stop with the needle ""up""
Be able to stop with the needle ""buried"" (i.e., most of the way down)
Be able to press a button to move forward exactly one stitch
Be able to adjust -- probably with a knob -- the top speed of the motor
Have the motor off when I'm not actually sewing

The 1 HP motor is probably overkill for what I'm doing. I don't suppose I've ever used more than about 1/4 HP even on the toughest jobs. 
I'd appreciate any comments on my thinking so far: 
From what I've read, it looks as if a DC motor is the way to go (max torque at zero speed, which is nice for that initial ""punch through the material"" thing, and the ability to ""brake"" by shorting the + and - terminals). Brushless would be nice...but expensive. And I have a nice DC treadmill motor, and if I drive it at about 12-24V, it'll give me more or less the right speed; adjusting pulleys will do the rest. Such DC motors are powered (in electric lawnmowers, for instance) by running AC through a diode bridge rectifier to produce pulsating DC, and I've got such a bridge rectifier handy. I also have an old autotransformer that I can use to get 24VAC pretty easily. Thus I can get 24V pulsating DC to drive the thing ... but that may or may not be a good idea. 
I've also got an Arduino and the skills to program it, and several years of electronics tinkering, and some RC experience...but no experience handling larger DC motors like this one. I've been told the magic words ""H-bridge"", and found this motor driver which certainly seems as if it'll allow me to turn on/off the motor, and regulate the voltage going to the motor. I don't know whether, when presented with pulsating DC at the input, it'll still work. Any thoughts on this? 
I also can't tell -- there doesn't seem to be a handy datasheet or instruction page -- whether this thing can do braking. 
For position sensing, there are lots of places I can get information -- either from the needle baror the handwheel of the sewing machine, so I'm not too concerned about that part. To give a sense of the problem, a typical stitching speed is something like 1000 stiches per minute, so if I'm just sensing whether the needle is in the top 1/4 of its travels or the bottom quarter, we're talking about something on the order of 10-50Hz, which doesn't sound like a challenging control scenario. 
I guess my questions are these:

Will pulsating DC work with a controller like the one I've cited? 
Would I be better off with an RC motor-controller? I can't seem to find one designed for the 24V range that can handle 50 amps, unless it's for a brushless motor, which I don't have. And I think that I want one that has braking ability as well. And I worry that with an RC controller, the software in the controller may prevent me from making the motor stop at a particular position. 

Any comments/suggestions appreciated. (BTW, I'm happy with mathematics and with reading data sheets, and I've read (a few years ago) about half of ""The Art of Electronics"", just so you have an idea of the level of answer that's appropriate.) 
To answer @jwpat's questions:

I got my voltage value by saying that the motor is rated for (I think) 130V, and is 2.5HP (yikes), but turns at something like 6700 RPM. (Here's one that looks just like mine).  Dividing by 5 or 6, I got ""about 24 V"" to give me about 1400 RPM. (I'm at the office; the motor's at home, so I can't tell you the exact part number, alas.) I honestly don't think that the no-load vs load condition is a big deal, because I can wangle a factor of 2 with pulleys. 
The sewing machine is a Juki 562 
Current motor/clutch are similar to this

Sorry for the lack of detail here, 
","motor, control, power"
Panasonic MSMO22F2G (stepper motor hook-up),"I have a Panasonic MSMO22F2G Servo Motor that I'm using as a stepper motor. The motor has 4 wires coming out of a port farther to the front, and 11 wires coming out of a port towards the rear of the unit (presumed to be the encoder). I am trying to drive the motor with an arduino motor shield. 
My question is this, how do i hook it up?
I have read that a stepper with 4 leads is a bipolar stepper, and that you group same coil wires together. I found that 3 wires on my stepper were on the same coil and the fourth seems to have no effect on the stepper. My checking process was using an ohm meter to see what was connected to what, as well as connecting wires and feeling the resistance.
","arduino, stepper-motor"
Completely autonomous traversal of a planar graph,"I have to program an autonomous robot to traverse through a grid given in the following figure.
But the main problem is that the nodes to visit is not known beforehand, it will be received by the bot in real time.
E.g.- after reaching the node 19, the bot now has to go to node 6. The shortest path(19-17-7-6) can be calculated by Dijkstra algo but i don't know how to make the robot traverse that path.
Any idea ?

Edit: Sorry for not making the question clear enough.
I am facing the problem in determining the current position and the direction the robot is facing so i can't define the set of commands (turn left/right/forward) to traverse to the next desired node.
I am thinking about an extra array for previously visited nodes and the current node and an extra variable for facing direction.
But for that i will have to define command sets for each node from each and every node.
Any better idea ?
","mobile-robot, automatic, dynamic-programming"
Torque / Current control for BLDC motors,"I am working on a robotic application, and I want to control the torque (or current) of brushless DC motors. There are many BLDC speed controllers but I could not find anything related to torque or current. 
Instead of continuously spinning, the motor is actuating a robotic joint, which means I need to control the torque at steady-state, or low-speed, finite rotation. 
I am looking for a low-cost, low weight solution, similar to what Texas Instruments DRV8833C Dual 
H-Bridge Motor Drivers does for brushed DC motors.
",brushless-motor
Apps for Pepper robot,"I recently googled about Pepper robot and I wonder how one could write apps for it and get money for them. As far as I know they have app store, but does it sell apps or give them for free? (All info I googled myself is rather incomplete and old - probably outdated)
Also I believe that apps for such (or similar) robots is the potential multibillion-dollar market. What do you think about that?
",mobile-robot
Could a motor shaft be swapped for a threaded shaft?,"I'm looking at the N20 DC motor which is fairly popular. Does anyone know if the shaft could be swapped out for a threaded shaft?
",motor
FastSlam 2.0 Implementation?,"I have studied Claus Brenner's lectures on how to implement the FastSLAM 1.0 algorithm, where each particle maintains the robot pose, and maintains EKF's of the landmarks.
However, I'd like to implement FastSlam 2.0. Which I understand uses particle filters completely. Is this the same as FS 1.0, but instead of each particle maintaining an EKF of the landmark, each particle maintains yet another array of particle filters for landmarks?
",slam
Bulding a robot arm for neural networks understanding,"I am thinking about building a small robotic arm with 4 small servo motors and an arduino uno to apply basic neural networks concepts.
Is it  a good idea to use a hand made robotic arm to learn more the power of neural networks?
Thank your for your time and Merry Christmas
",control
Which algorithms are used in autonomous robot,"I am working on proposal of autonomous fire fight robots but I'm little bit confused about its sensor and algorithms. My friend suggested there are some path finding AI algorithms like BFS, DFS, A*, and Dijkstra's Algorithm which are used in robots, but I didn't believe it. 
I want to ask: Are these algorithms used in real world robots or some other genetic algorithms? How does a robot discover path to detect, and differentiate, a human from fire?  I only want some explanation that gives knowledge.
","algorithm, machine-learning"
Automatic activation of a spray can,"I would like to wirelessly control the button on a spray can such that on pressing it the spray comes. For example, a deodorant bottle.
What is the best way to do this ?
What thing can I mount on the bottle to do this?
",robotic-arm
Is it possible to achieve fully autonomous route following using PX4FMU module?,"I have a quadcopter equipped with PX4FMU board. You may download its datasheet from HERE.
I wonder whether it is possible to program the quadcopter to autonomously follow a path like circular motion without any human interference. Are the built-in sensors enough for this task?
I also wonder how accurate the built-in GPS is? I read that it gives coordinates with a radius of 5m as error.
",quadcopter
How do I go about implementing a Kalman Filter for a pose estimation algorithm?,"I am currently in the process of writing a pose estimation algorithm using image data. I receive images at 30 fps, and for every image, my program computes the x,y,z and roll, pitch, yaw of the camera with respect to a certain origin. This is by no means very accurate, there are obvious problems such as too much exposure in the image, not enough feature points in the image, etc., and the positions go haywire every once in a while; so I want to write a Kalman filter that can take care of this part.  
I have read through the basics of KF, EKF etc. and then I was reading through an OpenCV tutorial that has an implementation of a Kalman Filter inside an algorithm for the pose estimation of an object. While this matches my use case very well, I don't understand why they are using a linear Kalman Filter while explicitly specifying parameters like (dt*dt) in the state transition matrix. For reference, the state transition matrix they are considering is 
             /* DYNAMIC MODEL */
//  [1 0 0 dt  0  0 dt2   0   0 0 0 0  0  0  0   0   0   0]
//  [0 1 0  0 dt  0   0 dt2   0 0 0 0  0  0  0   0   0   0]
//  [0 0 1  0  0 dt   0   0 dt2 0 0 0  0  0  0   0   0   0]
//  [0 0 0  1  0  0  dt   0   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  1  0   0  dt   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  1   0   0  dt 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   1   0   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   1   0 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   0   1 0 0 0  0  0  0   0   0   0]
//  [0 0 0  0  0  0   0   0   0 1 0 0 dt  0  0 dt2   0   0]
//  [0 0 0  0  0  0   0   0   0 0 1 0  0 dt  0   0 dt2   0]
//  [0 0 0  0  0  0   0   0   0 0 0 1  0  0 dt   0   0 dt2]
 /  [0 0 0  0  0  0   0   0   0 0 0 0  1  0  0  dt   0   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  1  0   0  dt   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  1   0   0  dt]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   1   0   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   1   0]
//  [0 0 0  0  0  0   0   0   0 0 0 0  0  0  0   0   0   1]

I'm a little confused, so my main question can be broken down into three parts:

Would a linear Kalman Filter suffice for a 6DOF pose estimation filtering? Or should I go for an EKF?
How do I come up with the ""model"" of the system? The camera is not really obeying any trajectory, the whole point of the pose estimation is to track the position and rotation even through noisy movements. I don't understand how they came up with that matrix.
Can the Kalman Filter understand that, for instance, if the pose estimation says my camera has moved half a meter between one frame and other, that's plain wrong, because at 1/30th of a second, there's no way that could happen?

Thank you!
","kalman-filter, pose"
Camera Calibration fails to run on ROS,"I am running ROS Indigo on Ubuntu 14.04. I am doing a mono-camera calibration and trying to follow the camera calibration tutorial on the ROS Wiki.
I give the following command:

rosrun camera_calibration cameracalibrator.py --size 8x6 --square
  0.108 image:=/my_camera/image camera:=/my_camera

I get the following error:

ImportError: numpy.core.multiarray failed to import Traceback (most
  recent call last): File
  ""/opt/ros/indigo/lib/camera_calibration/cameracalibrator.py"", line 47,
  in  import cv2 ImportError: numpy.core.multiarray failed to
  import

I thought it was to do with updating numpy and did a rosdep update but no difference.
What is a possible way to solve this problem?
UPDATE:
I uninstalled and reinstalled ROS completely from scratch. I still get the same error. Should I have to look somewhere outside ROS?
","ros, cameras, calibration"
Phantom Omni type robot Inverse kinematics solution,"Guys my robot looks like this,
Thanks to @croco I came to know that it looks much similar to Phantom Omni. Since it looks similar to the phantom Omni I am trying to get the inverse kinematics geometric solution for it. Using this inverse kinematics solution I can build my FPGA design. There is a very good research paper on this
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6318365&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6318365
but I do not understand in page 2122 how they find the L3 and L4 (shown in the image below). If I find this for my robot then my project is almost done. How do I find L3 and L4 for my robot ? Should I just bring my end effector to the (0,0,0) position as shown in Fig2  amd then measure the L3 and L4, would this work ?? Would be great if you guys could help. Cheers.

","inverse-kinematics, geometry"
Quadrocopter first build: how to tell if components play well together?,"I'm building my first quadrocopter, and I'm trying to come up with a parts list that is suitable for a first build. 
I will use this to learn how to fly a quadrocopter manually (lots of crashes!), and to do some experiments with running AI for piloting it.
A couple questions about the below list of parts:

Is this a good choice for a first build? Are we missing any crucial parts?
Do these components work together? Is this battery strong enough to fuel all the components that need power?

Here's the current list of parts:

Frame - 450 mm
Propellers - 10x4.5"", two pairs
Motor (4x) - 900kv brushless outrunner motor; max current: 18A; ESC: 25-30A; cell count:3s-4s lipoly
Electronic speed controllers (4x) - 20A constant current, 25A burst current; battery: 2-4S lipoly
Battery - 3300mAh lipoly, 11.1v, 3 cell; constant discharge: 30C, peak discharge: 40C. charge plug: JST-XH. 3300 mAh x 30C = 99 amps?
Charger - lipoly, 50W, 6A, 12v power supply
Power supply - input: AC 100-240v 50/60Hz; output: DC15v 5A
Arduino board
Gyroscope for arduino
Accelerometer for arduino
GPS sensor for arduino?
RC transmitter for arduino
RC controller

","arduino, motor, sensors, quadcopter"
DC motor shaft and gear installation,"I'm hoping to use a DC motor to drive a cog bar horizontally along a track. AFAIK, I'd need to install a (plastic) cog on the motor shaft, which itself grips on the (plastic) cog bar. Does anyone know how to prevent the cog from shifting on the shaft? The shaft is 10mm long and I'd like to make sure the cog cog sits at 5mm, where the cog bar is.
Any help will be appreciated.
Thanks
",motor
3 degrees of freedom analytical solution,"
I have got a robot that exactly looks like as shown in the figure above. I have worked out the inverse kinematics analytical solution without the base rotation (considering 2 dof alone) but I am not able to find the analytical solution including the base(3 dof). How do I find the anlytical solution for this robot ?? 
",robotic-arm
"How to have a 'Auto Go Home' feature, like the DJI Phantom 3, on a project built quadcopter?","What should a quadcopter have, or have access to, in order to make this 'return home' feature work? Is GPS enough? What is the approach needed to make this happen?
I used a Arduino Mega 2560 with IMU to stable my quadcopter.
","arduino, quadcopter, sensors, localization, gps"
BiRRT: Getting path from an array of 7 DOF angle configurations,"I've kind of finished implementing a BiRRT for a 7 DOF arm, using a KD-tree from numpy.spatial in order to get nearest queries. A picture is below:

I'm currently having trouble with the fact that it is impossible to retrieve a path from the start node to a particular node using a KD-tree, and while I do have an array of of all the nodes, and there are edges that can be calculated between subsets of the array, but the edges are not in any useful order. Can anyone give me some tips on how I'd retrieve a path from the starting node in the first array, and the ending node in the second array? Are there any useful data structures that would let me do this? Below is my code
def makeLine(distance, q_near, xrand, nodes):
    num = int((distance)/0.01)

    for i in range(1, num+1):
        qnext = (xrand - q_near)/distance * 0.01 * i + q_near
        #check for collision at qnext, if no collision detected:
            nodes = numpy.append(nodes, qnext)
        #else if there is collision, return 0, nodes, ((xrand - q_near)/distance*0.01*(i-1)+q_near
    return 1, nodes, qnext

def BIRRT(start, goal):

    startNode = numpy.array([start])
    goalNode = numpy.array([goal])
    limits = numpy.array([[-2.461, .890],[-2.147,1.047],[-3.028,3.028],[-.052,2.618],[-3.059,3.059],[-1.571,2.094],[-3.059,3.059]])
    for i in range(1, 10000):
        xrand = numpy.array([])
        for k in range(0, len(limits)):
            xrand = numpy.append(xrand, random.uniform(limits[k,:][0], limits[k,:][1]))

        kdTree = scipy.spatial.KDTree(startNode[:, 0:7])
        distance, index = kdTree.query(xrand)
        q_near = kdTree.data[index]     


        success, startNode, qFinal = makeLine(distance, q_near, xrand, startNode)

        kdTree2 = scipy.spatial.KDTree(goalNode[:, 0:7])
        distance2, index2 = kdTree2.query(qFinal)
        q_near2 = kdTree2.data[index2]

        success, startNode, qFinal2 = makeLine(distance2, qFinal, q_near2, startNode)

        if success:
            return 1, startNode, goalNode, 1, qFinal, qFinal2

        xrand = numpy.array([])
        for k in range(0, len(limits)):
            xrand = numpy.append(xrand, random.uniform(limits[k,:][0], limits[k,:][1]))

        kdTree = scipy.spatial.KDTree(goalNode[:, 0:7])
        distance, index = kdTree.query(xrand)
        q_near = kdTree.data[index]     

        success, goalNode, qFinal = makeLine(distance, q_near, xrand, goalNode)

        kdTree2 = scipy.spatial.KDTree(startNode[:, 0:7])
        distance2, index2 = kdTree2.query(qFinal)
        q_near2 = kdTree2.data[index2]

        success, goalNode, qFinal2 = makeLine(distance2, qFinal, q_near2, goalNode)

        if success:
            return 1, startNode, goalNode, 2, qFinal, qFinal2
    return 0

","inverse-kinematics, motion-planning, planning, rrt"
Problems about Complementary Filter IMU tuning,"I'm developing a project consists of an IMU controlled by Arduino through which you can send via a radio module, the data to the PC of the three Euler angles and raw data from the sensors.
For filtering I used the code made available by SparkFun: Razor AHRS 9 dof
https://github.com/ptrbrtz/razor-9dof-ahrs/tree/master/Arduino/Razor_AHRS
The code does not provide radio transmissions and is tuned for 50 Hz sampling rate, in fact its parameters are:
// DCM parameters
#define Kp_ROLLPITCH (0.02f)
#define Ki_ROLLPITCH (0.00002f)
#define Kp_YAW (1.2f)
#define Ki_YAW (0.00002f)

in this project data is read every 20ms (50Hz) and records of the sensors are set to the accelerometer odr 50hz and 25 bandwidth. with the gyroscope 50 Hz odr.
In my project I used a gyroscope different, namely that I used L3G4200D frequency odr starting at 100Hz, I set then registers with the 100Hz. My global data rate is 33Hz max, beacouse the use of a radio, i read the complete date with a frequency of 33Hz.
How can i tune the Ki and Kp of my setup? the Kp is the period, I have to consider the frequency odr that I set to register in the individual sensors or i have to set the global system sample rate limited to 33Hz by the radio transmission?
","arduino, imu, gyroscope, sensor-fusion"
Weird magnetometer values,"I bought a 3-axis magnetometer (Similar to this one ) And plugged into an Arduino in order to read the heading value.
I mounted it on my robot and I drove with the robot for around 30 meters, turned 180 degrees and drove back to my starting position. I plotted the heading value and it shows inconsistent values. The 180 degrees turn started at sec 55, the rest is driving in one direction using a joystick and following a wall as reference so small deviations are expected but not that big as in the image.

When the robot is turning in-place, there is no such problem and the heading variation follows the rotation of the robot. The robot (Turtlebot) is a little bit shaky such that the magnetometer doesn't always have the x and y axes parallel to the floor but I don't think few degrees of offset can cause such a huge difference. I calculate the heading as follows:

heading = atan2(y field intensity, x field intensity)

Why does this happen? Could it be form some metals or electric wires under the floor?
Can you suggest a more robust method/sensor for estimating the heading in indoor environments?
EDIT:
I drove the same path again and the pattern similarity is making it even weirder

","mobile-robot, navigation, magnetometer"
Where to make changes for simulation project Bullet/Gazebo/ROS/Orocos,"I am starting to develop robotics project which involves simulation (and maybe real world programs) of soft-body dynamics (for food processing) and clothes/garment handling (for textile industry or home service robots). It is known that soft-body dynamics and garment handling are two less explored areas of robotics and simulation, therefore I hope to make some development of (contribution to) projects that are involved. The following projects are involed:

Bullet physics engine - for dynamics
Gazebo - simulation environment
ROS - robot OS, I hope to use Universal Robot UR5 or UR10 arms and some grippers (not decided yet)
Orocos - for control algorithms

Initially I hope use ""ROS INDIGO IGLOO PREINSTALLED VIRTUAL MACHINE"" (from nootrix.com), but apparently I will have to make updates to the Bullet, Gazeboo, add new ROS stacks and so on.
The question is - how to organize such project? E.g. If I am updating Bullet physics engine with the new soft-body dynamics algorithm then what executable (so) files should I produce and where to put them into virtual machine? The similar question can be asked if I need to update Gazebo. 
There seems to be incredibly large number of files. Is it right to change only some of them.
Sorry about such questions, but the sofware stack seems to be more complex than the robotics itself.
","ros, software, programming-languages, dynamics, gazebo"
Using another device instead of RC transmitter,"I want to make pc controlled quadrotor. All the tutorials/projects made with rc receiver. I want to use arduino or xbee instead of rc receiver for pc control of quadrotor. How can I do this. 
Note: I have arduino, beaglebone, xbee, hc-05, KK2 and multiwii parts. 
","pid, quadcopter"
Microcontroller flashing itself,"Can a micro controller flash itself?
What i mean to say is, I have an STM32F103RG with 1Mb Flash Size. 
I have a UART Communication modem connected to it. Can i send a firmware (.HEX or .BIN) to the microcontroller via the radio verify checksums, on sucess the microcontroller saves the file into a SD Card ( via SPI ) and then restarts itself and start flashing itself reading from the file ( in the sD card )?
Can something like this be done or an external MCU is required to carry out the flashing?
The purpose is the microcontroller and radio will be sitting at a remote location and i need a way to change the microcontroller's firmware by sending it a firmware update file remotely. 
",microcontroller
What are human-friendly terms for mobile-robot orientation and relative direction of non-robot objects?,"Within robotics programming, orientation is primarily given in terms of x, y, & z coordinates, from some central location.  However x, y, z coordinates aren't convenient for rapid human understanding if there are many locations from which to select (e.g., {23, 34, 45}, {34, 23, 45}, {34, 32, 45}, {23, 43, 45} is not particularly human friendly, and is highly prone to human error).  Yet more common English orientation descriptors are frequently either too wordy or too imprecise for rapid selection (e.g.,  ""front-facing camera on robot 1's right front shoulder"" is too wordy; but ""front""/ ""forward"" is too imprecise - is the camera on the leading edge or is it pointing forward?)
In the naval and aeronautical fields vehicle locations are generically talked about as fore, aft (or stern), port, and starboard. While, direction of movement that is relative to the vehicle is frequently given in reference to a clockface (e.g., forward of the the fore would be ""at 12"", rear of the aft would be ""at 6"", while right of starboard and left of port would be ""at 3"" and ""at 9"", respectively).  This language supports rapid human communication that is more precise than terms such as ""front"" and ""forward"".  Are there equivalent terms within mobile robotics?
","mobile-robot, control, design, localization, navigation"
How do I if know my quad is powerful enough to take off vertically smoothly and how do I calculate max Thrust(g) for given current(A) and voltage(V)?,"If I build my quadcopter with the following components, will it take off vertically smoothly?
Frame: F450 glass fiber and polyamide nylon [280g]
Landing Gear: High Landing Gear for F450 [90g]
Battery: 3000mAh 20C (30C burst) 3s/1p [260g]
Motor: XXD A2212 1400KV brushless outrunner motor [4x50g]
ESC: 30A (40A burst) Brushless with BEC - 2A/5V [4x25g]
Prop: 1045 Propeller set

Assume that the total weight of the quad is ~1.1Kg and I would like to have payload upto 400g, making the quad weigh ~1.5Kg

From all that I've learnt, the Thrust:Weight ratio should never fall below 1.7:1 (and a 2:1 is recommended to have better control) which if does creates problems in lifting my quad vertically smoothly. I'm neither planning to have very high maneuverability nor cruise the sky pushing my quad's limits. I just want it to fly.

Here's the motors pull(g)-amps(A)-voltage(V)-power(W)-specificThrust(g/W) information for A2212 1400kv with 10 x 4.5 Props


Since my battery's discharge rate is limited to 20C, if I'm not wrong, it can give out only 60A (since its 3000mAh) for the entire circuit with four motors, each of which can take 15A max, which from the above table would produce ~600g thrust, which is 2400g from all four motors.

Does this mean I can get 1.6:1 Thrust:Weight at 100% throttle? Can my quad still take off vertically smoothly?

I'm also confused about the efficacy of my motor, meaning if I buy a bigger motor, can get more than 600g thrust at 15A/11.1V with same/bigger props? If not, is this the most efficient combination of motor and prop? 

If yes, what is the maximum thrust (practical pulling force in g, not in N) I can get outta 15A/11.1V? Which equation tells defines that exact relation, provided I fly my quad at usual conditions (1013hPa/25degC extTemp/80%-90% relHumidity/max15mAlti)?

PS: I tried ecalc, Prop Power Thrust and Efficiency Calculations, and a few other online stuff.
Update: Will replacing the 3000mAh-20c/30c battery with the 5000mAh-20c/30c 3s1p keeping everything same solve some problems and increase the flight time, provided I keep everything else the same?
","quadcopter, brushless-motor, battery, lithium-polymer"
How to convert rotation matrix in to equivalent Quaternion using Eigen Library.,"Eigen library (http://goo.gl/cV5LY), which is used extensively in ROS and PCL.
Thank you.
",ros
Kinematics of a 4 wheeled differential drive robots,"I have a 4 wheeled differential drive robot, like the Pioneer 3-AT. There are only two motors, one for left wheels and one for right wheels.
I want to send velocity commands to the robot, I'm using ROS and the standard commands are: [linear_velocity, angular_velocity].
I need to convert them into left and right velocities, from literature if I had 2 wheels I should do this:
$v_l = linear_v - \omega * |r|$
$v_r = linear_v + \omega * |r|$
where |r| is the absolute value of the distance from the wheels to the robot ""center"".
How should I take into account that I have 4 wheels?
","wheeled-robot, inverse-kinematics, wheel"
Quadcopter application underwater,"Just out of curiosity, can the concept of a Quadcopter be applied to an ROV? Would it work the same way underwater as it would be in Air? If not what kind of modifications it would take to implement that idea, underwater?
",quadcopter
Discontinuity in device orientation,"Why is there a discontinuity in the quaternion representation of my device orientation?
I'm using a SENtral+PNI RM3100+ST LSM330 to track orientation. I performed the following test:

Place the device in the center of a horizontal rotating plate (""lazy susan"").
Pause for a few seconds.
Rotate the plate 360° clockwise.
Pause for a few seconds.
Rotate the plate 360° clockwise again.

I got this output, which appears discontinuous at sample #1288-1289.

Sample #1288 has  (Qx,Qy,Qz,Qw) = (0.5837, 0.8038, 0.0931, 0.0675), but sample #1289 has (Qx,Qy,Qz,Qw) = (0.7079, -0.6969, -0.0807, 0.0818).
Plugging in the formulas on page 32 of this document, this corresponds to a change in orientation from (Heading, Pitch, Roll) = (108°, 0°, 142°) to (Heading, Pitch, Roll) = (-89°, 0°, 83°).
The graph of (Heading, Pitch, Roll) is also not continuous mod 90°.
Does this output make sense? I did not expect a discontinuity in the first plot, since the unit quaternions are a covering space of SO(3). Is there a hardware problem, or am I interpreting the data incorrectly?
Edit: The sensor code is in central.c and main.c. It is read with this Python script.
","sensors, calibration, orientation"
Detect polyethylene,"First of all, I am in high school[to tell you that I am a newbie and lack knowledge]
What I want to achieve for now is a thing that can differentiate between poly bags[polyethylene] and other stuffs. Or a thing that could detect polyethylene.
I have to built a robot and therefore we have only a few method accessible.
Anyway any knowledge or suggestion or external links provided by you, about this topic would be welcomed by me.
","mobile-robot, sensors"
"Hector SLAM, Matching algorithm","I'm trying to understand the scan-matching part of Hector SLAM (PPT summary). It seems a little difficult to understand, in some cases, how is it possible to actually perform the alignment of the scans. Can anyone explain about it?
In my case, I'm working with a simulation. I'm moving my robot in a corridor-like featureless environment (only two walls) and I don't get a map. Nevertheless, if I move in a sinewave motion, I'm able to get a map. Moreover, if I have an additional feature, the algorithm even shows the real path as long as this feature is seen (right part of the image), otherwise it shows a very weird-looking oscillatory path which does not resemble a sinewave at all. Something important to notice is that the width of the map is pretty accurate (real=4m, map's=4.014m), and the length of the movement is also somehow accurate (real=15m, map's= 15.47). I'm using a Hokuyo URG-04LX laser range finder, no odometry, no IMU. I'm running in Ubuntu 14.04 and using ROS Indigo.

I more or less understand how Hector works, but I have no idea about why I'm getting this map and specially trajectory.
Thank you.
","localization, slam, ros, mapping, rangefinder"
Problem with HC-SR04 sensor,"I have three HC-SR04 sensors connected to a PIC18F4431 with the schematic provided below. Before building the PCB I've tried testing all three sensors with a bread board and they worked fine. But now I have my PCB and when I connect them to that and tried testing, they work only for a few seconds and then stop working.
I set timers and a set of LEDs to lit if an item is in between 40cm from each sensor. As I've tried from the bread board, when I cross my arm withing that range, the appropriate bulbs are lit and when I take my arm off the other bulbs are lit. But using the PCB, when I upload the code through a PICKIT2 they work fine for a few seconds and then they freeze. If I reset the MCLR pin it works again for another few seconds and freeze again. And sometimes randomly if I touch the receiving part of the sensor it works but that happens randomly. Not always working. What could be the issue? 
Is my oscillator burnt while I was soldering it? Once I connected two 0.33uF polar capacitors and found out that for one second, it takes one minute or more to blink a bulb.


",mobile-robot
How to use an IMU to hover at a fixed location in a quadcopter in the presence of gravity?,"There's an accelerometer in the IMU. The output can then be integrated to estimate the position, at least in theory.
But in practice, there's a huge acceleration from gravity, which varies rather randomly across locations. Vibrations etc can be filtered out with low-pass filters, but how do you filter out gravity? Is it simply the case that the vertical vector is ignored when doing any calculations?
My application is, I want to build a quadcopter that could hover in one place even in the presence of (reasonable) winds: the quadcopter ideally would tilt towards random gusts to maintain a certain position. Every single tutorial I could find on the Internet only uses the accelerometer to estimate where down is when stationary, and simply assumes that using the gyroscope to hold the quadcopter level is enough. 
I also want to use the IMU to estimate altitude if possible, of course as an input to something like a Kalman filter in conjunction with a sonar system.
Obviously, for my application GPS is far too slow.
","quadcopter, imu"
calculating position based on accelerometer data,"Please help me with the following task. I have MPU 9150 from which I get acceleration/gyro and magnetometer data. What I'm currently interested in is to get the orientation and position of the robot. I can get the position using quaternions. Its quite stable. Rarely changes when staying still.
But the problem is in converting accelerometer data to calculate the displacement.
As I know its required to to integrate twice the accel. data to get position.
Using quaternion I can rotate the vector of acceleration and then sum it's axises to get velocity then do the same again to get position. But it doesn't work that way. First of all moving the sensor to some position and then moving it back doesn't give me the same position as before. The problem is that after I put the sensor back and it stays without any movement the velocity doesn't change to zero though the acceleration data coming from sensors are zeros.
Here is an example (initially its like this):
the gravity: -0.10  -0.00   1.00
raw accel: -785 -28 8135
accel after scaling to +-g: -0.10   -0.00   0.99
the result after rotating accel vector using quaternion: 0.00   -0.00   -0.00
After moving the sensor and putting it back it's acceleration becomes as:
 0.00   -0.00   -0.01
 0.00   -0.00   -0.01
 0.00   -0.00   -0.00
 0.00   -0.00   -0.01
and so on.
If I'm integrating it then I get slowly increasing position of Z.
But the worst problem is that the velocity doesn't come back to zero
For example if I move sensor once and put it back the velocity will be at:
-0.089 for vx and
0.15 for vy
After several such movements it becomes:
-1.22 for vx
1.08 for vy 
-8.63 for vz
and after another such movement:
vx -1.43
vy 1.23
vz -9.7
The x and y doesnt change if sensor is not moving but Z is changing slowly.
Though the quaternion is not changing at all.
What should be the correct way to do that task?
Here is the part of code for integrations:
vX += wX * speed;
vY += wY * speed;
vZ += wZ * speed;

posX += vX * speed;
posY += vY * speed;
posZ += vZ * speed;

Currently set speed to 1 just to test how it works.
EDIT 1: Here is the code to retrieve quaternion and accel data, rotate and compensate gravity and get final accel data.
        // display initial world-frame acceleration, adjusted to remove gravity
        // and rotated based on known orientation from quaternion
        mpu.dmpGetQuaternion(&q, fifoBuffer);
        mpu.dmpGetAccel(&aaReal, fifoBuffer);
        mpu.dmpGetGravity(&gravity, &q);

        //Serial.print(""gravity\t"");
        Serial.print(gravity.x);
        Serial.print(""\t"");
        Serial.print(gravity.y);
        Serial.print(""\t"");
        Serial.print(gravity.z);
        Serial.print(""\t"");


        //Serial.print(""accell\t"");
        Serial.print(aaReal.x);
        Serial.print(""\t"");
        Serial.print(aaReal.y);
        Serial.print(""\t"");
        Serial.print(aaReal.z);
        Serial.print(""\t""); 

        float val = 4.0f;
        float ax = val * (float)aaReal.x / 32768.0f;
        float ay = val * (float)aaReal.y / 32768.0f;
        float az = val * (float)aaReal.z / 32768.0f; 

        theWorldF.x = ax;            
        theWorldF.y = ay;
        theWorldF.z = az;

        //Serial.print(""scaled_accel\t"");
        Serial.print(ax);
        Serial.print(""\t"");
        Serial.print(ay);
        Serial.print(""\t"");
        Serial.print(az);
        Serial.print(""\t""); 

        theWorldF.x -= gravity.x;
        theWorldF.y -= gravity.y;
        theWorldF.z -= gravity.z;

        theWorldF.rotate(&q);
        //gravity.rotate(&q);
        //Serial.print(""gravity_compensated_accel\t"");
        Serial.print(theWorldF.x);
        Serial.print(""\t"");
        Serial.print(theWorldF.y);
        Serial.print(""\t"");
        Serial.print(theWorldF.z);
        Serial.print(""\t"");
        Serial.print(deltaTime); 
        Serial.println();


EDIT 2:

dmpGetQuaternion, dmpGetAccel functions are just reading from the FIFO buffer of MPU. 
dmpGetGravity is:
uint8_t MPU6050::dmpGetGravity(VectorFloat *v, Quaternion *q) {
    v -> x = 2 * (q -> x*q -> z - q -> w*q -> y);
    v -> y = 2 * (q -> w*q -> x + q -> y*q -> z);
    v -> z = q -> w*q -> w - q -> x*q -> x - q -> y*q -> y + q -> z*q -> z;
    return 0;
}

EDIT 3:
the library for using MPU 9150:
https://github.com/sparkfun/MPU-9150_Breakout
EDIT 4: Another example 
gravity vector: -1.00   -0.02   0.02
raw accel data: -8459   -141    125 
accel data scaled (+-2g range): -1.03   -0.02   0.02
gravity compensation and rotation of accel data: -0.01  0.00    0.33    
","accelerometer, algorithm"
PixHawk or Naza M V2 for Aerial Imaging of Small Study Area with Hexacopter,"I have a project that is going to involve capturing ~80 images of a 35m x 50m agricultural study area for image processing.
I am wondering whether to use a NAZA v2 or PixHawk controller to outfit my ship (on a DJI F550 Flamewheel airframe).
My understanding is that Naza is limited as far as the amount of waypoints I can have during a particular mission. Can I break my imaging mission up into 8 or 10 sub-missions?
I also understand that PixHawk is in many ways superior but getting it up and running can be more finicky - I am on a limited time frame for this project.
","gps, uav, radio-control"
4dof or 5dof robot arm with stepper motors tool-chain for an hobbyist,"In the past I built some simple robot arms at home, using RC servo motors or stepper motors (till 3dof). I would like to build a new arm with 4dof or 5dof with the steppers. Until now I used Arduino and A4988 stepper drivers and Gcode.
For calculating inverse kinematics in real time for a 4dof or 5dof I think the Arduino is not enough powerful. So I'm searching for a new tool-chain Gcode Interpreter + inverse kinematics calculation + stepper controller.
I see LinuxCNC + beaglebone black + cnc cape. Not too expensive for an hobbyist.
But this is the only possibility I found. There are other possibilities for an hobbyist to implement a 4dof or 5dof robot arm working with the stepper motors?
","stepper-motor, arm"
How to perform odometry on an arduino for a differential wheeled robot?,"I am using a differential wheel robot for my project. I need to know the current coordinates of the robot with respect to it's initial position taken as the origin. I m doing the computation on an Arduino UNO and the only sensory input that I get is from the two encoders. I have the function updateOdomenty() called in the loop() and this is it's corresponding code:
void updateOdometry()
{

  static int encoderRPosPrev = 0;
  static int encoderLPosPrev = 0;


  float SR = distancePerCount * (encoderRPos - encoderRPosPrev);
  float SL = distancePerCount * (encoderLPos - encoderLPosPrev);

  encoderRPosPrev = encoderRPos;
  encoderLPosPrev = encoderLPos;


  x += SR * cos(theta);           
  y += SL * sin(theta);
  theta += (SR - SL) / wheelDistance;

  if(theta > 6.28)
    theta -= 6.28;
  else if(theta < -6.28)
    theta += 6.28;
}

This is the code that me and my team mates made after reading this paper. I am wondering if this is the best possible way to solve this problem with an Arduino. If not, how is odometry done in differential wheeled systems?
","arduino, kinematics, odometry, differential-drive"
Dynamic model of a tank like robot,"I am planning a tank like robot for hobby purpose. I have control engineering background, however I never applied on robotics. 
I would like to test different control theory, namely MPC. I saw a lot of publications regarding the kinematics and inverse kinematics of such a robot, however I am wondering if somebody can point out regarding the dynamics modelling of such a system,taking into account the forces, mass etc?
",wheeled-robot
Is there a way to use a stress-ball-like device as acceleration control interface,"I am thinking of a project proposal for my robotics course and we are required to make one that has a potential application on physical therapy or medical fields. One thing that came across my mind is a motorized wheelchair that moves when a stress ball control is squeezed by the user. As a robotics novice, I wonder if I could integrate a sensor circuit with a rubber ball so that when it is pressed, perhaps by a stroke patient, it triggers some driver circuit. is this possible? if so, how? My experience with robotics is limited to arduino, servo motors and basic sensors.
",wheeled-robot
Visual servoing - tracking a point,"I am trying resolve some issues i am having with some inverse kinematics. 
the robot arm i am using has a camera at the end of it, at which an object is being tracked. I can from the the camera frame retrieve a position, relative to that that frame but how do i convert that position in that frame, to an robot state, that set all the joint in a manner that  the camera keep the object at the center of the frame?...
-- My approach -- 
From my image analysis i retrieve a position of where the object i am tracking is positioned => (x,y) - coordinate.
I know at all the time the position (a) of the end tool by the T_base^tool - matrix, and from the image analysis i know the position (b) of the object relative to the camera frame for which i compute the difference as such c = b - a. 
I then compute the image jacobian, given the C, the distance to the object and the focal length of the camera. 
So... thats where i am at the moment.. I am not sure whether the position change retrieved from the cam frame will be seen as position of the tool point, at which  the equation will become un undetermined as the length of the state vector would become 7 instead of 6.  
The equation that i have must be 
$$J_{image}(q)dq = dp$$

J_image(q)[2x6]: being the image jacobian of the robot at current
state q 
dq[6x1]: wanted change in q-state 
dp[2x1]: computed positional change...

Solution would be found using linear least square.. 
but what i don't get is why the robot itself is not appearing the equation, which let me doubt my approach.. 
","inverse-kinematics, visual-servoing"
increase current draw from serial port of icreate 2,"This is from the icreate 2's document:
""Pins 1 and 2 (Vpwr) are connected to the Roomba battery through a 200 mA PTC resettable fuse. The continuous draw from these two pins together should not exceed 200 mA. Do not draw more than 500 mA peak from these pins, or the fuse will reset.""
My project just need to draw a bit more than that number. So is there anyway to disable - short circuit - that fuse or replace that fuse with a bigger one? where does that fuse reside on the bot's circuit board?
If the above 2 is not possible (because the fuse is embedded inside some chip or too difficult to access for example), is it safe to run a small wire from the battery pole to the pin to by pass that fuse?

I know I can run a wire directly from battery pole to my circuit or draw power from the motor wires, but I love running through the serial port with keep things simple.

","power, irobot-create, serial"
Is there a .NET library for conveyor belt automation?,"I'm a software developer and I work for a company that I think could use some automation in its warehouse. I thought it would be fun to put together a prototype of a conveyor system that automates a manual sorting process that we do on our warehouses. I'm primarily a .NET developer so I'm wondering if there is an .NET SDK for conveyor automation. 
Any other information on where to start would be helpful but is not my main question here. 
",automation
Robotic Manipulator,"I have started working on robotic manipulators and got into a project which deals with control of robotic manipulator using artificial neural networks (solution of inverse kinematics and trajectory generation, to be precise!).
Can someone please suggest me where to start as I have no prior knowledge about robotic manipulator and ANN and how to code them?
","arduino, robotic-arm, inverse-kinematics, machine-learning"
Finding cubic polynomial equation for 3 joints,"My professor gave us an assignment in which we have to find the cubic equation for a 3-DOF manipulator. The end effector is resting at A(1.5,1.5,1) and moves and stops at B(1,1,2) in 10 seconds. How would I go about this? Would I use the Jacobian matrix or would I use path planning and the coefficient matrix to solve my problem. I'm assuming coefficient matrix but I am not given the original position in angle form. I was only taught how to use path planing when the original angles are given.
","motion-planning, inverse-kinematics, motion, jacobian"
Color sensor alternatives,"I am making a white line follower. I am using an IR sensor module based on TCRT5000. I am directly taking the 8bit ADC reading from Arduino Uno and printing the values on serial monitor. I observe that the values for white are around 25-35, which is ok. The problem arises when I try detecting an Orange (158C) surface. The sensor gives me values very close to that of white which is around 25-40. 
I can use a color sensor but they are bulky and I am not sure how I can get readings faster with them since they take a finite time for sampling 'R','G' and 'B' pulses. Can someone please tell me an alternate approach to detecting the colours or any other possible solution to my problem.
EDIT: I would like to add that the line I wish to follow is 3cm in width. Hence I plan to use three sensors. Two just outside the line on either sides and one exactly at the centre. The sampling frequency of Arduino UNO is around 125KHz. Sampling IR is not an issue because it is quick but using a color sensor takes a lot of time.
","sensors, line-following"
Filtering angular velocity spikes of a cheap Gyroscope,"I would like to filter angular velocity data from a ""cheap"" gyroscope (60$). These values are used as an input of a nonlinear controller in a quadcopter application. I am not interested in removing the bias from the readings.
Edit:
I'm using a
 l2g4200d gyroscope connected via i2c with an Arduino uno. The following samples are acquired with the arduino, sent via serial and plotted using matlab.
When the sensor is steady, the plot shows several undesired spikes.

How can I filter these spikes?
1st approach: Spikes are attenuated but still present...
Let's consider the following samples in which a couple of fast rotations are performed. Let's assume that the frequency components of the ""fast movement"" are the ones I will deal with in the final application.

Below, the discrete Fourier transform of the signal in a normalized frequency scale and the second order ButterWorth low pass filter.

With this filter, the main components of the signal are preserved. 

Although the undesired spikes are attenuated by a factor of three the plot shows a slight phase shift...

And the spikes are still present. How can I improve this result?
Thanks.
EDIT 2:
1./2. I am using a breakout board from Sparkfun. You can find the circuit with the Arduino and the gyro in this post: Can you roll with a L3G4200D gyroscope, Arduino and Matlab? 
    I have added pullup resistors to the circuit. I would exclude this option because other sensors are connected via the i2c interface and they are working correctly.
    I haven't any decoupling capacitors installed near the integrated circuit of the gyro. The breakout board I'm using has them (0.1 uF). Please check the left side of the schematic below, maybe I am wrong.

Motors have a separate circuit and I have soldered all the components on a protoboard.

The gyro is in the quadcopter body but during the test the motors were turned off.
That is interesting. The sampling frequency used in the test was 200Hz.  Increasing the update freq from 200 to 400 hz doubled the glitching.

I found other comments on the web about the same breakout board and topic. Open the comments at the bottom of the page and Ctrl-F virtual1
","quadcopter, gyroscope, filter"
Is RoboCup still vivant and significant robotic issue?,"A couple years ago RoboCup competitions seems to be quite vivant issue. Now when I'm looking for some info about it, it seems to be some kind of insignificant but this may be only my first impression (I was looking for 2D simulator league and it seems that it does not even exist anymore).
So is RoboCup still alive and significant robotic issue?
",soccer
Calculating required torques for a given trajectory using Lagrange-Euler,"I have a 2DOF robot with 2 revolute joints, as shown in the diagram below. I'm trying to calculate (using MATLAB) the torque required to move it but my answers don't match up with what I'm expecting.

Denavit-Hartenberg parameters:
$$
\begin{array}{c|cccc}
joint & a & \alpha & d & \theta \\
\hline
1 & 0 & \pi/2 & 0 & \theta_1 \\
2 & 1 & 0 & 0 & \theta_2 \\
\end{array}
$$
I'm trying to calculate the torques required to produce a given acceleration, using the Euler-Lagrange techniques as described on pages 5/6 in this paper.
Particularly,
$$ T_i(inertial) = \sum_{j=0}^nD_{ij}\ddot q_i$$
where
$$ D_{ij} = \sum_{p=max(i,j)}^n Trace(U_{pj}J_pU_{pi}^T) $$
and
$$ 
J_i = \begin{bmatrix}
{(-I_{xx}+I_{yy}+I_{zz}) \over 2} & I_{xy} & I_{xz} & m_i\bar x_i \\
I_{xy} & {(I_{xx}-I_{yy}+I_{zz}) \over 2} & I_{yz} & m_i\bar y_i \\
I_{xz} & I_{yz} & {(I_{xx}+I_{yy}-I_{zz}) \over 2} & m_i\bar z_i \\
 m_i\bar x_i & m_i\bar y_i & m_i\bar z_i & m_i \end{bmatrix}
$$
As I was having trouble I've tried to create the simplest example that I'm still getting wrong. For this I'm attempting to calculate the inertial torque required to accelerate $\theta_1$ at a constant 1 ${rad\over s^2}$. As $\theta_2$ is constant at 0, I believe this should remove any gyroscopic/Coriolis forces. I've made link 1 weightless so its pseudo-inertia matrix is 0. I've calculated my pseudo-inertia matrix for link 2:
$$
I_{xx} = {mr^2 \over 2} = 0.0025\\ I_{yy} = I_{zz} = {ml^2 \over 3} = 2/3
$$
$$
J_2 =\begin{bmatrix}
1.3308 & 0 & 0 & -1 \\
0 & 0.0025 & 0 & 0 \\
0 & 0 & 0.0025 & 0 \\
-1 & 0 & 0 & 2 \\
\end{bmatrix}
$$
My expected torque for joint 1:
$$ 
T_1 = I\ddot \omega \\
T_1 = {ml^2 \over 3} \times \ddot \omega \\
T_1 = {2\times1\over3}\times1 \\
T_1= {2\over3}Nm
$$
The torque calculated by my code for joint 1:
q = [0 0];
qdd = [1 0];
T = calcT(q);
calc_inertial_torque(1, T, J, qdd) 

$$
T_1={4\over3}Nm
$$
So this is my problem, my code $T_1$ doesn't match up with my simple mechanics $T_1$.
The key functions called are shown below.
function inertial_torque_n = calc_inertial_torque(n, T, J, qdd)
    inertial_torque_n = 0;
    for j = 1:2
        Mnj = 0;
        joint_accel = qdd(j);
        for i = 1:2
            Uij = calcUij(T, i, j);
            Ji = J(:,:,i);
            Uin = calcUij(T, i, n);
            Mnj = Mnj + trace(Uin*Ji*transpose(Uij));
        end
        inertial_torque_n = inertial_torque_n + Mnj * joint_accel;
    end
end

function U=calcUij(T,i,j)  
    T(:,:,j) = derivative(T(:,:,j));

    U = eye(4,4); 
    for x = 1:i
        U = U*T(:,:,x);
    end
end

function T = derivative(T)
    dt_by_dtheta = [0 -1  0  0
                    1  0  0  0
                    0  0  0  0
                    0  0  0  0];

    T = dt_by_dtheta*T;
end

I realise this is a fairly simple robot, and a complicated process - but I'm hoping to scale it up to more DOF once I'm happy it works. 
","dynamics, matlab, torque"
How to program an NXT brick to always hit a ball?,"I am using a mindstorm robot with an NXT brick, using the graphical interface to create the program. Part of the course my robot will take includes a black line on a white background. At the end of the line there is a gap, and after the gap there is a semi circular line. There is a ball that the robot has to hit soon after the robot crosses the gap. 
The robot has a small code to follow the black line for a certain amount of time, enough time so that it stops just before the gap. The the robot just runs forward for 1 second across the gap, then the robot swings an arm to hit the ball, and after that I have the code for line-following again. However, once it crosses the gap, the robot stops in a different place every time, so the arm usually misses the ball. Is it possible to program the robot to hit the ball every time/almost every time in the GUI?   
Among other things I have tried using the ultrasonic sensor to detect the ball, but the sensor does not pick it up.
","nxt, mindstorms"
Developing a Quadrotor using ROS,"I
 suppose who know ROS and how it works (at least most of you)
I have some question regarding the implementation of a quadrotor in that
 framework.

3D movements: A quadrotor has 6DOF and moves in a 3D 
environment. Looking at the various ROS packages I could not find any 
package that allows to drive a ""robot"" in the 3D space. The package 
/move_base for instance allows only 2D. Make sense to use this 
package for such a project? I thought to use 2D navigation projecting 
the ""shadow"" of a quadrotor on the ground...
MoveIt: it seems a real interesting and promising package, but I
read that it is for robotic arms and not expressly indicate for 
quadrotor. Maybe one can use the possibility to create a virtual 
floating joints in MoveIt to let the quadrotor any movement in a 3D 
environment...that's ok, but I cannot understand whether is ""too much"" 
and not useful for a flying robot.
Trajectories: The possibility to create a 3D trajectory in the 
space seems to be not a standard package of ROS. I found Octomap 
which allows the creation of 3D maps from sensor datas. Very interesting
and for sure very useful. But...I don't think it could be useful for 
creating 3D trajectories.  Should I in that case create an extra package
to compute 3D trajectories to be feed into the quadrotor? Or there 
already something like that?

There is already an existing project hector_quadrotor which seems to 
acclaim a good success ans it is very considered in the field. Most 
people refer to that project when speaking or answering question 
regarding quadrotors in ROS. I saw many times that project...since 
weeks. And due to the total lack of documentation I didn't try anymore 
to understand how it works. Really too difficult. 
Another interesting project, ArDrone, has comments in the source 
code...in Russian!!! @_@
Could you me give any good suggestions? Or point me in the right direction 
please?
It would help me to understand how to focus my searches and which 
package I can/cannot use.
UPDATE: my goal is to let the quadrotor flying and using gmapping to localize itself. I've heard and read al lot of stuff about that but I found all this tutorials very hard to understand. I cannot get a global vision of the software and sometime I run in problems like: ""is there a package for this task, or should I invent it from scratch?""
Thanks!
","ros, quadcopter"
Manipulator end-effector orientation with quaternions,"I have the following problem:
Given 3 points on a surface, I have to adjust a manipulator end-effector (i.e. pen) on a Baxter Robot, normal to that surface.
From the three points I easily get the coordinate frame, as well as the normal vector. My question is now, how can I use those to tell the manipulator its supposed orientation.
The Baxter Inverse Kinematics solver takes a $(x,y,z)$-tuple of Cartesian coordinates for the desired position, as well as a $(x,y,z,w)$-quaternion for the desired orientation. What do I set the orientation to? My feeling would be to just use the normal vector $(n_1,n_2,n_3)$ and a $0$, or do I have to do some calculation?
","inverse-kinematics, orientation"
How would i go about learning to code a flight controller?,"I'm interested in quadcopters/multi-rotors and want to eventually code my own flight controller ala an APM and/or Pixhawk. I've got a little experience in programming (i.e i know about if/else/else if conditionals), and have done a little programming with PHP, though it was procedural code.
I currently have a quadcopter that i built/assembled myself that is running on a f450 frame, using a APM 2.6 flight controller,so i have a reasonable grasp of how a quad works, and i would like to take it a step further and make my own adjustments to the code base, with the eventual aim of coding my own flight controller.
I've had a look at the code base, but am still unable to get a grasp of what the code is actually doing....yet. How would i go about learning how to code a flight controller?
I'm thinking that i would have to learn C++ & OOP first, but how familiar/proficient would i have to be with C++ before i can reasonably attempt to edit the code base?Also, what else would i need to learn apart from C++ & OOP?I am looking at setting a 6 month timeframe/deadline for me to do this, would it be possible?
","arduino, quadcopter, microcontroller, multi-rotor"
Real-time camera localisation in known environment,"I am young researcher/developer coming from different (non-robotic) background and I did some research on camera localisation and I came to the point, where I can say that I am lost and I would need some of your help.
I have discovered that there is a lot of SLAM algorithms which are used for robots etc. As far as I know they are all in unknown environments. But my situation is different.
My problems and idea at the same time is:

I will be placed in an known room/indoor environment (dimensions would be known)
I would like to use handheld camera
I can use predefined landmarks if they would help. In my case, I can put some "" unique stickers"" on the walls at predefined positions if that would help in any way for faster localisation.
I would like to get my camera position (with its orientation etc) in realtime(30 Hz or faster).

For beginning I would like to ask which SLAM algorithm is the right one for my situation or where to start. Or do you have any other suggestions how to get real time camera positions inside of the known room/environment. It must be really fast and must allow fast camera movements. Camera would be on person and not on robot.
thank you in advance.
","localization, slam, real-time"
IMU sensor and compensation,"Hi I'm using ""minImu 9"" 9 DOF IMU (gyro, accelerometer and compass) sensor and it gives pitch roll and yaw values with a slope on desktop (no touch, no vibration, steady). Y axis is angle in degree and X axis is time in second. X axis length is 60 seconds. How can fix this?
Pitch

Roll

Yaw

Note1: minIMU code
","sensors, imu, sensor-fusion"
Choosing motors for quadcopter frame,"I bought this drone frame : q450 glass fiber quadcopter frame 450mm from http://hobbyking.com/hobbyking/store/__49725__Q450_V3_Glass_Fiber_Quadcopter_Frame_450mm_Integrated_PCB_Version.html
I'm considering buying 4 AX-4005-650kv Brushless Quadcopter Motor's from http://hobbyking.com/hobbyking/store/__17922__AX_4005_650kv_Brushless_Quadcopter_Motor.html
Will these motor's fit this frame ? How can I determine what motor's will fit the frame ?
",quadcopter
Combustion engine controlled with a remote,"How can one control a combustion engine using a remote control.
Or how would you make a car controlled using a remote.
","robotic-arm, first-robotics"
sum_error in PID controller,"I'm trying to implement a PID controller by myself and I've a question about the sum_error in I control. Here is a short code based on the PID theory.
void pid()
{
  error = target - current;

  pTerm = Kp * error;

  sum_error = sum_error + error * deltaT ;
  iTerm = Ki * sum_error;

  dTerm = Kd * (error - last_error) / deltaT;
  last_error = error;

  Term = K*(pTerm + iTerm + dTerm);
}

Now, I start my commands:
Phase 1, If at t=0, I set target=1.0, and the controller begins to drive motor to go to the target=1.0,
Phase 2, and then, at t=N, I set target=2.0, and the controller begins to drive motor to go to the target=2.0
My question is, in the beginning of phase 1, the error=1.0, the sum_error=0, and after the phase 1, the sum_error is not zero anymore, it's positive. And in the beginning of phase 2, the error=1.0 (it is also the same with above), but the sum_error is positive. So, the iTerm at t=N is much greater than iTerm at t=0.
It means, the curves between phase 2 and phase 1 are different!!!
But to end-user, the command 1, and the command 2 is almost the same, and it should drive the same effort.
Should I set the sum_error to zero or bound it? Can anyone tell me how to handle the sum_error in typical?
Any comment will be much appreciated!!
Kevin Kuei
",pid
IPhone controlled RC car,"I have an R.C car and there is a program in my computer in which I can code the car to perform movements.I would like to have an application with a visual design.Where it shows the cars path.
Is there available software code for this? Saves me lots of time.
","software, research"
Does C have advantages over C++ in robotics?,"I want to build robots, and right now I aim to work with Arduino boards
I know that they are compatible with c and c++, so was wondering which language is better for robotics in general?
I know how to write in java, and the fact that c++ is object oriented makes it look like a better choice for me
does c have any advantages over c++?
","arduino, c++, c"
Implementing a position control for UAV through a flight controller. Plant model is unknown,"We are using Naza-M-Lite for our flight controller without GPS. The localization is obtained through our RGB-D camera sensor. We are able to teleoperate and even implement PID controllers for Roll, Pitch, Yaw and Throttle channels for our quadrotor. However, we do not know the plant model because what we are inputting from Arduino to the Naza-M-Lite are servo PWM ranging from 1000 to 2000. 

For throttle:   1500 altitude hold, 2000 maximum throttle, 1000
  minimum throttle
For Pitch, Roll, Yaw: 1500 maintain 0 angle, 2000 and 1000 moves the
  quadrotor towards its respective axes.

However, even at 1500 on every channel, the quadrotor drifts, maybe due to flying indoors and the wind pushes the quadrotor. Once it gains momentum, it drifts. We are having trouble tuning this because we do not know the relationship of the output is to the position. If the output were velocity, it would have been easier. But as in our case, it is not. Is there a way to find the plant model of the Naza-M-Lite and how can we tune this?
","localization, pid, quadcopter, uav"
What is $\alpha \sin(\theta) + \beta \frac{d \theta}{d t}$ in the inverted pendulum problem?,"I am preparing for an exam in neural networks. As an example for self-organizing maps they showed the inverted pendulum problem where you want to keep the pole vertical:

Now the part which I don't understand:

$$f(\theta) = \alpha \sin(\theta) + \beta \frac{\mathrm{d} \theta}{\mathrm{d} t}$$
  Let $x= \theta$, $y=\frac{\mathrm{d} \theta}{\mathrm{d} t}$, $z=f$.
Solution with SOM:

three-dimensional surface in $(x,y,z)$
adapt two-dimensional SOM to surface
Method of control
  
  
For a given $(x,y)$ find neuron $k$ for wich $w_k = [w_{k1}, w_{k2}, w_{k2}, w_{k3}]$
$f(\theta)$ is then $w_{k3}$



I guess we use the SOM to learn the function $f$. However, I would like to understand where $f$ comes from / what it means in this model.
","control, stability, machine-learning"
Open source implementations for GPS+IMU sensor fusion?,"Are there any Open source implementations of GPS+IMU sensor fusion (loosely coupled; i.e. using GPS module output and 9 degree of freedom IMU sensors)? -- kalman filtering based or otherwise.
I did find some open source implementations of IMU sensor fusion that merge accel/gyro/magneto to provide the raw-pitch-yaw, but haven't found anything that includes GPS data to provide filtered location and speed info.
","kalman-filter, imu, sensor-fusion, gps"
Robotic part to dispense candy,"I'm a complete newbie trying to build a simple robot that dispenses candy (M&M, skittles, etc).  However, since I'm not familiar with the field, I'm having a hard time googling because I don't know the correct terms to search for.  I'm looking for a piece to build a robotic 'trap door' of sorts that will open for a specified amount of time to release candy.  What parts can I use and what is are called?  I've tried robotic lever, robotic door, etc with no luck.
",design
Denavit Hartenberg parameters,"Can anybody help figure out HD parameters for the case where two links with a revolute joint are in the same plane, thus that the variable angle is 0, but the twist is not 0. This is a simple drawing. I think that x-axis that is perpendicular to both z-axis, points away and goes through the intercection of z-axis. The link length is 0, the twist is a and the offset is d. Whould it be correct?
Thanks.


",dh-parameters
Mechanical robustness/shock resistance LiPo batteries,"How mechanically robust are LiPo batteries? How much force or acceleration can they maximally withstand before failure? What is their (mechanical) shock resistance?
For some electrical components used in robots, such as IMU's, it can be found in datasheets that they can suffer mechanical failure if accelerated or loaded beyond given values. For IMU's, this is typically somewhere between $2000g$ and $10000g$ (where $1g = 9.81 m/s^2$).
I'm wondering if similar values are known for LiPo batteries, since they are known to be vulnerable components. But, is there any quantification known for their claimed vulnerability?
",battery
ROS + kinect depth data duplication,"I am trying to get depth data from a Kinect in a ROS project. It currently looks like this:

To arrive at this, I've done:
depth_sub = rospy.Subscriber(""/camera/depth/image"", Image, depth_cb)
...
def depth_cb(data):
    img = bridge.imgmsg_to_cv2(data, ""32FC1"")
    img = np.array(img, dtype=np.float32)
    img = cv2.normalize(img, img, 0, 1, cv2.NORM_MINMAX)
    cv2.imshow(""Depth"", img)
    cv2.waitKey(5)

I also launch openni.launch from the openni_launch package, which publishes the depth data.
I also get this weird warning from the node (can be seen in the image):
    ComplexWarning: Casting complex values to real discards the imaginary part.
But as I understand it the data type is an array of 32-bit floats. Yet some of the values appear as nan.
I would like a depth image that directly corresponds to a RGB image array of the same size. I will be doing some tracking in the RGB space, and using the tracked coordinates (X,Y) from that to index into the depth array. Thanks. 
edit:
Turns out, /camera/depth/image is published as an array of uint8s, but the actual data is 32bit floats (which is not listed anywhere, had to hunt it down in other people's code). Thus an array of 480x640 uint8s, interpreted as 32bit floats, in effectively ""quartered"" in the number of data points. Which could explain how the image is 4 times smaller (and hence accessing datapoints out of bounds = nan?), but not why there are two of them.
","ros, kinect"
Fusion of GNSS position data and prefused 9-dof AHRS data,"Bosch, FreeScale, InvenSense, ST and maybe others are releasing 9-dof AHRS platforms containing their own fusion software and outputting filtered/sane/fused data (attitude as quaternion and linear acceleration).
I would like to use these for the quality of their respective company fusion algorithm. And would like to merge GNSS position and velocity data to it.
I have found multiple examples of heavy (> 20) states Kalman filters merging raw 9-dof IMU data and GNSS position/velocity.
But I have a hard time finding a computationally lighter version of GPS+AHRS fusion as these new 9-dof AHRS already fuse IMU raw data themselves and this process should'nt be done twice.
Would you maybe have pointers on the algorithm(s) or type of filter to use ? Thank you.
","quadcopter, kalman-filter, imu, gps, sensor-fusion"
Brushless DC motor - Electronic Speed Control - Quadcopter,"I'm a student who is doing electrical and electronics engineering. I'm currently doing my final project which is a quadcopter. One of my objectives in that is to make a Electronic Speed Controller (ESC) for the brushless motors that are being used. 
I made a design for the ESC using proteus and I made the PCB also. I have attached the schematic. I used PIC16F628A for the ESC and wrote a small code in mikroC for the ESC to work when powered up. Unfortunately it didn't work properly. I tried sensorless control of brushless motors without getting any feedback.
Can I know how much of current that I should provide for the motor? According to some articles that I read the brushless DC (BLDC) motor requires around 10A at the startup for around 20 ms. I have posted the code also. I used two codes to run the motor. One with PWM and other without PWM (100% duty cycle). 
I am a rookie to the subject of BLDC motor controlling. I am very grateful if anybody can help me to clear out the doubts and figure out the mistakes in my design to make it work properly. 
Below given is the code that I tried. Please help me to figure out the right way to program the chip.
    const delay = 7000;

void main() {
 TRISB = 0x00;
 PORTB = 0x00;

 while(1)
 {
   PORTB = 0x24;
  delay_us(delay);

  PORTB = 0x36;
  delay_us(delay);

  PORTB = 0x12;
  delay_us(delay);

  PORTB = 0x1B;
  delay_us(delay);

  PORTB = 0x09;
  delay_us(delay);

  PORTB = 0x2D;
  delay_us(delay);
 }


}

When I uploaded the above given code and when I set the delay to around 3000 μs, the motor spun but at each time one of the MOSFETs got heated up until I cannot touch it anymore. Here is the video of this scenario.
This is the other code (PWM);
const delay1 = 2000;
const delay2 = 1000;
int count = 0;
int cnt;
int arr[6] = {0x24, 0x36, 0x12, 0x1B, 0x09, 0x2D};
int i = 0;
int x =  0x32;


 void init(void)
 {
  TRISB = 0x00;
 PORTB = 0x00;
 //OPTION_REG = 0x87;
 //INTCON = 0xA0;
 CCP1CON = 0;
 CMCON = 0x07;
 }



void main() {
   init();
   while(1){

   for (cnt = 0; cnt < 10; cnt++)
       {
         PORTB = arr[i];
         delay_us(2);
         PORTB = 0x07;
         delay_us(2);

        }

     i++;

         if (i == 6)
         {
         i =0;
         }

   };


 }

","quadcopter, brushless-motor, esc"
Comparison of lifting systems,"What kind of systems can be used to make a torso lifting system like the one used by this robot (the black part) :


Rack and pinion
lead screw
scissor lift
can a triple tree help ? 

What are the pro and cons of each system ?
How do they ensure stability ?
And finally, is there a way to draw current when lowering instead of drawing current when lifting ?
",mechanism
Choosing a proper sampling time for a PID controller,"I have a robotic system I'm controlling with Arduino, is there an heuristic way to determine a proper sampling time for my PID controller? Considering I have some other things to compute on my sketch that require time, but of course a good sampling time is crucial.
Basically I have a distance sensor that needs to detect at a constant rate an object that is moving, sometimes slow, sometimes fast. I don't have a good model of my system so I can't actually tell the physical frequency of the system.
",pid
What can this picture/data tell?,"I've implemented a model of a ball-on-plate plant and am controlling it over a network. Below is the open loop output when excited by successive sinusoidal inputs with increasing frequencies. I know that the plant is open loop unstable, and it is cool that this figure so nicely captures the instability. 
What I'd like to know is if there is other information that I can glean about the plant from the relationship between the input and the output state.
(The state is clipped at 3.1 units.)

","control, balance, distributed-systems"
DH parameters and Kinematic Decoupling,"Is it possible to decouple a 5DOF manipulator? 
This question I asked earlier and I believe I got the right answers but I never show the drawings of the manipulator and now I'm hesitating during setup of the DH parameters for Forward Kinematics. See drawing depicted here. 

","kinematics, forward-kinematics, dh-parameters"
How can I measure the height of an object with a single sharp sensor (GP2Y0A21YK0F)?,"I have one sharp sensor and I have to use it to measure the height of a block (6cm - 12 cm). How can I accomplish this ?
Actually it is to be connected to a robot which will move near the box and determine its height.
About GP2Y0A21YK0F:
http://www.sharpsma.com/webfm_send/1489
The robot is like this: http://i.imgur.com/8qT8zeQ.jpg

If possible please suggest a solution that doesn't require moving the sensor.
But any method will do fine.
","mobile-robot, first-robotics"
PID tuning for 6 dof robotic arm,"I'm currently developing a 6 dof robotic arm. The arm is vibrating when it stop moving and I want to reduce it. Another thing is that arm is so heavy (because there is a projector inside it, lol) and I have to use spring between joints. So, can anyone tell me 1. how to select springs because my supervisor told me that proper selection of springs can reduce vibration? 2. how do I tune the PID parameters? All the joints are dynamixel servos and their PID parameters are tunable. I read article about tuning for a single servo. How do I tune these parameters for the whole arm?
","control, pid, robotic-arm"
How can I measure the actual speed and distance traveled of the robot with an external setup?,"Good day to all.
First of all, I'd like to clarify that the intention of this question is not to solve the localization problem that is so popular in robotics. However, the purpose is to gather feedbacks on how we can actually measure the speed of the robot with external setup. The purpose is to be able to compare the speed of the robot detected by the encoder and the actual speed, detected by the external setup.
I am trying to measure the distance traveled and the speed of the robot, but the problem is it occasionally experiences slippage. Therefore encoder is not accurate for this kind of application. 
I could mark the distance and measure the time for the robot to reach the specified point, but then I would have to work with a stopwatch and then transfer all these data to Excel to be analyzed. 
Are there other ways to do it? It would be great if the external setup will allow data to be automatically sent directly to a software like Matlab. My concern is more on the hardware side. Any external setup or sensors or devices that can help to achieve this?
Thanks.
","mobile-robot, control, wheeled-robot"
Change PWM values according to encoder output,"I have a motor with an encoder. When I set the speed of the motor it should change its speed so that encoder readings per second should fit an equation $y = ax^2 + bx + c$ where x is speed value that is given to the motor and y is the encoder readings per second that should get with motor.
Encoder reading is counted in every 1ms and if it is not equal to the value of the encoder output should get from motor (it is calculated using the equation), the PWM input to the motor should vary in-order to get desired encoder output.
I want to control this value using a PID controller but I'm confused in writing equations. Any help would be appreciated..
","motor, pid, pwm"
Arduino Power Adapters,"I'm shopping for my first Arduino with a specific goal in mind. I need to attach 3 standard servo motors, an ArduCam Mini 2MP camera, and several LEDs. I'm trying to figure out power requirements. I assume that USB power won't be sufficient. I'm looking at 12V AC-to-DC outlet adapters and I noticed that Amps vary from ~500MA to 5A. I don't want to use batteries.
What would you recommend as minimum amperage for this setup? Is there a maximum amperage for Arduino boards? I don't want to plug it in and burn it out. If I plug in both the USB cable and a power adapter at the same time, is power drawn from both cables?
Thanks!
","arduino, power"
Calculate robot heading to follow wall and avoid obstacles,"I have a task that involves implementing robot behaviour that will follow wall and avoid obstacles along it's path. The robot must stay at desired distance from the wall but also stick to it so it should not loose sight of it. Robot is sensing it's surrounding with ultrasonic sensor that is oscillating from left to right and filling an array of small length (10 values) with detected distances (every 10 degrees). From this reading I would like to calculate heading vector that will result in robot path similar to one shown in bottom picture:
Black(walls), red(obstacles), blue(robot), green(desired path)

","wheeled-robot, navigation"
Can i connect the arduino usb to laptop after the arduino is started,"I have some sensors attached to arduino uno r3 and an esc. I start the Motor attached to esc through ardiuno with no usb connected to laptop. It starts correctly. There is a must that i will have to start the arduino from non usb supply so that esc is correctly started, means that my motor doesnt start with usb connected to pc. Now how can i get the sensor values to laptop. If i connect the usb to pc after starting the motor, will this work.
","arduino, esc"
Humanoid balancing,"I'm currently working on Humanoid robot. I've solved the Forward & Inverse Kinematic relations of the robot, and they turn out to be fine. Now I want to move onto Walking. I've seen tons of algorithms & research papers but none of them make the idea clear. I understand the concept of ZMP & what the method tries to do, but I simply can't get my head around all the details that are required to implement it in a real robot. Do I have to plan my gait & generate the trajectories beforehand, solve the joint angles, store them somewhere & feed it to the motors in real-time? or Do I generate everything at run-time(a bad Idea IMO)? Is there a step-by-step procedure that I can follow to get the job done? or Do I have to crawl all my way through those Research papers, which never make sense(at least for me).
","mobile-robot, stability, humanoid"
Is it possible to design robot software or AI to function in different devices?,"so i'm really interested in robotics.. I'm not really a robot expert as i have no experience on creating one. I just like them.  Anyway, I am always wondering if its possible to build a robot that can transfer itself to different devices and still function. I mean, if you want that robot to transfer itself(THE DATA that making it function or whatever you call it) to your laptop so you can still use it while you are away or anything.. Does creating one require advanced computing and knowledge? Is it kind of creating a artificial intelligence?. When it think of this i would always thought of J.A.R.V.I.S since he can go to Stark Suit and communicate with him.

Translated into robotics terminology by a roboticist:
Is it possible to create software for controlling robot hardware that can transfer itself to different devices and still function. Could it transfer itself to your laptop and collaborate with you using information it gathered while it was in it's robot body? 
  Does creating software like this require advanced knowledge and computing? Is software like this considered to be artificial intelligence?

I am serious about this question sorry to bother or if anyone will be annoyed./ 
",mobile-robot
trapezoidal vs sinusoidal commutation,"How do you know if a commercial driver is working with trapezoidal or sinusoidal commutation? If you measure the 3-phase voltage applied to the PMSM by means of an oscilloscope, will you see a difference?
",brushless-motor
How does a two-gear pull-back car toy work?,"This is not a robotics question, but this Stack Exchange is the closest I could find to mechanical engineering. Please refer me to a better place to ask this, if one exists. Hopefully someone might just know this.

I got a pull-back car for my boy at McDonalds, and it has two gears. It starts slow, then speeds up after about two seconds. It's impressive to me, especially given the inherent cheapness of toys sold by McDonalds. It feels solidly built as well.
I couldn't find anything related to this concept. The wiki on pullback motors does not include any information on multiple gears.
Any ideas on how this works? 
",mechanism
Which trajectory planning algorithm for minimizing jerk,"In order to perform a cyclic task, I need a trajectory planning algorithm. This trajectory should minimize jerk and jounce.
When I search for trajectory planning algorithms, I get many different options, but I haven't found one which satisfies my requirements in terms of which values I can specify. An extra complicating factor is that the algorithm should be used online in a system without too much computing power, so mpc algorithms are not possible...
The trajectory I am planning is 2D, but this can be stripped down to 2 trajectories of 1 dimention each. There are no obstacles in the field, just bounds on the field itself (minimum and maximum values for x and y)
Values that I should be able to specify:

Total time needed (it should reach its destination at this specific
time) 
Starting and end position
Starting and end velocity
Starting and end acceleration
Maximum values for the position.

Ideally, I would also be able to specify the bounds for the velocity, acceleration, jerk and jounce, but I am comfortable with just generating the trajectory, and then checking if those values are exceeded.
Which algorithm can do that?
So far I have used fifth order polynomials, and checking for limits on velocity, acceleration, jerk and jounce afterwards, but I cannot set the maximum values for the position, and that is a problem...
Thank you in advance!
","control, algorithm"
"How Should I tie My quadcopter to some thing, to adjust pid on one axis","I am stuck in adjusting the PID of my quadcopter, I cant adjust them on the fly because it just get out of control. I am adjusting them while attaching my quadcopter to something.
Is this method correct. Will the pid values required will be different on the fly or same. Please suggest me how to attach my quad to some thing.
","quadcopter, pid, balance"
What erector sets will function with normal servo motors?,"I need a basic erector set that the parts will fit with servo motors and dc motors. Preferably below $100. I've looked at Minds-i basic set and it looks good except I don't know if it will function with my servos without hot glue or extensive modifications. 
If it matters, I am making a bipedal robot so I don't require any wheels or anything pre-built. I just need a basic set that I can add on to to build a whole bunch of different robots. 
","mobile-robot, rcservo"
Is Venetian mirror possible in Autodesk Inventor?,"I see there are things like glass and mirror in Autodesk Inventor Professional 2016 but is there a possibility to have Venetian mirror? So that from one side it would look like a mirror and from the other side it would look like a transparent glass?
","design, mechanism, 3d-printing, 3d-model, visualization"
Can a 5S LIPO battery be changed to a 3S and a 2S?,"Newbie to robotics here! 
I bought a 5S LIPO but now realise that it is overkill. And these things are expensive!
So, given that (as far as I know) the pack is apparently made up of individual cells of 3.7 volts each, is there any way in which I could somehow (safely) separate out the cells to get a 3S and a 2S or even single 1S cells?
",battery
Software to simulate mechanics of production line,"Is there any software where I can simulate production line elements (joints, motors, springs, actuators, movement)? For example I want to simulate mechanism to unwind paper from big roll to weld it later with bubble foil and finally make bubble foil envelope, mechanism will look like this:

I need it as simple as possible and preferably free.
","mechanism, simulator"
differences between SCARA arm design,"I am currently interested in SCARA arm designs and I have few, beginner questions for which I didn't find answers yet. 
1/ While comparing professional arms (made by epson, staubli...) I noticed that the actuator used for the translation on the Z axis is at the end of the arm. On ""hobby"" arms like the makerarm project on kickstarter they use a leadscrew with the actuator at the beginning of the arm.
I thought it was smarter to put the actuator handling this DoF at the begining of the arm (because of its weight) and not at the end, but I assume that these companies have more experience than the company behind the makerarm. So I'm probably wrong, but I would like to understand why :)
2/ Also I would like to understand what kind of actuators are used in these arms. The flx.arm (also a kickstarter project) seems to be using stepper motors but they also say they are using closed loop control, so they added an encoder on the stepper motors right?
Wouldn't it be better to not use stepper and, for instance, use DC brushless motors or servos instead ?
3/ I also saw some of these arms using belts for the 2nd Z axis rotation, what is the advantage ? it only allows to put the actuator at the begining of the arm ?
","robotic-arm, design, actuator"
Modelling Point Clouds for Collision Detection in Gazebo,"I am currently applying path planning to my robotic arm (in Gazebo) and have chosen to use an RRT. In order to detect points of collision, I was thinking of getting a Point Cloud from a Kinect subscriber and feeding it to something like an Octomap to have a collision map I could import into Gazebo. However, there is no Gazebo plugin to import Octomap files and I do not have enough experience to write my own. The next idea would be to instead feed this point cloud to a mesh generator (like Meshlab) and turn that into a URDF, but before starting I'd rather get the input of somebody far more experienced. Is this the right way to go? Keep in mind the environment is static, and the only things moving are the arms. Thank you. Below is just a picture of an octomap.
","robotic-arm, localization, slam, kinect, gazebo"
Backstepping Integrator: changing the virtual control,"given the following differential equation 2°ODE in the following form:
$\ddot{z}=-g + ( cos(\phi) cos(\theta))U_{1}/m $
found in many papers (example) and describing the dynamic model of a quadrotor (in this case I'm interested as an example only for the vertical axis $Z$) , I get the movement about $Z$ after integrating the variable $\ddot{z}$ two times. As control input I can control $U_{1}$, which represents the sum of all forces of the rotors.
A Backstepping Integrator (as in many of papers already implemented) defines a tracking error for the height $e_{z} = z_{desired} - z_{real}$ and for the velocity $\dot{e}_{z} = \dot{z}_{desired} - \dot{z}_{real}$ to build virtual controls.
Through the virtual controls one can find the needed valueof $U_{1}$ to drive the quadrotor to the desired height (see the solution later on)
But wait...as said above I need to track both: position error and velocity error.
Now I asked myself, how can I transform such equation and the corresponding virtual controls to track only the velocity??
In my code I need to develop an interface to another package which accepts only velocity inputs and not position information.
I should be able to drive my quadrotor to the desired position using only velocity informations, tracking the error for the z displacement it not allowed.
The solution for the more general case looks like:
$U_{1}=(m/(cos(\phi)cos(\theta))*(e_{z} + \ddot{z}_{desired} + \alpha_{1}^{2}\dot{e}_{z} - \alpha_{1}^{2}e_{z} + g + \alpha_{2}\dot{e}_{z})$
for  $\alpha_{1}, \alpha_{2} > 0$
I could simply put brutal the $\alpha_{1} = 0$ for not tracking the position on Z but I think that is not the correct way.
Maybe could you please point me in the right direction?
Regards
","control, quadcopter"
How can I increase the resolution of a PWM signal?,"Say I have a motor and I want it to spin at exactly 2042.8878 revolutions per minute.  Say I have a very precise sensor to detect the RPM of the motor to a resolution of 1/1000th of a revolution per minute.  

Can I produce a PWM signal which can match the speed to that degree of 
precision?  
What variables in the signal parameters would I have to adjust to get the precision if possible?  
Would I have to use additional circuitry between the motor and the driver?  
Would I have to design the signal/circuitry around the specific specifications of the motor?
Should I just use a stepper motor?

This is assuming I am using a microcontroller to measure the motor's speed and adjust the signal in real-time to maintain a certain speed.
","motor, microcontroller, stepper-motor, pwm, stepper-driver"
Pole-balancing / inverted-pendulum; is there a need for active control?,"Not sure if I am posting this question in the correct community, as it relates primarily to reinforcement learning. Apologies early on if this is not so.
In reinforcement learning many algorithms exist for 'solving' the cart-pole problem; that of balancing a mass on the edge of a stick, connected to a cart on a hinge, which has 1 DoF. There is TD learning, Q-learning and many other on and off-policy methods. There is also the more recent, model-based policy search method PILCO.
What I am really wondering, I suppose is more of a physics question: is there a need for active control? Why is it not possible to find the one point for the cart, which prevents the mass to move, even incrementally, left or right as it sits atop the pole? Why does it always 'fall'?
",control
making a robot that knows your location,"I've just made a radio frequency remote control using PIC microcontroller and I want to do something useful with it. I am thinking of a robot that gets things for you while you are at bed but here comes the question: How am I going to have the PIC determine the location of the remote control calling for it? It can't really be done using a GPS module because it will all be in the house.
What options do I have?
","microcontroller, localization"
Recommendation for 3D mechanism modeling and simulation software,"I'm working on a robotic hand and I would like to simulate different joints and tendon insertion points before starting to actually build it.
I've been googling and found things like Solidworks and Autodesk, that seem very costly for a hobbyst like me but also I don't quite fully understand their capabilities (just CAD? 3D modelling but not simulation? Simulation but not interactive?). I've also found things like FreeCAD which seem to me somehow abandoned or just for CAD and not for simulation.
Another requirement would be interactivity of the simulation, not just rendering.
I don't have a problem with commercial software, but I'm looking for a reasonable cost for a hobbyst, not an engineering company.
Is there a software out there that meets all this requirements? Or should I use several programs each for a specific purpose?
Thanks!
","design, mechanism, software, simulator, 3d-model"
Lifting robotic leg with only one servo,"Note before I start: I have not actually put anything together yet, i'm still just planning, so any changes that require a shape change or anything like that are accepted.
I'm working on making a walking robot with my arduino and 3d printing all the pieces I need. It will have four legs, but since it needs to be mobile, I didn't want the power supply to be huge. I've decided it would be best if I can get each leg to only require 1 servo, at 5V each. I know how to get the leg to move back and forth, but i want to be able to lift it in between; before it brings the leg forward, it needs to lift up the foot.
The only thing I can think of is the rotation maybe locking some sort of gear.
When a motor begins rotating clockwise, how can I have it power a short motion to move an object toward itself, and when it begins moving counterclockwise to power the same object a short distance away from itself?
The servos I am using have 180* of rotation, so they don't go all the way around in a loop.
also: don't know if it will be important or not, but because of the peculiar construction of the foot, it would be best if it was lifted straight up, rather than up at an angle, but it isn't 100% necessary. 
Are there any robots that already do this? if so, I'm unaware of them. Thanks for your time.
","mechanism, motion-planning, servomotor, legged, gearing"
What are the approaches for indoor robot positioning?,"I understand that most of the self-driving cars solutions are based on Lidar and video SLAM.
But what about robots reserved for indoor usage? Like robot vacuums and industrial AGVs? I see that Lidar is used for iRobot and their latest version uses VSLAM. AGVs also seem to use Lidar.
","slam, lidar"
Quadrocopter problem with stability,"I'm building quadcopter from scratch, software is implemented on STM32F4 microcontroller. Frequency of main control loop equals 400Hz.
I've though everything is almost finished but when i've mounted everything and started calibration of PIDs i faced a problem.
It was impossible to adjust PID parameters properly.
So i started test with lower power (not enough to fly) and i've managed quite fast adjust PID for roll but when i've increased power problems with control came back.
After that i've done more measurements.


I didn't make test with blades but probably this is even worse and that is why i cannot calibrate it.

If problem is due to vibration how can i fix it?
If something else is cause of that symptom, what is it?
Can i solve this through better controls and data fusion algorithms?
Now i use complementary filter for acc and gyro sensors data fusion in roll and pitch.

","control, imu, accelerometer, gyroscope"
Which USB interface for Android device I can use for motor driver,"I am new to robotics,
I will be controlling DC motors from Android device through USB
For this I have selected L298N motor controller(After watching YouTube videos )
And got some DC motors
I have no idea how do I connect this to Android device via USB cable
Help appreciated
Ref:
https://www.bananarobotics.com/shop/L298N-Dual-H-Bridge-Motor-Driver
https://youtu.be/XRehsF_9YQ8
PS: All I know is programming android
","motor, usb"
Which brushless dc and propeller to choose?,"I have a small bot(around 4-5kg with wheels) which is to be pushed without contact by another bot. I plan to do this using a bru and a propeller. I am having problems selecting the right combination. Please help me with these questions:-

Should the bldc be high kv or low kv(will i need high rpm or low rpm)
What is the  ideal propeller to use with the motor so that i can create enough thrust to get the 'small' bot in motion and keep it in motion?
What are the other criteria i should keep in mind while selecting.

",brushless-motor
Forward kinematic and inverse kinematic... When to use what?,"I am not quite sure if I quite understand the difference between these two concepts, and why there is a difference between these two concept. 
Yesterday I was trying to compute the jacobian needed for an inverse kinematics, but the usual input I provided my transformation in the Forward kinematics being the Points P and xyz could not be applied, The transformation matrix was given a state vector Q, at which the the Tool position could be retrieved... 
I am not sure if understand the concept quite well, and can't seem to the google the topics, as they usually  include terminologies which makes the concepts too simple (Angle calc and so on.. )
I know it might be pretty much to ask, but what form of input is needed to compute the jacobian ?, and what and why is there a difference between forward and inverse kinematics?.. 
","inverse-kinematics, forward-kinematics, jacobian"
"Fanuc Robot ""Heat"" control","My work has an older Fanuc robot ( Arc Mate 100-iBe RJ3iB, Fanuc AWE2 teach pendant with  Powerwave 355M) and the old operator/programmer has left. I have taken over his job and cant find out how to turn down the voltage and wire feed speed because it occasionally burns through parts. I tried manually putting in voltage and wire feed speed but it seems it will only accept the previous weld schedules 1-8 and if i mess with them that will affect other programs using those. I just need someone to please point me in the right direction. 
P.S. Typed on phone , sorry if sloppy.
",industrial-robot
Is there a way to disconnect and reconnect from a Create 2 that was streaming sensor readings without having to unplug/replug my USB-serial cable?,"I am working with a Create 2 and I am executing a simple sequence like (in pseudocode):
create serial connection from Macbook to Create

start the OI with by sending the 128 code

send a pause-stream command (just to be safe)

initiate the data streaming with ids: [29, 13]

every 0.5 seconds for 15 seconds:
    poll the streamed sensor data and print it

send a pause-stream command before shutdown

send a 128 to put the robot in ""passive mode"" (I have also tried 173)

close the serial connection

The outcome when I run the above program repeatedly is that it works the first time, I see sensor data (that seems to not change or be reactive) printing to the screen, but on future runs no serial can be read and the program crashes (because I am throwing an exception because I want to get this problem ironed out before getting to far along with other things). If I unplug and replug my USB cable from my Macbook, then the program will work for another run, and then fall back into the faulty behavior.
I do not experience this issue with other things like driving the robot, I am able to run programs of similar simplicity repeatedly. If I mix driving and sensor streaming, the driving works from program run to program run, but the data streaming crashes the program on the subsequent runs.
I have noticed that if I want to query a single sensor, I need to pause the stream to get the query response to come through on the serial port, and then resume it. That is why I am so inclined to pause/restart the stream.
Am I doing something wrong, like pausing the stream too often? Are there other things I need to take care of when starting/stopping the stream? Any help would be appreciated!
EDIT:
I should note that I am using Python and pyserial. I should also note, for future readers, that the iRobot pushes its streamed data to the laptop every 15ms where it sits in a buffer, and the data sits there until a call to serial.read() or to serial.flushInput(). This is why it seemed that my sensor values weren't updating when I read/polled every half second, because I was reading old values while the current ones were still buried at the back of the buffer. I worked around this issue by flushing the buffer and reading the next data to come in.
EDIT 2:
Sometimes the above workaround fails, so if I detect the failure, I pause the stream, re-initialize the stream, and read the fresh data coming in. This seems to work pretty well. It also seems to have solved the issue that I originally asked the question about. I still don't know exactly why it works, so I will still accept @Jonathan 's answer since I think it is good practice and has not introduced new issues, but has at least added the benefit of the robot letting me know that it has started/exited by sounding tones.
","mobile-robot, irobot-create"
What engineering problems needs to be solved to build a potato-peeling robot?,"OK, let's say we have a tech request for a robotic system for peeling potatoes, and a design is as follows:

One ""arm"" for picking up a potato and holding it, rotating when needed.
Another ""arm"" for holding a knife-like something which will peel the skin from the potato.
Arm picks up a potato from first container, holds it over trash bin while peeling, then puts peeled potato in second container.
For simplicity a human rinses peeled potatoes, no need to build automatic system for it.
In first iteration even 100% spherical peeled potatoes are OK, but ideally would be good to peel as little as possible, to minimize the wastes.

Question:
I know that we're very, very far away from building such a system. Nevertheless, what are the purely technical difficulties which needs to be solved for such a robot to be built? 
EDIT
Let's assume we stick to this design and not invent something radically different, like solving the problem with chemistry by dissolving the skin with something. I know that the problem of peeling the potatoes is currently being solved by other means - mainly by applying friction and a lot of water.
This question is not about it. I am asking specifically about the problems to be solved with the two-arms setup using the humanlike approach to peeling.
",robotic-arm
How to connect absolute encoder on the rotating shaft. Please see the three options?,"

Hi,
Here I have added 2 options for connecting encoder on shaft.
Motor, gearhead and shaft is connected using coupling. But where will be best place for encoder (To avoid backlash from coupling and gearhead).
whether through hollow encoder is available? (see option 1).
I dont know which one will be best for this kind of system. 
Which one is widely using arrangements?
Options 3 is Encoder will be placed before the motor.
",motor
Mapping between camera pose and image features in visual servoing,"I have a robotic arm and a camera in eye-in-hand configuration. I know that there is a relationship between the body velocity $V$ of the camera and the velocities $\dot s$ in the image feature space that is $\dot s=L(z,s) V$ where $L$ is the interaction matrix. I was wondering if one can find a mapping (a so called diffeomorphism) that connects the image features' vector $s$ with the camera pose $X$. All I was able to find is that it is possible to do that in a structured environment which I don't fully understand what it is.
","mapping, visual-servoing"
How to build a fast quadcopter,"im currently in a (risky) proyect that involves me building the fastest quad i can afford.
Im trying to get something close to this extremely fast warpquad
After reading a lot about quadcopters, as i know i can buy all this and it should fit together and fly without any problem.
Motors: Multistar Elite 2306-2150KV
ESC: Afro Race Spec Mini 20Amp
Quanum neon 250 carbon racing frame(I love how it looks)
6Inch Props
CC3D flight controller
4S 1400mah 40-80C Battery
Any 6ch radio

My questions are, first if im wrong or im missing something as i had only read about it (thinking this is a common build for racer quad).
Then:
Will this overheat (bad consecuences) if i let it drain the full battery at 100% throttle?
Will this fly at least 4 minutes under the previous conditions?
Should i get a higher C-rating battery?
As i can't find better motors of that size, is the only way to improve its speed by putting a 6S battery? and what would happen if i do it?
Should i put the 6inch props or 4inch? I know 4inch should get faster rpm changes but will it be noticeable at this sizes?
And in general any tips to make it faster will be welcome.
Thanks. 
",quadcopter
Relative orientation of two robots,"Given two robot arms with TCP (Tool Center Point) coordinates in the world frame is:
$X_1 = [1, 1, 1, \pi/2, \pi/2, -\pi/2]$
and
$X_2 = [2, 1, 1, 0, -\pi/2, 0]$
The base of the robots is at:
$Base_{Rob1} = [0, 0, 0, 0, 0, 0]$
$Base_{Rob2} = [1, 0, 0, 0, 0, 0]$
(The coordinates are expressed as successive transformations, X-translation, Y-translation, Z-translation, X-rotation, Y-rotation, Z-rotation. None of the joint axes are capable or continuous rotations.)
How many degrees does the TCP of robot 2 have to rotate to have the same orientation as the TCP of robot one?
Is the calculation 
$\sqrt{(\pi/2 - 0)^2 + (\pi/2 - (-\pi/2))^2 + (-\pi/2 - 0)^2}$
wrong? If yes, please specify why.
UPDATED:
is the relative orientation of the two robots [π/2,π/2,−π/2]−[0,−π/2,0]=[π/2,π,−π/2]? but the euclidean distance cannot be applied to calculate angular distance?
In other words:

While programming the robot, and tool frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \pi/2, \pi, -\pi/2$) command, but the executed motion would have magnitude of $\pi$?
While programming the robot, and world frame is selected for motion, to match the orientation of the other one, i would have to issue a move_rel($0, 0, 0, \pi, 0, 0$) command, and the executed motion would have magnitude of $\pi$?

","robotic-arm, kinematics, geometry"
Small IR distance sensor that works on black surfaces,"Can anyone recommend an IR distance sensor that works on black surfaces? I'm looking for something to use as a ""cliff"" sensor, to help a small mobile robot avoid falling down stairs or off a table, and I thought the Sharp GP2Y0D805Z0F would work. However, after testing it, I found any matte black surface does not register with the sensor, meaning the sensor would falsely report a dark carpet as a dropoff.
Sharp has some other models that might better handle this, but they're all much larger and more expensive. What type of sensor is good at detecting ledges and other dropoffs, but is small and inexpensive and works with a wide range of surfaces?
",sensors
Kinematic decoupling,"Is kinematic decoupling of a 5DOF revolute serial manipulator also valid?
The three last joints is a spherical joint. Most literatures only talks about decoupling of 6DOF manipulators.
Thanks in advance,
Oswald
",kinematics
Need help calculating the thrust on quadcopter motors,"I'm trying to calculate the lifting capability of my four quadcopter motors. I tried using eCalc but it doesn't have battery I'm using. Are there any equations to keep in mind for doing these calculations? Here are some relevant details:
Battery: 2200mAh 3S 25~50C LiPo
ESC: 25A
Motor: 1240kV Brushless
Propeller: 8x4
Any help would be much appreciated, thanks!
",quadcopter
iRobot Create without ROS?,"Is it possible to control the Create without using any ROS whatsoever? I know it has all these serial/Digital I/O pins that connect to ROS which controls it using drivers/libraries. But how hard would it be to do so using, say, a PCduino?
I'm asking this because I'm having trouble launching the create using ROS (question)
",irobot-create
How can I reduce a motor's maximum current draw?,"I have a motor with a stall current of up to 36A. I also have a motor controller which has a peak current rating of 30A. Is there any way I could reduce the stall current or otherwise protect the motor controller?
I realize the ""right"" solution is to just buy a better motor controller, but we're a bit low on funds right now.
I thought of putting a resistor in series with the motor and came up with a value of 150mΩ, which would reduce the maximum current draw to 25A (given the 12V/36A=330mΩ maximum impedance of the motor). Is there any downside to doing this? Would I be harming the performance of the motor beyond reducing the stall torque?
","motor, microcontroller, current"
Has a robot ever taken a complete IQ test?,"And if so, what was the highest score so far?
Some news articles suggest only parts of tests were aced.

Update since people censored this question and closed it. There was an AI that has taken an IQ test and scored similar to a 4 year old.
http://phys.org/news/2015-10-ai-machine-iq-score-young.html

The AI system which they used is ConceptNet, an open-source project run by the MIT Common Sense Computing Initiative.
  Results: It scored a WPPSI-III VIQ that is average for a four-year-old child, but below average for 5 to 7 year-olds

Abstract

We administered the Verbal IQ (VIQ) part of the Wechsler Preschool and Primary Scale of Intelligence (WPPSI-III) to the ConceptNet 4 AI system. The test questions (e.g., ""Why do we shake hands?"") were translated into ConceptNet 4 inputs using a combination of the simple natural language processing tools that come with ConceptNet together with short Python programs that we wrote. The question answering used a version of ConceptNet based on spectral methods. The ConceptNet system scored a WPPSI-III VIQ that is average for a four-year-old child, but below average for 5 to 7 year-olds. Large variations among subtests indicate potential areas of improvement. In particular, results were strongest for the Vocabulary and Similarities subtests, intermediate for the Information subtest, and lowest for the Comprehension and Word Reasoning subtests. Comprehension is the subtest most strongly associated with common sense. The large variations among subtests and ordinary common sense strongly suggest that the WPPSI-III VIQ results do not show that ""ConceptNet has the verbal abilities a four-year-old."" Rather, children's IQ tests offer one objective metric for the evaluation and comparison of AI systems. Also, this work continues previous research on Psychometric AI. 


Update. A robot has passed the Japanese college entrance test and has an 80% chance of being accepted. Since it scored more than the average, that would make the IQ > 100, especially since college applicants have an IQ greater than average, and especially since Japanese are smarter than average humans. http://gizmodo.com/an-ai-program-in-japan-just-passed-a-college-entrance-e-1742758286

The Wall Street Journal reports that the program, developed by Japan’s National Institute of Informatics, took a multi-subject college entrance exam and passed with an above-average score of 511 points out of a possible 950. (The national average is 416.) With scores like that, it has an 8 out of 10 chance of being admitted to 441 private institutions in Japan, and 33 national ones.

",artificial-intelligence
Create 2 CAD files,"I found CAD files for the Create on the ROS TurleBot download page (.zip),
and shells on the gazebo sim page. 
Any ideas where the files for the Create 2 could be found?
","design, irobot-create"
What are hardware components to build a modular robot which consists of several 5x5x5cm modules?,"I am computer science student and I have no knowledge on robotics. 
In my project, I am trying to find controllers for modular robots to make them do specific tasks using evolutionary techniques. For the moment I am doing this in a simulator, but if I want to make physical robots I have to know a priori the components to add to the robot, where do I place them, especially if modules of robot are small (cubes of 5*5*5cm)...
So my questions are:

What are must have components to make physical robot ? (arduino, batteries, sensors, ...)
For a small robot how many batteries do I need ?
If modules have to communicate with wifi, do I have to put a wifi card on each module?
I want to add an IMU. Is its position important, I mean do I have to put it in the middle of the robot ?

Thank you very much.
","arduino, mechanism"
Sizing high current power supplies for large robots,"I'm a researcher in a lab that's starting work on some larger humanoid/quadruped robots as well as a quadcopter. Currently, we have several power supplies that have a max rating of 30V/30A and our modified quadcopter easily maxes out the current limit with only half of its propellers running. It seems like most power supplies are meant for small electronics work and have fairly low current limits. I think that I want to look for power supplies that are able to provide between 24-48V and higher than 30A for an extended period of time. 
1.) Is this unreasonable or just expensive? 
2.) Do most labs just connect PSUs in series to get higher voltages?
Thanks for the input.
","quadcopter, power, humanoid"
Angular velocity to translational velocity,"I have a 3D point in space with it's XYZ Coordinates about some Frame A. I need to calculate the new XYZ coordinates, given the angular velocities of each axis at that instant of time about Frame A
I was referring to my notes, but I'm a little confused. This is what my notes say:
http://imgur.com/yJD8OUi
As you can see, i can calculate the angular velocity vector w given my angular velocities. But I'm not sure how this translates to how to calculate my new XYZ position! How can i calculate the RPY values this equation seems to need from my XYZ, and how can i calculate my new position from there
",mobile-robot
Real-time video processing on video feed from a drone's camera,"I am working on a project where I want to run some computer vision algorithms (e.g. face recognition) on the live video stream coming from a flying drone. 
There are many commercial drones out there that offer video streams, like

http://www.flyzano.com/mens/
https://www.lily.camera/
etc..

But none of them seem to give access to the video feed for real-time processing. 
Another idea is to have the drone carry a smartphone, and do the processing on the phone through the phone's camera. Or just use a digital camera and an arduino that are attached to the drone. 
Although these ideas are feasible, I would rather access the video-feed of the drone itself. So my question is that are there any drones out there that offer this feature? or can be hacked somehow to achieve this? 
","computer-vision, cameras"
What is the easiest yet precise method one can make a track and a train,"I would like to put a train on a track and control its movement with high precision left and right using a wireless controller.
What is the best way to do it?
",arduino
Switching scheme for vector controlled pmac drive,"I have 28  pmac motors (3-ph, 230 volt of 0.5 kw, 1.05kw an 1.21kw) in motor control center. Please suggest a time staggered switching scheme in order to avoid tripping due to voltage sag, swell , flicker etc
",power
Solar panel set rotation: How to achieve both vertical and horizontal rotation?,"
How can we achieve this kind of rotation to enable maximum trapping of solar rays during the day?
",mobile-robot
Is there a way to turn the sound off of a Roomba?,"I am working with an iRobot Create 2 and I work with others around me. Whenever I turn the robot on, send it an OI reset command, etc., it makes its various beeps and noises. I would like to not have this happen since I find it a little annoying and I'm sure those who have to work around me would like to have things quiet so they can concentrate on their work. Is there a way to accomplish turning off the beeps (while still being able to easily re-enable them), or am I out of luck?
","irobot-create, roomba, digital-audio"
How to detect when a stepper motor has stalled?,"How can I detect when a stepper motor has stalled?
A google search led me to some people who say that
when the stepper motor stalls, the current spikes up,
which is easily detectable with a Hall sensor.
(Or, I suppose, by any of the other current sensors mentioned at
""How can I sense the motor's current?""
).
However, I measured the current through (one of the 4 wires of) my stepper motor,
and it's always within a few percent of 0.5 A, whether my stepper driver is holding one position, moving it normally (which in my application is very slowly), or the stepper driver thinks it is telling the stepper to move normally, but the motor has pegged out against the hard limit.
Measuring the current in the +12V power supply going to the stepper motor driver, also seemed to give a fairly constant current.
This may be because I turned down the current limit to that amount on my ""chopper"" stepper motor driver.
Am I missing some key detail in the ""measure the current"" approach?
A google search led me to some other people that measure the back-EMF (BEMF) in one coil of the stepper during the time the stepper driver is only driving the other coil.
But that only seems to distinguish between ""a motor moving quickly"" vs ""a motor stopped"", and doesn't seem to distinguish between my case of ""a motor moving slowly"" vs ""a motor stopped"".
Is there some way to apply the BEMF approach even in a system where I always drive the stepper slowly, and never spin it quickly?
I'm currently using a stepper driver board with the TI DRV8825 chip on it, and I hoped the ""fault"" pin would tell me when the stepper motor has stalled against my hard stop.
But it doesn't seem to be doing anything -- is it supposed to tell me about a stall, but I just have it wired up wrong?
Is there some other chip or drive technique that detects when the stepper has stalled out against the hard stop?
Is there some other technique for detecting a hard stall that I can ""add on"" to a system using an off-the-shelf stepper motor driver?
(Is there some other StackExchange site that is more appropriate for questions about motors and motor drivers?)
","stepper-motor, stepper-driver, force-sensor"
Torque control in eye-in-hand visual servoing,"In most papers about IBVS the camera velocity is computed and then used as a pseudo-input for the manipulator. (e.g. this one) Is there any work in which the dynamic Lagrange model $H(q) \ddot q +C(q,\dot q)\dot q+g(q)=\tau$ of the manipulator is taken into consideration in order to compute the torque required to move the joints accordingly?
","dynamics, torque, visual-servoing"
Relationship between earth frame attitude and acceleration for a quadcopter,"For a quadcopter, what is the relationship between roll, pitch, and yaw in the earth frame and acceleration in the x, y, and z dimensions in the earth frame? To be more concrete, suppose roll ($\theta$) is a rotation about the earth frame x-axis, pitch ($\phi$) is a rotation about the earth frame y-axis, and yaw ($\psi$) is a rotation about the z-axis. Furthermore, suppose $a$ gives the acceleration produced by all four rotors, i.e. acceleration normal to the plane of the quadcopter. Then what are $f, g, h$ in
$$a_x = f(a,\theta,\phi,\psi)$$
$$a_y = g(a,\theta,\phi,\psi)$$
$$a_z = h(a,\theta,\phi,\psi)$$
where $a_x$, $a_y$, and $a_z$ are accelerations in the $x$, $y$, and $z$ dimensions.
I've seen a number of papers/articles giving the relationship between x,y,z accelerations and attitude, but it's never clear to me whether these attitude angles are rotations in the earth frame or the body frame.
","quadcopter, dynamics"
Why does it require more force to turn a servo if it is electronically connected to another servo?,"I have two servo motors that I rigged up to use as a telescope remote focuser. The idea is to turn one servo by hand and use the power generated to turn the other, which is geared to a telescope focuser knob. I noticed that when the two servos are electrically connected, it is noticeably harder to turn a servo compared to turning it by itself. I tried changing the polarity of the connection hoping it would help, but it is still harder to turn the servo when they are connected. Does anyone know why this is?
","servos, servomotor"
Removing PCB from a Dynamixel RX-24F servo?,"For a mod on the Dynamixel RX-24F I need to remove the enclosed PCB. I removed all screws but the PCB doesn't come out easily (without applying more force than I'm comfortable with). It seems to be stuck on the three large solder points in the white area. Has someone experience with this particular servo? 
It might be glued/soldered to the case, but I'm not quite certain. Any help is appreciated.

",dynamixel
Joint Space Singularities,"I would like to clarify my self on singularity configurations. If I am moving the robot in joint space only one joint at a time, can I come to a singular configuration? If so how?
Thanks
","joint, jacobian"
"Using an RGB + Depth Camera to locate X,Y,Z coordinates of a ball","I've recently been trying to use Gazebo to do some modelling for a couple tasks. I have a robot that's effectively able to locate a ball and get x,y coordinates in terms of pixels using a simple RGB camera from the Kinect. I also have a point cloud generated from the same Kinect, where I hope to find the depth perception of the ball using the X,Y coords sent from the circle recognition from my RGB camera. My plan earlier was to convert the X,Y coordinates from the RGB camera into meters using the DPI of the Kinect, but I can't find any info on it. It's much, much harder to do object recognition using a Point Cloud, so I'm hoping I can stick to using an RGB camera to do the recognition considering it's just a simple Hough Transform. Does anybody have any pointers for me?
","localization, computer-vision, kinect, cameras, gazebo"
One propeller Drone?How well it works?Hope,"Can a one propeller drone work efficiently for a good flight and stable camera footage in a drone flight
",quadcopter
What is inverse depth (in odometry) and why would I use it?,"Reading some papers about visual odometry, many use inverse depth. Is it only the mathematical inverse of the depth (meaning 1/d) or does it represent something else. And what are the advantages of using it?
","slam, computer-vision, odometry"
What is the easiest and efficient way to detect human in close range distance and make the robot follow it?,"I am having a thesis right now regarding a robot. My research requires the robot to be attached to linear guide rail. A robot has to detect human in a very close range (of about 2 meters distance). What easiest and efficient method or components shall I use?
","sensors, wheeled-robot, industrial-robot"
3 DOF Inverse Kinematics Implementation: What's wrong with my code?,"I am currently trying to implement an Inverse Kinematics solver for Baxter's arm using only 3 pitch DOF (that is why the yGoal value is redundant, that is the axis of revolution). I for the most part copied the slide pseudocode at page 26 of http://graphics.cs.cmu.edu/nsp/course/15-464/Fall09/handouts/IK.pdf .
def sendArm(xGoal, yGoal, zGoal):
    invJacob = np.matrix([[3.615, 0, 14.0029], [-2.9082, 0, -16.32], [-3.4001, 0, -17.34]])
    ycurrent = 0
    while xcurrent != xGoal:
        theta1 = left.joint_angle(lj[1])
        theta2 = left.joint_angle(lj[3])
        theta3 = left.joint_angle(lj[5])
        xcurrent, zcurrent = forwardKinematics(theta1, theta2, theta3)
        xIncrement = xGoal - xcurrent
        zIncrement = zGoal - zCurrent
        increMatrix = np.matrix([[xIncrement], [0], [zIncrement]])
        change = np.dot(invJacob, increMatrix)
        left.set_joint_positions({lj[1]: currentPosition + change.index(0)/10}) #First pitch joint
        left.set_joint_positions({lj[3]: currentPosition + change.index(1)/10}) #Second pitch
        left.set_joint_positions({lj[5]: currentPosition + change.index(2)/10}) #Third Pitch joint


def forwardKinematics(theta1, theta2, theta3):
    xcurrent = 370.8 * sine(theta1) + 374 * sine(theta1+theta2) + 229 * sine(theta1+theta2+theta3)
    zcurrent = 370.8 * cos(theta1) + 374 * cos(theta1+theta2) + 229 * cos(theta1+theta2+theta3)         
    return xcurrent, zcurrent

Here is my logic in writing this:
I first calculated the Jacobian 3x3 matrix by taking the derivative of each equation seen in the forwardKinematics method, arriving at:
[370cos(theta1) + 374cos(theta1+theta2) .....   
0                                               0                      0
-370sin(theta1)-374sin(theta1+theta2)-......                            ]
In order to arrive at numerical values, I inputted a delta theta change for theta1,2 and 3 of 0.1 radians. I arrived at a Jacobian of numbers:
[0.954  0.586   .219
0.0000          0.000         0.0000
-.178   -.142   -0.0678]
I then input this matrix into a pseudoinverse solver, and came up with the values you see in the invJacob matrix in the code I posted. I then multiplied this by the difference between the goal and where the end effector is currently at. I then applied a tenth of this value into each of the joints, to make small steps toward the goal. However, this just goes into an infinite loop and my numbers are way off what they should be. Where did I go wrong? Is a complete rewrite of this implementation necessary? Thank you for all your help.
","inverse-kinematics, python, joint, jacobian"
Accelerometer calibration - how to get cross-axis sensitivities,"I've already asked a related question (accelerometer bias removal) here on robotics and got a bit better results on corrected accelerometer output. To get even better results I found the calibration equations (7th & 8th paragraph) from Vectornav which are just a bit enhanced than the solution in the linked question:
 
However, six more variables are needed:

Sensitivity of sensor X-axis to Y-axis inputs ($M_{xy}$)
Sensitivity of sensor X-axis to Z-axis inputs ($M_{xz}$)
Sensitivity of sensor Y-axis to X-axis inputs ($M_{yx}$)
Sensitivity of sensor Y-axis to Z-axis inputs ($M_{yz}$)
Sensitivity of sensor Z-axis to X-axis inputs ($M_{zx}$)
Sensitivity of sensor Z-axis to Y-axis inputs ($M_{zy}$)

Below it is also stated:

IEEE-STD-1293-1998 [...] provides a detailed test procedure for
  determining each of these calibration parameters

However, after searching through the 1293-1998 standard (especially page 201 in Google Docs) I didn't find any clue on how to calculate the $M$ values. Also, $B_{d}$ and $V_x$ values from Vectornav equations is not explained anywhere. Can someone point me further?
","sensors, accelerometer, calibration, errors"
Forward Kinematics/D-H parameters for perpendicular joint axes,"I am trying to compute forward kinematics of the Kuka youBot using DH convention: 
http://www.youbot-store.com/youbot-developers/software/simulation/kuka-youbot-kinematics-dynamics-and-3d-model
The arm joint 1 and arm joint 5 are revolute and rotate about the world z-axis (pointing to the sky)
But the other 3 joints are all revolute and rotate about x-axis, let's say (points horizontally)
DH convention says the ""joint distance"" is along the ""common normal"". But unless I am mistaken, the only common normal is the y-axis, and that is also horizontal, meaning there is no joint distance.
I was thinking I would use link offset for joint1 - joint2, but then I ran into a problem with joint4 - joint5. Link offset is supposed to be along the previous z-axis, and in that case it would point horizontally out to nowhere. But link distance STILL doesn't work either, because that is the common normal distance, and as established the common normal is x-axis, also horizontal. So now I feel very screwed. I am sure there is a simple solution but I can't see it. 
So I guess the question is, how do I use the DH convention for the links between 1-2 and 4-5, when the joint rotational axes are perpendicular?
","kinematics, forward-kinematics, dh-parameters"
EKF-SLAM initialize new landmark in covariance matrix,"I am trying to implement an EKF-SLAM using the algorithm for unknown correspondences proposed in the book ""Probalistic Robotics"" by Sebastian Thrun in Table 10.2 . 
By now I understand actually all of the algorithm except of the initialization of new landmarks in the covariance matrix $ P_{new} $. 
In that algorithm when a new landmark is detected the procedure is just the same as if a normal measurment update for an already observed landmark is done: the Kalman gain $ K $ is calculated for the new landmark and then the covariance is updated with that Kalman gain and the jacobian $ H $ of that new landmark like this $ P_{new}=  (I  -  K * H) * P$ . 
In my understanding a just new observed landmark would not have any effect on the rows and columns that correspond to already mapped landmarks or the robot pose in the covariance matrix. Instead I think that just two rows and columns for x and y should be created with some uncertainity like proposed here: the uncertainty of initializing new landmark in EKF-SLAM .
I tried to split down the calculation of $ P_{new}$ via claculating it blockwise to see if I could somehow come to the same initialization as shown in the link above. But I end up having a different covariance matrix where apparently the new landmark is effecting the rows and columns of the parts of the old covariance, which in my view can't be right.
I hope I don't understand the pseudo code of the book wrong or I did a mistake in my try to come to the same initialization. Any advice how the initialization of new lnndmarks work in that code or if it actually is the same as in the link will be appreciated.
Edit
So basically what I am asking is: why would they do a normal Kalman update of the covariance matrix in line 24 of table 10.2 for a new observed landmark? Why is there no explicit case for the initialization of new rows/columns of new observed landmarks in the covariance matrix? It seems to me like they just do a normal measurement update even for a just newly observed landmark.
","mobile-robot, slam, ekf"
kk2.1.5 gyro bubble is not at centre,"I have a quadcopter controlled through KK2.1.5 flight controller. I have been flying with it without problems, but now i am facing a problem. When i start and arm the kk2.1.5, by giving throttle it starts turning towards some direction with acceleration. I double checked the motor pins locations and all other things, they are correct. When i took a look to the gyro bubble of kk2.1.5 it wasnt at mid of the crosshair. I turned the quad off and then on. I check bubble again it was at centre. now again when i gave it throttle it started turning towards some direction. I checked the again, and it wasnt on centre this time too. So at the armed state the gyro bubble moves away from centre by giving throttle. due to which quad overcorrects itself. Now i have understood that the gyro is off centre due to vibration  of the FC. What should i do to antivibrate it. What material should i keep in between so that vibrations are almost zero.
",quadcopter
Is it possible to make Kite flying robot?,"This question was asked in electronics stackexchange. I want to know if is it possible to make a robot that can fly kites. Is this idea practical? I was thinking that making a kite is just like making some flying quadcopter or helicopter. I just want to know is this idea really implementable?  Is there an example or similar work in reference to this?     
","control, quadcopter, radio-control"
Razor IMU Arduino interfacing,"I was looking into the  Razor IMU from Sparkfun, and realized that the only example code on sparkfun's website was for it was meant for hooking it up to a computer (the AHRS head tracker used a serial to usb chip). I looked on google and saw nothing but stories about how it did not work. 
Is there any good way to hook up the  Razor IMU to an arduino uno (or any arduino without hardware support for more than one serial port), and if so does example code exist?
","arduino, imu"
can we detects animals through PIR(passive infrared sensors),"as i dont know that what kind of radiation animals emit. as humans emit IR radiations so PIR sensors help to identify humans. pls suggest me if someone have knowledge about sensors which detects animals.
","arduino, raspberry-pi, first-robotics"
Cannot command irobot create 2,"This might be a dumb question. I have started to play with this robot with raspberry pi two days ago. I did some simple stuff, like- move around and sensor reading etc. But since yesterday night, It seems like I cannot send any command. The built in clean, dock functions are working perfectly but I cannot do anything using the same python code that I already used before. Its behaving like nothing is going through the Rx.
Can you suggest what might go wrong? Thanks
",irobot-create
Solving for DH Parameters,"Given three sets of joint angles in which the end effector is in the same position, is it possible to find the DH parameters?
If the robot has 2 DOF in shoulder, 2 DOF in elbow, and 1 dof in wrist, with DH parameters as upper arm length, elbow offset in 1 axis, lower arm length, can this be solved, if so how?
I tried iterating through DH parameters to minimize position of end effector with forward kinematics, but this doesnt seem to work as DH parameters of 0 for everything makes 0 minimal distance.
Reason for this; given a physical robot, no DH parameters are known, and measuring by hand is not accurate.
",dh-parameters
Power on iRobot Create 2 via serial port,"I have Arduino talk with Create 2 via serial interface. But before sending commands to the robot, I have to power it on by manually pushing the power button on the robot. How to make the robot turned on via that mini din 7 port, instead of pushing that power button? 
I notice when plugin iRobot serial-2-USB cable into that port on the robot, the robot is immediately turned on, ready for received the first command (command 128), so apparently there is way to turn on the robot via that port.
",irobot-create
6DOF robot arm: Velocity of end effector vs. joint velocities,"I have a 6 DOF arm whose velocities I'm controlling as a function of force applied to the end effector. The software for the robot allows me to input either the desired end effector velocity or the desired joint angular velocities, which I know can be found using the inverse Jacobian.
Are there any benefits of using one scheme over the other? Would, for example, one help avoid singularities better? Does one lead to more accurate control than the other does?  
","robotic-arm, jacobian, force-sensor"
using a device with os instead of microcontrollers,"Im working on a robot that needs image processing to analyze data recieves from cameras. 
As i searched for ARM and AVR libraries i found that there is no dip library for these micros and their limited ram is hard for image data process. i want to know is there any hardware that connects to a win or android or... devices and make it possible to that device connect to actuators and sensors?
thank you or helping.
",microcontroller
Using DC motor as a generator to recharge battery of my robot,"I am trying to recharge my 12V lead acid battery with a 12V DC motor.  I am using the battery to power the robot when it climbs. When it descends, I notice that I dont need to apply reverse voltage but the dc motor just backdrives instead. This can act as generator to recharge back the battery, am I right?
I know that i need to step up the low voltage that is generated by the backdriven motor to 12V needed to recharge the battery. This is the board that I think can do the job:
https://www.pololu.com/product/799
Is this all I need to make it work? With this method, should I be concerned about the 3 stages of battery charging: bulk, absorption and float?
Please advise. Any feedbacks are greatly appreciated.
","mobile-robot, battery"
Very low output voltage at the output of L298n?,"I am using arduino and L298n motor driving IC to drive 4 12V dc motors (150rpm).
Also I am using 11.1V LiPo battery (3cell, 3300 mAh, 20C).I have connected two PWM pins of L298n to digital HIGH from arduino.Battery positive terminal is connected to the 12V input of IC.Battey negative terminal and arduino ground is connected to the ground input of IC.Also a 5V input is given from arduino to IC and ground from arduino is connected to other gnd pin which is adjacent to INT3 pin.Motor1 pins from L298n are connected with two motors (connected parallely on right side of bot) and Motor2 pins are connected with other two motors (connected parallely on left side of bot).Appropriate inputs are given to INT1,INT2,INT3,INT4 to drive the bot in forward direction.But the bot is moving too slowly.The voltage measured across motor1 pins is only 5V.I have connected the battery directly to the motors,then it is running very fast.How to run it fast.Please help.....

","arduino, motor"
How to drive robot without driving actuator?,"I am participating in a robotics competition. I am supposed to design and build two robots. Out of these, one cannot have a driving actuator (it can have a steering actuator though, fed by a line following circuit). The other is supposed to drive the non-driving robot through an obstacle course, without touching it. This is kind of driving me crazy since at one point the separation between the two robots is 60 cm (23 inches).
Ways I've considered:

Wind Energy (wont work, need huge sails)
Magnetic Repulsion of some sort

Now repulsion I've spent a lot of time studying. 
My solution was to use strong permanent magnets on the non-driving robot (Neodymium,N52) and electromagnets on the driving robot.
But, after doing a huge load of calculations came to the conclusion that not enough force can be transmitted over the distance as magnetic fields fall off too quick.
Rulebook: http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip
I am really looking for even a pointer here. Is there a trick somewhere that I am missing?
",line-following
Manipulator link applied torque,"I want to implement a manipulator link using a physic library. I can only apply some torque to the centre of mass, but the torque should be applied at the beginning of the link.
Shifting a reference frame from the centre of mass and recalculating inertia tensor in the new frame is not a problem, neither is recalculating a new torque, based on the change of distance, but I think it is not the correct solution.
In short, how can I scale a torque of a control signal applied at the beginning of the link to a torque of a physic simulation applied to the centre of mass. Thanks.
","simulator, torque"
"IMU rotate about one axis, other two angles change too","I am trying to use Invensense's MPU9250. I am using provided library to read euler angle. When the IMU rotates about one axis, angles about other two axes change too. What could be potential cause to it? 
",imu
Changing STM32 Nucleo Board's Microcontroller,"I have a STM32F072RB Nucleo Board which has a 64Pin Microcontroller.
For my application I chose the sTM32F103RG which has a bigger RAM size and Flash size too.
Can i Remove an F072R from a Nucleo board put a F103R on top of it?
I am testing my code with a F103C, but the flash and ram size is not meeting my requirement. I have a F072R Nucleo Board lying around so for a quick developmental test could I swap it for the 103R ? The R series is Pin Compatible!
Anyone Has done microcontroller swapping before? 
",microcontroller
how to plot $\pm 3 \sigma$ of a landmark in EKF-SLAM,"I have implemented 2D-SLAM using EKF. The map is based-feature in which there is only one landmark for the sake of simplicity. I've read some papers regarding this matter. They plot the $\pm3\sigma$ plus the error. I would like to make sure that I'm doing the right thing. In my project, I have the estimate of the landmark's position and its true values. The true values here are the ones that the sensor measure not the ideal case. For example, the ideal case of the landmark position  is (30,60) but this value is not accessible by any means, therefore I will consider the true values the ones that are coming from the sensor. 
Now the error in the landmark's position in x-axis is formulated as follows 
$$
\text{error}_{x} = \hat{x} - x
$$
The below picture shows the error in blue color. The red color represents the error bounds which is $\pm 3 \sigma_{x}$
My question is now is this the way people plot the errors in the academics papers because I've seen some papers the bounds doesn't not look like mine. Even though mine decreases monotonically however in some papers it is more curved and it seems more reasonable to me. Any suggestions?

","slam, ekf, simulation, mapping"
How much of a pause should there be between messages? (IRobot Create-2),"When I send several commands in a row some don't get executed. For example I have a script which starts the roomba driving in a circle and plays the john cena theme song through its speakers but sometimes it will only play the music and not drive. I have noticed that in all the guides there are pauses after every command. Is there any documentation which describes when pauses are needed?
",irobot-create
Trying to calculate the Thrust of my quadcopter motors,"I am trying to Calculate the thrust my 4 quadcopter motors will have.
I am not sure how to do it. Here are the parts I am Using
4S 6600mAh 14.8V Lipo Pack 
15x5.5 Prop
274KV motor max output is 28A
ESC 35 Amp
Thank You
",quadcopter
Relation between pole placement and marginal stability?,"I'm given an assignment in which I have to design a full state feedback controller by pole placement. The state space system is fully controllable and I've been using Matlab/Simulink to determine the required feedback gain K using the place() command for several sets of poles, however once I use poles that are ""too negative"", for example p=[-100,-200,-300,-400,-500], my controlled system starts showing bounded oscillatory behaviour. 
Is it possible that too negative poles can cause marginal stability? And if so, why? I've read that this is only possible when the real part of one or more poles equals 0, which certainly isn't the case here. 
","control, stability"
Filtering IMU angle discontinuities,"I try to measure Euler angles from an IMU, but some discontinuities happens during measurement, even in vibrationless environment, as shown in the images below. 
Can someone explain which type of filter will be the best choice to filter this type discontinuities?



","sensors, imu, matlab, noise, filter"
Designing a 5 bar linkage robot: Plot Clock,"I am a beginner at robotics. 
I recently stumbled across this robotic clock on youtube.
I am an electrical engineering student and am interested in submitting it as my minor project.
I have studied the basics on forward and inverse kinematics, Greubler's Equation, four bar linkage but this robot seems to be a 5 bar linkage. I want to know how to implement it in a 5 bar linkage.
How to use the inverse kinematics solutions described in Combined synthesis of five-bar linkages and non-circular gears for precise path
generation, to make the robot follow desired trajectory?
I have been stuck at this for days... any sort of help would be appreciated.
","robotic-arm, beginner, first-robotics"
Need an idea: automated sim card switcher,"First off, sorry if my question is too naive or not related to the forum (this is the best matching one I've found on StackExchange).
I have some amount of SIM-cards. I can programmatically access a single SIM-card if it is inserted into a USB-modem. I want to be able to access the specified card in the set. The best way to achieve this I can think of is to create a device that would somehow replace the current card in the modem with one in the set. I can not use several modems for this because I don't really know the amount of cards and I would like to automate this process anyway.
I am more of a programmer than an engineer so everything that follows (including the entire concept of switching cards) looks pretty weird to me. There probably is a better solution, but this is the best I've come up with. For now I consider building some sort of conveyor that would move cards and insert the ones I need with some sort of a feed device. This looks like an overkill to me that would be both expensive to build and uneffective to work with.
I want an idea of a device that would replace SIM-cards into the modem (or maybe a better solution to the problem). Any disassembly of a modem needed is possible.
This is required to automate receiving SMS from clients that have different contact phones. Unfortunately, a simple redirection of SMS is not an option.
","automatic, automation"
Transforming angular velocity?,"I have the following system here:
http://imgur.com/UTqswOi

Basically, I have a range finder which gives me $R_s$ in this 2D model. I also have the model rotate about the Centre of Mass, where I have angular values and velocities Beta ($\beta$) and BetaDot ($\dot{\beta}$).
I can't see, for the life of me, how to figure the formula for the angular velocity in the Range Finder frame. How am I supposed to do this? I have all the values listed in those variables. The object there doesn't move when the vehicle/system pitches. It's stationary.
","robotic-arm, sensor-fusion"
Multiple EKFs or one big,"Let's say I would like to use an EKF to track the position of a moving robot. The EKF would not only estimate the position itself but also variables affecting the position estimate, for example IMU biases, wheel radius, wheel slip and so on.
My question is, is it better to use one big EKF (state vector containing all estimated variables) or multiple smaller EKFs (each one responsible for tracking a subset of all variables to be estimated)? Or is there no difference?
As for the example above, the EKF could be split into one for tracking position, one for estimating wheel radius and slip and one for estimating IMU biases. The position EKF would of course use the estimations output from the other concurrent EKFs and vice versa.
To me it seems it would be easier to tune and test multiple smaller EKFs rather than just one big. Are there any other advantages/disadvantages (execution time, ease of debugging etc.) assuming the resulting estimates are equal in the two approaches (or close enough at least)?
Thanks,
Michael
","kalman-filter, ekf"
Why do 3-axis accelerometers seemingly have a left-handed coordinate system?,"Careful inspection of page 35 (figure 58) of the ADXL345 datasheet shows that under gravitational loading only, the chip uses a left-handed coordinate system.  My own experiments with this chip confirm this.  
I typically only use the chip to indicate the gravity vector.  So when using this chip, I simply negate the values to get a right handed coordinate system.  But this doesn't seem right.  I assume there is a logical and mathematical explanation for the left-handed coordinate system but i can't figure out what it might be. 

","sensors, imu, accelerometer, calibration"
Is modelling a robot and deriving its Equations of Motions more applicable to a system that is inherently unstable?,"As someone who is new and is still learning about robotics, I hope you can help me out.
Let's say I have two systems: 

(a) Inverted Pendulum (unstable system)
(b) Pole Climbing Robot (stable system)

For system (a), I would say that generally, it is a more dynamic system that produces fast motion. So, in order to effectively control it, I would have to derive the Equations of Motions (EOM) and only then I can supply the sufficient input to achieve the desired output. Eventually, the program will implement the EOM which enables the microcontroller to produce the right signal to get the desired output.
However for system (b), I assume that it is a stable system. Instead of deriving the EOM, why cant I just rely on the sensor to determine whether the output produced is exactly what I want to achieve? 
For unstable system, controlling it is just difficult and moreover, it does not tolerate erratic behavior well. The system will get damaged, as a consequence. 
On the contrary, stable system is more tolerant towards unpredictable behavior since it is in fact stable.
Am I right to think about it from this perspective? What exactly is the need for deriving the EOM of systems (a) and (b) above? What are the advantages?  How does it affect the programming of such systems?
Edited:
Some examples of the climbing robot that I'm talking about: 

i.ytimg.com/vi/gf7hIBl5M2U/hqdefault.jpg
ece.ubc.ca/~baghani/Academics/Project_Photos/UTPCR.jpg

","mobile-robot, control"
What is required to build a simple XY-stage?,"In the scope of my PhD, I would like to build an automated microscopy set-up that should take images of a sample of 2cm by 2cm. This should be done by taking pictures of 500 micrometers by 500 micrometers.
Therefore I need to design an XY-stage moving my sample over the optical setup. I would use a Raspberry Pi to steer all the hardware. 
Could you direct me to material about how to best make an XY-stage ? My questions are about what types of motors to use (stepper?), how many, how to create a good sliding mechanism to avoid jerky steps, etc. 
Simple links to basic engineering of such set-ups would be more than enough for me to start, as I am a complete layman in this field.
EDIT: I have found this blogpost. It does what I require, if I get small enough angle step stepper motors.
EDIT2: I need a maximal range of motion of 10 cm in both directions. The overall size should not exceed 30x30 cm^2. Step sizes should not exceed 10 microns. I do not care about moving speed. Based upon the design in the link, buying a stepper motor with a 100:1 gear box could allow my very small radial steps (<0.05 deg) which would result in about 5 micron steps, assuming a rotor radius of about 1cm.
As far as price goes, it should not exceed commercially available options which start at about 5k USD
","raspberry-pi, stepper-motor"
What is the required theory behind building a robotic arm?,"I am currently planning on building a robotic arm. The arm's specs are as follows: 

3 'arms' with two servos each (to move the next arm)
single servo clamp
mounted on revolving turntable
turntable rotated by stepper motor
turntable mounted on baseplate by ball bearings to allow rotation
baseplate mounted on caterpillar track chassis
baseplate is smaller in length and width than caterpillar chassis

What are the required formulas in determining how much torque each servo must produce, keeping in mind that the arm must be able to lift weights of up to 1 kilogram? Also, considering that the ball bearings will take the load of the arm, how strong does the stepper have to be (just formulas, no answers)?
As far as overall dimensions are concerned, the entire assembly will be roughly 255mm x 205mm x 205mm (l x w x h). I have not finalized arm length, but the aforementioned dimensions give a general estimate as to the size. 
","arduino, robotic-arm, stepper-motor, rcservo, torque"
Stereo vision in Matlab,"I am working on a project about robot soccer vision. 
How I utilize two webcams as a stereo vision in matlab for robot soccer matters?
","stereo-vision, matlab, soccer"
Need a mobile robot simulator that provides easier odometry funtions,"I want a mobile robot to go from a starting position to a goal position. But, I don't want to calculate the pose from encoders. Instead I want to know if there exist such a simulator that provides pose function that makes the work easier, like go_to(x_coordinate, y_coordinate). That means, the robot will automatically calculate its current position and leads itself to the goal position. 
","mobile-robot, odometry, simulation"
Advanced Line Following Robot of Maze Solving,"I know how to make a line follower. But in this video what have they done exactly? They are giving the source and destination in the map but how the robot moves based on the instruction given in map?
What is the procedure to do it? They have mapped the path. Please do watch the video.
","wheeled-robot, mapping, line-following"
Are there any aerodynamics modeling/simulation software that is capable to consume a SolidWorks model and to interface with MATLAB/Simulink?,"Currently I am developing a control system for an aircraft of a unique design (something in between a helicopter and a dirigible). At this moment I can model only the dynamics of this vehicle without any aerodynamic effects taken into account. For this I use the following work-flow:
Mechanical model in SolidWorks -> MSC ADAMS (Dynamics) <--> MATLAB/Simulink (Control algorithms)
Thus, the dynamics of the vehicle is modeled in ADAMS and all control algorithms are in MATLAB/Simulink. Unfortunately, ADAMS can not simulate any aerodynamic effects. As a result, I can not design a control system that is capable to fight even small wind disturbances.
","control, simulator, uav, matlab, simulation"
Sourcing motors by physical dimensions,"I have a 1inch square tube that I would like to place a motor into. 
The motor I have takes up approximately 1/2 of the available space (roughly 3/4 inch I.D.) I would like to find the largest motor that will fit in the space without having to cobble too much of a housing. 
Where/how can i find motors by physical dimensions?
",motor
How to connect an Arduino Uno to an Android phone via USB cable?,"Is it possible to set up communication between an Arduino Uno and an Android phone using a wire that directly connects the Android phone and the Arduino?
","arduino, usb"
What is torque bandwidth in actuated joints and how does it affect the control systems?,"The rest of my student team and I are in the process of redesigning an exoskeleton and building it based on an existing one. From the papers that we have been reading there are some references to low, high and zero impedance torque bandwith.
What is that? Does it have to do with the control system?
It is measured in Hz. Here is a table from one of the papers:

","control, design, mechanism, joint"
Where should I put the angle sensor on my cart-pole robot?,"I'm using an accelerometer and gyroscope to detect the angle and tilt rate on my two-wheeled cart-pole robot.  Is there an optimal height to place the sensors?  Should I place them closer to the bottom (near the wheels), the middle (near the center of mass), or the top?  Justification for the optimal choice would be appreciated.
","sensors, balance"
Is it possible to do SLAM with few IR sensors like Buddy?,"I saw Buddy's page and want to purchase for my SLAM research. However, I wonder is it possible to program Buddy for SLAM? 
According to Buddy's spec, they're only few IR's, sonars and a camera. As I know, most SLAM algorithms are implemented with powerful sensors such as RGBD/stereo camera, or even laser range finder.
Are there any pepers mention about IR-based SLAM?
","mobile-robot, localization, slam, mapping, rangefinder"
"""Thermal Imaging"" with Arduino and/or Lego Mindstorm NXT 2.0?","I'm trying to build a robot that can be sent into rooms/buildings and detect people using nxt and/or Arduino. In addition to this I would like to be able to view what my robot is ""seeing"" in real-time on my PC as an infrared image. The sensors I've shortlisted for this are:

Thermal Infrared NXT Sensor from Dexter industries - £44
RoBoard RM-G212 16X4 Thermal Array Sensor - £94
Omron D6T MEMS Thermal IR Sensor - £31

I believe the RoBoard and Omron sensors are capable of thermography, so I was wondering if anyone here has experience with these sensors and give me some advice.
I was also thinking about using an idea from this project: www.robotc.net/blog/tag/dexter-industries.
In this case I'd use the data read from the sensor to plot a graph showing different temperatures.   
","mobile-robot, sensors, nxt"
Wifi module for Zumo robot,"I'm a CS student trying to implement a clustering algorithm that would work for a set of robots in an indoor controlled environment. I'm still starting on Robotics and don't have much experiencing in figuring out what will work together.
My plan is to get 6 of these Zumo robots and plug in a wifi module like the Wifi shield. Then, I would use this to do inter communication and execute my algorithm.
My question: Can the wifi module just be plugged in and would it work? If not, how can I go about achieving this task. I see lots of Arduino boards with different names and I'm not sure which works with which, and whether they can be plugged in. Any help would be appreciated.
","arduino, wifi"
MCU architecture design,"Are there any standards regarding single vs multiple MCU in a robotic system? More specifically, if a single MCU can handle all of the sensor data and actuator controls, is it better to use a single MCU or multiple MCUs in a hierarchical manner? Are there any references/papers regarding this topic? What are the arguments towards one or the other? I am looking for facts, not personal opinions, so pros, cons, standards and such.
",design
Should the therotical parameters match the physical setup constraints when modeling a robot?,"I'm working on modeling and simulation of robotic arm, after I obtained the mathematical model of the robot, I used that to implement some control techniques, to control the motion of the robot. The dimensions and masses of each links are taken from available kit, basically, it's RA02 robot with servo at each joint. After the modeling, different parameters, can be plotted: like the joint angles\speeds\torques ... etc. The point now is that, the value obtained for the joint toque is much more higher that the torque limit of the servo, does it mean my design\modeling is not realizable? Is it necessarily to get close (torque)  value for servo's torque? 
Any suggestion?
","control, robotic-arm, servomotor, dynamics, torque"
Ways to estimate the drift rate of the gyrometer,"I found not so much literature to the topic, this is why I ask here.
Does someone know some ways to estimate the drift rate of the gyrometer.
I was thinking about basically two approaches. 
One would be to use a low pass filter with a low cut-off frequency to estimate the drift of the angular velocity.
Second would be to use the accelerometer, calculate the attitude dcm and by this also the angular velocity. The difference between the acc angular velocity and gyrometer would be maybe also a drift rate.
Nevertheless, I am not so sure whether this is a good way to get reliable drift rates :D
","sensors, gyroscope"
How much should I expect a Kalman filter to converge?,"I am learning about Kalman filters, and implementing the examples from the paper Kalman Filter Applications - Cornell University.
I have implemented example 2, which models a simple water tank, filling at a constant rate. We only measure the tank level, and the Kalman filter is supposed to infer the fill rate.

According to the model, the fill rate is a constant, so I assumed that over time, the Kalman filter would converge more and more accurately (and with less and less noise) on the correct fill rate. However, the amount of noise in the fill rate never seems to reduce after the first few iterations:

This graph shows how the fill rate part of the state vector changes over the course of 1000 iterations of the simulation.
Adjusting the Measurement Variance Matrix seems to have very little effect on the fill rate noise.
Also, the Kalman gain vector and State Variance matrix seem to be constant throughout the simulation. I assumed that the State Variance would reduce as the filter became more and more confident in its state estimate.
Questions:
- Is this graph what I should expect to see?
- Should the Kalman Gain vector and State Variance matrix change over time in this situation?
",kalman-filter
"ROS MoveIt!, virtual joints, planar joints, prismatic joints","I do have a robotic application, where a 7Dof robot arm is mounted on a omnidirectional mobile platform. My overall goal is to get MoveIt! to calculate a sequence of joint movements, such that the robot EEF reaches a desired goal in Cartesian space.
In order to combine a robot platform with a world, the MoveIt! setup assistant lets you assign virtual joints between the ""footprint"" of the platform and the world it is placed in.
I do have two strategies. Either 

select a planar joint as a virtual joint. (What are the degrees of freedom or respectively the joint information that I can gather from this joint)

or 

select a fixed joint and add a (prismatic-x -> prismatic-y -> revolute-z) chain to the robot model.

Are there any significant differences (advantages/ disadvantages) to either of the approaches?
","mobile-robot, robotic-arm, ros, motion-planning"
Quad Copter flight module can replace with smart phone?,"I want to replace the flight module with smart phone because it has all sensors that are required, like gyroscope, magnetometer, etc. Is that possible?
I am using an Google Nexus 4 Android (OS model 5.1). I will control using another mobile, I am able write an app, with an Arduino acting as a bridge between smartphone and copter. I am using flight controller OpenPilot CC3D CopterControl.
","arduino, quadcopter"
DC Motors for a ROV?,"I am planning to build a homemade ROV, and I wanted to know a couple of things about the motors.  First is: Will it be Ok, if I use a brushed DC motor,  instead of a brushless motor, and is there any major disadvantages ? Second : What RPM DC motor should I aim for ? High RPM or low RPM ? Will 600rpm be enough ? The specific motor that I am talking about is    http://www.ebay.ca/itm/37mm-12V-DC-600RPM-Replacement-Torque-Gear-Box-Motor-New-/320984491847?hash=item4abc2aa747:m:mEBEQXXpqmNg4-vxmFaZP5w
Will this be a good motor for the propellers of the ROV.  I am planning to have 4 motors / propellers. Two for upward and downward thrusting,  and 2 for forward and side thrusting. The propellers that I plan to use, are basic plastic 3 blade propellers,  with diameter,  between 40mm and 50mm. 
My main question is, what RPM and torque should I aim for when choosing the DC motor ?
",motor
what's confidence level? and how can we use it for vehicle detection using OpenCV?,"I'm working on project for the Autonomous vehicle, and i want to know what's confidence level means and how can we use confidence level for vehicle detection in OpenCV ?
","opencv, statistics"
Solution for INS and GPS integration,"I have a GPS module and an IMU (gyro, accelerometer and magnetometer) and I need to build an autonomous navigation system for a quadcopter. It must know its position at any time so that it can track a predefined path. I know that, in order to improve precision, I need to merge both sensors data through a Kalman Filter (or any other technique for that matter, the thing is that the Kalman Filter is way more common according to my research).
The problem is that I am seriously stuck and I know this might be something very simple but I don't seem to find a solution or at least the answer for some of the most basic questions.
As a start, I know how to get the position from the accelerometer readings. I have some filters that help eliminate noise and minimize the integration errors. I also have the GPS readings in latitude and longitude. The first question is, during sensor fusion, how can I make both measurements compatible? The latitude and longitude from the GPS won't simply mix with the displacement given by the accelerometer, so what is the starting point for all of this? Should I calculate the displacement from the GPS readings or should I assume a starting latitude and longitude and then update it with the accelerometer prior to applying the filter?
I have once developed a simple Kalman Filter in which I could plug the new reading values to obtain the next estimate position of a two wheeled car. Now I have two sources of inputs. How should I merge those two together? Will the filter have two inputs or should I find a function that will somehow get the best estimate (average, maybe?) from the accelerometer and GPS? I am really lost here.
Do you guys have any examples of code that I could use to learn? It is really easy to find articles full of boxes with arrows pointing the direction in which data must flow and some really long equations that start to get confusing very soon such as those presented on this article: http://isas.uka.de/Material/Samba-Papierkorb/vorl2014_15/SI/Terejanu_tutorialUKF.pdf (I have no problems with equations, seriously) but I have never seen a real life example of such implementation.
Any help on this topic would be deeply appreciated.
Thank you very much.
","kalman-filter, sensor-fusion, gps"
Aligning datasets with drift,"I have a dataset that contains position information from tracking a robot in the environment. The position data comes both from a very accurate optical tracking system (Vicon or similar) and an IMU. I need to compare both position data (either integrating the IMU or differentiating the optical tracking data).
The main problem is that both systems have different reference frames, so in order to compare I first need to align both reference frames. I have found several solutions; the general problem of aligning two datasets seems to be called ""the absolute orientation problem"".
My concern is that if I use any of these methods I will get the rotation and translation that aligns both datasets minimizing the error over the whole dataset, which means that it will also compensate up to some extent for the IMU's drift. But I am especially interested in getting a feeling of how much the IMU drifts, so that solution does not seem to be applicable.
Anyone has any pointer on how to solve the absolute orientation problem when you do not want to correct for the drift?
Thanks
","sensors, localization, imu, calibration"
Accurate Wheeled Robot Odometry,"I'm looking for a ""good"" algorithm/model for wheeled odometry estimation. We have encoders on the two back wheels of the tricycle robot, and IMU on the controller board. Currently we use MEMS gyro for angular velocity estimation and encoders for linear velocity, then we integrate them to get the pose. But it's hard to calibrate gyro properly and it drifts (due to temperature or just imperfect initial calibration). How can we improve the pose estimation? Should we consider model that incorporates both encoders and gyro for heading estimation? Model slippage, sensor noise? Is there some nice standard model? Or should we just use more/better gyro? Not considering the visual odometry.
","kalman-filter, wheeled-robot, odometry"
3D Angular velocity to 3D velocity to predict next state,"I have a sensor that gives R, Theta, Phi (Range, Azimuth and Elevation) As such:
http://imgur.com/HpSQc50
I need to predict the next state of the object given the roll, pitch yaw angular velocities given the above information. But the math is really confusing me.
So far all I've gotten is this:
Xvel = (R * AngularYVel * cos(Theta))
YVel = (R * AngularXVel * cos(Phi))
ZVel = (R * AngularYVel * -sin(Theta)) + (R * AngularXVel * -sin(Phi))

i worked this out by trigonometry, so far this seems to predict the pitching about the x axis and yawing about my y axis (sorry i have to use camera axis)
But i dont know how to involve the roll (AngularZVel)
",kalman-filter
Overcorrecting Kalman Filter,"I'm trying to get an extended Kalman Filter to work. My System Model is:
$ x = \begin{bmatrix}
 lat \\
 long \\
 \theta
\end{bmatrix}$
where lat and long are latitude and longitude (in degree) and $\theta$ is the current orientation of my vehicle (also in degree).
In my Prediction Step I get a reading for current speed v, yaw rate $\omega$ and inclination angle $\alpha$:
$z = \begin{bmatrix}
 v \\
 \alpha\\
 \omega 
 \end{bmatrix}$
I use the standard prediction for the EKF with $f()$ being:
$
\vec{f}(\vec{x}_{u,t}, \vec{z}_t) = \vec{x}_{u,t} + 
 \begin{bmatrix}
  \frac{v}{f} * \cos(\theta) * \cos(\alpha) * \frac{180 °}{\pi * R_0} \\
  \frac{v}{f} * \sin(\theta) * \cos(\alpha) * \frac{180 °}{\pi * R_0} * \frac{1}{\cos(lat)} \\
  \frac{\omega}{f}
 \end{bmatrix}
$
$f$ being the prediction frequency, $R_0$ being the radius of the earth (modelling the earth as a sphere)
My Jacobian Matrix looks like this:
$
C = v \cdot \Delta t \cdot cos(\alpha) \cdot \frac{180}{\pi R_0}
$
$
F_J =
\begin{pmatrix}
  1 & 0 & -C \cdot sin(\phi) \cdot \frac{1}{cos(lat)} \\
  -C \cdot sin(\phi) \cdot \frac{sin(lat)}{{cos(lat)}^2} & 1 & C \cdot cos(\phi) \cdot \frac{1}{cos(lat)}\\
  0 & 0 & 1
\end{pmatrix}
$
As I have a far higher frequency on my sensors for the prediction step, I have about 10 predictions followed by one update.
In the update step I get a reading for the current GPS position and calculate an orientation from the current GPS position and the previous one. Thus my update step is just the standard EKF Update with $h(x) = x$ and thus the Jacobian Matrix to $h()$, $H$ being the Identity.
Trying my implementation with testdata where the GPS Track is in constant northern direction and the yaw rate constantly turns west, I expect the filter to correct my position close to the track and the orientation to 355 degrees or so. What actually happens can be seen in the image attached (Red: GPS Position Measurements, Green/blue: predicted positions): 
I have no Idea what to do about this. I'm not very experienced with the Kalman filter, so it might just be me misunderstanding something, but nothing I tried seemed to work…
What I think:
I poked around a bit: If I set the Jacobian Matrix in the prediction to be the identity, it works really good. The Problem seems to be that $P$ (the covariance Matrix of the system model) is not zero in $P(3,1)$ and $P(3,2)$. My interpretation would be that in the prediction step the Orientation depends on the Position, which does not seem to make sense. This is due to $F_J(2,1)$ not being zero, which in turn makes sense.
Can anyone give me a hint where the overcorrection may come from, or what I should look at / google for?
","kalman-filter, gps, sensor-fusion"
UWSim Pressure Sensor Units,"I am attempting to use the data Underwater Simulator (UWSim) provides through the ROS interface to simulate a number of sensors that will be running on a physical aquatic robot. One of the sensors detects the current depth of the robot so, I want to simulate this with the data provided by the UWSim simulated pressure sensor. The Problem is that nowhere in the UWSim wiki or source code can I find any reference to what units UWSim uses to measure pressure.
That being said, what units does UWSim use to measure pressure? Additionally, I would appreciate general information about what units UWSim uses for the data provided by it's virtual sensors.
","ros, simulation, underwater"
Angular velocities and rotation matrices,"Let us assume I have an object O with axis $x_{O}$, $y_{O}$, $z_{O}$, with different orientation from the global frame S with $x_{S}$, $y_{S}$, $z_{S}$ (I don't care about the position).
Now I know the 3 instantaneous angular velocities of the object O with respect to the same O frame, that is $\omega_O^O = [\omega_{Ox}^O \omega_{Oy}^O \omega_{Oz}^O]$.
How can I obtain this angular velocity with respect to the global frame (that is $\omega_O^S$)?
Thank you!
","mobile-robot, imu, gyroscope"
Gyro measurement to absolute angles,"Let us assume we have a gyro that is perfectly aligned to a global frame ($X,Y,Z$). 
From what I know the gyro data give me the angular rate with respect to the gyro axis ($x,y,z$). So let's say I got $\omega_x,\omega_y,\omega_z$. Since I know that the 2 frames are perfectly aligned I perform the following operations:

$\theta_X = dt * \omega_x$
$\theta_Y = dt * \omega_y$
$\theta_Z = dt * \omega_z$ 

where $\theta_X$ is the rotation angle around $X$ and so on.
My question is: what is this update like in the following steps? Because this time the measurement that I get are no more directly related to the global frame (rotated with respect to the gyro frame).
Thank you!
","imu, gyroscope, frame"
rotation matrix to euler angles with gimbal lock,"How do i determine which angle i can negate when gimbal lock occurs. 
As i've understood with gimbal lock that it remove one degree of freedom, but how do i determine which degree can be removed when a  value R[1][3]  of a rotation matrix (size 3x3) has the value 1.  Is it the Roll, Pitch or yaw which can be taken out from the equation?
",motion-planning
Nano Quadcopters Microcontroller and battery,"I am looking into building a nano quadcopter, But as i watch more resources and videos i get more confused, regarding some of the things that i hope would be answered here. I am in very basic level of expertise here, i haven't built any robots or quadcopters to be exact.
What i want to know is, when i program a quadcopter say using intel edison chip, how do i power the quadcpter? i could not find that small size battery to move the propellers and start the chip.
Further more what is the procedure i should follow while developing a nano or small quadcopter, i saw a link on instructable that uses python on raspberry pi and then that raspberry pi control the arduino to control the robot. Can it be done only by using raspberry pi itself? 
I am getting confused and i would like to know if i have to make small or nano quadcopter what should i be doing to get started?
Most of the latest chip support linux and high level programming language like python, so i hope i can go about programming the entire quadcopter using python or similar high level language and i don't suppose i have to stick with c langauge now. If i am wrong please help me understand the matter, there is high chance that i could be wrong.
","arduino, quadcopter, raspberry-pi, python"
Linear actuators in a cartesian robots,"I would like to make a Cartesian robot with maximum speed of up to $1ms^{-1}$ in x/y plane, acceleration $2ms^{-2}$ and accuracy at least 0.1mm. Expected loads: 3kg on Y axis, 4kg on X axis. Expected reliability: 5000 work hours. From what I have seen in 3D printers, belt drive seems not precise enough (too much backlash), while screw drive is rather too slow. 
What other types of linear actuators are available? What is used in commercial grade robots, i.e. http://www.janomeie.com/products/desktop_robot/jr-v2000_series/index.html
","actuator, industrial-robot, cnc"
How would I replicate a tank/zero-turn steering system in a small robotic vehicle?,"I'm working on a project that requires me to build a small vehicle (footprint of ~ 14 x 14 inches, less than 6.5 pounds) that can traverse sand. For the steering system, I was thinking of replicating the way tanks and lawn mowers navigate (ability to do zero-point turns), but I want to do this with four wheels instead of tracks like a tank.
I need help with implementing this idea. My preliminary thoughts are to have two motors where each motor power the wheels on one side of the vehicle (I think this would require a gearing system) or to have a motor to power each individual wheel which I'd rather avoid.
","mobile-robot, motor, wheeled-robot"
EKF-SLAM: Shrink covariance matrix on one direction,"I have implemented an EKF on a mobile robot (x,y,theta coordinates), but now I've a problem.
When I detect a landmark, I would like to correct my estimate only on a defined direction. As an example, if my robot is travelling on the plane, and meets a landmark with orientation 0 degrees, I want to correct the position estimate only on a direction perpendicular to the landmark itself (i.e. 90 degrees).
This is how I'm doing it for the position estimate:

I update the x_posterior as in the normal case, and store it in x_temp.
I calculate the error x_temp - x_prior.
I project this error vector on the direction perpendicular to the landmark.
I add this projected quantity to x_prior.

This is working quite well, but how can I do the same for the covariance matrix? Basically, I want to shrink the covariance only on the direction perpendicular to the landmark.
Thank you for your help.
","mobile-robot, slam, kalman-filter, ekf"
How to implement Bounded Angle Vision in Particle Filter?,"I have built a Particles Filter simulator and I wanted to add the following functionalities.

Limited Range Vision (Robot can see up to 50 meters)
Limited Angle Vision (Robot can see within a certain angle w.r.t its current orientation.  e.g. If the current orientation is 30 degree then it can see in the range from 0 to 60 degree.)

I have managed to add the Limited Range Vision functionality but unable to add Limited Angle Vision.
Method to Sense the landmarks distance within the range
public double[] sense(boolean addNoise) {
    double[] z = new double[World.getLandmark().getLandmarks().size()];
    for (int i = 0; i < z.length; i++) {
        Point lm = World.getLandmark().getLandmarks().get(i);
        double dx = x - lm.getX();
        double dy = y - lm.getY();
        double dist = Math.sqrt(Math.pow(dx, 2) + Math.pow(dy, 2));
        if (addNoise) {
            dist += Util.nextGaussian(0, sense_noise);
        }
        if (isBoundedVision()) {
            // TODO Limited angle vision
            // if robot can see within 60 degree angle w.r.t its orientation
            if (dist <= laserRange) {
                z[i] = dist;
            }
        } else {
            z[i] = dist;
        }
    }
    return z;
}

Method to calculate the probability of this particle
@Override
public double measurement_prob(double[] measurements) {
    double prob = 1.0;
    int c = 0;
    double[] myMeasurements = sense(false);
    for (int j = 0; j < measurements.length; j++) {
        if (measurements[j] != 0) {
            prob *= Util.gaussian(myMeasurements[j], sense_noise, measurements[j]);
            c++;
        }
    }
    if (isBoundedVision()) {
        if (c > 0) {
            // increase the probability if this particle can see more landmarks
            prob = Math.pow(prob, 1.0 / c);
        } else {
            prob = 0.0;
        }
    }
    return prob;
}

Coordinates are relative to the robot and for distance calculation I am using the Euclidean Distance method and my Robot gets localized correctly. 
","localization, particle-filter, visualization"
choice for camera sensor to be used with LiDAR,"I am doing research on autonomous car and looking for a sensor to be used along with LiDAR laser scanner. Ladybug could be a very good option but the cost!! too expensive. 
Could you please suggest me options for camera sensors with good FOV and which will cost me around $1000. 
Thank you so much!!
-CHIANG CHEN
",computer-vision
Balancing Robot Control Model,"I am trying to find a control model for the system of a balancing robot. The purpose of this project is control $\theta_2$ by the 2 motors in the wheels i.e. through the torque $τ$ I started with the dynamic equations and went to find the transfer function. 
Then I will find the PID gains that will control the robot and keep it balanced with the most optimum response. For the time being I am only interested in finding the transfer function for the dynamic model only.
Here is an example: https://www.youtube.com/watch?v=FDSh_N2yJZk
However, I am not sure of my result.Here are the free body diagrams for the wheels and the inverted pendulum (robot body) and calculations below: 

Dynamic Equations:
$$
\begin{array}{lcr}
m_1 \ddot{x}_1 = F_r - F_{12} & \rightarrow & (1)& \\
m_2 \ddot{x}_2 = F_{12} & \rightarrow &  (2) &\\
J_1 \ddot{\theta}_1 = F_r r - \tau & \rightarrow & (3) &\\
J_2 \ddot{\theta}_2 = \tau - mgl\theta & \rightarrow & (4) & \mbox{(linearized pendulum)}\\
\end{array}
$$
Kinematics:
$$
x_1 = r\theta_1 \\
x_2 = r\theta_1 + l\theta_2 \\
$$
Equating (1) and (3):
$$
m_1 \ddot{x}_1 + F_{12} = F_r \\
\frac{J_1 \ddot{\theta}_1}{r} + \frac{\tau}{r} = F_r
$$
Yields:
$$
\frac{J_1 \ddot{\theta}_1}{r} - m_1 \ddot{x}_1 + \frac{\tau}{r} = F_{12} \rightarrow (5)
$$
Equating (5) with (2):
$$
\frac{J_1 \ddot{\theta}_1}{r} - m_1 \ddot{x}_1 + \frac{\tau}{r} - m_2 \ddot{x}_2  = 0 \rightarrow (6) \\
$$
Using Kinematic equations on (6):
$$
(J_1 - m_1 r^2 - m_2 r^2) \ddot{\theta}_1 + m_2 l r \ddot{\theta}_2 = -\tau \rightarrow (7) \\
$$
Equating (7) with (4):
$$
\begin{array}{ccc}
\underbrace{(J_1 - m_1 r^2 - m_2 r^2) }\ddot{\theta}_1 &+& \underbrace{(m_2 l r + J_2 ) }\ddot{\theta}_2 &+& \underbrace{m_2 gl}\theta &= 0 \rightarrow (8) \\
A & &B & & C & \\
\end{array}
$$
Using Laplace transform and finding the transfer function:
$$
\frac{\theta_1}{\theta_2} = -\frac{Bs^2 + C}{As^2} \\
$$
Substituting transfer function into equation (7):
$$
(J_1 - m_1 r^2 - m_2 r^2) \frac{\theta_1}{\theta_2}\theta_2 s^2 + m_2 lr\theta_2 s^2 = -\tau \\
$$
Yields:
$$
\frac{θ_2}{τ} = \frac{-1}{(mlr-B) s^2+C}
$$
Simplifying:
$$
\frac{θ_2}{τ}=  \frac{1}{J_2 s^2-m_2 gl}
$$
Comments:
-This only expresses the pendulum without the wheel i.e. dependent only on the pendulums properties.
-Poles are real and does verify instability.
","arduino, control, pid, wheeled-robot"
Balancing a plate with an IMU offset from the center,"I recently bought a IMU . I am new at this. 
My question: Does the positioning of the IMU matter? Are there any differences between placing it at the center of the plate or if it is offset from the center?
I am still learning about this topic. So any help would be greatly appreciated.
Thanks.
","control, sensors, imu, sensor-fusion"
Quadcopter stability vs (PID error signal lag and sample time),"The question I am asking is that, what is the effect on stability of increasing or decreasing both the sample time and lagging of error signal to PID. Does it helps in stability or degrade it?
","quadcopter, pid, stability"
"Are there off the shelf solutions for GPS+INS (accelerometer,gyro,magneto) sensor fusion for getting filtered/fused location and speed output?","I am working on a project that needs tracking location and speed of pedestrians/runners/athletes (so not really robotics, but I see a lot of related usage and posts in the robotics domain, and an answer to this question could help with follower robots). I'm interested in just the 2D location (latitude-longitude).
Using just the GPS position has noisy/jump samples and also the degradation due to multi-path near trees etc. From reading about filtering solutions, I understand that sensor fusion that fuses GPS with the data from inertial sensors (INS) helps improve a lot of these issues. Also, this kind of sensor fusion seems to be used in a lot of places -- robotics, wearables, drones etc. Hence I think there might be off the shelf chips/modules/solutions for this, but I couldn't find any.
I found a sensor hub from Invensense that integrates the 9 dof inertial sensors and comes with the fusion firmware, but it doesn't seem to have hookups and firmware for fusing GPS and providing filtered latitude-logitude. 
So, what should I be looking for? Are there any off the shelf chips/modules/solutions that come with the built in sensor fusion Software/firmware for doing GPS+INS fusion? 
I understand that it will still need tuning some params as well as some calibration.
","localization, kalman-filter, sensor-fusion, gps"
Two DC motors and single output?,"I saw one old industrial robot(Year 1988) end effector is having 2 DC motor for roll drive. After roll drive, yaw and pitch drives are connected and it has dc motors separately.
But roll drive has two DC motors. Why are they used like this? why not single with higher torque.
All the roll, pitch and yaw motors are same spec. Total 4 DC motors.
Two DC motor connected to single shaft using gears in roll.
",industrial-robot
How can I avoid Roomba Error 10 Code?,"I am trying to run my 600 series Roomba in a large, open space (1700+sf) and it does not recognize the large, open space and throws the Error 10 code.  It does not recognize an edge of 2 12""-3"" either; it will fall off the edge and become stuck.  Any suggestions?
","mobile-robot, irobot-create, roomba"
velocity of the end-effector,"The joint velocities are constant and equal to $\dot{\theta}_{2}$ = 1 and $\dot{\theta}_{1}$ = 1. How to  Compute the
velocity of the end-effector when $\theta_{2} =\pi/2$ and $\theta_{1} = \pi/6$

",robotic-arm
Can digital servo motors be modified for continuous rotation?,"In an autonomous mobile robot, we're planning on using digital servo motors to drive the wheels. Servo motors usually don't rotate continuously. However, they can be modified to do so based on many tutorials online which only mention modifying [analog] servo motors.
My question is, can the same method(s) or any other ones be used to modify digital servo motors?
Thanks
","mobile-robot, rcservo"
Wires or columns which contract on passing electricity,"Background: Introductory robotics competition for college freshmen; Bot has to open 8 jars (with two balls in each of them) in ten minutes and load the balls into a shooting mechanism.
So, we were doing this project and we hit upon a challenge that the jar is not opening like we originally intended to. So we decided to get a rack-pinion mechanism and use it for unscrewing the lid. However, it is too large and we are unable to fit the bot in the required dimensions
The actual question: Are there any wires or rigid columns/things which can contract ~1 cm when electricity is passed through it? And what would their price range be? Our budget is also in the list of constraints for the bot
Edit: We can include a wire of length <1m or a column of length <30 cm. Also, the wire needs to contract only more than 7mm
",mechanism
USB instead of RS232,"RS232 is not popular as it used to be and is mainly replaced by USB [wikipedia]. Problems such as mentioned in this question doesn't help its reputation either.
In a new system design therefore, one could think of using USB instead of Serial Port for communication. However, it still seems like RS232 is the serial communication protocol/port of choice.
Why is that? I understand changing old machinery that work with RS232 is costly, but what prevents new system designers from using USB instead of RS232?
","rs232, usb"
Robot wire follower + how to position on wire,"I'm designing my lawn mower robot, and I am in the perimeter stage. 
The electronic part is done, and works quite good, now comes the software.
I need an advice on how to deal with the problem of line following. I mean, once the robot is on the line, parallel to the line, that's relatively easy. 
But how to manage the situation when the robot is driving around and approaches the line (wire)?
I have two sensors, left and right, turned 45° with respect to the forward direction. 
The robot could arrive from any angle, so the signal amplitude read from the sensor could be completely random.. 
So I don't understand what to do in order to move it in the right position on the wire...
What's the usual approach? 

The idea is the same as here:

The wire is all around the yard. on the mower there are 2 sensors, left and right, that sense the signal emitted from the wire, a square wave signal at 34 KHz. The signal amplitude read from the sensors on the mower is about 2 V when it's above the wire.
","line-following, magnetometer"
Is there a simpler way than ROS for 5 DOF Dynamixel arm control,"I will have a 5 or 6 DOF arm build with Dynamixel or HerculeX smart servos. I need  to move the gripper along Cartesian trajectory, which I will calculate in my C++ application. I looked at ROS, but the learning curve is pretty steep and it looks like a major overkill for this use case. I don't need a distributed system with all the complexity it brings. Preferably, I would like to call a standalone C++ library or libraries to get the arm actuated. 
What are my options? What will be the limitations of not using a full blown robotics framework like ROS or YARP in this case.
EDIT
Here is how I would like to code it:
vector<Point> way_points;
vector<Pose>  way_poses;

compute_Cartesian_trajectory(way_points, way_poses);   // my code
execute_Cartesian_trajectory(way_points, way_poses);   // library call

The last line can be spread over several library function calls and intermediate data structures, if needed. The end result should be the gripper physically following Cartesian trajectory given by way_points and way_poses.
","robotic-arm, ros, motion-planning, c++"
Particle filter weight function,"I am trying to implement a particle filter in MATLAB to filter a robot's movement in 2D but I'm stuck at the weight function. My robot is detected by a camera via two points, so a single measure is a quadruple (xp1, yp1, xp2, yp2) and states are the usual (x,y,alpha) to detect its pose in 2D. As far as my understanding goes I should assign a weight to each particle based on its likelihood to be in that particle position with regards to the current measurement.
I also have the measure function to calculate an expected measurement for a particle, so basically I have, for each instant, the actual measurement and the measurement that a single particle would have generated if it were at the actual state. 
Assuming all noises are Gaussian, how can I implement the weight function? I kind of noticed the mvnpdf function in MATLAB, but I can't actually find a way to apply it to my problem.
","mobile-robot, wheeled-robot, particle-filter"
image processing,"I am making a Robot goalie, the robot is supposed to detect whether a ball has been thrown in its direction , sense the direction of the ball and then stop it from entering the goal post. A webcam will be mounted on top of the goal post. The robot is required to only move horizontally (left or right), it shouldn't move forwards or backwards. The robot will have wheels, the image processing will be performed by raspberry pi which will then send the required information to a micro controller which will be responsible for moving the robot in the required direction(using servo motors). Which image processing algorithm will be the best to implement this scenario?
","mobile-robot, computer-vision, algorithm, beagle-bone, first-robotics"
Firmware upgrade for iRobot Create 2,"Is there a firmware upgrade for available for the Create 2? I had some issues in March when using these for assigning a University of Tennessee  programming project. We are getting ready to use them again (we have 10 now) and I'd like to get them all updated to the latest firmware.
",irobot-create
Algebraic and geometric in inverse kinematic,"I'm just wondering that is there any case that when algebraic way can't solve the problem while the geometric can ? Cause I'm working on a 2DOF robotics arm This one, I know the length of L1 and L2, location that I want for the end effector, then I tried calculating the angles by using algebraic but it gave me cos(alpha) > 1, but when I tried solving with geometric, I can find the solution, so is it because I use a wrong way in algebraic ? 
Thank you very much.
","localization, kinematics, robotic-arm, inverse-kinematics"
Robot arm reachability of a pose in Cartesian space,"Given a set of robot joint angles (i.e. 7DoF) $\textbf{q} = [q_1, ... , q_n]$ one can calculate the resulting end-effector pose (denoted as $\textbf{x}_\text{EEF}$), using the foward kinematic map. 
Let's consider the vice-versa problem now, calculating the possible joint configurations $\textbf{q}_i$ for a desired end-effector pose $\textbf{x}_\text{EEF,des}$. The inverse kinematics could potentially yield infinitely many solutions or (and here comes what I am interested in) no solution (meaning that the pose is not reachable for the robot).
Is there a mathematical method to distinguish whether a pose in Cartesian space is reachable? (Maybe the rank of the Jacobian) Furthermore can we still find a reachability test in case we do have certain joint angle limitations? 
","mobile-robot, robotic-arm, kinematics, inverse-kinematics, jacobian"
When to use multiple batteries vs a UBEC,"When should you use multiple separate batteries vs a single battery with multiple UBECs?
I'm trying to design the power system for a small 2-wheeled robot. Aside from the 2 main drive motors, it also has to power an Arduino, a Raspberry Pi and a couple small servos to actuate sensors.

the motors are each rated for 6V with a peak stall current of 2.2A
the Arduino uses about 5V@100mA
the Raspberry Pi uses about 5V@700mA
the servos each use 6V and have a peak stall current of 1.2A.

So the theoretical max current draw would be 2.2*2+.1+.7+1.2*2 = 7.6A.
Originally I was planning to use three separate Lipo batteries:

one 12V using a step-down converter to power the main drive motors for 6V@4.4A peak
two 3.7V lipos each with step-up converter (rated for 5v@3A) to handle the servos and logic separately

Then I discovered UBECs, which sound too good to be true, and they seem to be both cheap (<$10) and efficient (>90%) and able to handle my exact volt/current requirements.
Should I instead use a single high-current 12V lipo with three UBECs to independently power my drive motors, sensor motors and logic? Or will this still suffer from brown-out and power irregularities if a motor draws too much current?
What am I missing?
","battery, bec"
Relative frame calculation,"what's the quickest way to calculate a relative coordinate of a frame, as shown in the code below. The Kuka robot language this "":"" is referred to as the geometric operator. 
I would like to perform this calculation in matlab, scilab, smathstudio or java, could you please advise on which library to use and/or how to proceed?
Frame TableTop=[x1 y1 z1 a1 b1 c1]
Frame RelCoord=[x2 y2 z2 a2 b2 c2]

Frame AbsCoord= TableTop:RelCoord

",programming-languages
How much offset speed of motors on an axis is required before adjusting pid,"For adjusting the pid for quadcopter, how much speed of motors are required before adjusting the pid. Do we need to give so much offset speed so that it cancels weight? I am sure we cant start adjusting pid with zero speed of motors initially.
","quadcopter, pid"
EMAX ESC Simon Series with arduino,"I want to control a brushless motor with the ""EMAX Simon Series 30amp ESC"" and Arduino (Leonardo) board. I am really confused how to do that. I can't understand which beep sounds mean what. I have tested many code examples but they weren't useful.  
","arduino, brushless-motor, esc"
Can I connect a UDOO to a PC using a straight-through ethernet cable or do I need a cross over?,"Can I connect a UDOO board to a PC using a straight-through ethernet cable? Or do I need a cross-over cable?
As far as I know, most modern devices can use the two interchangeably. However, I am not sure if a UDOO can do that. Anyone with any experience?
Thank you for your help.
(PS: I don't have a UDOO on me at the moment, so I can't test it myself. Couldn't find any information in the documentation either).
",embedded-systems
Sensor orientation of an external magnetometer,"On many drones are already external magnetometers. Unfortunately, the orientation of such sensors is sometimes unknown. E.g. the sensor can be tilted 180° (pitch/roll) or X° in yaw. I was wondering, whether one could calculate the rotation of the sensor relative to the vehicle by application of the accelerometer and gyrometer?
Theoretically, the accelerometer yields a vector down and can be used for calculation of the coordinate system. The discrepancy between magnetometer and gyrometer then, may be used for calculation of the correct orientation of the compass. Later the compass should be used for yaw calculation. 
Below is the starting orientation of both sensors (just an example, the orientation of the compass can be anything). Does someone know a good way to figure out the rotation of the compass? 

","magnetometer, orientation"
Two exclusive inputs control,"I have a system with two inputs (throttle and brake) and one output (speed). How does one design a controller in such a way that the two outputs of the controller (throttle and brake) are never both greater than zero (so that it doesn't accelerate and brake simultaneously)?
Thanks
","control, automation"
Can someone explaine to me this code?,"static void set_default_param(DPMTTICParam& param)
{

    param.overlap = 0.4;
    param.threshold = -0.5;
    param.lambda = 10;
    param.num_cells = 8;
}

",c++
How can a load be balanced between multiple AC electric drive motors?,"I have a three wheeled vehicle in a tricycle configuration attached to a fixed frame. Each wheel is powered by an AC electric motor. The AC motors are fed by motor controllers that take a speed demand. The single main wheel (which is also steerable) has a lower gear ratio than the rear wheels so it has a theoretical higher top speed. 
When the vehicle drives in a straight line each of the motor controllers are given identical speed requests. Unfortunately feedback from the controller indicates that some motors are pushing while some are pulling. In particular we have a common scenario where one rear wheel is pushing while the front wheel is trying to slow down. The third wheel will often have almost no current. 
What can be done to make all three motors work together and avoid situations where they fight?  Is there a way to change the request to the motor controller to encourage the drives to work together? Do we have to switch from a speed request setup to a current control setup? If so what is the appropriate way to control the motors then? 
Let me know if I haven't included any important details and I will update my question.
","control, motor"
Using SLAM to create 2D topography,"I have a small mobile robot with a LidarLite laser range finder attached to a servo. As of now I have the range finder side-sweeping in a 30 degree arc, taking continuous distance readings to the side of the robot (perpendicular to the robots forward motion).
My goal is to have the robot drive roughly parallel to a wall, side-scanning the entire time, and create a 2D map of that wall it is moving past. The 2D topography map is created post processing (I use R for much of my data processing, but I don't know is popular for this kind of work).
From what I know of it, SLAM sounds like a great tool for what I want to do. But I have two issues:
1: I know my robot will not have a consistent speed, and I have no way to predict or measure the speed of my robot. So I have no way to estimate the odometry of the robot.  
2: The robot will also move further and closer to the wall as it proceeds down it's path. So I can not depend on a steady plane of travel from my robot.
So given that I don't have any odometry data, and my realtive distance to the wall changes over the course of a run, is it possible to use SLAM to create 2D maps?
I'm looking into stitching algorithms that are used for other applications, and some of these can handle the variances in relative distance, but I was hoping SLAM or some other algorithm could be of use here.
","slam, servos, laser, rangefinder"
High-traction thin tires vs Wide moderate-traction tires? [Sumo-bot],"I am building a sumo-bot and our competitors have thin sticky tires, while we have wider and less sticky tires. The diameter is the same, and the gearbox/motor is the same. Who will win?
PS: Sticky tires: https://www.pololu.com/product/694 & wide tires: https://www.pololu.com/product/62
Thanks!
","movement, wheel, two-wheeled"
How to calculate the current consumed by a brushless motor on a quadcopter,"I want to create a virtual quadcopter model, but I am struggling to come up with a satisfying model for the brushless motors & props.
Let's take an example, based on the great eCalc tool:

Let's say I want to know how much current is consumed by the motor in a hovering state. I know the mass of the quad (1500g), so I can easily compute the thrust produced by each motor:
Thrust = 1.5 * 9.81 / 4 = 3.68 N per motor

Thrust is produced by moving a mass of air at an average speed of V:
Thrust = 0.5 * rho * A * V²

Where rho (air density) is 1.225kg/m3 and A (propeller disk area) is PI * Radius² = 0.073m² (12"" props). So I can compute V:
V = sqrt(Thrust / 0.5 / rho / A) = 9.07 m/s

All right, now I can calculate the aerodynamic power created by the propeller:
P = Thrust * V = 3.68 * 9.07 = 33.4 W

All right, now I can calculate the mechanical power actually produced by the motor. I use the PConst efficiency term from eCalc:
Pmec = Paero * PConst = 33.4 * 1.18 = 39.4W

Here, eCalc predicts 37.2W. It's not too far from my number, I imagine they use more sophisticated hypotheses... Fair enough.
From this post, I know that this power is also equal to:
Pmec = (Vin - Rm * Iin) * (Iin - Io)

Where I know Rm (0.08 Ohms) and Io (0.9 A). So, finally, my question: How do you calculate Vin and Iin from here? Of course, if I knew the rotation speed of the engine I could get Vin from:
n = Kv * Vin

Where Kv = 680 rpm/V. But unfortunately I don't know the rotation speed...
(Note that Vin is assumed to be averaged from the pulse-width-modulated output produced by the ESC)
Thanks for your help!
","motor, quadcopter, brushless-motor, electronics, power"
How is a brushless gimbal motor different from a regular brushless motor?,"How are the brushless motors in a gimbal assembly designed?
Obviously it doesn't need continual rotation, but it does need accurate control of precise position. I've noticed that the motors in my gimbal don't have the usual magnetic 'snap' positions that my other motors do. 
What are the primary design differences in these kinds of motor, if any?
",motor
Covariance and optimization,"I am trying to build a map containing lamps as landmarks. I drive around with a robot and a monocular camera looking to the ceiling.
The first step is detect the edges of each observed rectangular lamp and save the position in pixels and also the current position from odometry of the robot. After the lamp disappears from the field of view, there is enough base-line to do a 3D reconstruction based on structure from motion. Once this reconstruction is done there will be uncertainty in the position of the lamps that can be modelled by covariance. Imagine if the robot was driving for a while, its own position estimated from odometry will also have a relatively high incertitude, how can I integrate all of those incertitudes together in the final covariance matrix of the position of each lamp?
If I understand well there would be the following covariances:

noise from camera
Inaccurate camera calibration matrix
inaccurate result from optimization
drift in odometry

My goal is to manually do loop closure using for example g2o (graph optimization) and for that I think correct covariances are needed for each point.
","slam, cameras, 3d-reconstruction"
"""Ambiguous up to scale"" , Explanation required","I am reading ""Computer Vision: Models, Learning, and Inference"" in which author writes at several points (like on page 428-429) that although matrix A seems to have 'n' degree of freedom but since it is ambiguous up to scale so it only has 'n-1' degree of freedom? Can anyone explain what this thing means? Why one degree of freedom is decreased?
",computer-vision
Building Robotic arm joint,"I am very new to robotic design and I need to determine what parts I will need to assemble an arm joint.  The joint will contain one timing belt pulley which a remote motor will be turning, a forearm that the pulley will be rotating and an upper-arm piece that will actually be two parallel arms that will grip the pulley on top and bottom in order to brace the pulley from off axis torque from the timing belt.
I am kind of at a lost as to how to mount all of these together.  I would like to mount the forearm directly to the pulley and then the two parallel arms (comprising the upper-arm) sandwich the top of the pulley and the lower part of the forearm.  This would be attached using a turn table.  Any ideas on how a shaft would mount to these?  Or how to attach the pulley to the arms themselves?
Any kind of direction or links would be greatly appreciated, I don't even know the names of the parts I would be looking for.
In this ASCII art model the dashed lines (-) are the arms.  The arm on the left is the forearm and the two arms on the right are the two parallel parts of the upper arm.  The stars are the belt and the bars (||) are the pulleys at the elbow |E| and shoulder |S|.  
              -----------------
              |E|***********|S|
-----------------
              -----------------

I am thinking of mounting the pulley to the left arm directly (a bushing?) and then maybe using turntables to mount the pulley to the top arm and another turn table to mount the left arm to the bottom arm.
Here is a picture of the design to help you visualize:

","design, arm, joint"
Line following robot path planning,"I have built a mobile robot with several ultrasonic sensors to detect obstacles and an infrared sensor to track a line as a path. I have written a simple algorithm to follow the line which works fine, but avoiding obstacles are a problem because the robot doesn't know the layout of the path, so even if it does move around the obstacle, it is not guaranteed that it will find the path line again(unless the line is perfectly straight). Therefore, I think I may need to use a path/motion planning algorithm or find a way to store the layout of the path so that robot could predict where to move and get back to the path line and keep on following after overcoming an obstacle. I would like to hear suggestions or types of algorithms I should focus on for this specific problem.
Picture might help specifying the problem I'm facing.

Thank you.
",motion-planning
Why are bipedal robots difficult?,"Not sure if this has been asked, but there are lots of simulations of bipedal locomotion algorithms online, some of the evolutionary algorithms converge to very good solutions. So it seems to me that the algorithm part of bipedal locomotion is well-understood.
If you can do well on simulations, you should be able to do it well in the real world. You can model delay and noise, you can model servo's response curve.
What I don't understand is then why is it still difficult to make a walking robot? Even a robot like the Big Dog is rare.
",walk
Powering my robot with 12V battery which is charged by a gas/petrol generator while the robot is operating?,"A have designed a robot to perform tasks in farms.  But the problem now is I'm not sure on the best way to supply continuous power to my robot.  All the motors are rated at 12V and only Arduino and a few sensors work at 5V or less.  
Can I continuously charge a 12V lead acid battery with an adapter (comes with the battery) plugged into the AC output of the generator while the robot is operating? Do I have to worry about overcharging the battery?
Or should I use the generator's DC output which can supply 12V and up to 8.3Amp. Or is there any other suggestions?
Some information about the adapter which are stated on the package:
1. Built-in over-charge protection device.
2. Built-in thermal protection device
3. Output: 6v/12v 2Amp
This is the generator that I have: 
http://global.yamaha-motor.com/business/pp/generator/220v-60hz/0-1/et950/
This is my first robot which is quite big that requires a lot of electrical/electronic knowledge to power it. I do not have a lot of experience in this field. So any feedback is greatly appreciated.
","electronics, power, battery"
Device to generate screen tap response,"I have extremely limited knowledge in the general topic of robotics and therefore this question is a shot in the dark. Please let me know if the topic is unsuitable for the site.
I am interested in creating a device that would generate a touchscreen tap. In a nutshell, I would like to replicate on a touchscreen the automated mouse functionality you can obtain with software like AutoHotKey in Windows. Since, without jailbreaking the phone, a software solution is basically impossible, it occurs that one of the first components would be a physical device that simulates a tap. Do any options for such a component exist?
I recognize that there are philosophical implications with creating such a device. I am assuming the entire conversation to be theoretical and solely related to the hardware design.
Thanks,
Alex
",automatic
Do you know where to get the original iRobot Create?,"Does anyone out there know where I can get the original iRobot Create?
The company no longer sells them. 
It was only 2 years ago that it was sold. It is white and its value is the physical design, that it has a large exposed deck for mounting armatures.
It is preprogrammed to operate in different configurations, eg. spinning, figure 8, following the outline of a wall, etc.
I have an ongoing art project using this model and as they are in operation everyday, I will eventually need to replace them with new ones.
To see a video of one of my projects you can go to https://vimeo.com/119486779
I currently have it working in a spinning motion.
",irobot-create
Are there systematic ways to tune the Kalman filter in engineering practice?,"Including Q, R, and initial states of x and P.
",kalman-filter
Multiple control loops with overlapping effects,"I'm familiar with using PID to perform closed loop control when there is a single output and a single error signal for how well the output is achieving the desired set-point.
Suppose, however, there are multiple control loops, each with one output and one error signal, but the loops are not fully independent.  In particular, when one loop increases its actuator signal, this changes the impact of the output from other loops in the system.
For a concrete example, imagine a voltage source in series with a resistor, applying a voltage across a system of six adjustable resistors in parallel.  We can measure the current through each resistor and we want to control the current of each resistor independently by adjusting the resistance.  Of course, the trick here is that when you adjust one resistor's resistance, it changes the overall resistance of the parallel set, which means it changes the voltage drop due to the divider with the voltage source's resistance and hence changes the current through the other resistors.
Now, clearly we have an ideal model for this system, so we can predict what resistance we should use for all resistors simultaneously by solving a set of linear equations.  However, the whole point of closed loop control is that we want to correct for various unknown errors/biases in the system that deviate from our ideal model.  The question then: what's a good way to implement closed loop control when you have a model with this kind of cross-coupling?
","control, pid"
"How to use specific ESC,BLDC motor through Arduino Uno R3?","Attempt to clean up:
I'm trying to use this motor with this ESC and an Arduino Uno R3.
Typically, I used the PWM pins when I use the Arduino and an ESC, but I can't control the motor even if I use the servo library, and I've also tried sample code from different websites.
The ESC has a beep I can't understand. Sometimes it's high-low-high or high for 4 seconds, but I can't find anything on Google. 
Sometimes the motor spins periodically for a short time, but I don't know why. Some sites recommend using flash or bootloader, but I'd prefer to use Arduino PWM or the servo library. 
Original post
Specific ESC is Rctimer Mini ESC16A OPTO SimonK Firmware SN16A ESC..
I can only using ESC(Discussed above..) and RCTimer 1806-1450KV Multi-Rotor BLDC Motor.
Typically, I used PWM pins(3, 9, 10, 11-because similar Signal frequency) when using Arduino-ESC.. but, I can control BLDC Motor even i used to Servo library.. 
I've been used usual websites example code.
Just ESC had unknowable beep .. sometime di-ri-di or di(for 4 seconds).. 
I couldn't find that way.. in google (or my country websites)
Sometimes, The Motor spins(In a certain value, periodically) for a short time but I don't know why The motor spins
In google sites, just using flash or Bootloader, but I'll use Arduino PWM or Servo..
So.. Please! would you please help me?
Thank you for reading my thread.


","arduino, brushless-motor, esc, pwm"
Arduino triggers a camera to start recording,"I've already made an Arduino device which detects the trigger event, but now I want it to trigger the recording and storage of video when this event occurs.  If the camera could be wirelessly triggered a few feet away from the Arduino unit, that would be optimal, but I can settle for running wires if need be.
I'm looking for suggestions because I'm on a limited budget for this project. I want to avoid reinventing the wheel and ordering parts which I can't get to work with an Arduino.
I'm considering the use of this camera.
https://www.sparkfun.com/products/11418
This is my first Arduino project.  Any help is very welcome.
","arduino, wireless"
2D Positioning of mobile robot,"I am just starting to explore an idea and I am somewhat of a novice in robotics. I am looking to position a mobile robot as accurately as possible on a concrete slab. This would be during new construction of a building and probably not have many walls or other vertical points for reference. the basic premise behind the robot is to print floor plans straight on to the slab. I will have access to the BIM (building information models, CAD, Revit) files of the building. I want the robot to position itself as accurately as possible on the blank slab using the BIM files as a map. What would be the best avenue to track and adjust positioning of the robot in the open space of a slab? Low frequency, Lidar, wifi? Lastly what sensors would be best?
","mobile-robot, sensors, localization"
Easiest way to submit a longer non standard character string via MAVLink,"I want to submit my gains for the PID regulator via MAVLink. 
Unfortunately, I am not very used to MAVLink and there are several functions which may be used for that purpose (I think). My string is currently JSON formatted and I was directly sending it to the serial port before. 
Is there a straight forward way to submit the data like it is (see below) with MAVLink, or is it better not to transfer a JSON string with MAVLink and submit each single value? If yes, what is the function of choice. 
So far I noticed that for most of the sensors, there are already MAVLink function defined. For the PID gains I found not so much.
AP_HAL::UARTDriver *pOut = uartX == UART_C ? hal.uartC : hal.uartA;
pOut->printf( ""{\""t\"":\""pid_cnf\"",""
              ""\""p_rkp\"":%.2f,\""p_rki\"":%.2f,\""p_rkd\"":%.4f,\""p_rimax\"":%.2f,""
              ""\""r_rkp\"":%.2f,\""r_rki\"":%.2f,\""r_rkd\"":%.4f,\""r_rimax\"":%.2f,""
              ""\""y_rkp\"":%.2f,\""y_rki\"":%.2f,\""y_rkd\"":%.4f,\""y_rimax\"":%.2f,""
              ""\""p_skp\"":%.2f,\""r_skp\"":%.2f,\""y_skp\"":%.4f}\n"",
              static_cast<double>(pit_rkp), static_cast<double>(pit_rki), static_cast<double>(pit_rkd), static_cast<double>(pit_rimax),
              static_cast<double>(rol_rkp), static_cast<double>(rol_rki), static_cast<double>(rol_rkd), static_cast<double>(rol_rimax),
              static_cast<double>(yaw_rkp), static_cast<double>(yaw_rki), static_cast<double>(yaw_rkd), static_cast<double>(yaw_rimax),
              static_cast<double>(pit_skp), static_cast<double>(rol_skp), static_cast<double>(yaw_skp) );

","c++, mavlink"
Overheating/Jamming MG996R servo,"I have recently purchased my first ever servo, a cheap unbranded Chinese MG996R servo, for £3.20 on eBay.

I am using it in conjunction with a Arduino Servo shield (see below):

As soon as it arrived, before even plugging it in, I unscrewed the back and ensured that it had the shorter PCB, rather than the full length PCB found in MG995 servos. So, it seems to be a reasonable facsimile of a bona-fide MG996R.
I read somewhere (shame I lost the link) that they have a limited life, due to the resistive arc in the potentiometer wearing out. So, as a test of its durability, I uploaded the following code to the Arduino, which just constantly sweeps from 0° to 180° and back to 0°, and left it running for about 10 to 15 minutes, in order to perform a very simple soak test.
#include <Servo.h> 

const byte servo1Pin = 12;

Servo servo1;                // create servo object to control a servo 
                             // twelve servo objects can be created on most boards

int pos = 0;                 // variable to store the servo position 

void setup() 
{ 
  servo1.attach(servo1Pin);  // attaches the servo on pin 9 to the servo object 
  Serial.begin(9600);
} 

void loop() 
{ 
  pos = 0;
  servo1.write(pos);         // tell servo to go to position in variable 'pos' 
  Serial.println(pos);
  delay(1000);               // waits 15ms for the servo to reach the position 

  pos = 180;
  servo1.write(pos);         // tell servo to go to position in variable 'pos' 
  Serial.println(pos);
  delay(1000);               // waits 15ms for the servo to reach the position 
} 

When I returned, the servo was just making a grinding noise and no longer sweeping, but rather it seemed to be stuck in the 0° position (or the 180°). I picked the servo up and whilst not hot, it was certainly quite warm. A quick sniff also revealed that hot, burning motor windings smell.  After switching of the external power supply and allowing it to cool, the servo began to work again. However, the same issue occurred a little while later. Again, after allowing it to rest, upon re-powering, the servo continues to work.  However, I am reluctant to continue with the soak test, as I don’t really want to burn the motor out, just yet.
Is there a common “no-no” of not making servos sweep from extreme to extreme, and one should “play nice” and just perform 60° sweeps, or is the cheapness of the servo the issue here?
I am powering the servo from an external bench supply, capable of 3A, so a lack of current is not the issue.

Please note that I also have a follow up question, Should a MG996R Servo's extreme position change over time?
","arduino, rcservo"
What PID values should i keep,"I have built quadcopter but the problem is of balancing. It doesnt goes up. I am using PID techniqe for balancing. But i am not finding the suitable values for PID tuning. I am using mpu6050 as a sensor. I get the accelerometer values of x and y axis and find the error from them. That is lets say if accel on x is not zero then it error cause it should be zero if balanced. I am using +-2g sensitivity scale of accelerometer. The motors i am using are dji 920 kva. What values for kp, ki and kd should i set. I cant set them while in flight cause it completely out of balance.

This is the design. Completely home made. I have modified it a little after this photo. Accelerometer is at 2g so at balance z will be 32768/2 .
short PID()
{
short error,v;
error = desired-current; 
//error/=390;
integ += error;
der = error - perror;
x=error; 
x2=integ; 
x3=der; 
x*=kp;
x2*=ki;
x3*=kd; 
v=kpi;x/=100;
v=kii;x2/=1000;
v=kdi;x3/=1000;
x=x+x2+x3; 
//x/=390; 
perror = error;
return x;
}

There are also few more questions, should i scale error or pid output, because error is from ranging from 0 to 16380 at 2g setting, so i am scaling it from 0 to 42. So should i divide error or pid by some value?
","quadcopter, pid, balance"
Is it safe to give 5v through 5v pin of arduino uno r3 while usb cable inserted,"Is it safe to give 5v through 5v pin of arduino uno r3 while the USB cable is inserted? I have ESCs connected to it which aren't likely to start in other cases. The 5v and gnd is coming from the BEC circuit of a connected ESC. Please help me. Thanks
","arduino, esc"
How can I control fast real time sensor (250Hz) with slow system display(60Hz),"We do some experiments of real time representation of sensor position on TV. In this experiments, we used sensors for collect real time position in 3D at 250Hz and TV for Display the sensor position at 60Hz. Also, we used MATLAB and C++ for programming with OpenGL platform.
In programming, Every iteration dat display on the TV, erase and draw the circle (Object, which is represent real time position on the display). In this program I collect to only 60 points and loose other 190 points in every second, becuase, I think that refresh rate of TV is 60Hz.
I have gone through the thread ""How can I control a fast (200Hz) realtime system with a slow (30Hz) system?""(How can I control a fast (200Hz) realtime system with a slow (30Hz) system?), but i don't understand, How to implement two loop on 200Hz and 30Hz.
My Question is, How can we implement in MATLAB/C++? So I can store 250 data of sensors as well as 60 points for real time display on the TV.
If you help me through pseudo code, I appreciate your help.
Thank You in advance.
Please help me.
P.S. Code
%Display main window using Psychtoolbox

win=Screen(2,'OpenWindow',[1 1 1])

while (1)
  % Setup for data collection at 250Hz
  Error   = calllib('ATC3DG64', 'GetSynchronousRecord',  hex2dec('ffff'), pRecord, 4*numBoards*64);
  errorHandler(Error);
  Record = get(pRecord, 'Value');

  %sensor number
  count=2;

  evalc(['tempPos(1, count) ='  'Record.x' num2str(count - 1)]);
  evalc(['tempPos(2, count) ='  'Record.y' num2str(count - 1)]);
  evalc(['tempPos(3, count) ='  'Record.z' num2str(count - 1)]);

  % Record X and Y position of  sensor 2

  if SensorNumAttached(count)
    % Real time position and minus world origin so that real
    % time position display on the TV
    table1(count,1)=(2.54*tempPos(2,count))-X_world_origin;
    table1(count,2)=(2.54*tempPos(3,count))-Y_world_origin;
  end 

  % Some conversion for the Pixel to centimeter ratio
  x_center_new = x_center - (x_ratio * table1(2,1));
  y_center_new = y_center - (y_ratio * table1(2,2));

  % conversiorn for display circle on the TV, is represent the real time poistion of the sensor
  x1  =round(x_center_new - R_num_data);
  y1 = round(y_center_new - R_num_data);

  x2 = round(x1 + 2*R_num_data);
  y2 = round(y1 + 2*R_num_data);


  % Display command for TV.      
  Screen('FrameOval',win,[255 0 0], [x1 y1 x2 y2]);
  Screen('Flip',win);
end

","design, communication, matlab, c++"
Robotic legs technologies,"what robotic leg technologies are available.
i'm sorry if this is a basic question i am a software developer looking to get into the field of robotics. i am particularly interested in robotic legs that are similar to those used on Boston Dynamics ATLAS robot.
what is the mechanism required that allows it to move its joints so quickly. if you see any videos of many of Boston Dynamics robots they make an engine sound (presumably because it uses an engine), but i cant find any details in the configuration that is being used.
",legged
DualCopter Degree Of Freedom,"I am a newbie in this drone field. I am curious to know what type of rotation and translation a dualcopter can achieve ? By rotation and translation i mean can it be able to roll, pitch and yaw like quadcopters?
If not, in any copter what makes them to roll pitch and yaw? Furthermore are there any dualcopter design that have movable wings that will rotate the rotors itself or do up and down motion while flying?
","quadcopter, multi-rotor"
Inverse kinematics solution for 6DOF serial arm,"My 6 joint robot arm structure doesn't meet the requirements for a closed form solution (no 3 consecutive axes intersecting at a point or 3 parallel axes...).  
What would be best method to adopt to get solution in 1ms or less? Estimation accuracy of 1mm. I'm assuming the computation is done on an average laptop Intel Core i3, 1.7GHz, 4GB RAM
",inverse-kinematics
"Could anyone tell me what are these things in a Roomba robot and how to clean them, please?","I'm really in doubt whether it is proper to ask this question here, so I'm apologizing if it is not, I'll delete it.
I have a Roomba robot which has worked for me for more than three years, and now while it is working it is producing some strange sounds, so I've decided to clean it thoroughly.
But when I disassembled it down to this point:
 
I got stuck with these sort of glass things (marked with the red rectangles at the picture). They are really filthy from the inside and I cannot figure out how to clean them.
Does anyone know how one can remove dust from the inside on these things? May be there are some Roomba creators here.
Thanks in advance.
",roomba
How to use a POMDP-based planner on top of a probabilistic filter,"POMDPs extend MDPs by conceiling state and adding an observation model. A POMDP controller processes either

action/observation histories or
a bayesian belief state, computed from the observations (belief-MDP transformation)

In a complex, real-world system like a robot, one usually preprocesses sensory readings using filters (Kalmann, HMM, whatever). The result of which is a belief-state.
I am looking for publications that discuss the problem of fitting a (probably more abstract) POMDP model on top of an existing filter-bank. 

Do you have to stick to the belief-MDP, and hand over the filtered belief-state to the controller?
Is there any way of using history-based POMDP controllers, like MCTS?
How do you construct/find the abstract observations you need to formulate the POMDP model?

","kalman-filter, particle-filter, planning, filter"
Mobile Robot path reconstruction by using IMU acceleration and Yaw angle,"I hope you can help me with my project.
I'm using a skid-steering wheeled mobile robot for autonomous navigation and I'd like to find a way to be able to perform path reconstruction in Matlab.
By using only the robot encoders (installed on the robot) and the yaw rate information (which come from a very accurate IMU sensor mounted on the robot frame), I can successfully do the path reconstruction.
(I'm using XBOW-300CC sensor)
The problem is that I would like to try to reconstruct the path by using only the IMU yaw rate and the IMU acceleration values for X and Y axis.
I'm able to obtain velocity and distance by integrating two times the IMU acceleration values but my problem is that I don't know how to use this data.
Do I have to use a rotation matrix to pass from the IMU frame to the robot frame coordinates? 
I'm asking this because I use a rotation matrix for the encoder values which come from the robot encoder.
At the moment, I use these equations for robot encoders and IMU yaw rate:
tetha(i)=(yaw(i)+yaw(i-1))/2*(encoder(i)-encoder(i-1))+tetha(i-1); %trapezoidal integral

Rx=[1 0 0;0 cos(-roll_angle(i)) -sin(-roll_angle(i)); 0 sin(-roll_angle(i)) cos(-roll_angle(i))];
Ry=[cos(-pitch_angle(i)) 0 sin(-pitch_angle(i)); 0 1 0; -sin(-pitch_angle(i)) 0 cos(-pitch_angle(i))];
Rz=[cos(-tetha(i)) -sin(-tetha(i)) 0; sin(-tetha(i)) cos(-tetha(i)) 0; 0 0 1];
R2=Rz*Ry*Rx;

disp=R2 *[encoder_displacement(i) 0 0]';
X_r(i)=disp(1);
Y_r(i)=disp(2);
Z_r(i)=disp(3);

X(i)=x0+sum(X_r(1:i));
Y(i)=y0+sum(Y_r(1:i));
Z(i)=z0+sum(Z_r(1:i));

Do I still have to use R2 matrix?
Thank you a lot
","mobile-robot, kinematics, imu, navigation, matlab"
Optimal-time acceleration sequence of a line-following robot following a moving obstacle,"Say we have a line-following robot that has a moving obstacle in front, that is a one-dimensional problem. The moving obstacle is defined by its initial state and a sequence of (longitudinal) acceleration changes (the acceleration function is piecewise constant). Let's say the robot can be controlled by specifying again a sequence of acceleration changes and its initial state. However, the robot has a maximum and minimum acceleration and a maximum and minimum velocity. How can I calculate the sequence of accelerations minimizing the time the robot needs to reach a goal. Note that the final velocity must not necessarily be zero.
Can you briefly explain how this problem can be addressed or point me to some references where an algorithm is described? Or point out closely related problems?
Furthermore, does the solution depend on the goal position or could the robot just brake as late as possible all the time (avoiding collisions) and still reach any goal in optimal time?
A more formal problem description:
Given the position of the obstacle $x_B(t) = x_{B,0} + \int_{t_0}^t v_B(t) dt$, and the velocity of the obstacle $v_B(t) = v_{B,0} + \int_{t_0}^t a_B(t) dt$, where $a_B$ is a known piecewise constant function:
$$a_B(t) = \begin{cases} a_{B,1} & \text{for } t_0 \leq t < t_1 \\
a_{B,2} & \text{for } t_1 \leq t < t_2 \\
\dots & \\
\end{cases}$$
and given the initial state of the line-follower $x_{A,0}, v_{A,0} \in \mathbb{R}$ we search for piecewise constant functions $a_A$, where $a_{min} \leq a_A(t) \leq a_{max}$, $v_{min} \leq v_A(t) \leq v_{max}$ and $x_A(t) \leq x_B(t)$ (collision freeness) holds at all times. Reasonable assumptions are e.g. $v_B(t) \geq 0$ and $x_{B,0} \geq x_{A,0}$. Among the feasible solutions I would like to pick those minimizing $\int_{t_0}^{\infty} x_B(t) - x_A(t) dt$ or a similar objective. Approximation algorithms are also ok.
Some numbers for those who would like a test input:
http://pastebin.com/iZsm2UhB
","mobile-robot, control, motion-planning, line-following"
Comparing LQR and PID controllers for inverted pendulum problem,"As far as i can tell, both LQR and PID controllers can both be applied to the cart-pole (inverted pendulum) problem.  What are the pros/cons to using one controller over the other for this particular problem?  Are there any reasons/situations where I should prefer one over the other for this problem?
","control, pid"
Motor for DIY Remote controlled shades,"I'm currently undertaking a project to build remote controlled shades from scratch. I currently have every piece figured out except I don't know know much about the motors involved in something like this. I am looking for suggestions on what type of motor to search for. I imagine I need a type that can go forward and back as well as stop when the shade is fully retracted. I don't know what to search for though.
Any help is much appreciated. 
",motor
How to determine x-axis if the two z-axis are intersecting in Denavit Hartenberg representation,"Suppose I have a 3 link(1 dimensional) chain in which all the joints are revolutes, the axis of first revolute joint is along Z-axis(global) and axis of second joint is along X-axis(global). The first link is along X-axis(global) and second link is along Z-axis(global).
Now in order to use DH representation I introduced a local frame for link 1 at joint 1(z axis along Z and x axis along X) and another frame at joint 2.Here z-axis is along axis of rotation(global X) and here I am clueless how to determine x-axis for joint 2 because the two z axis are intersecting.(standard procedure is to find common normal between two z axis)
Thanks for your time.
","robotic-arm, dh-parameters"
What are the best ways to transmit force through air efficiently?,"I am taking part in a robotics competition, where the challenge is to create a pair of robots which successfully navigate a series of obstacles. However, the rules state that of the two robots, only one must have a driving actuator. The other must somehow be moved by the other robot, WITHOUT PHYSICAL CONTACT. 
I could think of either having sails on the non-driving robot, and moving it with fans on the driving one OR electromangnets on the driving one and permanent magnets with the opposite polarity on the non-driving one. However the problem with both is that efficiency falls off drastically with distance. Thus, I am looking for possible ways to overcome this problem. Thanks :)
Also, the driving robot has a cable power supply, while the non-driving one may only have batteries.
Rulebook: http://ultimatist.com/video/Rulebook2016_Final_website_1_Sep_15.zip
",force
How to convert between classic and modified DH parameters?,"I currently have a description of my 22 joint robot in ""classic"" DH parameters.  However, I would like the ""modified"" parameters.  Is this conversion as simple as shifting the $a$ and $alpha$ columns of the parameter table by one row?
As you can imagine, 22 joints is a lot, so I'd rather not re-derive all the parameters if I don't have to.  (Actually, the classic parameters are pulled out of OpenRave with the command: planningutils.GetDHParameters(robot).  
","kinematics, dh-parameters"
IR 40kHz receiver,"These days I'm trying to build IR 40kHz long range receiver. I use ir phototransistor. I don't want to use components like TSOP... I need to make
daylight filter and intensify filtred signal because out of this sensor I wanna use with some microcontroller. Can someone help me? Any idea? Thanks.
",sensors
What factors should i consider when selecting a motor for a free wheeled cart-pole balancing robot?,"I'm developing a small scale cart-pole balancing robot consisting of two wheels driven by a single motor at the base (essentially like a unicycle, but with two wheels to constrain balance to a one dimensional problem).
I'm not sure what qualities to look for in that motor.  I think the motor should be able to accelerate quickly in directions opposite of motion as dictated by the control system.  However, i'm not sure if this rapid acceleration should correlate with higher torque motors or faster speed motors.  I think higher torque motors would be too slow to react to control commands.  In contrast, fast speed motors may not be able to overcome the momentum of the cart.
Are there any design equations or other calculations i can make based on my robot's dimensions and weight to determine the right specs needed for my robot's motor?  How can i determine the right motor specs for this application without resorting to brute-force trial & error experiments?
","motor, design, balance"
How to use the Homogeneous transformation matrix?,"I am trying to understand how to use, what it requires compute the homogenous transformation matrix. 
I know 2 points from 2 different frames, and 2 origins from their corresponding frames. 
I how transformation matrix looks like, but whats confusing me is how i should compute the (3x1) position vector which the matrix needs.  As i understand is, this vector a origin of the old frame compared to the new frame.  But how to calculate it, the obvious answer (I think) would be to subtract both ($O_{new} - O_{old}$ ), but it does not feel right. 
I know its a simple question but my head cannot get around this issue, and how can i prove it the right way, with the information i know?
","kinematics, frame"
"what kp,ki,kd should i keep","  // MPU-6050 Short Example Sketch
// By Arduino User JohnChi
// August 17, 2014
// Public Domain
#include<Wire.h>
#include <Servo.h>
Servo firstESC, secondESC,thirdESC,fourthESC; //Create as much as Servoobject you want. You 
const int MPU=0x68;  // I2C address of the MPU-6050
int speed1=2000,speed2=0,speed3=0,speed4;
int16_t AcX,AcY,AcZ,Tmp,GyX,GyY,GyZ;
float integ=0,der=0,pidx=0,kp = .5,ki=0.00005 ,kd=.01,prerror,dt=100;
void setup(){
  firstESC.attach(3);    // attached to pin 9 I just do this with 1 Servo 
  secondESC.attach(5);    // attached to pin 9 I just do this with 1 Servo 
  thirdESC.attach(6);    // attached to pin 9 I just do this with 1 Servo 
  fourthESC.attach(9);    // attached to pin 9 I just do this with 1 Servo 
  Wire.begin();
  Wire.beginTransmission(MPU);
  Wire.write(0x6B);  // PWR_MGMT_1 register
  Wire.write(0);     // set to zero (wakes up the MPU-6050)
  Wire.endTransmission(true);
  Wire.beginTransmission(MPU);
  Wire.write(0x1c);  // PWR_MGMT_1 register
  Wire.write(0<<3);     // set to zero (wakes up the MPU-6050)
  Wire.endTransmission(true);
  Serial.begin(9600);
  firstESC.writeMicroseconds(0);
  secondESC.writeMicroseconds(0);
  thirdESC.writeMicroseconds(0);
  fourthESC.writeMicroseconds(0);
  firstESC.writeMicroseconds(2000);
  secondESC.writeMicroseconds(2000);
  thirdESC.writeMicroseconds(2000);
  fourthESC.writeMicroseconds(2000);
  delay(2000);
  firstESC.writeMicroseconds(700);
  secondESC.writeMicroseconds(700);
  thirdESC.writeMicroseconds(700);
  fourthESC.writeMicroseconds(700);
  delay(2000);
}
void loop(){
  Wire.beginTransmission(MPU);
  Wire.write(0x3B);  // starting with register 0x3B (ACCEL_XOUT_H)
  Wire.endTransmission(false);
  Wire.requestFrom(MPU,4,true);  // request a total of 14 registers
  AcX=Wire.read()<<8|Wire.read();  // 0x3B (ACCEL_XOUT_H) & 0x3C (ACCEL_XOUT_L)    
  AcY=Wire.read()<<8|Wire.read();  // 0x3D (ACCEL_YOUT_H) & 0x3E (ACCEL_YOUT_L) 
  firstESC.writeMicroseconds(0);
  secondESC.writeMicroseconds(700-(pidx/10));
  thirdESC.writeMicroseconds(700+(pidx/10));
  fourthESC.writeMicroseconds(0);
  PID();
  //if(Serial.available()) 
    //speed1 = Serial.parseInt(); 
  //Serial.print(""AcX = ""); Serial.print(AcX);
  //Serial.print("" | AcY = ""); Serial.print(AcY); 
  //Serial.println();
  //delay(333);
}

void PIdD()
{
  float error;
  error = (atan2(AcY,AcZ)*180/3.14);
  now = millis(); 
  dt = now-ptime;
  if(error>0)error=180-error;
  else error = -(180+error); 
  error=0-error;
  integ = integ+(error*dt) ;
  der = (error - prerror)/dt ;
  prerror=error;
  pidx = (kp*error);
  pidx+=(ki*integ);
  pidx+=(kd*der);
  if(pidx>1000)pidx=1000; 
  if(pidx<-1000)pidx=-1000; 
  ptime = now; 
  }

the above is my program for my quadcopter, but now i have to tune the PID values, that is kp, ki and kd. my accelome is at 2g. Please point to me what is wrong with the program, is the error signal not appropriate? Please also give me or help me choose correct pid tuning. my limitation is I always have to connect my arduino to pc and change kp ki or kd values, that is i have no remote control available currently.
","quadcopter, pid"
Maximum ball screw speeds,"What is the maximum rotational velocity of miniature ball-screw (diameters up to 12mm) for approximately 1000 thrust cycles, and which type/brand would that be, if the speed is limited by the ball return mechanism? The fastest I could find was 4000 rpm at 3000 N thrust, but this was from a datasheet with a big safety margin (millions of cycles).
I'm looking for either experience and data, or a general method/formula that can be used to find the maximum velocity (and load) as function of cycles or the other way round (similar to those of ball bearings). Suggestions and knowledge about faster types and brands of ballscrews than the ones I have been able to find is welcome as well.
Some more background information:
Ball screws are very interesting transmissions for electrically actuated legged robotics, since they provide a high-geared rotary-to-linear transmission that is accurate, precise, energy efficient and possibly backlash-free. However, the big downside is their limited rotational speed. The maximum rotational velocity is limited by resonance and the ball return mechanism. The former limit is easy to calculate (eigenfrequency calculation), and mostly not problematic for small spindles. However, the latter is a bigger problem. The balls in a ball screw roll through the threaded spindle and have to be recirculated to the other end of the nut. The recirculation limits the rotational velocity of the ballscrews. The corresponding maximum rotational velocities are not calculate-able (for as far as I know) and are provided by manufacturers in catalogues, either directly in rpm or via a so-called $D_n$-value, where the rotational velocity in rpm is $n=D_n/d$ where d is the diameter of the ball screw. But even then, the maximum rotational velocity of ball screws is capped at 4000 rpm or lower according to datasheets (depending on brand and ball return mechanism). The highest permissible rotational velocities I found were those of Steinmeyer ballscrews, at 4000 rpm, using an end-cap-return mechanism. Note that for electrical motors (up to 200W) ideal (maximum power) velocities are higher than 4000 rpm, and even more than twice as high for many brushless motors. It appears however that ball screws can run at higher speeds than what they are specified for in reality, because the specifications hold for many millions of cycles. I can only find a single unofficial source where someone claims to have run their ball-screws up to 6000 rpm, and in missiles (one-time-use) up to 7500 rpm. I'm interested in a theory or more experimental data that backs this up.
",driver
IRobot Create 2: Powering Up after Sleep,"I've notice the IRobot Create 2 does not respond to the app's commands when it has been sleeping.  If I press the Clean button and re-run the app then the robot is responsive to the commands.
My initialization sequence (Android/Java) using usb-serial-for-android:
port.open(connection);
port.setParameters(115200, 8, UsbSerialPort.STOPBITS_1,UsbSerialPort.PARITY_NONE);
command(Opcode.START);
command(Opcode.SAFE);

The physical architecture is IRobot Create 2 connected by IRobot Serial Cable to Google Project Tango Tablet.
How can my app wake up the Roomba from it's sleep?
",irobot-create
Line following robot with EV3 Colour Sensor,"I am trying to build an advanced coloured lines following robot with the ability to differentiate between many different coloured lines and follow them. I am looking for the right sensor that will help my robot achieve its objective.
As I was researching I came across the EV3 Colour Sensor which can detect up to 7 colours.
Is this sensor suitable for my project?
What other sensors can I use and how?
Thank You
","mobile-robot, sensors, line-following"
composition of rotation matrices,"I am the moment learning about rotation matrices.  It seems confusing how it could be that  $R_A^C=R_A^BR_B^C$ is the rotation from coordinate frame A to C C to A, and A,B,C are different coordinate frames.
$R_A^C$ must for a 2x2 matrix be defined as 
$$
R_A^C=
\left(
\begin{matrix}
xa⋅xb  &  xa⋅xb \\
ya⋅yb &  ya⋅yb
\end{matrix}
\right)
$$
$x_a, y_a and x_b,y_b$ are coordinates for points given in different coordinate frame.
I don't see how, using this standard, the multiplication stated above will give the same matrix as for $R_A^C$. Some form for clarification would be helpful here.
",frame
How to sumarize Kalman filter covariances for display?,"I'm implementing an extended Kalman filter and I'm facing a problem with showing the covariances to the user.
The covariance matrix estimate contains all the information we have about the current value estimate, but that is too much to display.
I would like to have a single number that says ""our estimate is really good"" when close to 0 and ""our estimate is not worth much"" when large.
My intuitive simple solution would be to average all the values in the covariance estimate matrix (or maybe just the diagonal), except that in my case the values have different units and different ranges.
Is it possible to do something like this?
",kalman-filter
Raspberry pi quadcopter drifts like crazy,"I have recently built a raspberry pi based quadcopter that communicates with my tablet over wifi. The problem is that it drifts a lot. At first I thought that the problem was vibration, so I mounted the MPU-6050 more securely to the frame. That seemed to help a bit, but it still drifts. I have tried tuning the PID, tuning the complementary filter, and installing a real time OS. Nothing seems to help very much. Below is my code written completely in java. Any suggestions are appreciated.
QuadServer.java:
package com.zachary.quadserver;

import java.net.*;
import java.io.*;
import java.util.*;

import com.pi4j.io.i2c.I2CBus;
import com.pi4j.io.i2c.I2CDevice;
import com.pi4j.io.i2c.I2CFactory;

import se.hirt.pi.adafruit.pwm.PWMDevice;
import se.hirt.pi.adafruit.pwm.PWMDevice.PWMChannel;

public class QuadServer {
    private final static int FREQUENCY = 490; 

    private static final int MIN = 740;
    private static final int MAX = 2029;

    private static Sensor sensor = new Sensor();

    private static double PX = 0;
    private static double PY = 0;
    private static double PZ = 0;

    private static double IX = 0;
    private static double IY = 0;
    private static double IZ = 0;

    private static double DX = 0;
    private static double DY = 0;
    private static double DZ = 0;

    private static double kP = 1.95; //2.0
    private static double kI = 10.8; //8.5
    private static double kD = 0.15; //0.14

    private static long time = System.currentTimeMillis();

    private static double last_errorX = 0;
    private static double last_errorY = 0;
    private static double last_errorZ = 0;

    private static double outputX;
    private static double outputY;
    private static double outputZ;

    private static int val[] = new int[4];

    private static int throttle;

    static double setpointX = 0;
    static double setpointY = 0;
    static double setpointZ = 0;

    static double errorX;
    static double errorY;
    static double errorZ;

    static long receivedTime = System.currentTimeMillis();

    private static String data;

    static int trimX = -70;
    static int trimY = 70;

    public static void main(String[] args) throws IOException, NullPointerException {
        DatagramSocket serverSocket = new DatagramSocket(40002);

        PWMDevice device = new PWMDevice();

        device.setPWMFreqency(FREQUENCY);

        PWMChannel esc0 = device.getChannel(0);
        PWMChannel esc1 = device.getChannel(1);
        PWMChannel esc2 = device.getChannel(2);
        PWMChannel esc3 = device.getChannel(3);

        /*Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
                public void run() {
                    System.out.println(""terminating"");
                    try {
                        esc0.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(MIN/1000.0, FREQUENCY));
                } catch (IOException e) {
                    e.printStackTrace();
                }
                }
            }));
            System.out.println(""running"");*/


        Thread read = new Thread(){
                public void run(){
                    while(true) {
                    try {
                            byte receiveData[] = new byte[1024];
                            DatagramPacket receivePacket = new DatagramPacket(receiveData, receiveData.length);
                            serverSocket.receive(receivePacket);
                            String message = new String(receivePacket.getData());

                            data = """"+IX;
                        addData(IY);

                        addData(sensor.readAccelAngle(0));
                        addData(sensor.readAccelAngle(1));

                        byte[] sendData = new byte[1024];
                            sendData = data.getBytes();
                                InetAddress IPAddress = InetAddress.getByName(""192.168.1.9"");
                                DatagramPacket sendPacket = new DatagramPacket(sendData, sendData.length, IPAddress, 1025);
                                serverSocket.send(sendPacket);

                            setpointX = Double.parseDouble(message.split(""\\s+"")[0])*0.7;
                        setpointY = Double.parseDouble(message.split(""\\s+"")[1])*0.7;

                            throttle = (int)(Integer.parseInt((message.split(""\\s+"")[3]))*12.67)+MIN;

                            kP = Math.round((Integer.parseInt(message.split(""\\s+"")[4])*0.05)*1000.0)/1000.0;
                            kI = Math.round((Integer.parseInt(message.split(""\\s+"")[5])*0.2)*1000.0)/1000.0;
                            kD = Math.round((Integer.parseInt(message.split(""\\s+"")[6])*0.01)*1000.0)/1000.0;

                            trimX = (Integer.parseInt(message.split(""\\s+"")[7])-50)*2;
                            trimY = (Integer.parseInt(message.split(""\\s+"")[8])-50)*2;

                            double accelSmoothing = 0.02;//(Integer.parseInt(message.split(""\\s+"")[8])*0.05)+1;
                            double gyroSmoothing = 0.04;//(Integer.parseInt(message.split(""\\s+"")[7])*0.01);

                            sensor.setSmoothing(gyroSmoothing, accelSmoothing);

                            //System.out.println(""trimX: ""+trimX+"" trimY: ""+trimY);

                            System.out.println(""kP: ""+kP+"", kI: ""+kI+"", kD: ""+kD+"", trimX: ""+trimX+"", trimY: ""+trimY);

                        receivedTime = System.currentTimeMillis();

                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
        };
        read.start();

        while(true)
        {
            Arrays.fill(val, throttle);

            errorX = sensor.readGyro(0)-setpointX;
            errorY = -sensor.readGyro(1)-setpointY;
            errorZ = sensor.readGyro(2)-setpointZ;

            double dt = (double)(System.currentTimeMillis()-time)/1000;

            double accelAngleX = sensor.readAccelAngle(0);
            double accelAngleY = sensor.readAccelAngle(1);

            if(dt > 0.005)
            {

                PX = errorX;
                PY = errorY;
                PZ = errorZ;

                IX += (errorX)*dt;
                IY += (errorY)*dt;
                //IZ += errorZ*dt;

                IX = 0.98*IX+0.02*accelAngleX;
                IY = 0.98*IY+0.02*accelAngleY;

                DX = (errorX - last_errorX)/dt;
                DY = (errorY - last_errorY)/dt;
                //DZ = (errorZ - last_errorZ)/dt;

                last_errorX = errorX;
                last_errorY = errorY;
                last_errorZ = errorZ;

                outputX = kP*PX+kI*IX+kD*DX;
                outputY = kP*PY+kI*IY+kD*DY;
                outputZ = kP*PZ+kI*IZ+kD*DZ;

                time = System.currentTimeMillis();
            }

            //System.out.println(IX+"", ""+IY+"", ""+throttle);

            add(-outputX-outputY-outputZ-trimX+trimY, 0);       //clockwise
            add(-outputX+outputY+outputZ-trimX-trimY, 1);   //counterClockwise
            add(outputX+outputY-outputZ+trimX-trimY, 2);    //clockwise
            add(outputX-outputY+outputZ+trimX+trimY, 3);        //counterclockwise

            //System.out.println(val[0]+"", ""+val[1]+"", ""+val[2]+"", ""+val[3]);

            try {
                if(System.currentTimeMillis()-receivedTime < 1000)
                {
                    esc0.setPWM(0, calculatePulseWidth(val[0]/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(val[1]/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(val[2]/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(val[3]/1000.0, FREQUENCY));
                } else 
                {
                    esc0.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc1.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc2.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                    esc3.setPWM(0, calculatePulseWidth(800/1000.0, FREQUENCY));
                }
            } catch (IOException e) {
                e.printStackTrace();
            }

        }
    }

    private static void add(double value, int i)
    {
        if(val[i]+value > MIN && val[i]+value < MAX)
        {
            val[i] += value;
        }else if(val[i]+value < MIN)
        {
            //System.out.println(""low"");
            val[i] = MIN;
        }else if(val[i]+value > MAX)
        {
            //System.out.println(""low"");
            val[i] = MAX;
        }
    }

    static void addData(double value)
    {
        data += "" ""+value;
    }

    private static int calculatePulseWidth(double millis, int frequency) {
        return (int) (Math.round(4096 * millis * frequency/1000));
    }
}

Sensor.java:
package com.zachary.quadserver;

import com.pi4j.io.gpio.GpioController;
import com.pi4j.io.gpio.GpioFactory;
import com.pi4j.io.gpio.GpioPinDigitalOutput;
import com.pi4j.io.gpio.PinState;
import com.pi4j.io.gpio.RaspiPin;
import com.pi4j.io.i2c.*;

import java.net.*;
import java.io.*;

public class Sensor {
    static I2CDevice sensor;
    static I2CBus bus;
    static byte[] accelData, gyroData;
    static long accelCalib[] = {0, 0, 0};
    static long gyroCalib[] = {0, 0, 0};

    static double gyroX;
    static double gyroY;
    static double gyroZ;

    static double smoothedGyroX;
    static double smoothedGyroY;
    static double smoothedGyroZ;

    static double accelX;
    static double accelY;
    static double accelZ;

    static double accelAngleX;
    static double accelAngleY;

    static double smoothedAccelAngleX;
    static double smoothedAccelAngleY;

    static double angleX;
    static double angleY;
    static double angleZ;

    static boolean init = true;

    static double accelSmoothing = 1;
    static double gyroSmoothing = 1;

    public Sensor() {
        try {
            bus = I2CFactory.getInstance(I2CBus.BUS_1);

            sensor = bus.getDevice(0x68);

            sensor.write(0x6B, (byte) 0x0);
            sensor.write(0x6C, (byte) 0x0);
            System.out.println(""Calibrating..."");

            calibrate();

            Thread sensors = new Thread(){
                    public void run(){
                        try {
                            readSensors();
                        } catch (IOException e) {
                        e.printStackTrace();
                    }
                    }
            };
            sensors.start();
        } catch (IOException e) {
            System.out.println(e.getMessage());
        }
    }

    private static void readSensors() throws IOException {
        long time = System.currentTimeMillis();
        long sendTime = System.currentTimeMillis();

        while (true) {
            accelData = new byte[6];
            gyroData = new byte[6];

            int r = sensor.read(0x3B, accelData, 0, 6);

            accelX = (((accelData[0] << 8)+accelData[1]-accelCalib[0])/16384.0)*9.8;
            accelY = (((accelData[2] << 8)+accelData[3]-accelCalib[1])/16384.0)*9.8;
            accelZ = ((((accelData[4] << 8)+accelData[5]-accelCalib[2])/16384.0)*9.8)+9.8;
            accelZ = 9.8-Math.abs(accelZ-9.8);


            double hypotX = Math.sqrt(Math.pow(accelX, 2)+Math.pow(accelZ, 2));
            double hypotY = Math.sqrt(Math.pow(accelY, 2)+Math.pow(accelZ, 2));

            accelAngleX = Math.toDegrees(Math.asin(accelY/hypotY));
            accelAngleY = Math.toDegrees(Math.asin(accelX/hypotX));

            //System.out.println(accelAngleX[0]+"" ""+accelAngleX[1]+"" ""+accelAngleX[2]+"" ""+accelAngleX[3]);

            //System.out.println(""accelX: "" + accelX+"" accelY: "" + accelY+"" accelZ: "" + accelZ);

            r = sensor.read(0x43, gyroData, 0, 6);

            gyroX = (((gyroData[0] << 8)+gyroData[1]-gyroCalib[0])/131.0);
            gyroY = (((gyroData[2] << 8)+gyroData[3]-gyroCalib[1])/131.0);
            gyroZ = (((gyroData[4] << 8)+gyroData[5]-gyroCalib[2])/131.0);

            if(init)
            {
                smoothedAccelAngleX = accelAngleX;
                smoothedAccelAngleY = accelAngleY;

                smoothedGyroX = gyroX;
                smoothedGyroY = gyroY;
                smoothedGyroZ = gyroZ;

                init = false;
            } else {
                smoothedAccelAngleX = smoothedAccelAngleX+(accelSmoothing*(accelAngleX-smoothedAccelAngleX));
                smoothedAccelAngleY = smoothedAccelAngleY+(accelSmoothing*(accelAngleY-smoothedAccelAngleY));

                smoothedGyroX = smoothedGyroX+(gyroSmoothing*(gyroX-smoothedGyroX));
                smoothedGyroY = smoothedGyroY+(gyroSmoothing*(gyroY-smoothedGyroY));
                smoothedGyroZ = smoothedGyroZ+(gyroSmoothing*(gyroZ-smoothedGyroZ));

                /*smoothedAccelAngleX = accelAngleX;
                smoothedAccelAngleY = accelAngleY;

                smoothedGyroX = gyroX;
                smoothedGyroY = gyroY;
                smoothedGyroY = gyroY;*/


                /*smoothedAccelAngleX += (accelAngleX-smoothedAccelAngleX)/accelSmoothing;
                smoothedAccelAngleY += (accelAngleY-smoothedAccelAngleY)/accelSmoothing;

                smoothedGyroX += (gyroX-smoothedGyroX)/gyroSmoothing;
                smoothedGyroY += (gyroY-smoothedGyroY)/gyroSmoothing;
                smoothedGyroZ += (gyroZ-smoothedGyroZ)/gyroSmoothing;*/

            }

            angleX += smoothedGyroX*(System.currentTimeMillis()-time)/1000;
            angleY += smoothedGyroY*(System.currentTimeMillis()-time)/1000;
            angleZ += smoothedGyroZ;

            angleX = 0.95*angleX + 0.05*smoothedAccelAngleX;
            angleY = 0.95*angleY + 0.05*smoothedAccelAngleY;

            time = System.currentTimeMillis();

            //System.out.println((int)angleX+""  ""+(int)angleY);
            //System.out.println((int)accelAngleX+"", ""+(int)accelAngleY);
        }
    }

    public static void calibrate() throws IOException {
        int i;
        for(i = 0; i < 100; i++)
        {
            accelData = new byte[6];
            gyroData = new byte[6];
            int r = sensor.read(0x3B, accelData, 0, 6);
            accelCalib[0] += (accelData[0] << 8)+accelData[1];
            accelCalib[1] += (accelData[2] << 8)+accelData[3];
            accelCalib[2] += (accelData[4] << 8)+accelData[5];

            r = sensor.read(0x43, gyroData, 0, 6);
            gyroCalib[0] += (gyroData[0] << 8)+gyroData[1];
            gyroCalib[1] += (gyroData[2] << 8)+gyroData[3];
            gyroCalib[2] += (gyroData[4] << 8)+gyroData[5];
            try {
                Thread.sleep(1);
            } catch (Exception e){
                e.printStackTrace();
            }
        }
        gyroCalib[0] /= i;
        gyroCalib[1] /= i;
        gyroCalib[2] /= i;

        accelCalib[0] /= i;
        accelCalib[1] /= i;
        accelCalib[2] /= i;

        System.out.println(gyroCalib[0]+"", ""+gyroCalib[1]+"", ""+gyroCalib[2]);
        System.out.println(accelCalib[0]+"", ""+accelCalib[1]+"", ""+accelCalib[2]);
    }

    public double readAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return angleX;
            case 1:
                return angleY;
            case 2:
                return angleZ;
        }

        return 0;
    }

    public double readGyro(int axis)
    {
        switch (axis)
        {
            case 0:
                return smoothedGyroX;
            case 1:
                return smoothedGyroY;
            case 2:
                return smoothedGyroZ;
        }

        return 0;
    }

    public double readAccel(int axis)
    {
        switch (axis)
        {
            case 0:
                return accelX;
            case 1:
                return accelY;
            case 2:
                return accelZ;
        }

        return 0;
    }

    public double readAccelAngle(int axis)
    {
        switch (axis)
        {
            case 0:
                return smoothedAccelAngleX;
            case 1:
                return smoothedAccelAngleY;

        }

        return 0;
    }

    public void setSmoothing(double gyro, double accel)
    {
        gyroSmoothing = gyro;
        accelSmoothing = accel;
    }
}

","pid, raspberry-pi, quadcopter"
Weave Weld Lincoln Electric Mig Robot,"This is a simple question that I can't seem to find the answer for but when setting up the weave function how exactly does frequency (Hz) determine how fast it moves back and forth? 
In other words if I raise frequency will it move quicker or slower and what factors must I consider? 
","robotic-arm, industrial-robot"
continuous vs discrete simulation in robotics,"As far as I know, a robot sends orders as discrete signals. However, isn't computer simulation based on continuous simulation? Do you know if it may happen any important difference when comparing reality to simulation in some cases? I heard that cable-driven robots were quite sensitive.
","control, simulation"
Using a Bitmap maze image to navigate the maze,"I'm working on an robot that would be able to navigate through a maze, avoid obstacles and  identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.
I am just a first year electrical engineering student, and so need help on how I can use the bmp image. I will be making my robot using the Arduino mega microcontroller.
So how should I get started on it.
If you need me to elaborate on anything kindly say so. 
Link: http://ceme.nust.edu.pk/nerc/files/theme_ind_2015.pdf
","arduino, control, localization"
What I need to learn to build robots,"What subjects are involved in robotics. If I want to build robots then what necessary things I need to learn consecutively as a beginner.
","artificial-intelligence, embedded-systems, first-robotics"
PID quaternion contoller,"I want to control the attitude(roll, pitch, yaw) of a vehicle capable of pitching and rolling. To do this I have created a quaternion PID controller. First I take the current attitude of the vehicle converting it to a quaternion Qc and do the same for the desired attitude with the quaternion Qd. I then calculate the input of my PID controller as Qr = Qc' x Qd. The imaginary parts of the quaternions are then fed as force requests on the roll, pitch, yaw axes of the vehicle. I test on a simulator and the control works but becomes unstable in some cases (request for R: 60 P: 60 Y:60). I also want this to work around singularities (i.e. pitch 90)
Does anyone know why I get this behavior and if so explain (thoroughly) what I'm doing wrong?
","control, pid, stability"
indoor positioning system: which is better?,"Which method is better, in term of accuracy, for detection of indoor localization of a drone. Camera based system or wireless techniques like WLAN or Bluetooth?
",slam
Arduino mobile robot,"Is there a way I can control my arduino robot from anywhere in the world. The robot goes out of range of my home wifi so my wifi shield can't help. Is there a way to make sure the robot is always on the Internet no matter where it goes? 
","arduino, mobile-robot, raspberry-pi"
Quadcopter accelerating or not,"I am on the project quadcopter. So i have to use PID for stabalizing it. I think i am going wrong because i am adding the pid output to motors thrust. While the motors thrust means to be its acceleraTion. The reason of my previous statment is that when the quad is static in air(not goin up nor below), that time the thrust is enough to cancel gravity, means thrust is negative gravity, that is acceleration. So if i add pid output to thrust that is acceleration of motors, it will be wrong. I have to add pid to speed of motors, which is not visible. My quad is not stabalizing the reason i see is this, that i am adding pid to acc, while it should be added to speed(virtually). What should i do. Should i derivate the pid output and add to thrust? https://mbasic.facebook.com/photo.php?fbid=1545278952394916&id=100007384772233&set=a.1447457675510378.1073741830.100007384772233&refid=17&ft=top_level_post_id.1545278952394916%3Athid.100007384772233%3A306061129499414%3A69%3A0%3A1443682799%3A-1394728329505289925&tn=E
https://mbasic.facebook.com/photo.php?fbid=1545281645727980&id=100007384772233&set=a.1447457675510378.1073741830.100007384772233&refid=17&tn=E
This is the drawing of my circuit. I am giving the current from one esc to whole of the circuit. Other esc's has only pwm wire connected to circuit.
","quadcopter, pid"
Ceiling depth with a monocular camera,"Having a camera mounted on my robot and looking upwards, I want to estimate the distance of the ceiling as the robot moves and also the position of landmarks observed on the ceiling (lamps for example).
I know this is a structure from motion problem but I was very confused on how to implement it. This case is a much simpler case than bundle adjustment as the intrinsic calibration of the camera is existing, the camera pose changes just in x and y directions, and the observed thing is a planar ceiling. Odometry might also be available but I would like to start solving it without. Do you know any libraries that offer a good and simple API to do such a thing? preferably based on levenberg-marquardt or similar optimization algorithms taking in more than just two observations. (Python bindings would be nice to have)
","cameras, 3d-reconstruction"
How to calculate vehicle detection distance,"I would like to know how to calculate the distance to each car when I run my application for an autonomous vehicle in real time. In addition I want to know how implement the calculation in C++. 
You can see in the images we can know the distance for each vehicle but I don't know what code I should use to make all these calculations for every vehicle .
Please check the photo to understand more about what I'm trying to achieve.


","control, sensors, localization, ros, cameras"
Axis of rotation via IMU,"Using an IMU (gyro, accelerometer and magnetometer), as found in most smartphones, can I detect the differences between tilting the device, say forward, along different (parallel) axis positions? 
To clarify, if the axis of rotation is far from the sensor, the the motion contains a translational component.
Can the distance and position of this axis be extracted from the IMU data and if so how?
Is there some data fusion algorithm that can do all this?
","imu, accelerometer, gyroscope, magnetometer"
Arm to disassemble and assemble notebook at home?,"Suppose I have perfect AI to control robotic arm. 
What characteristics should it fulfill to be able to take such common tools as screwdriver and linesman's and disassemble and then assemble conventional notebook computer?
Are there such models available?
Is seems to me, that such arms as OWI-535 are only toys, i.e. they can just relocate lightweight objects and that's all. Am I right?
UPDATE
Also suppose that my AI can look at assembly area with multiple HD cameras and can perfectly ""understand"" what is sees.
",robotic-arm
Air hockey with a robot as an opponent,"I'm not sure if this is the right place to post this but here goes.
So, as the title states, I'm planning on building a desk that doubles as an air hockey table which has a robot on the other side.
The robot would be mounted on a rail which should be able to go left and right using a linear actuator. It should be able to ""attack"" the puck using two servos.
The real problem is how should I detect the puck's location?
My idea:
Since the table would have tiny holes in the corners of a every square(0.5inx0.5in), I could fit in a laser on the bottom part of the table, a laser for ever 1in so a 1inx1in square, the same location would be reflected on the ""ceiling"" of the table but instead of laser diodes, they would be replaced by an ldr. 
So I'm planning on doing a matrix and reading the signals of the ldr's columns and rows then performing some logic to locate the center of the puck.
PROBLEMS:
While I don't see any performance flaws in my plan, I see tons of flaws when done imperfectly even to the tiniest bit.

I have to be exactly accurate regarding the laser diode's position,
it has to be on the center of the holes, right below the z-axis.
This should be easy if I'm just going to place 4 or 5. But I'm not.
According to my estimations, I'm going to have to use 300-700 laser
diodes, depending on if I'm planning on putting the lasers only on
the opponent's side or on the entire board. It would definitely be
costly. Imagine 300...
This isn't really a huge problem, more like a hassle. Wiring 300 of
these. Forget the pcbs, the project area is just to large.

I have thought of numerous way to lessen these, like using a color sensor to get the x-axis location and a laser situated on a negative x-axis pointing to the positive x-axis to locate the puck's y location, but I'm still comparing ideas.
Advantages:
I could get a 3d-like graphical representation with 3d-like controls (3d-like in reality but technically 2d since the lasers are only plotted in the x and y axis though facing the z-axis). 
Since this project is going to be my room desk, situated in an automated room, I was thinking of making ""desk modes"" which should toggle between a game that takes advantage of the lasers and their controls, A control desk for my room, ordinary desk mode, and an air hockey mode.
My question: (More like a request)
Does anyone have another idea regarding how I should be able to locate the puck's x and y location accurately in real time?
EDIT: The table is roll-able and stored underneath a loft bed which has an under-area height of 5'4"". Which means I can't go grande on the a vertical solution.
EDIT #2: Thanks to the helpful people here, I have come to the conclusion of using a camera.
The camera will be that of a smartphone's, I'll create an app that tracks an object by color and a has fixed size comparison to identify the distance of the robot from the puck. The phone will then process this and send signals via bluetooth.
The phone is anchored at the end of the robot's moving part so the camera is reminiscent of those games with a first-person view.
Incoming problems: I'm looking forward to some delay, given the delay in processing.
","sensors, microcontroller, design, electronics, laser"
Measuring angular displacement using the TI-SensorTag,"I've looked around but can't find the answer, to what I hope is a simple question. I'm working with a TI-SensorTag, and I want to be able measure the rotation around the unit's Z-axis. Basically I want to attach the tag to a clock pendulum, lie the clock on a table so the tag and clock face point up, and to measure the angular displacement of the pendulum as it swings back and forth. I'm hoping the mental image translated well! 
My understanding is that I can solve for displacement by multiplying my gyroscope readings by my sampling period, but I'm not sure how to compensate for drift. So my questions are: is my approach sound, and is the answer to drift to use the changing x and y accelerations? Or would I need to somehow incorporate the magnetometer readings?
Thanks!
","sensors, kinematics, gyroscope"
Book on mechanisms,"I wanted to know if there is any sort of archive of mechanisms that contains a brief description of mechanisms like there type of motion and forces involved. Not lengthy derivations and other stuff. 
","mobile-robot, mechanism"
How to make door opening to the top,"I need to make this construction (door is closed by default, door is opened to the top). This is the scheme:

Red rectangle on the picture is the aperture, blue rectangle is the door (weight is about 0.5 kg), which moves top when door need to be opened. Green stripe on the picture is the rail for the door.
Which electrical engine should I use?
Estimated time of door opening is about 10 seconds, I want to send signal to up the door, it should be drop down when power is lost or I should send a signal to drop it down.
","design, electronics, actuator"
Total Hand calculations procedure & formulaes of Mega-Quadcopter,"I am a student of BE taking Mega-Quadcopter as my final year project.Can u please help me with the total hand calculations of the mega-copter i.e its procedure and formulaes? . I wanted to know how to calculate the dimensions of frame,specifications of motor and propeller,the rating of ESC's and the power rating of the batteries and its total no.s.I do not want direct answers but its procedure and formulaes.I want to lift aload of around 20-30 kgs .Please feel free to help.
",quadcopter
Ubuntu ARM lacking /sys/devices/cape-bone-iio,"I'm trying to pull analog input from a beaglebone black using this tutorial. However when I go to /sys/devices there is no cape-bone-iio. I have spoken with several other programmers and one of them suggested that the cape-bone does not work with the newer versions of Linux. However downgrading could have negative impact on the rest of the project. Is there any other solution?
","beagle-bone, linux"
How to numerically calculate the Jacobian?,"I'm trying to calculate the Jacobian for days now. But first some details. Within my Master's Thesis I have to numerically calculate the Jacobian for a tendon-driven continuum Robot. I have all homogeneous transformation matrices as I already implemented the kinematics for this Robot. Due to it's new structure there are no discrete joint variables anymore but rather continuous parameters. Therefore I want to compute the Jacobian numerically.
It'd be awesome if someone could provide a detailed way how to compute the numerical Jacobian for a 6-DoF rigid-link robot (only rotational joints => RRRRRR). From that I can transfer it to the continuum robot.
I've already started computing it. Let T be the homogeneous transformation matrix for the Endeffector (Tip)  with
$$T=\begin{bmatrix}R & r \\ 0 & 1 \end{bmatrix} $$
with R = rotational matrix (contains orientation) and $ r = \begin{bmatrix} x & y & z \end{bmatrix}^T$ endeffector position.
My approach is to compute the first three rows of J by successively increasing the joints, computing the difference to the ""original"" joint values and dividing it by the increment delta, the joint-space is $ q = \begin{bmatrix} q_1 & q_2 &  q_3 & ... &q_6 \end{bmatrix}T $
$q_1 = q_1 + \delta$ => $J(1,1) = (X_{increment} - X_{orig})/\delta$ 
$q_2 = q_2 + \delta$ => $J(1,2) = (X_{increment} - X_{orig})/\delta$   
and so on. I do the same for the y and z coordinates. So I get the first 3 rows of J. 
Now I don't know how to compute the last three rows as they refer to the rotational Matrix R. Since it's a 3x3 matrix and no scalar value I don't know how to handle it. 
",jacobian
Positioning Sensor,"I would like to locate the position of a stationary autonomous robot in x-y-z axis relative to a fixed starting point.
Could someone suggest sensors that would be suitable for this application?
I am hoping to move the robot in 3D space and be able to locate it's position wirelessly. The rate of position update is not important as I would like to stop the robot from moving and relay the information wirelessly.
The range I am looking for is roughly 2 KM + (the more the better) with accuracy of  +/- 1 CM.
Is there any system that could do this? Thanks for your help.
","sensors, imu"
Wall following using hokuyo lidar and sharp IR sensors,"I have a mobile robot and I would like it to follow the walls of a room.
I have:

A map of the room. 
Wheel encoders for the odometry.
A Kalman filter for fusing data from wheel encoders and IMU. 
A Hokuyo lidar for localization and obstacle avoidance
A Kinect to see obstacles which can not be seen by the Hokuyo. 
Amcl for localization.
A couple of sharp sensors on the side for wall following. 

I am not planning to use the global or local costmap because the localization of the robot is not perfect and the robot might think that it is closer (or further away) to the wall than it actually is and therefore, wall following might fail. So, I am planning to just use the data from Hokuyo lidar and sharp sensors to do wall following and maintain constant distance from the wall (say 10 cm). 
Now, I would like to know what is the best technique for doing wall following in this manner? Also, how can one deal with the issue of open gaps in the wall (like open doors, etc..) while doing wall following using the above approach?
I know this is a very general question but any suggestions regarding it will be appreciated. Please let me know if you need more information from me.
Update:
I am just trying to do wall following in a given room (I have the vertices of the room in a global reference frame) For example, Lets say I have a map of a room (shown below). I want to make the robot follow the wall very closely (say 10 cm from the wall). Also, if there is an open space (on bottom left), the robot should not go in the adjacent room but should keep on doing wall following in the given room (For this, I have the boundary limits of the room which I can use to make sure the robot is within the given room).
The approach which I am thinking is to come up with an initial global path (set of points close to the wall) for wall following and then make sure robot goes from one point to the next making sure that it always maintains a certain distance from the wall. If there is no wall, then the robot can just follow the global path (assuming localization is good). I am not sure about its implementation complexity and whether there is a better algorithm/ approach to do something like this.

","sensors, localization, navigation"
"Why the name ""combinatorial""?","Why are 'cell decomposition' methods in motion planning given the name, ""combinatorial"" motion planning?
",motion-planning
Arduino mega shield v2.0 compatibility with arduino due,"Like the title says.. Will it work? I know about the due 3.3 volt limitations.
I want to build a hexapod with 18 servo's.
The shield I am looking at:
http://yourduino.com/sunshop2/index.php?l=product_detail&p=195
If it isn't compatible. Is there an alternative shield which will work? I can't seem to find much for the due.
",arduino
recommendation for really high precision attitude measurement sensors,"I am new in this field, I am looking for some high precision gyroscopes and accelerometers for attitude measurements.The precision requirement is around 0.2~0.5 deg/s dynamic.
    I have done some digging myself, not a single integrated MEMS sensor can do that without costing too much. So some heavy math is needed but that's fine.I need to make sure the prefect sensors are chosen, the budget is less than 100USD.
    can any one help, thanks in advanced.
","sensors, research"
Why do we generally prefer DH parameters over other kinematic representations of robot arms?,"I am specifically interested in DH parameters versus other representations in terms of kinematic calibration.  The best (clearest) source of information I could find on kinematic calibration is in the book ""Robotics: Modelling, Planning and Control"" by Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo, chapter 2.11.  Which requires a description of the arm in DH parameters, multiplying out the kinematics equation, partial differentiation w.r.t. each DH parameter, then a least-squares fit (with the left pseudo-inverse), then iterate.  
Is there some fundamental reason why DH parameters are used instead of a different representation (like xyz + euler angles).  I understand that there are fewer parameters (4 versus 6 or more), but for a calibration procedure like this I will be taking much more data than unknowns anyway.  All the robotics textbooks i have read just present DH parameters and say ""this is what you should use"", but don't really go into why.  Presumably this argument can be found in the original paper by Denavit, but I can't track it down.
","robotic-arm, kinematics, calibration, dh-parameters"
Robot path planning,"My goal is to move robot in certain points as shown in the figure. It's initial position is (x0,y0) and move along other coordinates.

I am able to track robot position using a camera which is connected to pc and camera is located at the top of the arena. I've mounted a ir beacon on the robot, camera find this beacon and locates it's coordinate(in cm) in the arena. Using this coordinate how can I move my robot to another position, say new position (x1,y1)
My robot has arduino mega 2560 with two DC motors, communication between pc and robot is done using bluetooth
Update:
Thanks @Chuck for the answer, however I still have few doubts regarding turning angle.
My robot position setup is as shown in the image.
(xc, yc) is the current position and (xt, yt) is the target position.

If I want to align robot in the direction of the target coordinates, I've to calculate atan2 between target and current coordinates. But the angle remains same since it's current position is not changing with respect to the target point. so I assume robot simply makes 360' rotation at current position?
Update:
The path points is as show below in the image, is my initial heading angle assumption is correct? 
'1' is the starting point.

Update
Thank you for your patience and time, I'm still struck at turning, my code goes like this
//current points
float xc = -300;
float yc = 300;

//target points

float xt = -300;
float yt = -300;

//turning  angle
float turnAngle;

void setup() {
  // pin setup
  Serial.begin(9600);
}

void loop() {

  turnAngle = atan2((yt-yc), (xt-xc)); //calculate turning angle

  turnAngle = turnAngle * 180/3.1415; //convert to degrees

  if (turnAngle > 180) {
    turnAngle = turnAngle-360;
  }

  if (turnAngle < -180) {
    turnAngle = turnAngle+360;
  }

  if (turnAngle < -10) {
    //turn right
  }

  if (turnAngle > 10) {
    //turn left
  }
}

Since angle is always -90' robot only makes right turn in loop at current point, since angle is not changing. I think I'm missing something here.
","arduino, navigation"
One BEC for multiple ESC (Quadcopter),"I'm building a quadcopter and have discovered that most ESC have a built-in BEC, but I was wondering if it wouldn't be better to use only one.
What if I delivered power to my four ESC with a unique BEC ? Would that work ?
I think this would be easier to configure (you have to set it up only once for the four ESC) and it would prevent each ESC from having it's own behavior.
Am I doing it wrong ?
Here is an image of what I'm talking about :
Edit : trying to find the original image and upload it.
Given the answer by Ian McMahon it appears that this schema is not the right thing to do, since I had misunderstood the role of BECs.
So would the right schema would look like this ?
Edit : trying to find the original image and upload it.
I'm still not sure if I'm getting it.
Do I need 4 ESCs with integrated BECs and connect all three cables to flight controller ?
","electronics, quadcopter, bec, esc"
Can i charge a lipo nano tech battery over imax b3 charger,"Can i charge a lipo nano tech battery over imax b3 charger. 2650mah 35/70c 3s is the battery
","battery, lithium-polymer"
"looking for a miniature joystick, but in reverse","Does anyone know if small mechanical actuators exist which can be controlled electrically, sort of like a miniature joystick, but in reverse.  Instead of it picking up mechanical movement and outputting electrical signals, I want it to generate mechanical movement controlled via my electrical input signals.  I’ve searched for : electromechanical actuators, not finding what I need. Think of a pencil attached to a surface which can pivot to point anywhere in its half dome.  I’m thinking small, on the order of an inch.  It will not be load bearing. 
My goal is to programmatically control the normal pointed to by a small flat surface attached to the end of each joystick rod.  Accuracy is more important than speed.  From across a small room, say 10' by 10', I'd like the surface normal to accurately point to arbitrary objects in the room, say a person walking across the room.  If I can cheaply buy/build such mechanisms to control the movement of these small flat surfaces, I would like dozens places across the walls of the room.
Its for an electromechanical sound project I’m planning. 
","control, actuator"
How can I get the values of a IMU from the serial message received in Simulink via UART?,"I try to read IMU sensor data from an Arduino mega 2560 UART with serial receive block of Arduino support package for simulink. The IMU can send binary packets and also nmea packets and I can configure it to any output. When the serial recieve block output is directly used, it displays just the numbers between 0-255. l need help about how to parse the coming data which contains the euler angles that I want to use.
Here is binary structure ;
""s"",""n"",""p"",packet type(PT),Address,Data Bytes (D0...DN-1),Checksum 1,Checksum 0
The PT byte specifies whether the packet is a read or a write operation, whether it is a batch operation, and the length of the batch operation (when applicable). The PT byte is also used by the UM7 to respond to commands. The specific meaning of each bit in the PT byte is given below.
Packet Type (PT) byte;
7 Has Data,
6 Is Batch,
5 BL3,
4 BL2,
3 BL1,
2 BL0,
1 Hidden,
0 CF
Packet Type (PT) Bit Descriptions;
7...Has Data: If the packet contains data, this bit is set (1). If not, this bit is cleared (0). 
6...Is Batch: If the packet is a batch operation, this bit is set (1). If not, this bit is cleared (0) 
5:2..Batch Length (BL): Four bits specifying the length of the batch operation. Unused if bit 7 is cleared. The maximum batch length is therefore 2^4 = 16 
1...Hidden: If set, then the packet address specified in the “Address” field is a “hidden” address. Hidden registers are used to store factory calibration and filter tuning coefficients that do not typically need to be viewed or modified by the user. This bit should always be set to 0 to avoid altering factory configuration.
0...Command Failed (CF): Used by the autopilot to report when a command has failed. Must be set to zero for all packets written to the UM7.
The address byte specifies which register will be involved in the operation. During a read operation (Has Data = 0), the address specifies which register to read. During a write operation (Has Data = 1), the address specifies where to place the data contained in the data section of the packet. For a batch read/write operation, the address byte specifies the starting address of the operation.
The ""Data Bytes"" section of the packet contains data to be written to one or more registers. There is no byte in the packet that explicitly states how many bytes are in this section because it is possible to determine the number of data bytes that should be in the packet by evaluating the PT byte.
If the Has Data bit in the PT byte is cleared (Has Data = 0), then there are no data bytes in the packet and the Checksum immediately follows the address. If, on the other hand, the Has Data bit is set (Has Data = 1) then the number of bytes in the data section depends on the value of the Is Batch and Batch Length portions of the PT byte.
For a batch operation (Is Batch = 1), the length of the packet data section is equal to 4*(Batch Length). Note that the batch length refers to the number of registers in the batch, NOT the number of bytes. Registers are 4 bytes long.
For a non-batch operation (Is Batch = 0), the length of the data section is equal to 4 bytes (one register). The data section lengths and total packet lengths for different PT configurations are shown below.
The two checksum bytes consist of the unsigned 16-bit sum of all preceding bytes in the packet, including the packet header.
Read Operations;
To initiate a serial read of one or more registers aboard the sensor, a packet should be sent to the UM7 with the ""Has Data"" bit cleared. This tells the device that this will be a read operation from the address specified in the packet's ""Address"" byte. If the ""Is Batch"" bit is set, then the packet will trigger a batch read in which the ""Address"" byte specifies the address of the first register to be read.
In response to a read packet, the UM7 will send a packet in which the ""Has Data"" bit is set, and the ""Is Batch"" and ""Batch Length"" bits are equivalent to those of the packet that triggered the read operation. The register data will be contained in the ""Data Bytes"" section of the packet.
here is an Example Binary Communication Code;
{
uint8_t Address;
uint8_t PT;
uint16_t Checksum;
uint8_t data_length;
uint8_t data[30];
} 
UM7_packet;
// parse_serial_data.This function parses the data in ‘rx_data’ with length ‘rx_length’ and attempts to find a packet in the data. If a packet is found, the structure ‘packet’ is filled with the packet data.If there is not enough data for a full packet in the provided array, parse_serial_data returns 1. If there is enough data, but no packet header was found, parse_serial_data returns 2.If a packet header was found, but there was insufficient data to parse the whole packet,then parse_serial_data returns 3. This could happen if not all of the serial data has been received when parse_serial_data is called.If a packet was received, but the checksum was bad, parse_serial_data returns 4. If a good packet was received, parse_serial_data fills the UM7_packet structure and returns 0.

uint8_t parse_serial_data( uint8_t* rx_data, uint8_t rx_length, UM7_packet* packet )
{
uint8_t index;
// Make sure that the data buffer provided is long enough to contain a full packet The minimum packet length is 7 bytes
if( rx_length < 7 )
  {
  return 1;
  }
// Try to find the ‘snp’ start sequence for the packet
for( index = 0; index < (rx_length – 2); index++ )
  {
  // Check for ‘snp’. If found, immediately exit the loop
  if( rx_data[index] == ‘s’ && rx_data[index+1] == ‘n’ && rx_data[index+2] == ‘p’ )
    {
    break;
    }
  }
uint8_t packet_index = index;
// Check to see if the variable ‘packet_index’ is equal to (rx_length - 2). If it is, then the above loop executed to completion and never found a packet header.
if( packet_index == (rx_length – 2) )
  {
  return 2;
  }
// If we get here, a packet header was found. Now check to see if we have enough room left in the buffer to contain a full packet. Note that at this point, the variable ‘packet_index’contains the location of the ‘s’ character in the buffer (the first byte in the header)
if( (rx_length – packet_index) < 7 )
  {
  return 3;
  }
// We’ve found a packet header, and there is enough space left in the buffer for at least the smallest allowable packet length (7 bytes). Pull out the packet type byte to determine the actual length of this packet
uint8_t PT = rx_data[packet_index + 3];
// Do some bit-level manipulation to determine if the packet contains data and if it is a batch.We have to do this because the individual bits in the PT byte specify the contents of the packet.
uint8_t packet_has_data = (PT >> 7) & 0x01; // Check bit 7 (HAS_DATA)
uint8_t packet_is_batch = (PT >> 6) & 0x01; // Check bit 6 (IS_BATCH)
uint8_t batch_length = (PT >> 2) & 0x0F; // Extract the batch length (bits 2 through 5)
// Now finally figure out the actual packet length
uint8_t data_length = 0;
if( packet_has_data )
  {
  if( packet_is_batch )
    {
    // Packet has data and is a batch. This means it contains ‘batch_length' registers, each // of which has a length of 4 bytes
    data_length = 4*batch_length;
    }
  else // Packet has data but is not a batch. This means it contains one register (4 bytes)
    {
    data_length = 4;
    }
  }
else // Packet has no data
  {
  data_length = 0;
  }
// At this point, we know exactly how long the packet is. Now we can check to make sure we have enough data for the full packet.
if( (rx_length – packet_index) < (data_length + 5) )
  {
  return 3;
  }
// If we get here, we know that we have a full packet in the buffer. All that remains is to pullout the data and make sure the checksum is good. Start by extracting all the data
packet->Address = rx_data[packet_index + 4];
packet->PT = PT;
// Get the data bytes and compute the checksum all in one step
packet->data_length = data_length;
uint16_t computed_checksum = ‘s’ + ‘n’ + ‘p’ + packet_data->PT + packet_data->Address;
for( index = 0; index < data_length; index++ )
  {
  // Copy the data into the packet structure’s data array
  packet->data[index] = rx_data[packet_index + 5 + index];
  // Add the new byte to the checksum
  computed_checksum += packet->data[index];
  }
// Now see if our computed checksum matches the received checksum 
// First extract the checksum from the packet
uint16_t received_checksum = (rx_data[packet_index + 5 + data_length] << 8);
received_checksum |= rx_data[packet_index + 6 + data_length];
// Now check to see if they don’t match
if( received_checksum != computed_checksum )
  {
  return 4;
  }
// At this point, we’ve received a full packet with a good checksum. It is already fully parsed and copied to the ‘packet’ structure, so return 0 to indicate that a packet was processed.
return 0;
}

","arduino, electronics, embedded-systems, matlab"
iRobot Create Serial Cable for Turtlebot I,"I need an iRobot Create Serial Cable (one end 7-pin Mini-DIN Connector and the other end is USB) for Turtlebot I.  How can I connect my bot to my PC?
","mobile-robot, irobot-create, serial, roomba"
How can I recognize animals in a video stream or static images with openCV or other library/software?,"I'm a software developer not experienced in AI or machine learning, but I'm now interested in developing this kind of software. I want to develop software that recognizes some specific objects, specifically, animals from a video stream (or a sequence of static images).
I saw there's a library called openCV which is often commented in this forum, but what I saw so far is this library is a helper for working with images, I didn't find the object recognition or self learning part.
Is openCV a good starting point? better go for some theory first? or there are other already developed libraries or frameworks aimed for object recognition?
EDIT
To give some context: I will have ona camera checking a landscape, mostly static but some leaves may move with the wind or some person may step in, and I want to get an alert when some animal is into view, I can reduce the ""animals"" to only birds (not always I will have a nice bird/sky contrast).
I did some work with supervised neural networks some 15 years ago and studied some AI and machine learning theory, but I guess things have improved way too much since then, that's why I was asking for some more practical first steps.
Thank you
",computer-vision
Robot localization without any sensors,"Is it possible to localize a robot without any sensors, odometer and servo motors?
Assume robot has dc motors and no obstacles.
","localization, mapping"
Non linear control system?,"I have a dual (sequential) loop control system controlling the angle of a rotational joint on a robot using an absolute encoder. I have tuned the inner control loop (for the motor) and am now working on tuning the outer loop (for the joint).
Example of a dual loop controller

When I disturb the system the response isn't what I would expect.
Kp = 0.4

Kp = 0.1 Kd = 0.001

I didn't add a Ki term because I don't have any steady state error.
I'm confused by the fact that the second overshoot in the first plot is larger than the first one. No matter how I adjust the parameters I can't seem to get rid of the oscillation in the velocity of the joint (seen in the second plot). One limitation I have is if I increase both Kp and Kd too high the gearbox of the becomes very noisy because the noise in the encoder signal creates larger adjustments in the position of the motor. I'm working on adding a filter to the output using the method described here.
The code I'm using for the outer loop is:
static float e_prev = 0.0;

e = joint_setpoint - joint_angle;
e_i += e/0.001; // dt = 0.001s
e_d = (e - e_prev)/0.001; // dt = 0.001s

e_prev = e;

motor_setpoint += k_p * e + k_i * e_i + k_d * e_d;

I'm beginning to think that the system might not be able to be modeled by a first order equation, but would this change the implementation of the control loop at all? Any advice is appreciated!
Ben
",pid
How to rotate a dc motor at a fixed rpm,"I am using 8051 microcontroller and a dc motor.What to do if i have to rotate the motor at any fixed rpm. Let's say 120rpm.
And if it is possible by generating pwm,how to do the calculations for the relation between duty cycle and rpm?
","control, motor, microcontroller"
Powering a Project Tango Tablet with iRobot Create 2,"Project Tango Development Kits come with a mini-dock (see picture below).  
I am controlling the iRobot Create 2 by the mounted Tablet using the USB cable provided plugged into the mini-dock. (see docs).

The USB 3.0 port on the mini-dock is only functional when the tablet is docked. The port can be used to attach an external memory drive or standard peripherals to the tablet.

I wish to recharge the tablet using the power from the iRobot. The mini dock comes with a port for external charging:

The mini-dock accepts a power adapter for faster charging (not provided). The power adapter output must be 12V, 2A, and the connector must be a barrel plug with 5.5mm outer diameter, 2.1mm inner diameter, center positive.

Ideally the charging would happen only when the iRobot is also charging, but charging all the time is acceptable. 
Is this possible?  If so, how?

",irobot-create
Robot docking for self-recharging,"I want to build a simple obstacle avoider robot, but this time I want it to be self-recharging so I am building a dock for this purpose, so I want it to be able to locate the dock and go for it when battery voltage is lower than a fixed value.
I am having trouble to chose the right components for locating the dock, I think I am going to use an IR emitter on the dock so the robot can head toward it when battery is low (let's forget about the orientation problem for the moment, but if you have any thoughts about it that will be helpful) but I am not sure if the robot is able to detect the IR LED (or whatever) from a long distance (over 10 meter)
Is it possible to use this solution for this distance? If not, what do you suggest?
(If there is a simple ready solution to buy that's ok, let's say I have no budget limit)
","sensors, localization, wheeled-robot, battery, wireless"
make hc-sr04 receive from another one,"I have 3 ultrasonic Sensor (HC-SR04), i want to use one of them as transmitter, and the other as receiver, i want to let the first one send ultra Sonic waves and the other receive these waves from the same transmitter.
how can i do that ? 
i tried to send trigger for each ultrasonic and connect them on different pins on PIC, but its now work.
its something like this project but using HC-SR04 
","microcontroller, ultrasonic-sensors"
How do I dispense a greasy fluid?,"I'm a agricultural engineering student and complete newbie trying to build a simple mechanism attached to a drone that dispenses a grease-type fluid. However, since I'm not familiar with the field, I'm having a hard time googling because I don't know the correct terms to search for. 
I'm looking for a mechanism that will remotely push the grease out. The problem is carrying the necessary weight for an hectare (300g to 1,5kg of fluid) and the dispenser mechanism within the drone. So I'm looking for a lightweight dispenser mechanism capable of deliver small amounts of this fluid (3g) distributed on the trees canopy. The grease do not need to be heated as it flows naturally in normal temperatures (like a toothpaste). Both pump or syringe-type arrangement would be fine as long as I can control it remotely.
","motor, quadcopter, design, battery, actuator"
How can I tell if a servo motor is capable of being controlled degree by degree?,"I want to create a rotating control mechanism that can turn a surface to face any direction in a sphere. My dad (an electrical engineer) said I can probably do it by connecting two servo motors together. 
I am looking for a servo motor that can do what I want to do, which is moving the sphere with decent precision (within ~1 degree) but I don't know which kinds of motors are able to handle such precision. 
Another challenge is that one servo will have to hold the second servo on top. As I understand it, the torque rating determines the maximum amount of force the servo can exert on its load so I can figure out if the servo is strong enough through some math?
","control, servos"
What's an efficient way to visit every reachable space on a grid with unknown obstacles?,"I'm trying to create a map of the obstacles in a fairly coarse 2D grid space, using exploration.  I detect obstacles by attempting to move from one space to an adjacent space, and if that fails then there's an obstacle in the destination space (there is no concept of a rangefinding sensor in this problem).
example grid http://www.eriding.net/resources/general/prim_frmwrks/images/asses/asses_y3_5d_3.gif (for example)
The process is complete when all the reachable squares have been visited.  In other words, some spaces might be completely unreachable even if they don't have obstacles because they're surrounded.  This is expected.
In the simplest case, I could use a DFS algorithm, but I'm worried that this will take an excessively long time to complete — the robot will spend more time backtracking than exploring new territory.  I expect this to be especially problematic when attempting to reach the unreachable squares, because the robot will exhaust every option.
In the more sophisticated method, the proper thing to do seems to be Boustrophedon cell decomposition.
Boustrophedon cell decomposition http://planning.cs.uiuc.edu/img2836.gif
However, I can't seem to find a good description of the Boustrophedon cell decomposition algorithm (that is, a complete description in simple terms).  There are resources like this one, or this more general one on vertical cell decomposition but they don't offer much insight into the high-level algorithms nor the low-level data structures involved.
How can I visit (map) this grid efficiently?  If it exists, I would like an algorithm that performs better than $O(n^2)$ with respect to the total number of grid squares (i.e. better than $O(n^4)$ for an $n*n$ grid).
","algorithm, coverage, planning"
Beginner Soldering question,"So, I need to know a couple of things about soldering. My primary workspace, robotics and otherwise is a desk with a computer and only a little bit of free space (4ft. by 6 in.). I am wondering if it is safe to solder in such a small area. Also, what level of ventilation do I need to solder safely? My desk is in a normal house room and my desk is write next to an air vent. My house has heating and A/C? Do I need a fan or a fume sucker thing? I plan to only solder a little to get things to stay in my solder less bread board (soldering header pins onto wires and such). So, basically, what are the minimum requirements for soldering safely (space and ventilation). Also, if anyone could point me to some hobby/beginner level soldering must-haves on amazon that would be great, thanks.
",beginner
Using Blob detection in V-Rep,"I was trying to reproduce this youtube tutorial in V-rep and I came across some problems concerning blob detection. There are some complaints on this matter under the video. I don't believe that blob detection stopped working in recent v-rep versions, but I was unable to make it work (as a new v-rep user myself). Has anyone any idea how to properly implement it? 
More specifically, I have a vision sensor named cam and I want it to follow a red ball. The vision sensor will detect the position of the ball and I will use it to control the joints that steer the sensor (yaw and pitch). My script follows
threadFunction=function()
    yaw=simGetObjectHandle(""yaw"")
    pitch=simGetObjectHandle(""pitch"")
    cam=simGetObjectHandle(""cam"")
    while simGetSimulationState()~=sim_simulation_advancing_abouttostop do
        result,pack1,pack2=simReadVisionSensor(cam)
        if result>0 then
            xtarget=pack2[5]
            ytarget=pack2[6]
            simAuxiliaryConsolePrint(out,string.format(""\n x: %0.2f, y: %0.2f"",xtarget,ytarget))
            simSetJointTargetVelocity(yaw,1*(0.5-xtarget))
            simSetJointTargetVelocity(pitch,1*(0.5-ytarget))
        end
    end
end

simSetThreadSwitchTiming(2)
out = simAuxiliaryConsoleOpen(""Debug"",8,1)
res,err=xpcall(threadFunction,function(err) return debug.traceback(err) end)
if not res then
    simAddStatusbarMessage('Lua runtime error: '..err)
end

When I run the simulation I can see that the sensor sees the red ball at some point but result is always 0 meaning that no detection takes place.  
Here is my scene
","simulator, visual-servoing"
Forward kinematics of constrained double pendulum,"I was wondering whether maybe you could help me with this problem. I have a double pendulum. I have set the origin of cartesian coordinates to be the ""head"" of the first arm, which is fixed. The end of the second arm is attached to a block that slides along the x-axis. What I want to do is to derive the equations relating the pendulum's angles with the distance from the origin to the block. 
Now, I know how I could go about deriving the equations without the constraint. 
$$x_1 = L_1cos(a_1)$$
$$y_1 = L_1sin(a_1)$$
Where $x_1$ and $y_1$ is where the first arm joins the second arm and $a_1$ is the angle between the horizontal and the first arm. 
Similarly, I can derive the equations for the end of the second arm 
$x_2 = x_1 + L_2 cos(a_2)$ and $y_2 = y_1 - L_2 sin(a_2)$
Now then, if I attach a sliding block to the end of my second arm, I don't know whether my equation for $x_2$ would change at all. I don't think it would but  would I have to somehow restrict the swing angles so that the block only moves along the x direction? 
Well, basically the problem is finding the equation of $x_2$ if it's attached to a block that only moves along the x- direction. 
",forward-kinematics
How do robotics startups work?,"In software engineering startups, you generally go to a room with a computer or bring your own laptop, and write code. I'm interested in how robotics startups work: Is there a separate location for designing the robots? Take for example, Anki. Do they have separate research labs for designing robots? How does a robot get from a single design to being manufactured?
I couldn't find a better place on SE to post this (the startups business section is defunct): Please link me to another SE site if there is a better place to ask this question.
",manufacturing
An architecture for testing autonomous flight and sensors,"I'm designing a simple autopilot software on top of Ardupilot, my goal is to possibly interface an Raspi on top of ArduPilot Mega (APM). I am stuck on setting up a simulation environment using either V-Rep or Gazebo. 
The quadcopter will have basic sensors plus advanced sensors. basic sensors talks directly with ArduPilot, while advanced sensors talks with my own autopilot software. I am trying to wrap my head around a feasible setup to test the software while using ArduPilot Mega in the Hardware-In-The-Loop. I am planning on having three stages of Simulation:
Stage 1. Simulate quadcopter physics in Gazebo/V-Rep, run ArduPilot software and my autopilot software in a VM (not sure if it's even do-able)
Stage 2. Simulate quadcopter physics in computer, run my autopilot software in a VM, and run APM in a hardware-in-the-loop fashion.
Stage 3. deploy my autopilot onto Raspi and interface with APM then run both hardwares in Hardware-in-the-loop fashion.
","quadcopter, simulator"
What commands make the irobot create 2 go left and right not just forwards and backwards?,"I am new to the create 2 and I downloaded real term to program, opened an interface to the robot and send numbers with it to the robot.
I can only get the drive command to work. I only know how to make the robot go faster, turning around or slower.
I would like to know how to make the other commands work along with making it go left and right.
",irobot-create
Original paper of Kalman filter,"Recently we've encountered Kalman filter algorithm for state estimation in a course of Probabilistic Robotics.
After taking several days to try to read Kalman's original paper published in 1960, ""A New Approach to Linear Filtering and Prediction Problems"", it firstly feel a bit difficulty to read, and it seems the majority is to show the orthogonal projection is the optimal estimation under certain conditions and solutions to Wiener's problem.
But I did not find the exact algorithm in this original paper as the one in the textbook. 

For example, is there an explanation of ""Kalman gain"" in this paper ?
Does Kalman's paper provide a mathematical derivation of Kalman
filter algorithm?

",kalman-filter
Preventing leaks in motor shafts for underwater bots,"Whenever building an aquatic bot, we always have to take care to prevent leakages, for obvious reasons. Now, holes for wires can be made watertight easily--but what about motors? We can easily seal the casing in place (and fill in any holes in the casing), but the part where the axle meets the casing is still left unprotected. 

Water leaking into the motor is still quite bad. I doubt there's any way to seal up this area properly, since any solid seal will not let the axle move, and any liquid seal (or something like grease) will rub off eventually.
I was thinking of putting a second casing around the motor, maybe with a custom rubber orifice for the shaft. Something like (forgive the bad drawing, not used to GIMP):

This would probably stop leakage, but would reduce the torque/rpm significantly via friction.
So, how does one prevent water from leaking into a motor without significantly affecting the motor's performance?
(To clarify, I don't want to buy a special underwater motor, I'd prefer a way to make my own motors watertight)
","motor, underwater, auv, protection"
Forward kinematics: Why ω should remain same?,"Can anyone please explain me these lines found on page 5 of 
Kinematics Equations for Differential Drive and Articulated Steering
by Thomas Hellström?

Note that plugging in $r$ and $v$ for both left and right wheel result in the same $\omega $ (otherwise the wheels would move relative to each other). Hence, the following equations hold:
$$
\begin{align}
\omega~ \left(R+\frac{l}{2}\right) &= v_r\\
\omega~ \left(R-\frac{l}{2}\right) &= v_l\\
\end{align}$$
where $R$ is the distance between ICC and the midpoint of the wheel axis, and $l$ is the
  length of the wheel axis (see Figure 6). 

Figure 
  6
  .
  When left and right wheel rotate with different speeds, the robot rotates around
  a common point denoted ICC
  My questions are:


How do these equations come to be?
Why does $\omega$ have to be same if we want to analyse the behavior after changing the wheel velocity relative to other?
How do we know about the circle, in which the robot rotates by doing variations in one wheel velocity, surely passes through the center point between two wheels.

",mobile-robot
Simple Sensor Fusion for pose estimation,"I am currently working on a balancing robot project, which features fairly low-cost sensors such as an 9-Dof IMU with the measurement states
$\textbf{x}_\text{IMU} = \left[a_x, a_y, a_z, g_x, g_y, g_z, m_x,m_y,m_z \right]^\text{T}$.
Currently I use the accelerometer and gyroscope readings, fused by a complimentary filter to get the angular deviation of the robot's upright (stable) position. The magnetometer values are tilt-compensated and yield the robots orientation with respect to the earth-magnetic field (awful when close to magnetic distortion). Furthermore I have pretty decent rotational encoders mounted on the wheels which deliver information on a wheel's velocity.
$\textbf{x}_\text{ENC} = \left[v_l,v_r\right]^\text{T}$.
Given these measurements i want to try to get the robots pose (position + heading).
$\textbf{x}_\text{ROB} = \left[x,y,\theta\right]^\text{T}$
I do have minor theoretical knowledge on EKF or KF, but it is not sufficient for me to actually derive a practical implementation. Note that my computational resources are fairly limited (Raspberry Pi B+ with RTOS) and that I want to avoid using ROS or any other non-std libs. Can anybody help me on how to actually approach this kind of problem?

","sensors, kalman-filter, imu, sensor-fusion, odometry"
Simulation environment for conducting visual servoing experiment,"I want to conduct the following experiment:
I want to set up a scene with a kuka lwr4+ arm, a 3D model of an object and a camera overlooking them. I want to find the pose of the object using some pose estimation algorithm and move the arm towards the object. 
In general I want a piece of software or a combination of cooperating software that can do all that without having to reinvent the wheel. Is there anything available?
","software, simulation, visual-servoing"
Lifting Robot To Lift Small Crates,"I am trying to design a robot to lift tote-crates and transport them around in a localized area. I want to be able to carry 3 tote-creates at a time.  This robot needs to be able to pickup the creates. I only want the robot to carry three at a time so keep is small and mobile. I was thinking of a design with a central lift that could carry the crates. What would you suggest as a simple ingenious way to create this robot? 
","design, wheeled-robot, mechanism"
Wiring & driving TowerPro SG90 servos,"I got my hands on a few Tower Pro SG90 9G servos but cannot find their schematics or datasheet anywhere (besides that link).
I have the following concerns:

Looks like they're rated for 4.8V, but will they tolerate a 5V supply?
How do I determine the current they require, in amps, mA, etc.?
There's 3 wires: brown, red & yellow-orange, what do each of these guys do?

If I had to guess I'd say that red is power, another one is direction, and another one is the position to rotate to


","rcservo, wiring"
IMU based acceleration parameters for differential drive robot,"I have a differential drive robot whose motors are virtually quiet while driving on a completely flat surface, but the motors make a lot of noise when on a incline. This is likely due to the correction required to maintain speed with the high inertial load where the robot cannot accelerate fast enough for the PID to keep up.
But I noticed that some of the noise is related to acceleration, and the higher the acceleration, the smaller the amount of noise I hear, or the smaller the time the same level of noise lasts (up to a certain acceleration limit, otherwise the motors get really noisy again).
I am trying to find out of how to use an IMU that I have a available in order to change the acceleration based on how steep the path's incline is.
Any documentation (papers, tutorials, etc) about motion planning related to this topic that you can point me to?
","ros, imu, differential-drive, noise"
Maximum Distance using Ultrasonic sensor Arduino,"What is the maximum distance (of say , a car ) you could measure using an ultrasonic sensor that would be compatible with arduino? Is there any sensor(ultrasonic or not) that could measure the distance of a car , say upto 50 meters that can be used with arduino?
","arduino, ultrasonic-sensors"
How to tune PID for a Y(t) = k*X(t) system?,"Could I have your opinions on PID type selection?

System description

Here comes a very simple system: $\mbox{Output}(t) = k * (\mbox{Input}(t) + \mbox{systemVariable}(t))$. $k$ is constant and $\mbox{systemVariable}(t)$ is a system variable which may change according to time.
The goal of the whole system is to maintain system output at $0$. It has to be as close to zero as possible. The controller has to compensate the $\mbox{systemVariable}$.
The change ($\mbox{systemVariable}$ ) is modeled by a very slow ramp.

Controller description

The controller's input is the output of the system. However, the measurements are always noisy, and I modeled Band-Limited White Noise into the measurements.
After PID controller, the output goes into an integrator, since the PID controller always calculates the ""change"" of the plant input. 

Questions

My original thoughts: Add a PID controller with P=1/k is enough. Since every time the controller gets an error $e$, it can be calculated back that the compensation on controller output shall be $e/k$. However, Matlab auto-tuning always give me a PID. Why is that?
What is the relation between P of PID and measurement noises? If P is large, the system will tend to be rambling largely, due to the noises. If P is small, the system will tend not to converge to the correct value or very slow. How to make the trade-offs? Or how to prevent system from rambling largely and get quick system responses? 

Thanks a lot!
","control, pid"
Olf Futaba Rx & Tx with APM micro 2.7.2?,"Hi there I just found this old Rx & Tx in my loft and need to know weather it is compatible with my APM micro 2.7.2. I already have telemetry but that does not give me manual control. My guess is I need a new Rx because the current one will make a hash of the electronics on the APM. Thanks in advance[![enter image description here][1]][1]
[![enter image descriptere][2]][2]


",quadcopter
Robotics StackExchange vs ROS Answer,"Robotics Stackexchange vs. ROS Answer:
What is better and for what purpose?
","ros, robotc"
Which type of actuator will be suitable for a very strong robot arm,"I wish to build a robotic arm that can lift a useful amount of weight (such as 3-6kg on an arm that can extend to approx 1.25 meters). What actuators are available to accomplish this. The main factors and design points are:

Not Expensive
5 to 6 d.o.f.
to be mounted on a yet to be designed mobile platform
battery powered
stronger than hobby servos (at least for the 'shoulder' and 'elbow' joints)
not slow to actuate

","mobile-robot, robotic-arm, actuator"
How to find out how far a motor has taken a vehicle?,"I have a small motorized vehicle with gears as wheels running up and down a track made of gear racks. How can this robot know when it has run half the track? And what's the best method to keep it from running off its track at the end and then return to start.
The robot is carrying water, not exactly the same amount each time, so it will not weigh the same. Therefore it might not be the same amount of steps in the stepper-motor each time.
Here I have some ideas that might work, though I am a beginner, and don't know what's the best solution.

GPS tracking it (overkill on such a small scale?)
Some kind of distance measurer
Have a knob it will hit at the middle of the track, telling program to delay for a given time
Track amount of steps the motor has performed (won't be as accurate?)

","mobile-robot, arduino, sensors"
Electric Motor Speed Control - PWM vs analog voltage?,"I'm working on a 2-wheeled robot and have connected up a raspberry pi to an L298N motor driver.
I'm sending the enable pin of a particular motor a software-generated PWM signal at 100Hz with a 50% duty cycle.  I observe with an osciloscope:

a fairly clean square wave going into the enable pin as expected.
a fairly dirty square wave across the output motor terminals.

The motor turns at about 50% speed/torque as expected.
I find myself wondering if it would be better to control the speed of the motor by placing a flat lower constant voltage across its terminals, rather than oscillating a square wave.  ie to do 50% speed/torque - instead of oscilating between 0V and 5V - just put a constant 2.5V across the motor terminals.  I wonder if the oscillation is a waste of power/energy.
Is this true?  Or doesn't it make any difference?  Do high-end motor drivers use a variable flat analog voltage to control speed/torque, or do they use a PWM?  If a PWM, does the frequency make any difference?
",motor
Complementary and Kalman filter don't work for Y angle,"I'm working on a Python script which reads the data from the MPU6050 IMU and returns the angles using sensor fusion algorithms: Kalman and Complementary filter. Here is the implementation:
Class MPU6050 reads the data from the sensor, processes it. Class Kalman is the implementation of the Kalman filter. The problem is the next: None of the Kalman, neither the Complementary filter returns appropriate angle values from the Y angle. The filters work fine on the X angle, but the Y angle values make no sense. See the graphs below. I've checked the code million times, but still can't figure out where the problem is.  
class MPU6050():
    def __init__(self):
        self.bus = smbus.SMBus(1)
        self.address = 0x68

        self.gyro_scale = 131.072 # 65535 / full scale range (2*250deg/s)
        self.accel_scale = 16384.0 #65535 / full scale range (2*2g)

        self.iterations = 2000                    

        self.data_list = array('B', [0,0,0,0,0,0,0,0,0,0,0,0,0,0])
        self.result_list = array('h', [0,0,0,0,0,0,0])       

        self.gyro_x_angle = 0.0
        self.gyro_y_angle = 0.0
        self.gyro_z_angle = 0.0         

        self.kalman_x = Kalman()
        self.kalman_y = Kalman()

    def init_sensor()...

    def calculate_angles(self):
        dt = 0.01

        comp_y = 0.0
        comp_x = 0.0
        print(""Reading data..."")

        while True:             
            self.read_sensor_raw()

            gyro_x_scaled = (self.result_list[4] / self.gyro_scale)
            gyro_y_scaled = (self.result_list[5] / self.gyro_scale)
            gyro_z_scaled = (self.result_list[6] / self.gyro_scale)

            acc_x_scaled = (self.result_list[0] / self.accel_scale)
            acc_y_scaled = (self.result_list[1] / self.accel_scale)
            acc_z_scaled = (self.result_list[2] / self.accel_scale)

            acc_x_angle = math.degrees(math.atan2(acc_y_scaled, self.dist(acc_x_scaled,acc_z_scaled)))
            acc_y_angle = math.degrees(math.atan2(acc_x_scaled, self.dist(acc_y_scaled,acc_z_scaled)))

            comp_x = 0.95 * (comp_x + (gyro_x_scaled * dt)) + 0.05 * acc_x_angle
            comp_y = 0.95 * (comp_y + (gyro_y_scaled * dt)) + 0.05 * acc_y_angle

            kalman_y_angle = self.kalman_y.filter(acc_y_angle, gyro_y_scaled, dt)
            kalman_x_angle = self.kalman_x.filter(acc_x_angle, gyro_x_scaled, dt)

            self.gyro_x_angle += gyro_x_scaled * dt
            self.gyro_y_angle -= gyro_y_scaled * dt
            self.gyro_z_angle -= gyro_z_scaled * dt   

            time.sleep(dt) 

    def read_sensor_raw(self):
        self.data_list = self.bus.read_i2c_block_data(self.address, 0x3B, 14)

        for i in range(0, 14, 2):
            if(self.data_list[i] > 127):
                self.data_list[i] -= 256

            self.result_list[int(i/2)] = (self.data_list[i] << 8) + self.data_list[i+1]

    def dist(self, a,b):
        return math.sqrt((a*a)+(b*b))

class Kalman():
    def __init__(self):
     self.Q_angle = float(0.001)
     self.Q_bias = float(0.003)
    self.R_measure = float(0.03)

    self.angle = float(0.0)
    self.bias = float(0.0)
    self.rate = float(0.0)

    self.P00 = float(0.0)
    self.P01 = float(0.0)
    self.P10 = float(0.0)
    self.P11 = float(0.0)

def filter(self, angle, rate, dt):
    self.rate = rate - self.bias
    self.angle += dt * self.rate

    self.P00 += dt * (dt * self.P11 - self.P01 - self.P10 + self.Q_angle)
    self.P01 -= dt * self.P11
    self.P10 -= dt * self.P11
    self.P11 += self.Q_bias * dt

    S = float(self.P00 + self.R_measure)

    K0 = float(0.0)
    K1 = float(0.0)
    K0 = self.P00 / S
    K1 = self.P10 / S

    y = float(angle - self.angle)

    self.angle += K0 * y
    self.bias += K1 * y

    P00_temp = self.P00
    P01_temp = self.P01

    self.P00 -= K0 * P00_temp
    self.P01 -= K0 * P01_temp
    self.P10 -= K1 * P00_temp
    self.P11 -= K1 * P01_temp

    return self.angle

EDIT:
I've added some information based on @Chuck's answer:

self.result_list[3] contains the temperature
In my opinion the compl. filter is implemented correctly: gyro_x_scaled and gyro_y_scaled are angular velocities, but they are multiplied by dt, so they give angle. acc_?_scaled are accelerations, but acc_x_angle and acc_x_angle are angles. Check my comment, where the Complementary filter tutorial is.
Yes, there was something missing in the Kalman filer, I've corrected it.
I totally agree with you, sleep(dt) is not the best solution. I've measured how much time the calculation takes, and it is about 0.003 seconds. The Y angle filters return incorrect values, even if sleep(0.007) or sleep(calculatedTimeDifference) is used.

The Y angle filters still return incorrect values.
",kalman-filter
GraphSLAM: why are constraints imposed twice in the information matrix?,"I was watching Sebastian Thrun's video course on AI for robotics (freely available on udacity.com).  In his final chapter on GraphSLAM, he illustrates how to setup the system of equations for the mean path locations $x_i$ and landmark locations $L_j$.
To setup the matrix system, he imposes each robot motion and landmark measurement constraint twice.  For example, if a robot motion command is to move from x1 by 5 units to the right (reaching x2), i understand this constraint as
$$-x_2+x_1= -5$$
However, he also imposes the negative of this equation 
$$x_2-x_1=5$$
as a constraint and superimposes it onto a different equation and i'm not sure why.  In his video course, he briefly mentions that the matrix we're assembling is known as the ""information matrix"", but i have no why the information matrix is assembled in this specific way.
So, I tried to read his book Probabilistic Robotics, and all i can gather is that these equations come from obtaining the minimizer of the negative log posterior probability incorporating the motion commands, measurements, and map correspondences, which results in a quadratic function of the unknown variables $L_j$ and $x_i$.  Since it is quadratic (and the motion / measurement models are also linear), the minimum is obviously obtained by solving a linear system of equations.
But why are each of the constraints imposed twice, with once as a positive quantity and again as the negative of the same equation?  Its not immediately obvious to me from the form of the negative log posterior probability (i.e. the quadratic function) that the constraints must be imposed twice.  Why is the ""information matrix assembled this way?  Does it also hold true when the motion and measurement models are nonlinear?
Any help would be greatly appreciated.
",slam
How To Program Three Wheel Omni,"I have created a three wheeled omni robot like the diagram below. Now I am unsure of how to program it. I want to use a single joystick so one x and one y value. The values for x and y are between -1 and 1, also the motors can be set anywhere from -1 to 1. How do I use this data to make the robot move based on the joystick without changing orientations? After doing some initial research this seems like a complex problem, but I am hoping there is a formula that I can.

",wheeled-robot
Kalman filter for estimating position with “direction” measurements,"I am currently working on a pose estimation problem for which I would like to use filtering. To explain the system briefly, it consists of two cameras and each has its own GPS/IMU module. The main assumption is that Camera1 is fixed and stable, whereas camera2 has a noisy pose in 3D. I am using computer vision to obtain the pose (metric translation and rotation) of camera2 w.r.t. camera1, so that I can improve upon the inherent noise of GPS/IMU modules.
The problem here is that the translation obtained through the vision method is only up to an arbitrary scale, i.e. at any given instant, I can only obtain a unit vector that specifies the ""direction"" of the translation and not absolute metric translation. The camera based estimation, although accurate, has no idea about how much actual distance is between the cameras, which is why I have the GPS, which gives me position data with some noise.
Example: camera 2 is 5 m to the east of camera 1, the pose from my vision algorithm would say [1, 0, 0] ; 1 m north-east to camera 1, it would be something like [0.7, 0.7, 0]
Hence, would it be possible to consider the GPS estimate of the metric translation as well as its covariance ellipse, and somehow link it with the normalized camera measurements to obtain a final, more accurate estimate of metric translation? I am not sure what kind of filters would be happy to use a measurement that has no absolute value in it.
Thanks!
","kalman-filter, cameras, pose"
Which sensor type most accurately measures position?,"We're building an 6dof joystick, and we need to accurately measure the displacement of our central device. We can easily use a mechanical connection to the edges, but there has been some discussion about what the best way to achieve this is. The range of motion will be fairly small, but accuracy is incredibly important for us.
Which sensors are most easily and accurately measured?
My impulse response is that rotational and linear potentiometers are the most reliable, but others have been arguing for using gyros/accelerometers. I've also heard that hall effect sensors can be used to great effect.
","control, sensors, accelerometer"
What device do I need to project a laser to point at a specific location?,"I'm searching for a (commercial) projector that just projects a single laser point into the world (e.g. using two moving mirrors). However, I'm struggling because I'm not sure what such a thing is called. I either find area projectors that use lasers, party equipment or laser pointers. 
What is the name for such a device? 
",laser
Deciding length of quadcopter arms,"How quadcopter's arm length affect stability?
As per my view I'll have better control on copter with longer arms but with stresses in arms and also it doesn't affect lift capabilities.
","control, quadcopter, stability"
Implementing a boustrophedon algorithm in a given room with obstacles,"I have a mobile robot which is navigating around a room, I already have the map of the room. I am using the navigation_stack of ROS. I am using rotary encoders for odometry. I am fusing the data from Rotary encoders and IMU using robot_pose_ekf. I am using amcl for localization and move_base for planning. 
Now, I have to write a Complete coverage Path planning algorithm and I am following this paper and I would like to ask what is the best way to generate the Boustrophedon path (simple forward and backward motions) in a cell (can be rectangular, trapezium, etc.) with no obstacles? I read a paper where they use different templates and combine them in a certain way to come up with the Boustrophedon path. Is there any other way by which we can generate the boustrophedon path? If someone can suggest how to implement it in ROS, that will be great.
Please let me know if you need more information from me. Any help will be appreciated.
","ros, navigation, motion-planning, coverage"
Robot structure kit or materials,"I have an arduino, wires, resistors, all of that good stuff. However, I don't have materials to build the structure of the robot. What do you guys recommend? I don't have a place to solder yet so I can't solder but is there a kit or material that you guys recommend? Will it work well with motors and other stuff? Thanks! 
P.S. I plan on building a standard driving robot, but I want to be able to make other robots with the same materials/kit. I don't want a kit that only makes one robot, I want a Lego-esque approach to building the structure where I can build whatever I want with it. (Bump2)
","arduino, beginner"
Micro Quadcopter PID problem,"I designed a mini quadcopter which is about 4.5x4.5cm(Main Body). The main body is the PCB. 
![enter image description here][1]
It weighs about ~20 grams with the battery. I'm using the MPU6050 with the DMP using the i2cDevLib. I am using raw radians for pitch, roll, and yaw these measures are read from the MPU6050's DMP. The motors are attached to the body using electrical tape(Black thing  around motors). The motors are 8.5mm in diameter and are driven by a n-channel mosfet. The mode of control right now is bluetooth(HC-05 module). The code being used is my own.
I have a control loop on all axes, the pitch and roll have the same values since the quadcopter is symmetrical. The problem I have is that PID tuning is next to impossible, the best I got was a ~2 second flight ([Video in slow-motion][2]).
At first I was using my own code for the control loop, but it wasn't as effective as the Arduino PID library. 
The output of the PID loops are mapped to -90 to 90 on all axes. This can be seen in the code
myPID.SetOutputLimits(-90, 90); //Y angle    
myPID1.SetOutputLimits(-90, 90); // X angle
myPID2.SetOutputLimits(-90, 90); // Yaw angle
myPID.SetMode(AUTOMATIC);
myPID1.SetMode(AUTOMATIC);
myPID2.SetMode(AUTOMATIC);

My full code is below, but what do you think the problem is?
Code
http://pastebin.com/cnG6VXr8
","arduino, quadcopter, pid"
Rocker bogie suspension system - pitch angle,"What does this sentence mean :
""The chassis maintains the average pitch angle of both rockers.""
Put in other words, "" the pitching angle of the chassis is the average of the pitch angles of the two rocker arms"" 
What is a pitching angle in this context? 
Please explain both pitching angles.
","mobile-robot, wheeled-robot"
Which joints to discretize for IK,"I am using ikfast in OpenRave for my inverse kinematics.  This is an analytical solver, so if your robot's DOF matches the IK type's DOF, then you get all possible solutions.  But if your robot has more DOFs, then you need to pick some joints to have a constant value.  (However, if you use OpenRave's Python interface it will discretize that joint for you.  i.e. give you a set of solutions for every 0.1 radians of that joint.  But my question holds for either interface.)  I have a 7 DOF anthropomorphic arm with joints: Roll-Pitch-Roll-Pitch-Roll-Pitch-Yaw as seen in this image:

The discretized joints are call ""free joints"" in OpenRave's terminology.  If I let ikfast decide, it picks joint 3 (upper arm roll) to be the free joint.  However, I have been using joint 4 (elbow) to be the free joint because it is easier for me to think about.  But then I realized that perhaps joint 5, 6, or 7 would be better to discretize because they are closer to the end of the chain.  Won't the IK solutions suffer if joints closer to the start of the chain have a large discretization?  Or is OpenRave picking the optimal joint to discretize?
I was just wondering if there is some standard practices or known conventions for this sort of thing.
Put simply: I want a set of IK solutions for the end-effector at some pose.  I will fix a joint either near the start or end of the kinematic chain.  And what i set it to isn't going to be perfect.  Lets say it is off from some ""ideal"" position by some epsilon.  Now you can imagine that if i want the hand in-front of the robot, and I pick a bad angle for the shoulder (like straight up for example), the rest of the joints will have a hard time getting the end-effector to the target pose, if at all.  But If I fix the wrist to be at some awkward angle, there is still a good chance of getting the end-effector there, or at lease close.  What kind of trade-offs are there?  Which will have a ""better"" set of solutions?
",inverse-kinematics
Comparing industrial robot arms,"I'd like to study the capabilities of industrial robot arms. For example, to answer the question how does price vary with precision, speed, reach and strength?
Is there a database of industrial robot arms including information like the price, precision, speed, reach and strength of each model?
","robotic-arm, industrial-robot"
How do I decide the size of the time steps between sensing and control actuation?,"My Background:
My experience is in solid mechanics and FEA.  So I have zero experience in robotics/controls.  
Problem Description
I'm developing a control strategy to stabilize a complicated 6-legged dynamical system.  Torques Ti from each leg's joints will be used to create a net moment M on the body, stabilizing the system.  This moment M is known from the pre-determined control strategy.  (Side note: the dynamical solver is of the nonlinear computational type)
Due to my lack of background, I have a fundamental confusion with the dynamical system.  I want to use joint torques Ti to create this known net moment M on the body.  This moment M is a function of the

current positions/angles of all the leg segments
reaction forces and moments (that cannot be controlled) of each leg
controllable joint torques Ti of each leg
time

$(*)$ At a given time $(n-1)\Delta$t: 

--From the control strategy, the desired net moment M is computed/known
--One can read/sense the legs' positions, angles, reaction forces, and reaction moments (say, from well placed sensors), at this time $t = (n-1)\Delta$t.  
--From this information, vector algebra easily yields the desired joint torques Ti required to create the net moment M

$(**)$ At the time $(n)\Delta$t:

--one applies the previously determined joint torques Ti (determined at $t=(n-1)\Delta$t) to create the desired moment M 
--of course these torques Ti are applied at the immediate proceeding time step because they cannot be applied instantaneously

So this is exactly where my fundamental confusion exists.  The torques Ti were calculated in $(*)$, based on data of angles/positions/reactions in $(*)$, with the objective to create moment M.  However, these torques Ti are applied in $(**)$, where the data (angles/positions/reactions) are now different - thus the desired net moment M can never be created (unless you an magically apply actuation at the instantaneous time of sensing).  Am I understanding the controls problem correctly?  
Questions

Am I understanding the robotics problem correctly?  What are the terms and strategies around this dilemma?
Of course I could create the time steps between the sensing and the actuation to be infinitely small, but this would be unrealistic/dishonest.  What is the balance between a realistic time step, but also performs the task well?

","control, actuator, stability, legged"
ROS Calibration Camera Problems,"I am trying to calibrate a monocular camera using ROS with the help of this website: How to Calibrate a Monocular Camera. When I run rostopic list, I get:
/left
/right
/rosout
/rosout_agg
/usb_cam/image

When I run rosservice list, I get:
/cameracalibrator/get_loggers
/cameracalibrator/set_logger_level
/rosout/get_loggers
/rosout/set_logger_level

Finally, when I run:
rosrun camera_calibration cameracalibrator.py --size 10x7 --square 0.025 image:=/usb_cam/image camera:=/usb_cam 

It says:
('Waiting for service', '/usb_cam/set_camera_info', '...')
Service not found

I even added the parameter at the end, --no-service-check, but that just makes the terminal stall indefinitely. 
Could someone please help me figure out what is going wrong and how I can fix it? Also if it is important, usb_cam is saved at catkin_ws/src/usb_cam.
","ros, cameras, calibration"
Prototyping with IRobot roomba,"For a project I am building a Tele-Op Robot using the IRobot's Roomba as my drivetrain. In order for my robot to work, I need an extra castor. IRobot provides .stl and .stp files for me to use and I used them and printed the files. (The file I printed was from this link: Create® 2 Bin Modification.
This file is a new part to the drivetrain to allow another caster.
And I downloaded the first link called ""Full bin bottom with caster mount""
The piece was great but it made the castor a different height then the wheels. I was wondering if anyone had this file but saved as something different so I can edit it in preferably Solidworks. I was on the phone with IRobot for over 2 hours today and they told me to post here. So please help!!!! :)
","irobot-create, roomba"
Kinect v1 VS Kinect v2,"From a technical standpoint what are the differences between the Kinect v1 and the Kinect v2 ?
I'm interested both in the hardware equipment and the format of the data.
",kinect
MATLAB 3D Simulation with SOLIDWORKS model,"I'm learning  to make a 3D simulation in MATLAB based on a model designed from SOLIDWORKS.
There is an example: SIMULINK+SOLIDWORKS
The way used here is: 

Create a 3D model in SOLIDWORKS
Create a xml file applicable to import to MATLAB via SimMechanics Link
Import the model to MATLAB/SIMULINK. A simulink system is created.

After these steps, controlling the system will be implemented in SIMULINK.
But I feel simulink is kind of strict to control. I want to be more flexible, apply any algorithm to the model. And using matlab *.m file to control is more efficient way. 
So my question is this: Is there any way to do 3D simulation (MATLAB+SOLIDWORKS) by using only *.m file to control, no SIMULINK anymore? 
All model information will be contained in the *m.file. Maybe the step 1 and 2 are inherited, but step 3 is different.
","matlab, simulation"
How to cut throttle signal to ESC properly?,"I have a 16 Channel Servo Driver board from Adafruit (see here), and I communicate to it via I2C using a Raspberry Pi. The servo board is controlling a Qbrain by sending a PWM pulse between 1ms to 2ms and it works great.
Problem is, I'm trying to create a kill switch such that the signal from the servo board would cease, and the ESC would stop because it detects no PWM signal. I have placed a toggle switch that cuts the VCC to the servo board, so technically it should no longer produce any PWM signal, however when the power is cut, the ESC jumps to 100% throttle, I can only assume this is because the ESC believes the signal is 100% duty cycle, but how do I solve this?
","raspberry-pi, esc"
Modeling a robot to find its position,"The task of the robot is as follows.
My robot should catch another robot in the arena, which is trying to escape. The exact position of that robot is sent to my robot at 5Hz. Other than that I can use sonsor to identify that robot.
Is that possible to estimate the next position of other robot using a mathematical model. If so, can anyone recommend tutorials or books to refer..?
","arduino, kalman-filter, automatic, probability"
Duty cycle mapping,"I need to build a conversion/mapping algorithm from a controller (PID etc.) output to the duty cycle in order to command my bldc motor via esc. I couldn't do it yet because l think l dont know the meaning of controller output. Anybody highlights my way?
","arduino, motor, esc, microcontroller"
Drone Battery Question,"Background:

6 propeller drone w 20C 3s 6400 mAh 11.1 liPO battery
4 propeller drone w 25C 2s 5000 mAh 7.40 liPO battery

Behavior:

Drone 1 flies with ease
Drone 2 struggles hover 2-3 inches above ground

Question:
The microcontroller, all props, ESCs, and motors are the same. I'm thinking the reason the drones are flying so differently is because of the difference in batteries. IF the batteries are the reason, what would be the property that is most responsible for the difference in flight?
",battery
jacobian of Abb irb140 robot,"Can someone please help me with the jacobian matrix equations for Abb irb140 robot. Or an easy way by which I can derive it given the DH parameters. I need it to implement some form of control that am working on. Thanks
","control, robotic-arm, kinematics, dh-parameters"
Mathematical Moddeling of Elastic Robots,"We can easily compute the rigid robot kinematics and dynamics. There is many resources, simulators and modelling tools about it. But i couldnt find any of these for elastic robots. Can you suggest resources and modelling tools?
","kinematics, simulator, dynamics"
Calculus in robotics,"
I am still in high school and am a part of the robotics club that competes in the FTC (First Tech Challenge). I am just about finishing my first Calculus class (Calc 1), and would be ecstatic to be able to apply this someway in a real world example such as robotics. [Besides PID. It seems like only approximations anyways] 
So far, I've only been working with ""fabricated"" math problems. Would deriving an equation from real life situations be too complicated?

Thank you!
","control, software"
Orientation parameter for quadcopter with madgwick fusion algorithm,"I recently decided to build a quadricopter from scratch using Arduino and now I'm faced with an orientation estimation problem.
I bought a cheap 10DOF sensor with 3 axis magnetometer, 3 axis accelerometer, 3 axis gyro and a barometer and the complementary filter that I use to get orientation returns usable but noisy values.
I tried the Madgwick fusion filter too, but it returns unstable values that diverges from the ones I get with complementary filter. Given that the Madgwick filter implementation is correct, I pass acceleration values measured in Gs, gyro values measured in rps (radians per second) and Magnetometer values measured in uT, while sampling time is the same of my loop cycle. Is there anything I have missed?
Is there any advantage using Kalman filter?
EDIT1:
My problem was due to an wrong choice of sampling time and now seems to work, but convergence is very very slow (i.e. it takes about 3 seconds to reach the right value after a quick flip of the IMU). Rising value of Kp adds to much noise. I also tried to repeat filter update step more than once per cycle but it requires too much time exceeding the sampling time.
Here some graphs, from top to bottom Complementary filter, Madgwick filter and Madgwick filter with high Kp:



EDIT2:
Different values probably are caused by cable plug and unplug. Anyway raw data example from my sensor can be downloaded here
","arduino, quadcopter, kalman-filter, imu"
Is it possible to simulate vision (perception) in Gazebo (or other simulators),"Vision is important part of robotics and frequently it is unavoidable component of control loop. E.g. many clothes/garment handling algorithms rely on visual cues in deciding how to proceed. The question is - does simulation environments (Gazebo or some others) allow one to design world with robot and garment and simulate not only garment dynamics but simulate also what robot sees, how robot perceives garment in each simulation step? If it is not possible to simulate vision then how to simulate algorithms with vision as component of control loop?
Maybe simulation of vision can be good research theme? Are here some trend or good articles about it? Some initial projects that could be expanded?
Actually - it can be stated as more general question - is it possible to simulate sensors in Gazebo? E.g. food handling (soft-body handling) can involve tactile sensors. In principle Gazebo can calculate deformation and forces of soft-body and format these data as the simulated values of sensor readings. Maybe similar mechanism can be used for simulation of vision as well?
","computer-vision, simulator, research, simulation, gazebo"
"Building parts, and keeping laser alignment steady","I am building a laser gun for pentathlon targets (also doing one).
I would like to know how build a part of the gun and if I can count on a steady laser if it is attached to a motor.
The question is about the laser. I want it maybe attached to a small-(servo)motor to try to implement some cheat just for fun. Assuming the motor has a good torque, can I assume that the laser will not move (not the sightliest bit) when the motor is turned off? (I don't have any to test)
This is for precision shooting, so small vibrations and a moving pointer would be really prejudicial.
In case it does, what can I do to minimize the problem? Is it all about ordering the motor with the highest torque?
I also have a second question which is slightly off-topic, yet related, and robotics people usually have solutions for such problems.
I also need to build the sights. Here's a gun:

As you can tell, its sights are a fixed plastic point in the front, and an adjustable large back. There are two bolts, one on each side. One makes the sight higher or lower, and the other makes it point more to the right or left.
How can such part be built with simple tools?
Thanls
","motor, stability, laser"
Using a Sick laser with Matlab in Windows,"Is there a Matlab toolbox available to use Sick lasers in Windows?
I found one toolbox for Matlab in GNU/Linux.  Is there another way to use Sick laser via Matlab in Windows?
","mobile-robot, localization"
Navigating through a Maze using path-planning (Dijkstra),"I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.
Up till now I have processed the bitmap image, and converted it into an adjacency list. I will now use the dijkstra's algorithm to plan the path.
However the problem is that I have to extract the entrance point/node and exit node from the bmp image itself for dijkstra's algorithm to plan the path.
The robots starting position will be slightly different (inch or two before the entrance point) from the entrance point of maze, and I am supposed to move to the entrance point using any ""arbitrary method"" and then apply dijkstra algorithm to plan path from maze's entrance to exit.
On the way I have to also stop at the ""X's"" marked in the bmp file I have attached below. These X's are basically boxes in which I have to pot balls. I will plan the path from entrance point to exit point , and not from the entrance to 1st box, then to second, and then to the exit point; because I think the boxes will always be placed at the shortest path.
Since the starting position is different from the entrance point, how will I match my robot's physical location with the coordinates in the program and move it accordingly. Even if the entrance position would have been same as starting position there may have been an error. How should I deal with it? Should I navigate only on the bases of the coordinates provided by dijkstra or use ultrasonics as well to prevent collisions? And if we yes, can you give me an idea how should I use the both (ultrasonics, and coordinates)?

","arduino, mobile-robot, localization, motion-planning, mapping"
Estimation of Battery Life Time From PWM Signals in a Quadrotor,"Is there any way of estimation the battery life from pwm outputs which goes to motors in microcontroller level. I'm planning to estimate path range with this. Microcontroller, sensor and other electronic device should be neglected. 
","quadcopter, battery"
How to control a dc motor,"I have 2 of these 12v motors and a 12v battery
http://www.enigmaindustries.com/Motors/Bosch_EV_Warrior.htm
I would like to know what the best solution for controlling this motor with an Arduino Uno would be.
Does the motor controller need to have a maximum current of 100A?
I though of a 100A transistor connected to the pwm pin of arduino, and then, control the motor with pwm.
Is voltage regulator better than pwm?
","motor, esc, pwm, microcontroller"
Kalman Filter and the state noise vector?,"I'm reading Probabilistic Robotics by Thrun. In the Kalman filter section, they state that 
$$
x_{t} =A_{t}x_{t-1} + B_{t}u_{t} + \epsilon_{t}
$$
where $\epsilon_{t}$ is the state noise vector.  And in
$$
z_{t} = C_{t}x_{t} + \delta_{t}
$$
where $\delta_{t}$ is the measurement noise. Now, I want to simulate a system in Matlab. Everything to me is straightforward except the state noise vector $\epsilon_{t}$. Unfortunately, majority of authors don't care much about the technical details. My question is what is the state noise vector? and what are the sources of it? I need to know because I want my simulation to be rather sensible. About the measurement noise, it is evident and given in the specifications sheet that is the sensor has uncertainty ${\pm} e$.
","kalman-filter, noise"
"What type of control law is used in ""Reaction Control System"" of Apollo Lunar Module or Space Shuttle?","Reaction Control Systems (RCS) on these vehicles are implemented by using small rocket thrusters. For me it looks like these thrusters work in some kind of ""pulse"" mode. And I can't understand - do they use some optimal control to calculate in advance the required impulse to reach the new desired state of the system OR they use ""pulse"" mode just for precise magnitude variation of provided thrust (like average voltage in PWM(pulse-width modulation)) in a classic PID control loop?
","control, automatic, rocket"
position control for linear model of quadrotor (problem with tracking task),"Lately, if you notice I have posted some questions regarding position tracking for nonlinear model. I couldn't do it. I've switched to linear model, hope I can do it. For regulation problem, the position control seems working but once I switch to tracking, the system starts oscillating. I don't know why. I have stated what I've done below hope someone guides me to the correct path. 
The linear model of the quadrotor is provided here which is 
$$
\begin{align}
\ddot{x}    &= g \theta \ \ \ \ \ \ \ \ \ \ (1)\\
\ddot{y}    &= - g \phi \ \ \ \ \ \ \ \ \ \ (2)\\
\ddot{z}    &= \frac{U_{1}}{m} - g \\
\ddot{\phi} &= \frac{L}{J_{x}} U_{2} \\
\ddot{\theta} &= \frac{L}{J_{y}} U_{2} \\
\ddot{\psi} &= \frac{1}{J_{z}} U_{2} \\
\end{align}
$$
In this paper, the position control based on PD is provided. In the aforementioned paper, from (1) and (2) the desired angles $\phi^{d}$ and $\theta^{d}$ are obtained, therefore, 
$$
\begin{align}
\theta^{d}  &= \frac{\ddot{x}^{d}}{g}  \\
\phi^{d}    &= - \frac{\ddot{y}^{d}}{g}
\end{align}
$$
where 
$$
\begin{align}
\ddot{x}^{d} &= Kp(x^{d} - x) + Kd( \dot{x}^{d} - \dot{x} ) \\
\ddot{y}^{d} &= Kp(y^{d} - y) + Kd( \dot{y}^{d} - \dot{y} ) \\
U_{1} &= Kp(z^{d} - z) + Kd( \dot{z}^{d} - \dot{z} ) \\
U_{2} &= Kp(\phi^{d} - \phi) + Kd( \dot{\phi}^{d} - \dot{\phi} ) \\
U_{3} &= Kp(\theta^{d} - \theta) + Kd( \dot{\theta}^{d} - \dot{\theta} ) \\
U_{4} &= Kp(\psi^{d} - \psi) + Kd( \dot{\psi}^{d} - \dot{\psi} ) \\
\end{align}
$$
with regulation problem where $x^{d} = 2.5 m, \ y^{d} = 3.5 m$ and $z^{d} = 4.5 m$, the results are 


Now if I change the problem to the tracking one, the results are messed up. 


In the last paper, they state 

A saturation function is needed to ensure that the reference roll and
  pitch angles are within specified limits


Unfortunately, the max value for $\phi$ and $\theta$ are not stated in the paper but since they use Euler angles, I believe $\phi$ in this range $(-\frac{\pi}{2},\frac{\pi}{2})$ and $\theta$ in this range $[-\pi, \pi]$
I'm using Euler method as an ODE solver because the step size is fixed. For the derivative, Euler method is used. 
This is my code 
%######################( PD Controller & Atittude )%%%%%%%%%%%%%%%%%%%%

clear all;
clc;

dt = 0.001;
 t = 0;

% initial values of the system
 x = 0;
dx = 0;
 y = 0;
dy = 0;
 z = 0;
dz = 0;

   Phi = 0;
  dPhi = 0;
 Theta = 0;
dTheta = 0;
   Psi = pi/3;
  dPsi = 0;


%System Parameters:
m = 0.75;      % mass (Kg)
L = 0.25;      % arm length (m)
Jx = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jy = 0.019688; % inertia seen at the rotation axis. (Kg.m^2)
Jz = 0.039380; % inertia seen at the rotation axis. (Kg.m^2)
g  = 9.81;      % acceleration due to gravity m/s^2

errorSumX = 0;
errorSumY = 0;
errorSumZ = 0;

errorSumPhi   = 0;
errorSumTheta = 0;

pose = load('xyTrajectory.txt');

% Set desired position for tracking task
DesiredX = pose(:,1);
DesiredY = pose(:,2);
DesiredZ = pose(:,3);

% Set desired position for regulation task
% DesiredX(:,1) = 2.5;
% DesiredY(:,1) = 5;
% DesiredZ(:,1) = 7.2;


dDesiredX = 0;
dDesiredY = 0;
dDesiredZ = 0;

DesiredXpre = 0;
DesiredYpre = 0;
DesiredZpre = 0;

dDesiredPhi = 0;
dDesiredTheta = 0;
DesiredPhipre = 0;
DesiredThetapre = 0;



for i = 1:6000

   % torque input
   %&&&&&&&&&&&&( Ux )&&&&&&&&&&&&&&&&&&
   Kpx = 90; Kdx = 25; Kix = 0.0001; 


   errorSumX = errorSumX + ( DesiredX(i) - x );

   % Euler Method Derivative
     dDesiredX = ( DesiredX(i) - DesiredXpre ) / dt;
   DesiredXpre = DesiredX(i);


   Ux = Kpx*( DesiredX(i) - x  ) + Kdx*( dDesiredX - dx ) + Kix*errorSumX;
   %&&&&&&&&&&&&( Uy )&&&&&&&&&&&&&&&&&&
   Kpy = 90; Kdy = 25; Kiy = 0.0001; 


   errorSumY = errorSumY + ( DesiredY(i) - y );

   % Euler Method Derivative
   dDesiredY = ( DesiredY(i) - DesiredYpre ) / dt;
   DesiredYpre = DesiredY(i);


   Uy = Kpy*( DesiredY(i) - y  ) + Kdy*( dDesiredY - dy ) + Kiy*errorSumY;
   %&&&&&&&&&&&&( U1 )&&&&&&&&&&&&&&&&&&
   Kpz = 90; Kdz = 25; Kiz = 0; 


   errorSumZ = errorSumZ + ( DesiredZ(i) - z );

      dDesiredZ = ( DesiredZ(i) - DesiredZpre ) / dt;
   DesiredZpre = DesiredZ(i);

   U1 = Kpz*( DesiredZ(i) - z ) + Kdz*( dDesiredZ - dz ) + Kiz*errorSumZ;
   %#######################################################################
   %#######################################################################
   %#######################################################################
   % Desired Phi and Theta


   %disp('before')
   DesiredPhi   = -Uy/g;
   DesiredTheta =  Ux/g;



   %&&&&&&&&&&&&( U2 )&&&&&&&&&&&&&&&&&&
   KpP = 20; KdP = 5; KiP = 0.001;


   errorSumPhi = errorSumPhi + ( DesiredPhi - Phi );


   % Euler Method Derivative
      dDesiredPhi = ( DesiredPhi - DesiredPhipre ) / dt;
   DesiredPhipre  = DesiredPhi;


   U2 = KpP*( DesiredPhi - Phi ) + KdP*( dDesiredPhi - dPhi )  + KiP*errorSumPhi;

   %--------------------------------------
   %&&&&&&&&&&&&( U3 )&&&&&&&&&&&&&&&&&&

   KpT = 90; KdT = 10; KiT = 0.001;
    errorSumTheta = errorSumTheta + ( DesiredTheta - Theta );

   % Euler Method Derivative
      dDesiredTheta = ( DesiredTheta - DesiredThetapre ) / dt;
   DesiredThetapre = DesiredTheta;


   U3 = KpT*( DesiredTheta - Theta ) + KdP*( dDesiredTheta - dTheta ) + KiT*errorSumTheta;
   %--------------------------------------
   %&&&&&&&&&&&&( U4 )&&&&&&&&&&&&&&&&&&
   KpS = 90; KdS = 10; KiS = 0; DesiredPsi = 0; dDesiredPsi = 0;
   U4 = KpS*( DesiredPsi - Psi ) + KdS*( dDesiredPsi - dPsi );


   %###################( ODE Equations of Quadrotor )###################
   ddx = g * Theta;
    dx = dx + ddx*dt;
     x =  x +  dx*dt;
   %=======================================================================  
   ddy = -g * Phi;
    dy = dy + ddy*dt;
     y =  y +  dy*dt;
   %=======================================================================
   ddz = (U1/m) - g;
    dz = dz + ddz*dt;
     z =  z +  dz*dt;
   %=======================================================================  
   ddPhi = ( L/Jx )*U2;
    dPhi = dPhi + ddPhi*dt;
     Phi =  Phi +  dPhi*dt;
   %=======================================================================  
   ddTheta =  ( L/Jy )*U3;
    dTheta =  dTheta + ddTheta*dt;
     Theta =   Theta +  dTheta*dt;
   %=======================================================================  
   ddPsi =  (1/Jz)*U4; 
    dPsi = dPsi + ddPsi*dt;
     Psi =  Psi +  dPsi*dt;
   %=======================================================================  
   %store the erro
   ErrorX(i)   = ( x - DesiredX(i) );
   ErrorY(i)   = ( y - DesiredY(i) );
   ErrorZ(i)   = ( z - DesiredZ(i) );
   ErrorPsi(i)   = ( Psi - 0 );


   X(i) = x;
   Y(i) = y;
   Z(i) = z;

   T(i) = t;

   t = t + dt; 


end


Figure1 = figure(1);
set(Figure1,'defaulttextinterpreter','latex');


subplot(2,2,1)
plot(T, ErrorX, 'LineWidth', 2)
title('Error in $x$-axis Position (m)')
xlabel('time (sec)')
ylabel('$x_{d}(t) - x(t)$', 'LineWidth', 2)

subplot(2,2,2)
plot(T, ErrorY, 'LineWidth', 2)
title('Error in $y$-axis Position (m)')
xlabel('time (sec)')
ylabel('$y_{d}(t) - y(t)$', 'LineWidth', 2)

subplot(2,2,3)
plot(T, ErrorZ, 'LineWidth', 2)
title('Error in $z$-axis Position (m)')
xlabel('time (sec)')
ylabel('$z_{d} - z(t)$', 'LineWidth', 2)


subplot(2,2,4)
plot(T, ErrorPsi, 'LineWidth', 2)
title('Error in $\psi$ (m)')
xlabel('time (sec)')
ylabel('$\psi_{d} - \psi(t)$','FontSize',12);
grid on 


Figure2 = figure(2);
set(Figure2,'units','normalized','outerposition',[0 0 1 1]);

figure(2)
plot3(X,Y,Z, 'b')
grid on

hold on 
plot3(DesiredX, DesiredY, DesiredZ, 'r')

pos = get(Figure2,'Position');
set(Figure2,'PaperPositionMode','Auto','PaperUnits','Inches','PaperSize',[pos(3),pos(4)]);
print(Figure2,'output2','-dpdf','-r0');

For the trajectory code
clear all;
clc;

fileID = fopen('xyTrajectory.txt','w');

 angle = -pi;
radius = 3;
z = 0;
t = 0;

for i = 1:6000
    if ( z < 2 ) 
        z = z + 0.1;
        x = 0; 
        y = 0;
    end
    if  ( z >= 2 )
        angle = angle + 0.1;
        angle = wrapToPi(angle);
        x = radius * cos(angle);
        y = radius * sin(angle);
        z = 2;
    end

    X(i) = x;
    Y(i) = y;
    Z(i) = z;

    fprintf(fileID,'%f \t %f \t %f\n',x, y, z);
end

fclose(fileID);
plot3(X,Y,Z)
grid on

","control, quadcopter, matlab"
Position and Object Data Tracking,"For a class project, I'm working with a weight stack:

I'm trying to simultaneously measure:

the position of a moving weight stack
the value of the weight based on a calibrated/preloaded position in the stack, not via load sensor. (e.g. think a stack of plate weights where the sensor knows in advance that 1 plate = 10lbs, 2 plates = 20lbs, etc.)

The weight stack and the base camp chip/sensor/laser would be within two feet of the weight stack, so I don't need anything overly strong. My requirement is that it is small/unobtrusive and cost effective. I've looked into a few options, but I'm not an engineer so I'm not sure if I am on the right track.
How would you do this? Is there any research that I could check out?
",sensors
Conventional Land Vehicle Dynamic Models for GPS/INS augmentation,"I am looking to augment a GPS/INS solution with a conventional land vehicle (car-like) model. That is, front-wheel steered, rear wheels passive on an axle.  I don't have access to odometry or wheel angle sensors.
I am aware of the Bicycle Model (e.g. Chapter 4 of Corke), but I am not sure how to apply the heading/velocity constraint on the filter.
So my questions are:

Are there any other dynamic models that are applicable to the land vehicle situation, especially if they have the potential to provide better accuracy?
Are there any standard techniques to applying such a model/constraint to this type of filter, bearing in mind I don't have access to odometry or wheel angle?
Are there any seminal papers on the topic that I should be reading?

","gps, dynamics"
Optimal hardware for linear algebra operations,"I've been working lately on SLAM algorithms implementing extended kalman filtering to brush up on some localisation techniques and I have been thinking forward to the hardware side of things. Are there embedded chips such a microcontroller that are optimised for large linear algebra operations? What sort of embedded options are the best for processing these sorts of operations?
","slam, kalman-filter"
Formationing Algorithm for Multiple Robots,"I'm looking for an algorithm for formationing multiple robots in 2D simulation. Can you suggest resources about this topic. Also I need suggestions and comments about these topics:

Can I recruit algorithm from optimization algorithms like particle or ant?
Is there any way except ""go to goal"" for each robot
Is patter formationing algorithms feasible?
Suggestions about a fast way of formationing/ aligning

Notes:

Im not using a robotics simulator or physics engine for this. 
Robots are represented as dots.
multi robot system is homogeneous
every robot can sense obstacles and other robots in a sense range circle around the robot.  
number of obstacles and robots can vary from 2 to 100 
multi robot system is not a central 

","mobile-robot, multi-agent, swarm"
How to get the projection matrix from odometry/tf data?,"I would like to compare my results of visual Odometry with the groundtruth provided by the KITTI dataset.
For each frame in the groundthruth, i have a projection matrix.
For example:
1.000000e+00 9.043683e-12 2.326809e-11 1.110223e-16 9.043683e-12 1.000000e+00 2.392370e-10 2.220446e-16 2.326810e-11 2.392370e-10 9.999999e-01 -2.220446e-16

Here the instructions provided by the readme:

Row i represents the i'th pose of the
  left camera coordinate system (i.e., z
  pointing forwards) via a 3x4
  transformation matrix. The matrices
  are stored in row aligned order (the
  first entries correspond to the first
  row), and take a point in the i'th
  coordinate system and project it into
  the first (=0th) coordinate system.
  Hence, the translational part (3x1
  vector of column 4) corresponds to the
  pose of the left camera coordinate
  system in the i'th frame with respect
  to the first (=0th) frame

But I don't know how to produce the same kind of data for me.
What I have for each frame in my case:

The Tf transformation from the init_camera (the fix one from the (0,0,0)) to the left camera which is moving. So I have the translation vector and the quaternion rotation.
The odometry data: the pose and the twist
Camera calibration parameters

With those data, How I compare with the groundtruth ? So I need to find the projection matrix from the data above but don't know how to do it.
Can someone help me ?
Thank
","mobile-robot, odometry, stereo-vision"
DC Motor PID control with unstable velocity feedback,"Currently I am building a omnidirectional robot with 4 DC Motors with embedded incremental encoder.
However, with a constant pwm input, i am not able to control the motor to rotate in a ""relatively stable"" state, refer to the figure, it can be observed that the linear speed of the motors can varied in 10cm/s range. I believe one possible reason is the PWM signal generated from my Arduino Mega Controller is not good enough.
And my problem is how can I implement a stable PID controller in this case? As the speed of the motor varies even with the same input, I believe extra work like adding a filter is needed?
Any advice is appreciated >.< Thank you

","motor, pid"
How to set up binocular cameras on a car?,"I am trying to set up my stereo vision system on a car. However, I meet several problems and do not know how to solve them.

How to select the baseline? I want the distance measurement to be far at 30 or 50 meters and near at around 5-10m. Is it possible to choose a baseline that meets my requirement?
I have tried stereo calibration of two cameras and also learned how to compute depth value from disparity map. However I don't know how to compute depth value if the focal lengths of the two cameras are different. It seems all the theorems I can find on the Web only concern cameras of the same focal length. 

",stereo-vision
Quadcopter liPo battery weight/capacity trade off,"I'm trying to find where additional battery capacity becomes worthless in relation to the added weight in terms of a quadcopter. Currently with a 5500 mAh battery, 11.1V, I can get between 12 minutes and 12:30 flight time out of it. My question, then, is this - within the quads lifting capability of course, is there any way to find out where the added weight of a larger battery (or more batteries) cancels out any flight time improvement? Obviously it's not going to be as long as two separate flights, landing and swapping batteries; I'm just trying to maximize my continuous 'in air' time. I'm trying to figure out where the line is (and if I've already crossed it) with tacking bigger batteries onto the quad and seeing diminishing returns. Thanks!
(Again, for now presume that the quad is strong enough to lift whatever you throw at it. With one 5500mAh, ~ 470 grams, my max throttle is about 70%)
","battery, quadcopter, power"
Low variance resampling algorithm for particle filter,"For my particle filter, I decided to try using the low variance resampling algorithm as suggested in Probabilistic Robotics. The algorithm implements systematic resampling while still considering relative particle weights. I implemented the algorithm in Matlab, almost word-for-word from the text:
function [state] = lowVarianceRS(prev_state, weight, state_size)
    state = zeros(1,state_size);    % Initialize empty final state
    r = rand;                       % Select random number between 0-1
    w = weight(1);                  % Initial weight
    i = 1;
    j = 1;

    for m = 1:state_size
        U = r + (m - 1)/state_size; % Index of original sample + size^-1
        while U > w                 % I'm not sure what this loop is doing
            i = i + 1;
            w = w + weight(i);
        end
        state(j) = prev_state(i);   % Add selected sample to resampled array
        j = j + 1;
    end
end

As would be expected given the while loop structure, I am getting an error for accessing weight(i), where i exceeds the array dimensions.
To solve this, I was considering circularly shifting my weight array (putting the first index used as the first value in weight, so that I never exceed matrix dimensions). However, I wasn't sure if this would negatively impact the rest of the algorithm, seeing as I'm having trouble understanding the purpose of the U calculation and while loop.
Could anyone help clarify the purpose of U and the while loop, and whether or not a circular shift is an acceptable fix?
","mobile-robot, algorithm, particle-filter, probability"
Voltage rpm relation,"I measure the voltage ESC drawing while increasing the dc motor speed. Multimeter shows that as long as the speed increases the voltage value decreases. Can anybody explain why this is happening? 
","brushless-motor, electronics, esc"
Torque of coreless DC micro motor,"I would like to build a small two-wheeled robot similar to the one shown here.
In order to keep the robot small, I intend to use two coreless micro motors like the one shown bellow. The power source would be 2 AAA or AA batteries, in order to reach 3 V. These batteries would represent the bulk of the weight of the robot. The rest of the robot would be virtually weightless.
The specifications of one of such motor are:
Motor diameter: 6 mm
Motor length: 12 mm
Output shaft: 0.8 mm
Output shaft length: 4 mm
Voltage: 3 V 
Current: 17 mA (stall 120 mA) 
Frequency​​: 22000 RPM 

My question is if small DC motors of this type have enough torque to even make the robot start moving. I have been unable to find torque info on these kind of motors and I suspect the weight of the robot could be too much for them to handle. Do you know the typical torque of such motor? Is there another type of (cheap) motor more appropriate for this project?

","mobile-robot, motor, wheeled-robot, torque"
Motion Model for Holonomic Robot,"We are working with an holonomic robot equipped with three (120 degree shifted) omnidirectional wheels. The relative movement is estimated by dead reckoning using wheel encoders. To improve this estimation we installed an gyroscope to measure the change in orientation. Furthermore the robot has a 270 degree laser range finder. 
In order to solve the kidnapped robot problem we implemented a particle filter. In every step each particle is updated according to the odometry and gyroscope readings. Since these readings are distorted by noise we need a motion model to include these errors. As described in Probabilistic Robotics by Thrun (Page 118 - 143) there are two commonly used motion models (velocity motion model and odometry motion model). However these models seem to describe the behavior of differential drive robots not omnidirectional robots. I base this thesis on the fact that the error in relative y-direction is proportional to the error in orientation as far as the motion models by Thrun are concerned. This is appropriate for differential drive robots as the orientation and the heading of the robot are identical. For omnidirectional robots this assumption can not be made since the heading and the orientation are completely independent. Even if we assume perfect information about the robots orientation we can still obtain error in relative y-direction.
I would like to discuss if my assumption - that the velocity/odometry motion model fails for omnididrectional robots - is correct or not as i am not sure about that. Furthermore  i am curious if there are any other motion models for omnidirectional robots that might fit better.
","mobile-robot, localization, motion, particle-filter"
Guidance for compensating internal forces on closed loop chain,"I'm working on a legged robot and generating joint torques. Basically the robot seems to be statically stable to some extend. The robot goes instable if the center of pressure moves to the border of the feet. I'm looking for some method to move away the center of pressure  from the feet edges after having calculated my joint torques. In Sentis thesis ( http://ai.stanford.edu/~lsentis/files/Thesis-Sentis-2007.pdf ) , it is mentioned that he somehow manages to cancel out the internal forces to keep the feet flat against the supporting surfaces. 
Does anyone has got experience in dealing with internal forces? As far as I understood the literature one can modify the nullspace of the calculated torques to achieve that the COP remains in the geometrical center of the considered foot. I'm looking for methods apart from the virtual linkage model as it did not seem to work for me or someone with whom I could discuss the virtual linkage model described in ( http://ai.stanford.edu/~lsentis/files/tro-2010.pdf ) as I might not have it understood it correctly.
","stability, legged"
increase PID sampling rate on embedded system,"my robotic project is running at every 1ms and the processes are taking about 0.9ms. I am running PID so my max clock rate is 1kHz. About half of the processing time are taken by SPI peripherals, IMU and encoders. Is there any recommendation on how I can run faster PID sampling rate?
","pid, embedded-systems"
Degree of Freedom,"A robotic arm should pick a cuboid up of a table, rotate it around its vertical axis and put it down on all possible positions. How many degrees of freedom are at least necessary?
(All coordinates, that should be reached by the robotic arm, are in its workspace. It is not allowed to put the cuboid down and pick it up, once the robot has it )

The answer is  4 (3 translatory and 1 rotatory).

But I don’t understand why. I thouhgt that it should be 3.
2 prismatic joints:  1 to pick the cuboid up,  and another one to move it anywhere on the table.
1 revolute joint to rotate the cuboid around its vertical axis. => 2 translatory and 1 rotatory. 
",robotic-arm
7DOF inverse kinematics spherical wrist,"Is it possible to apply kinematic decoupling for a 7 DOF 7R manipulator with spherical wrist?  If it is possible, can anyone suggest a reference on how to apply this approach with a redundant manipulator with spherical wrist, or explain why it is not possible? 
I'm working with Robotic Toolbox (matlab) and the numeric algorithm can find the inverse kinematics solution without a problem if I don't specify the orientation.  And I was thinking about solving the problem a second time considering the spherical wrist.  Will this approach work?
","inverse-kinematics, manipulator, matlab"
Ackerman steering model,"I am trying to create a simulation of a robot with Ackerman steering (the same as a car). For now I'm assuming that it's actually a 3-wheeled robot, with two wheels at the back, and one steering wheel at the front:

Knowing the wheel velocity, and the steering angle a, I need to be able to update the robot's current position and velocity with the new values at time t+1.
The obvious way to do this would be to calculate the position of the centre of rotation, where the axles of the wheels would meet, however, this leads to an undefined centre of rotation when a = 0. This means that the model doesn't work for the normal case of the robot just driving in a straight line.
Is there some other model of Ackerman steering which works over a reasonable range of a?
","mobile-robot, simulation"
Measurement and physics model fusion,"I am combining two position measurements of a ball from two sensors in real time to obtain one triangulated position in x,y,z coordinates. As the data exchange of the measurements carries some latency, the data has to be extrapolated be able to obtain the current position. Due to extrapolation an error appears in the triangulated data.
I know that when the ball is in the air, the velocity of the ball should be constant in x and y directions and the velocity in the z direction should decay with g. The velocities in x and y however oscillate as function of time around a mean value which is the actual x respectively y velocity. The same goes for when I compute the acceleration in the z direction. It oscillates as function of time around g.
Given that I know how the ball should behave, i.e. that vx and vy should be constant and that the acceleration in the z direction should be z, how can I impose these conditions to better estimate the triangulated position? 
",sensor-fusion
KUKA Robotics API IDE,"I've got Robotics API library, demo-program and a robot. I want to develop app for it. The best solution is offline development on some kind of simulator. I'm completely new in such tasks - is there any IDE for this? Or a way do deliver byte-code to machine? Thanks in advance!
","robotic-arm, dynamic-programming"
Is anybody using robot simulators?,"Do you use simulators for developing your robot algorithms or do you test directly in your robot?
I would like to get introduced into the simulators world, but don't know from where to start... can you recommend me one?
Regards
",simulator
RGB-D SLAM - Compute Information Matrix,"currently im working on a RGB-D SLAM with a Kinect v1 Camera. In the front-end the SLAM estimates the pose with Ransac as an initial guess for the ICP. With the pose estimation i transform the pointcloud to a pointcloud-scene which represents my map.
To smooth the map im trying to implement a graph optimizing algorithm (g2o). 
Until now, there is no graph representation in my frontend, so i started to integrate that.
Im trying to build a .g2o file with the following fromat:
VERTEX_SE3 i x y z qx qy qz qw
where x, y, z is the translation and qx, qy, qz, qw ist the Rotation in respect to the initial coordinate system. And,
EDGE_SE3 observed_vertex_id observing_vertex_id x y z qx, qy, qz, qw inf_11 inf_12 .. inf_16 inf_22 .. inf_66
Translation and rotation for the edge is the pose estimate that i compute with Ransac and ICP (visual odometry). 
Now im getting stuck with the information matrix.
I read the chapter 3.4 THE INFORMATION FILTER in Thrun's Probabolistic Robotics and several threads in this forum, such as:
The relationship between point cloud maps and graph maps
and
information filter instead of kalman filter approach
From the second link, i got this here. 

The covariance update
  $$P_{+} = (I-KH)P$$
  can be expanded by the definition of K to be
$$ P_{+} = P - KHP$$
  $$ P_{+} = P - PH^T (HPH^T+R)^{-1} HP$$
Now apply the matrix inversion lemma, and we have:
$$P_{+} = P - PH^T (HPH^T+R)^{-1} HP$$
  $$ P_{+} = (P^{-1} + H^TR^{-1}H)^{-1}$$
Which implies:
  $$ P_{+}^{-1} = P^{-1} + H^TR^{-1}H$$
The term $P^{-1}$ is called the prior information,$$H^TR^{-1}H$$ 
  is the sensor information (inverse of sensor variance), and this gives us  $P^{-1}_+$, which is the posterior information. 

Could you please point this out for me. 
What data do i need to compute the information matrix? 
","slam, kinect, matlab"
Using 3DR Radio to communicate ArduPilot Data,"I am trying to send some data over to my PC from the Arduipilot, I used a Normal USB connection to send over a recurring string like this:-
const AP_HAL::HAL& hal = AP_HAL_BOARD_DRIVER;

void setup()
{
   hal.uartA->begin(38400);
}

void loop()
{
    hal.scheduler->delay(20);
    hal.console->println(""Recd_String"");
}
AP_HAL_MAIN();

I receive the string just fine when I open a serial monitor with baud of 38400 bits/sec. But, when I remove the USB port and plug in the 3DR radio module to the ardupilot and the PC, it gives me garbage. I know that the 3DR radios use MAVLink communication protocol, but I was wondering if it's possible to change this protocol and use a normal SPI so that I receive the data in the same format I receive when connected via USB.
If this is not possible, is there a way to convert this garbled data from the radio module to a useful string. 
It would be greatly appreciated if someone can help me with this.
","ardupilot, radio-control"
PID gains for motor position and velocity control,"I have a servo motor with quad optical encoder and I'm trying to control its position and velocity. By controlling both I meant that if I input that the motor should reach 90° at 200rpm then it should. How can I do that? I am using an Arduino Uno. Kindly share some code if possible. 
Though I have implemented the PID, I don't think it is correct because I didn't implement the feedforward controller (because I have no idea what that is) and I have not been able to find suitable gains for PID. The gains I find for small steps (or say degree rotation) do not work out well for large steps and vice versa. I have also not used a limit for integral sum (because I don't how much it should be).
I am using a Pittman motor.
",robotic-arm
Object Grasping Robot Arm Control,"I have a 2 DOF Robot Arm with a camera attached to it. It takes an Image and there's an object in that image, say a glass. Of course, in order to move the arm to the required position to grasp the object, I have to solve the inverse kinematic equations. In order to solve them, I need the x and y, the coordinates where the arm has to reach to grasp the object. My question is how can I find the x and y of say the midpoint of the object from the image. Thanks 
","mobile-robot, robotic-arm"
"Pose estimation, how to populate set of known edges and points?","I am building an estimator that solves for the camera pose relative to a reference frame which contains a known set of features and edges. Currently, the system works with an unscented kalman filter with four known points (red leds) in the reference frame. I am now hoping to improve robustness by adding edges to the model as well as robust features. I would like to add additional points that are uncovered by some opencv feature finding function (fast,cornerHarris,...).
So far I found the paper ""Fusing Points and Lines for High Performance Tracking"" and ""Robust Extended Kalman Filtering For Camera Pose Tracking Using 2D to 3D Lines Correspondences"" which seem to detail how to fuse edge and feature matching for pose estimation.
Is there a strategy to populate the known set of edges and features when it is impractical to measure them with a ruler/tape measure? My first thought is to start with a small known set of features, my red leds, then run some slam algorithm and keep all features/edges that have some minimum certainty.
Thanks a bunch!
I have misunderstood the RANSAC algorithm. This is not appropriate for my application. 
For those interested, I am hoping to use a similar approach to the one presented in the following paper.
Youngrock Yoon, Akio Kosaka, Jae Byung Park and Avinash C. Kak. ""A New Approach to the Use of Edge Extremities for Model-based Object Tracking."" International Conference on Robotics and Automation, 2005.
","kalman-filter, computer-vision, pose"
How to reduce battery power 10v 1.5A to 6v 1.5A,"What's least complex way to reduce power from a 10V 1.5A battery to 6V 1.5A
Thank you!
",battery
Extending iCreate battery power for auxilliary equipment,"I plan to use the icreate as a platform to carry a tablet, or notebook PC and want to have power for some time so I need more than the 3000 mAh battery. I want all to be powered from same battery system and use same charging source. So I need info as to how to wire in additional 14.4V NiMH batteries in parallel with the existing and how to deal with the additional temperature sensors (I could ignore of course but...). Can the built in power control deal with this? Do I need to upgrade it somehow? I would appreciate suggestions as I do not want a completely separate power system for aux devices. Charging all from standard home base is the goal even though it will take longer. I can deal with adapting the 14.4V to whatever aux devices I add. Thanks.
","power, battery, roomba"
What actuator types exist that remain locked in their last position like hydraulic piston?,"I would like to find an electronic actuator that mimics the characteristics of a hydraulic actuator, in that the position remains fixed without power drain when the actuator is not moving. Which actuators exist that match these criteria?
","electronics, actuator"
Mapping formats for small autonomous robots,"I have some robot software I'm working on (Java on Android) which needs to store a pre-designed map of a playing field to be able to navigate around. The field's not got any fancy 3d structure, the map can be 2d.
I've been trying to find a good format to store the maps in.
I've looked into SVGs and DXFs, but neither one is really designed for the purpose.
Is there any file format specifically designed for small, geometric, robotics-oriented maps?
The field I'd be modelling is this one:

",mapping
Kinect VS Stereo cameras,"As I'm advancing in my project I realized I need better hardware, particularly for video input and processing.
From an intuitive feeling sounds like stereo cameras offers a more powerful and flexible solution, on the other hand the Kinect looks like a great out-of-the-box solution for depth sensing and it also takes away a lot of computational complexity as it output directly the depth.
So I would like to know what are the upsides and downsides of the 2 solutions and if they have any well known limitation and/or field of application and why.
Thank you 
","kinect, cameras, stereo-vision"
Turning an epilog laser into a 3d printer?,"We have an epilog laser cutter around here and I was wondering if it would possibly work as a base for a 3d printer? Here is a Dropbox photo album of the laser cutter. I am thinking I will have to get a new control system but I am unsure if I will be able to use the motor controllers or if they are embedded in the current control's board. I am also unsure if it has fine enough control on the z axis but if not that can be modified.
What would be a good head to look at?
Any other thoughts?
","laser, 3d-printing"
"where to get this reference about Kalman filter, technical report","I'm sorry for this question that might not fit in here however, I would like to give it a shot. I've chosen this stack since the question is somehow related to mobile robots. I've came across a paper in Mobile Robot Localization that has cited the following reference, 

C. Brown, H. Durrant-Whyte, J. Leonard, B. Rao, and B. Steer.  Kalman
  filter algorithms, applications, and utilities. Technical Report
  OUEL-1765/89, Oxford U. Robotics Research Group, 1989.

I couldn't find this reference. Nothing show up in Google not even in Google Scholar. In my university which allows me to access to a massive database, also nothing show up. Since this is a technical report, I'm interested to read it to have more appreciation about Kalman Filter. Has anyone came across this reference?
","mobile-robot, localization, kalman-filter"
Do magnets affect IMU values?,"Im in the process of making a robot which requires 12 3x10mm cylindric magnets for the construction. They are 30mm from the center of the robot where I plan to have the IMU. 
I was thinking about using MPU-6050. Do magnets affect the values? If yes, is there a solution for it? like maybe I could have a shield or something around the IMU?
","sensors, imu"
How do I control the robotic arm motion?,"I have a Robotic arm mounted on a car. There's a camera attached to it. Suppose the camera takes the image of a room, and finds that there's something, say an object, that has to be picked up. Say it's 50 feet away from the robot. My question is that how will the robot reach the object in the first place, and secondly, when it has reached the object, how will it know the real world co-ordinates of the object, to pick the object up, using inverse kinematic equations. Any help would be appreciated. Thanks
","mobile-robot, robotic-arm"
How do I choose the best filter for dead reckoning with an IMU?,"I'm searching filter to reduce noise and smooth the signal while dead reckoning with an IMU (6dof gyro+accelerometer). What are the differences/advantages/disadvantages of the following filters:

Kalman
Complementary
moving average
Mahony 

I applied kalman and complementary filters to an IMU and both of them gives time lag to actions with respect to filter parameters. Also kalman filter works slower than moving average and complementary. How can I choose right filter and filter parameters?
","mobile-robot, localization, kalman-filter, imu"
Conceptual problem regarding electronic shutters,"I have been looking at CCD and CMOS sensors and cameras to decide which one to use in the process of automatic control of a printing process. By now I am getting the grips on almost all the essential numbers and abbreviations but there remains a problem with shutters.
I understand that there are different types of shutters, both mechanical and electronic, and I can understand how they work. My problem concerns shutter speed. If I use a mechanical shutter, well then the maximum shutter speed depends on that particular element in the assembly, but how does it work for electronic shutters? I have never read ""Max shutter speed"" in any specs. The only thing I usually see floating around are frames per second. But those do usally not pass a limit of about 120 fps. Depending on how the sensor it is built one could think that the maximum shutter speed therefore is 1/120 or 1/240 if it uses half frames.
Can this be right? It seems really slow. I will be faced with the task of recording crisp and clear images of paper which moves at about 17 m/s. That is never possible with shutter speeds that slow. Will I be forced to use a mechanical shutter or am I misunderstanding something?
","computer-vision, cameras"
Cascading PID DC Motor Position & Velocity Controllers,"I'm trying to build a robot with a differential drive powered by two DC Motors. First I implemented a PID Controller to control the velocity of each motor independently. Estimated the TF using the MATLAB's System Identification Toolbox, of the open loop system by the acquiring the velocity of each wheels encoder in function of the PWM signal applied by an Arduino microcontroller. All went well and i successfully dimensioned the PID gains for this controller.
What I'm trying to accomplish now is to control the exact (angular) position of the DC Motor. I thought in cascading a PID controller in the input of the other already implemented. So this way, I can give a position to the first controller, which will be capable of generate an output reference to the second (velocity) controller so it generates the appropriate PWM value signal to drive the DC Motor accordingly.
Will it work? Is that a good approach? Or should I try to implement a different controller which outputs the PWM signal in response to a position reference signal?
Many thanks for your attention and I hope somebody can help me with these doubts.
","arduino, control, microcontroller, pid, wheeled-robot"
Calculating the efficiency of Mecanum wheels,"I'm part of a FIRST Robotics team, and we're looking into using Mecanum wheels for our robot.
What are the advantages and disadvantages of using Mecanum wheel versus regular ones? From looking through Google, it looks like Mecanum wheels give more mobility but don't have as much traction. Are there any other advantages or disadvantages?
Compared to regular wheels, are Mecanum wheels less efficient or more efficient in any way? And if so, is there a quantifiable way to determine by how much?
Are there equations I can use to calculate efficiency (or inefficiency) and/or speed of moving forwards, sideways, or at arbitrary angles?
A picture of a robot with mecanum wheels:

","mobile-robot, design, movement, wheel, first-robotics"
Connecting USB Xbox Controller to National Instruments cRIO,"I have a FIRST Robotics spec National Instruments cRIO.  I would like to connect a USB wireless Xbox controller to it in order to control it from a distance with minimal extra hardware (which is why I am not using the more traditional WiFi radio method).  To this point I have been able to find either
A. A sidecar for the cRIO which allows it to act as a USB host or
B. A method that does not use NI specific hardware to connect the two together
If someone who is knowledgeable on the subjects of industrial system and robot control could provide some assistance that would be greatly appreciated, thanks!
","control, industrial-robot, wireless, usb, first-robotics"
Problem with acceleration sensor,"I’m using the BMA020 (from ELV) with my Arduino Mega2560 and trying to read acceleration values that doesn’t confuse me.
First I connected the sensor in SPI-4 mode. Means
CSB <-> PB0 (SS)
SCK <-> PB1 (SCK)
SDI <-> PB2 (MOSI)
SDO <-> PB3 (MISO)
Also GND and UIN are connected with the GND and 5V Pins of the Arduino board.
Here is the self-written code I use
#include <avr/io.h>
#include <util/delay.h>

#define sensor1     0
typedef int int10_t;

int TBM(uint8_t high, uint8_t low)
{   
    int buffer = 0;
    if(high & (1<<7)) {
        uint8_t high_new = (high & 0x7F);
        buffer = (high_new<<2) | (low>>6);
        buffer = buffer - 512;
    }
    else
        buffer = (high<<2) | (low>>6);


    return buffer;
}

void InitSPI(void);
void AccSensConfig(void);
void WriteByteSPI(uint8_t addr, uint8_t Data, int sensor_select);
uint8_t ReadByteSPI(int8_t addr, int sensor_select);
void Read_all_acceleration(int10_t  *acc_x, int10_t *acc_y, int10_t *acc_z, int sensor_select);


int main(void)
{
    int10_t S1_x_acc = 0, S1_y_acc = 0, S1_z_acc = 0;
    InitSPI();
    AccSensConfig();
    while(1) {
        Read_all_acceleration(&S1_x_acc, &S1_y_acc, &S1_z_acc, sensor1);
    }
}

void InitSPI(void) {

    DDRB |= (1<<DDB2)|(1<<DDB1)|(1<<DDB0);

    PORTB |= (1<<PB0);

    SPCR |= (1<<SPE);
    SPCR |= (1<<MSTR);  
    SPCR |= (0<<SPR0) | (1<<SPR1);  
    SPCR |= (1<<CPOL) | (1<<CPHA);  
}

void AccSensConfig(void) {

    WriteByteSPI(0x0A, 0x02, sensor1);
    _delay_ms(100);

    WriteByteSPI(0x15,0x80,sensor1);    //nur SPI4 einstellen
}

void WriteByteSPI(uint8_t addr, uint8_t Data, int sensor_select) {

    PORTB &= ~(1<<sensor_select);   
    SPDR = addr;                    
    while(!(SPSR & (1<<SPIF)));     

    SPDR = Data;                    
    while(!(SPSR & (1<<SPIF)));     

    PORTB |= (1<<sensor_select);    
}

uint8_t ReadByteSPI(int8_t addr, int sensor_select)
{
    int8_t dummy = 0xAA;

    PORTB &= ~(1<<sensor_select);   

    SPDR = addr;                    
    while(!(SPSR & (1<<SPIF)));     
    SPDR = dummy;                   
    while(!(SPSR & (1<<SPIF)));     

    PORTB |= (1<<sensor_select);    

    addr=SPDR;
    return addr;
}

void Read_all_acceleration(int10_t  *acc_x, int10_t *acc_y, int10_t *acc_z, int sensor_select)
{
    uint8_t addr = 0x82;
    uint8_t dummy = 0xAA;
    uint8_t high = 0;
    uint8_t low = 0;

    PORTB &= ~(1<<sensor_select);   

    SPDR = addr;
    while(!(SPSR & (1<<SPIF)));

    SPDR = dummy;           
    while(!(SPSR & (1<<SPIF)));
    low = SPDR;     
    SPDR = dummy;   
    while(!(SPSR & (1<<SPIF)));
    high = SPDR;                    
    *acc_x = TBM(high, low);

    SPDR = dummy;       
    while(!(SPSR & (1<<SPIF)));
    low = SPDR;     
    SPDR = dummy;   
    while(!(SPSR & (1<<SPIF)));
    high = SPDR;    
    *acc_y = TBM(high, low);

    SPDR = dummy;       
    while(!(SPSR & (1<<SPIF)));
    low = SPDR; 
    SPDR = dummy;   
    while(!(SPSR & (1<<SPIF)));
    high = SPDR;    
    *acc_z = TBM(high, low);

    PORTB |= (1<<sensor_select);    
}

And now here is what really confuses me. I got 5 of this sensors. One is working with this code perfectly fine. The Data I get is what I expect. I measure earth gravity in z-component if Iay the sensor on the table, if I start turning it I measure the earth gravity component wise in x-, y- and z- direction depending on the angle I turn the sensor.
From the other 4 sensors I receive data that is different. The values jump from -314 (about -1.2 g) to +160 (about 0.5g). With the same code, the same wires and the same Arduino.
I checked the register settings of all sensors, they are all the same. I checked the wire connection to the first component at the sensors, they are all around 0.3 Ohm. I used an Oscilloscope and made sure CSB, SCK and MOSI work properly.
Am I missing something? What causes this similar but wrong behavior of 4 out of 5 sensors?
","arduino, accelerometer"
Quadrotor control using ArduIMU,"We are using ArduIMU (V3) as our Quadrotor's inertial measurement unit. (we have a separate board to control all motors, not with ArduIMU itself). 
As mentioned here , the output rate of this module is only at about 8hz. 
Isn't it super slow to control a quadrotor ? I'm asking because as mentioned in this answer a quadrotor needs at least 200hz of control frequency to easily stay in one spot, and our ESCs is configured to work with 450hz of refresh rate. Any working PID controller I saw before for Quadrotors used at least 200-400hz of control frequency.
I asked similar question before from Ahmad Byagowi (one of the developers of ArduIMU ) and he answered:

The arduimu calculates the dcm matrices and that makes it so slow. If
  you disable the dcm output, you can get up to 100 hz gyro, acc and so
  on.

So, what will happen if I disable DCM from the firmware ? Is it really important ? We did a simulation before and our PID controller works pretty well without DCM.
","arduino, quadcopter, imu, pid"
How to efficiently do 3D mapping of an area on a MAV?,"I have been researching on a cost-effective way to scan an area on a MAV (exploraton) and later use it for CAD/civil purposes(use the point cloud data for CAD) but the major sensors available have their own problems.
kinect - can't use outside,high computation power
stereo - high computation power,somewhat expensive
lidar - very expensive + not real time + heavy
I need a system(on the MAV/quadrotor) that can work over wifi/wireless, can scan outdoors , not very expensive and that gives data real-time.Please suggest a system that can be as close to the above requirements.
Also can stereo be operated over wifi? 
","kinect, mapping, stereo-vision, 3d-reconstruction"
STM32F3 timers & computing,"I have an STM32F3 discovery board. I want to go to the next step and I want to try to use timers in a few configurations. 
How can I calculate variables (such as prescaler, period)? I looked in all datasheets, manuals and didn't find anything that can describe these values as  - Input capture mode, OP, PWM, etc. 
I think that prescaler is for downgrading a frequency from 1-65575. 
So if I have fcpu=72MHz and want to generate a signal of frequency=40kHz, am I supposed to do: 72MHz/40kHz=1800? 
Now should I subtract this prescaler with -1?
",microcontroller
LT1157 Logic Level Question,"I plan to use the LT1157 in my application PCB to act as a switch control from a micro controller side to control the On/Off state of 2 module boards which will be connected in the PCB. 

1st Load is 5V 1A.
2nd Load is 3.3V 500mA.

The LT1157 will get a 5V input at the Vs terminal. 
Does anyone know how much voltage is required to be used at the IN1 and IN2 pins? The datasheet doesn't say how much voltage can be used here. I am guessing it will be 5V, but can it do logic level with 3.3V? My microcontroller board gives an output of 3.3V and not 5V so I'll have to make a logic Level converter before feeding the pins IN1 and IN2 if it's not 3.3V tolerant. 
Please confirm, if anyone has used this IC before. 
",circuit
The IDE using for programming the Atlas robots,"ATLAS Gets an Upgrade - the new video of the Atlas robot is out so I'm curious about the IDE with which they are coding this thing.
",dynamics
Can ESC be programmed to run full throttle only on one side of a quadcopter?,"Can ESC in quads be programmed in such a way that only one side has throttle and no throttle at all on the other? This would cause the quad to flip I suppose? 
With that, is there a way we can program the controller to like trigger a switch when we want the quad to flip? Because I was thinking of doing a waterproof quad. So initially, it flies in the air normally with the 4 channel, and then I set it to float on water. After that, I was thinking of maybe triggering a switch on the controller so that this time it's just going to flip and nothing else. After it flips, I would trigger the switch back to normal operation. Is that possible?
",quadcopter
what are methods to compare PID controller performance?,"If there are input and the sensor measured outputs. What are the objective methods to compare performance besides looking at inputs and outputs matching or not?
",pid
"Raspberry Pi Hexapod 18DOF, Best servo control board?","Recently I've bought a hexapod kit and 18 TowerPro MG995 servos.
My objective is to apply also the Pi camera, sensors and perhaps a claw...
So I've been researching and I haven't found a clear answer when comes to the servo control board.
Which servo controller board shall I choose to complete my project?
","mobile-robot, raspberry-pi, servomotor, rcservo, hexapod"
Virtual model in PLC discrete / continuous,"How does one implement virtual model (continuous) while control system itself is discrete (PLC)?
I've done this in practice but what about theory, how does one explain this topic to a stranger? (lets say myself)
",control
POMDPs in robotics,"POMDPs are used when we cannot observe all the states.
However, I cannot figure out when these POMDPs can be useful in robotics. What is a good example of the use of POMDPs? (I have read one paper where they used them, but I didn't find it obvious why pomdps should be used) What would be good projects ideas based on POMDPs?
","algorithm, artificial-intelligence"
"What is the achievable stiffness of a impedance/admittance controlled robot (incl. haptic devices), given its structural and control stiffnesses?","EDIT: I realised I missed the point of the paper completely (thanks to very-skim reading ;) ). So, this part of it I'm relating to is about how much damping - not how much stiffness - should we display to obtain stability, given a structural stiffness. I changed the question accordingly - what is achievable stiffness of a impedance/admittance controlled robot, given its structural and control stiffnesses? (Stiffness/compliance is, of course, mathematically just one of the terms in total impedance/admittance)
Let us consider a haptic device with mechanical and control parts, and mechanical part is not infinitely rigid (compliant). Basically, it would be a robot with impedance or admittance control. I thought perceivable stiffness can be just as simple as serial connection of two stiffnesses - and so the stiffer mechanical structure is, the better it can display control stiffness:
$k = \frac{k_e k_c}{k_e + k_c}$
where $k_c$ is stiffness control. Still, I cannot find any confirmation to this, although something very similar is stated in Samur's ""Performance Metrics for Haptic Interfaces"". I would be very grateful if you could refer me to some sources or just plain prove it wrong or right (:
In a paper (here, p. 728) I only found stability condition for virtual damping value in relation to virtual stiffness, given structural stiffness.
","control, mechanism, reference-request"
DH-Parameters for Forward Kinematics for Translatory Motion only,"I am fairly new to the DH-transformation and I have difficulties to understand how it works. Why are not all coordinates (X+Y+Z) incorporated into the parameters? It seems to me that at least one information is useless/goes to the trash, since there is only a, d (translatory information) and alpha, theta(rotatory information). 
Example: 
The transition between two coordinate systems with identical orientation(alpha=0, theta=0) but with different coordinates(x1!=x2, y1!=y2, z1!=z2). 
DH only makes use of a maximum of two of these information.
Please enlighten me! 
Greetings
:EDIT: 
To clarify which part of the DH-Transform I don't understand, here is an example. 
Imagine a CNC-Mill(COS1) on a stand(COS0) without any variable length(=no motion) between COS0-COS1. For some reason I need to incorporate the transformation from COS0-COS1(=T0-1) into the forward transformation of my CNC-Mill. 

DH-Parameters for T0-1 would be a=5mm, alpha=90°, d=2mm and theta=90°. Assuming this is correct, the dX=10mm information is lost during this process?
If I recreate the relation between COS0 and COS1 according to the DH-Parameters, I end up like this: 

As far as I understand, on non parallel axis the information is not lost because the measurement of a/d would be diagonal, therefore include either dX/dY, dX/dZ or dY/dZ(pythagorean theorem) in one parameter. 
Where is the flaw in my logic?
","forward-kinematics, dh-parameters"
Homogenous Transformation Matrix for DH parameters,"I'm studying Introduction to robotic and found there is different equations to determine the position and orientation for the end effector of a robot using DH parameters transformation matrix, they are :



Example: Puma 560, All joints are revolute
Forward Kinematics:
Given :The manipulator geometrical parameters.
Specify: The position and orientation of manipulator.
Solution:

For Step 4:

for step 3 :Here I'm confused 
Here we should calculate the transformation matrix  for each link  and then multiply them to get the position and orientation for the end effector.
I've seen different articles using one of these equations when they get to this step for the same robot(puma 560)
What is the difference between them? Will the result be different? Which one should I use when calculating the position and orientation?
",dh-parameters
Extend robotic arm with wrist rotation,"I got an OWI Robotic arm, but was slightly disappointed at it having only horizontal position for gripper. What would be the easiest way to extend with gripper/wrist rotation, i.e. 6th degree of freedom? 

",robotic-arm
How to localise a underwater robot?,"I am building an autonomous underwater robot. It will be used in swimming pools. It should be capable of running in any normal sized pool, not just the pool in which I test. So I cannot rely on a particular design or feature. It has to know it's position in the pool, either with respect to the initial position or with respect to the pool. I have a IMU, which is a Pololu MiniIMU but finding the displacement with an IMU is a near impossible task. 
What sensor can I use for this task? It should not be very expensive. (below 200$)
Tank size: 25x20x2.5 meters
","sensors, localization, sensor-fusion, underwater"
How frequently should a PID controller update?,"I am developing a quadcopter platform on which will be extended over the next year. The project can be found on Github. Currently, we are using an Arduino Uno R3 as the flight management module.
At present, I am tuning the PID loops. The PID function is implemented as:
int16_t pid_roll(int16_t roll)
{
    static int16_t roll_old = 0;
    int16_t result = 
    (KP_ROLL * roll) + 
    (KI_ROLL * (roll_old + roll)) +
    (KD_ROLL * (roll - roll_old))
    ;
    roll_old += roll;
    result = constrain(result, PID_MIN_ROLL, PID_MAX_ROLL);
    return -result;
}

I am having trouble interpreting the system response on varying the constants. I believe the problem is related to the questions below.

How frequently should a PID controller update the motor values? Currently, my update time is about 100-110 milliseconds. 
What should be the maximum change that a PID update should make on the motor thrusts? Currently, my maximum limit is about +-15% of the thrust range.
At what thrust range or values, should the tuning be performed? Minimum, lift off, or mid-range or is it irrelevant?

","quadcopter, pid"
Troubleshooting Xbox Kinect 360,"I recently got libfreenect running on my mac and was able to test out freenect-glpclview which uses some of the 3D capabilities of the depth sensor.
I noticed that the Kinect would only respond / pick up movement that happened within a range of about 3-6 inches in front of the sensor.
I thought this may be because the lights where on so I turned them off. It seemed to get a little better but it still only ""works"" if something is block the sensor almost completely.
Does anyone know if this is something that can be solved? I know it's an old sensor but I got it for $20 so I could do some prototyping with it.
Notes:

laser project is ON
light starts out blinking then goes solid green
when not level light goes red
RGB camera works but is a little choppy and sometimes shows tears in the picture.

freenect-glcplview output (snippet):
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 70] Expected 1748 data bytes, but got 948
[Stream 70] Expected max 1748 data bytes, but got 1908. Dropping...
[Stream 70] Expected max 1748 data bytes, but got 1908. Dropping...
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948

freenect-regview output (snippet)
[Stream 70] Invalid magic 2dc5
[Stream 70] Invalid magic aaf5
[Stream 70] Invalid magic dddb
[Stream 70] Invalid magic 9272
[Stream 70] Invalid magic 9873
[Stream 70] Invalid magic 9b8b
[Stream 70] Invalid magic 59eb
[Stream 70] Invalid magic 88f1
[Stream 70] Invalid magic 75ee
[Stream 70] Invalid magic ffff
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Lost 1 packets
[Stream 80] Lost 15244 total packets in 514 frames (29.657587 lppf)
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Expected 1908 data bytes, but got 948
[Stream 80] Invalid magic 3b46
[Stream 80] Lost 1 packets

Found this which gives me the idea that this may be a USB issue: Regular receipt of undersized packet.
",kinect
Converting a 2D array of bits to a connectivity map (Code Debugging),"I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects (Boxes in which it has to pot the balls)  in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation.
Up till now, I have converted/read the bitmap image of the maze into a 2D array of bits. Right now I am writing a code that should convert the 2D array (that represents the maze) into a connectivity map so that I could apply a path planning algorithm on it. Mr. @Chuck has helped me by providing a code in MATLAB. i have converted that code into C++, however the code isn't providing the right output. Kindly see the code and tell me what I am doing wrong.
I am sharing the link to the 2D array that has been made, the MATLAB code, and my code in C++ to convert the array into a connectivity map.
Link to the 2D array:-
https://drive.google.com/file/d/0BwUKS98DxycUZDZwTVYzY0lueFU/view?usp=sharing
MATLAB CODE:-
Map = load(map.mat);
nRows = size(Map,1);
nCols = size(Map,2);
mapSize = size(Map);
N = numel(Map);
Digraph = zeros(N, N);

for i = 1:nRows
  for j = 1:nCols
    currentPos = sub2ind(mapSize,i,j);
    % left neighbor, if it exists
    if (j-1)> 0
      destPos = sub2ind (mapSize,i,j-1);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % right neighbor, if it exists
    if (j+1)<=nCols
      destPos = sub2ind (mapSize,i,j+1);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % top neighbor, if it exists
    if (i-1)> 0
      destPos = sub2ind (mapSize,i-1,j);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
    % bottom neighbor, if it exists
    if (i+1)<=nRows
      destPos = sub2ind (mapSize,i+1,j);
      Digraph(currentPos,destPos) = Map(currentPos)*Map(destPos);
    end
  end
end

Code in C++:-
int **digraph = NULL;
digraph = new int *[6144];

for (int i = 0; i < 6144; i++)
{
    digraph[i] = new int[6144];
}

for (j = 0; j < 96; j++)
{
    for (z = 0; z < 64; z++)
    {
        currentPos = sub2ind[j][z];
        digraph[currentPos][currentPos] = 0; //------NEW ADDITION-----------

    if ((z - 1) >= 0)
        {
            destPos = sub2ind[j][z - 1];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j][z - 1];
        }

    if ((z + 1) < 64)
        {
            destPos = sub2ind[j][z + 1];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j][z + 1];
        }

    if ((j - 1) >= 0)
        {
            destPos = sub2ind[j - 1][z];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j - 1][z];
        }

    if ((j + 1) < 96)
        {
            destPos = sub2ind[j + 1][z];
            digraph[currentPos][destPos] = bitarray[j][z] * bitarray[j + 1][z];
        }
    }

}

ofstream connectivityMap;
connectivityMap.open(""diGraph.txt"");

for (int l = 0; j < 100; l++) // printing only 100 elements
{
    for (int k = 0; k < 100; k++)
    {
        connectivityMap << digraph[l][k] << "" "";
    }
}

","mobile-robot, localization, mapping, planning"
Is Lego Mindstorm a good start?,"I would like to start experimenting with Robots. Is Lego Mindstorm a good start? Should I consider other platforms?
",platform
What are the frequencies used for within drones?,"What are these frequencies used for within the drone technology, and why these values?

35 MHz
433 MHz
868 MHz
2.4 GHz
5.8 GHz

","quadcopter, wireless, radio-control"
How to transform x y z coordinates to Tx Ty Tz?,"I need to get coordinates of the specific points from 2D CAD file and transform them so that I could use them to move the robotic arm to those points. The problem is that I only get x y z coordinates and the robotic arm needs x y z Tx Ty Tz coordinates to move to the certain position. 
Any suggestions?
Edited:
My task: I need robotic arm to go through certain points on PCB board and heat soldering paste. I could do it manually by setting points with pendant. But a much easier way would be to get coordinates of those points from CAD file and write a code using PC.
MOVL MotionSpeedType(0 - linear mm/s, 1 - angular °/s) Speed (0.1 - 1000 mm/s or Max angular 

speed) coordinate X Y Z Tx Ty Tz 

ToolNo [Type] (move robot in a cartesian coordinates in linear motion)

this is how code for linear motion to a certain point looks like
I only could find this manual.
This is pendant manual maybe it will be helpful.
I am second year student in ""Robotics and mechatronics"". I'm currently in a internship at the scientific research institution. I really appreciate your help!
",robotic-arm
Graph optimization with G2O,"I'm trying to do graph optimization with G2O, mainly in order to perform loop closure. However finding minimal working examples online is an issue (I've found this project, as well as this one. The second one though has the form of a library, so one cannot really see how the author uses things.)
In contrast to online loop closure, where people update and optimize a graph every time they detect a loop, I'm doing graph optimization only once, after pairwise incremental registration. So in my case, pairwise registration and global, graph-based optimization are two separate stages, where the result of the first is the input for the second.
I already have a working solution, but the way that works for me is quite different from the usual use of g2o:

As nodes I have identity matrices (i.e. I consider that my pointclouds are already transformed with the poses of the pairwise reg. step) and 
as edges, I use the relative transformation based on the keypoints of
the pointclouds (also the keypoints are transformed). So in this case
I penalize deviations of the relative pose from the identity matrix.
As Information matrix (inverse of covariance) I simply use a 6x6
identity matrix multiplied by the number of found correspondences
(like this case). 
The result of the graph is an update matrix,
i.e. I have to multiply with this the camera poses. 

Although this works in many/most cases, it is a quite unusual approach, while one cannot draw the graph for debugging (all nodes are identities in the beginning, and the result after optimization is a 3d path), which means that if something goes wrong getting an intuition about this is not always easy.
  
So I'm trying to follow the classic approach:

The vertices/nodes are the poses of the pairwise registration
The edges are the relative transformations based on the keypoints/features of the raw pointclouds (i.e. in the camera frame, not transformed by the poses of the pairwise registration)
The output are the new poses, i.e. one simply replaces the old poses with the new ones
Drawing the graph in this case makes sense. For example in case of scanning an object with a turntable, the camera poses form a circle in 3d space.
I'm trying to form all the edges and then optimize only at one stage (this doesn't mean only 1 LM iteration though).

However I cannot make things running nicely with the 2nd approach.
I've experimented a lot with the direction of the edges and the relative transformation that is used as measurement in the edges, everything looks as expected, but still no luck. For simplicity I still use the information matrix as mentioned above, it is a 6x6 identity matrix multiplied with the number of correspondences. In theory the information matrix is the inverse of covariance, but I don't really do this for simplicity (plus, following this way to compute the covariance is not very easy).
  
Are there any minimal working examples that I'm not aware of?
Is there something fundamentally wrong in what I describe above?
Are any rules of thumb (e.g. the first node in both approaches above is fixed) that I should follow and I might not be aware of them?
Update: More specific questions

The nodes hold the poses of the robot/camera. It is unclear though at which reference frame they are defined. If it is the world coordinate frame, is it defined according to the camera or according to the object, i.e. first acquired pointcloud? This would affect the accumulation of the pose matrices during incremental registration (before the g2o stage - I try to form and optimize the graph only once at the end, for all the frames/pointclouds).
The edge (Src->Tgt) constraints hold the relative transformation from pointcloudSrc to pointcloudTgt. Is it just the transformation based on the features of the two in the local coordinate frame of pointcloudSrc? Is there and tricky point regarding the direction, or just consistency with the relative transformation is enough?
The first node is always fixed. Does the fixed node affect the direction of the edge that departs/ends_up from/at the fixed node?
Is there any other tricky point that could hinter implementation?
I'm working in millimeter instead of meter units, I'm not sure if this will affect the solvers of g2o in any way. (I wouldn't expect so, but a naive use of g2o that was giving some usable results was influenced)

",slam
Sensors' field of view in car driving,"I want to develop an autonomous driving RC car. For detecting obstacles, I plan to mount 3-5 ultrasonic sensors in the front and in the back the car. What is the  minimum necessary combined field of view of the sensors so the car never hits an obstacle? I.e. what is the minimum angle of detection of the combined sensors the car should have to detect any obstacle in its path?
Some data about the car: (I don't know whether all the data is relevant)

Separation between right and left wheel : 19,5 cm
Wheelbase (distance between the front and the back wheels):  31,3cm
Steering axle: front.
Maximum angle of steering: around 30 degrees. The car uses Ackermann steering

","mobile-robot, sensors, wheeled-robot"
"Tried Normal Distributions Transform with my own files (in correct PCD format) and it throws errors, why?","http://pointclouds.org/documentation/tutorials/normal_distributions_transform.php#normal-distributions-transform
I've used this program with the sample PCD's given and it came out correctly. This was confirmed by experienced users on here. Now I'm trying to use my own pcd's. I didn't want to bother changing the program so I just changed the names to room_scan1 and room_scan2. When I attempt to use them, I get this error:

Loaded 307200 data points from room_scan1.pcd Loaded 307200 data
  points from room_scan2.pcd Filtered cloud contains 1186 data points
  from room_scan2.pcd normal_distributions_transform:
  /build/buildd/pcl-1.7-1.7.1/kdtree/include/pcl/kdtree/impl/kdtree_flann.hpp:172:
  int pcl::KdTreeFLANN::radiusSearch(const PointT&, double,
  std::vector&, std::vector&, unsigned int) const [with PointT =
  pcl::PointXYZ, Dist = flann::L2_Simple]: Assertion
  `point_representation_->isValid (point) && ""Invalid (NaN, Inf) point
  coordinates given to radiusSearch!""' failed. Aborted (core dumped)

This is the program I compiled: http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_1:_Installing_and_testing#Testing_.28OpenNI_viewer.29
Before you suggest it, I will let you know I already changed all of the PointXYZRGBA designations to just PointXYZ. It threw the same error before and after doing this. The thing that confuses me is that I looked at my produced PCD files and they seem to be exactly the same as the samples given for NDT.
Mine:
2320 2e50 4344 2076 302e 3720 2d20 506f
696e 7420 436c 6f75 6420 4461 7461 2066
696c 6520 666f 726d 6174 0a56 4552 5349
4f4e 2030 2e37 0a46 4945 4c44 5320 7820
7920 7a0a 5349 5a45 2034 2034 2034 0a54
5950 4520 4620 4620 460a 434f 554e 5420
3120 3120 310a 5749 4454 4820 3634 300a
4845 4947 4854 2034 3830 0a56 4945 5750
4f49 4e54 2030 2030 2030 2031 2030 2030
2030 0a50 4f49 4e54 5320 3330 3732 3030
0a44 4154 4120 6269 6e61 7279 0a00 00c0
7f00 00c0 7f00 00c0 7f00 00c0 7f00 00c0

Sample from NDT page:
2320 2e50 4344 2076 302e 3720 2d20 506f
696e 7420 436c 6f75 6420 4461 7461 2066
696c 6520 666f 726d 6174 0a56 4552 5349
4f4e 2030 2e37 0a46 4945 4c44 5320 7820
7920 7a0a 5349 5a45 2034 2034 2034 0a54
5950 4520 4620 4620 460a 434f 554e 5420
3120 3120 310a 5749 4454 4820 3131 3235
3836 0a48 4549 4748 5420 310a 5649 4557
504f 494e 5420 3020 3020 3020 3120 3020
3020 300a 504f 494e 5453 2031 3132 3538

Does anyone have any ideas?
","kinect, computer-vision, openni"
Inverting a transform (Reading J Craig's book on Robotics),"From Introduction to Robotics by J.J. Craig, chapter 2, Page no. 36:
Could anyone explain how that equation was derived/formed? I am stuck on this page due to failing to understand where the equation came from. Thank you.

","design, theory, books"
How to determine the trajectory reference on the real robot trajectory tracking,"I know that we can use some algorithms like LQR, MPC, or even PID to make the robot follows the trajectory references. In the simulation like MATLAB, I usually specify the trajectory reference by a function. Let say, given a sequence of points generated by a path planning algorithm, then I want to do a real experiment of trajectory tracking over those sequence of points. My question is:
- How to specify the errors towards the path in real situation. My impression is the generated path by path planning algorithm is uncertain due to the error of the robot sensing. And unlike the line following robot which has a real physical line for the reference, the generated path from path planning is virtual, e.g. it does not exist in the real world. I am really confused about these matter.
","mobile-robot, control"
Digital Controller Design for System with variable sample time,"Basically I got system with a sensor and an output. I want to apply a digital implemented feedback controller. The problem in this setup is the sensor. The specifications of the module says that the sampletime of the sensor does change in wide range, depending on the usecase; from 1.3 second to 10 second. But it stays constant until the system is disabled.
My first approach was tuning a digital PID-Controller for the longest sampletime. This works fine. Even if I change the sampletime to the shortest the system stays stable, which was expected because I'm still in ROC.
The problem now is that the system's response is pretty slow.
If I design the controller for my fastest samplingrate the results are satisfying but become instable for the slowest samplerate, which can be explained again by the ROC
I could use some kind of adaptive predefined gains which I change depending on the samplerate but I was wondering if there are  control strategies which are able to handle the sampletime changes?
EDIT: 
To give a better overview I will add some details: 
I'm talking about a heating system which heats with radiation. As a sensor I use a pyrometer module with a samplingrate of up 1kHz. The problem is, that the pyrometer is not able to produce reasonable readings whenever the radiator is turned on. (Yes there are other alternatives to the pyrometer, but they start at $50k and are too expensive). The radiator has to be pulsed to operate it. So to maintain a decent heat up time and steady-state temperature the ""duty-cycle"" has to be at a decent rate(target is 95%). The minimum ""off-time"" of the radiator is 0.2 seconds before the measured values are reasonable. So at the end my sensor got an effective sampletime of 1-10seconds (by varying the duty cycle).
The hardware is hard too change, radiator and sensor have been evaluted for months right now. Therefore I try to improve the results by ""just"" changing the control algorithm.
",control
Need help regarding EKF in MonoSLAM,"I am trying to understand the implementation of Extended Kalman Filter for SLAM using a single, agile RGB camera. 
The vector describing the camera pose is 
$$
\begin{pmatrix}
r^W \\
q^W  \\
V^W \\
\omega^R \\
a^W \\
\alpha^R
\end{pmatrix}
$$
where:

$r^W$ :   3D coordinates of camera w.r.t world
$q^W$ :   unit quaternion describing camera pose w.r.t world
$V^W$ :   linear velocity along three coordinate frames, w.r.t world
$\omega$ :   angular velocity w.r.t body frame of camera

The feature vector set is described as 
$$
\begin{pmatrix}
y_1 \\
y_2  \\
\vdots \\
y_n
\end{pmatrix}
$$
where, each feature point is described using XYZ parameters.
For the EKF acting under an unknown linear and angular acceleration $[A^W,\psi^R] $ , the process model used for predicting the next state is:
$$
\begin{pmatrix}
r^W + V^W\Delta t + \frac{1}{2}\bigl(a^W + A^W\bigr)\Delta t^2 \\
q^W \bigotimes q^W\bigl(\omega^R\Delta t + \frac{1}{2}\bigl(\alpha^R + \psi^R\bigr)\Delta t^2\bigr)   \\
V^W + \bigl(a^W + A^W\bigr)\Delta t\\
\omega^R + \bigl(\alpha^R + \psi^R\bigr)\Delta t \\
a^W + A^W \\
\alpha^R + \psi^R
\end{pmatrix}
$$

So far, I'm clear with the EKF steps. Post this prediction step, I'm not clear how to perform the measurement update of the system state.
From this slide, I was under the impression that we need to initialize random depth particles between 0.5m to 5m from the camera. But, at this point, both the camera pose and the feature depth is unknown.

I can understand running a particle filter for estimating feature
depth if camera pose is known. I tried to implement such a concept in this project: where I read the camera pose from a ground truth file and keep triangulating the depth of features w.r.t world reference frame
I can also comprehend running a particle filter for estimating the
camera pose if feature depths are known.

But both these parameters are unknown. How do I perform the measurement update?
I can understand narrowing down the active search region for feature matching based on the predicted next state of the camera. But after the features are matched using RANSAC (or any other algorithm), how do I find the updated camera pose? We are not estimating homography, are we?
If you have any idea regarding MonoSLAM (or RGB-D SLAM), please help me out with understanding the EKF steps.

To be more specific: is there a homography estimation step in the algorithm? how do we project the epipolar line (inverse depth OR XYZ) in the next frame if we do not have any estimate of the camera motion?
","slam, ekf"
Adding an Actuator or Force to a (Featherstone) Articulated Rigid Body Model,"I'm working on a project where I need to model a system that is essentially comprised of a series of ball-and-socket joints attached to a base, which is attached in turn to a prismatic joint (rail). 
I've read Roy Featherstone's Rigid Body Dynamics Algorithms cover-to-cover, and I've also read the Dynamics section from the Springer Handbook of Robotics (also written by Featherstone). 
It took me a long time to get acclimated to using his ""spatial vector"" and ""spatial matrix"" notation, but after re-creating all of his notation by hand as an exercise it works out to just be a nice way of concatenating 3x3 and 3x1 matrices and vectors into 6x6 and 6x1 matrices and vectors. The maths he invents to perform operations can be a bit tedious to read as he hijacks some standard notation, but overall everything is very compact, very easy to implement in MATLAB. 
My problem is this: How do I add actuators to the model? He walks through explicitly configuring the joint definitions, link definitions, etc., but when it comes to actuators or applied forces he says something like, ""Just add a $\tau_a$ here and Bob's your uncle!"" - it's not discussed at all. In the Handbook of Robotics he suggests introducing a false acceleration to the fixed base to add the gravitational force term, but doesn't show how to add it in local coordinates nor does he mention how to add the actuator input. 
Any help would be greatly appreciated. I've considered starting over with a different book, but it's going to be a great expense of my time to re-acclimate myself to a different set of notation. I'd like to move forward with this, but I feel like I'm just a few inches shy of the finish line. 
","actuator, dynamics, joint"
Calculate required motor torque through Harmonic Drive,"I have a term project which is controlling a two-link manipulator with harmonic drive installed at each joint.
To control, i used Computed control method to determine the torque needed for each joints based on the formula: 
 $$\tau_i =M(\theta)(\ddot{\theta_i}+K_d\dot{e}+K_pe)+V+G  $$
To calculate the torque that each motor needs to produce through harmonic drive, i use: 
$$\tau_{motor} =(J_m+J_g)\rho\ddot{\theta_i}+\frac{\tau_i}{\rho\eta_g}$$
where:
 $\rho$ and $\eta_g$ are gear ratio and efficiency of the harmonic drive. $J_m$ and $J_g$ are the motor and gear inertia, respectively. 
after these calculation, i can see the effect of harmonic drive in the system by comparing input torque from motor in the model with harmonic drive ($\tau_{motor}$) to that torque in the model without harmonic drive ($\tau_i$) 
But my professor doesn't agree the formula $\tau_{motor}$ i used. He want me to include the stiffness $k$ of the harmonic drive.
This is what i have done
P/S: This model which consists of two-link manipulator+harmonic drive at each joint is built in MATLAB.  
Can anyone suggest me the formula about it? 
Thank you so much.
","actuator, manipulator"
Mini Recorder from RC Heli Parts,"I had a RC Helicopter (with video,picture, and audio taking capabilities) that recently ""died"" (unrelated to short circuit). The reciever board short circuted, but the board that sent data to micro-sd card and had camera+mic was fine. I can access the data on the micro-sd card through the circuit, with a USB cable. The reciever board sent data via a 4 wire bundle to the camera board to make it take pictures/record audio. Is there any way to still do this from my computer (from the USB), and turn it into a mini spy camera? (Not remotely, jst through a cable)
I got this heli a while back so I don't have the heli number but the camera board number is TX6473 R1, and the reciever board number is 3319B rev.a
Reciever Board Image

Camera/Data Board Image

","control, cameras, circuit"
How to transfer signed integers with libusb?,"Folks at programmers stack exchange asked me ask here:
I want to communicate with an arduino and sent integers it. I code this program in C++. I initialy used bulk_transfer(), but it sends only char data.
This in the API reference for libusb:
http://libusb.org/static/api-1.0/group__syncio.html
Here is the prototype of bulk_transfer()
int     libusb_bulk_transfer (struct libusb_device_handle *dev_handle, unsigned char endpoint, unsigned char *data, int length, int *transferred, unsigned int timeout)
As you can see, data is an unsigned char pointer, that is, a pointer to a buffer containing length unsigned chars. I can successfully transcieve strings. How do I transfer integers with sign?
Currently I am thinking about a system in which the arduino asks for the digit by sending a character and my program sends the number as reply followed by the sign, which is requested next. Is this solution viable? Or should I transfer the integer as a string? Is there a better way?
","arduino, communication, usb, c++"
Structuring EKF to estimate pose and velocity with odometry inputs,"I have a differential drive robot for which I'm building an EKF localization system.  I would like to be able to estimate the state of the robot $\left[ x, y, \theta, v, \omega \right]$ where $x, y, \theta$ represent the pose of the robot in global coordinates, and $v, \omega$ are the translational and rotational velocities. Every mobile robot Kalman filter example I've seen uses these velocities as inputs to prediction phase, and does not provide a filtered estimate of them. 
Q: What is the best way to structure a filter so that I can estimate my velocities and use my measured odometry, gyroscope, and possibly accelerometers (adding $\dot{v}$ and $\dot{\omega}$ to my state) as inputs? 
My intuition tells me to use a prediction step that is pure feedforward (i.e. just integrates the predicted velocities into the positions), and then have separate updates for odometry, gyro, and accelerometer, but I have never seen anyone do this before. Does this seem like a reasonable approach?
","localization, kalman-filter, gyroscope, odometry"
Starter Looking For Advice,"I'm no professional. At 29 I just became seriously interested in robotics a few months ago and have been researching everything I can since. Now that I've come to understand how far robotics have truly come I have a desire to try to make my own.
Granted, I know nothing about coding or programming. I have no idea where to begin. And I know it'll probably, the first time at least, be something small rather than a huge life altering project.
Thus, if anyone could suggest to me good resources for a beginner I'd massively appreciate it.
",beginner
Which middleware for IPC and multi-threading in a autonomous robot?,"Aim: To use multi-threading and inter-process communication(IPC) when coding an autonomous robot.
Platform: Embedded Linux (Yocto)
Constraints : Limited CPU power.
We are building an Autonomous Underwater Vehicle, to compete in the RoboSub competition. This is the first time I am doing something like this. I intent to use a middleware like ROS, MIRA, YART, MOOS etc. The purpose of using one is that I want to modularise tasks, and divide the core components into subsystems, which should be run parallel(by multi-threading). But I have limited computational power (a dual core omap SoC), and the middleware, while robust should also be very efficient.
I need to use a middleware, because I don't want the program to be run on a single thread. My CPU has two cores, and it would be great if I could do some multi-threading to improve performance of the program. The middleware will provide for me the communication layer, so I don't have to worry about data races, or other problems associated with parallel processing. Also I have no prior experience writing multi-threaded programs, and so using parallel processing libraries directly would be difficult. Hence IMO, middlewares are excellent choices.
In your experience, which is the best one suited for the task. I don't really want to use ROS, because it will be having a lot of features, and I wont be using them. I am a computer science student(under graduate freshman, actually) and don't mind getting my hands dirty with one which has not that much features. That's true if only it will take less toll on the CPU.
","ros, communication, underwater, operating-systems"
Pull-down resistor for inter-chip and sensor-to-chip communication,"I understand the concept of using a pull-up/pull-down resistor when implementing a button/switch with Arduino to avoid a floating state, and in fact I have implemented this quite often.
But I am not too sure if a pull-down resistor is necessary in chip-chip or chip-sensor communication.
I am connecting a coin acceptor to the Arduino (common ground). The coin acceptor's output pin gives a short pulse each time there is a coin inserted. So far I am connecting the output pin of the coin acceptor directly to an Arduino pin and it works without any problem. Is a pull-down resistor (on this line) usually required as precaution in this case?
Also I have the same question when connecting 2 pins of 2 separate Arduino's (also common ground) so that one Arduino can read pulses from the other.
Thanks in advance for any experience shared!
Dave
",arduino
How to make a directed graph?,"I'm working on an robot that would be able to navigate through a maze, avoid obstacles and identify some of the objects in it. I have a monochromatic bitmap of the maze, that is supposed to be used in the robot navigation. 
Up till now, I have converted/read the bitmap image of the maze into a 2D array of bits. However, now I need guidance on how to use that array to plan the path for the robot. I would appreciate if you could share any links as well, because I am new to all this stuff (I am just a 1st year BS electrical engineering student) and would be happy to have a more detailed explanation.
If you need me to elaborate on anything kindly say so.
I would be grateful!
Here's the image of the maze.

This is just a sample image; the robot should be able to work with any maze (image) with similar dimensions. And you are welcome!
Thank you Chuck!
UPDATE
Heres the code for sub2ind in c++. Kindly see if the output is correct:-
ofstream subtoind;
subtoind.open(""sub2ind.txt"");

int sub2ind[96][64] = { 0 };
int ind2subROW[6144] = { 0 };
int ind2subCOL[6144] = { 0 };
int linearIndex=0;
j = 0;
z = 0;

for (j = 1; j <= 64; j++)
    {
        for (z = 1; z <= 96; z++)
            {
                    linearIndex = z + (j - 1) * 96;
                    sub2ind[z-1][j-1] = linearIndex-1;
                    //ind2subROW[linearIndex-1] = j-1;
                    //ind2subCOL[linearIndex-1] = z-1;
            }
    }
for (j = 0; j < 96; j++)
{
    subtoind << endl; //correction
    cout << endl;

    for (z = 0; z < 64; z++)
    {
        subtoind << sub2ind[j][z] << "" "";
    }
}
subtoind.close();

Heres the Link to the output file.
https://drive.google.com/file/d/0BwUKS98DxycUSk5Fbnk1dDJnQ00/view?usp=sharing
","arduino, mobile-robot, localization, mapping, planning"
Lagging sensor data for PID,"Let's say a PID is implemented and the errors are calculated using the sensor data, but the sensor data lags by certain amount of time because of the overhead. And the lag time is smaller than the sampling period, how well does PID performs? What I am thinking is that PID will calculate errors based on past data, and use that to control. How will using a Kalman filter to estimate the actual sensor data help? 
","pid, kalman-filter"
Jacobian for point on robotic arm,"currently i am programming for a robotic simulation. I have a Endeffector which aproaches a target, on the way to the target is an Obstacle. Now i redirect my Endeffector, so that it does not hit the target.
When i want to do the same for the whole arm i want to push the arm away from the Obstacle as well. Now i have it working so far that i can redirect the arm. But my calculation for the Jacobian seems to be faulty.
For my setup, and what i need for that.
I have a robotic arm, 7DOF. Let $x_0$ be the closest point on the arm to the obstacle. And $J_0$ the corresponding Jacobian.
Also i have given the following term:
$\dot{x_0} = J_0 * \dot{\theta}$ 
$\theta$ are my joint angles. I can calculate the Jacobian for the EndEffector, but do not know on how to calculate it for a point ob the arm.
Does anybody have an Idea on how to calculate the corresponding Jacobian.
Cheers
","robotic-arm, jacobian"
Capacitive touch input robot to remote access iPad,"I'd like to buy a capacitive touch input robot in order to remote access my iPad but I'm having trouble describing a correct kind of robot. 
I would like to keep lag down to an additional 60ms so that it is still a high quality interface. 
I would like to have a robotic arm equipped with a capacitive pen that moves to places on the ipad screen based on the mouse or I'd like a array of capacitive pens that emulate the touch of a user. 
I guess I'd use Squires software reflect and the mirror function but I'm open to using an SHD camera with the robotic arm and a pixel sensor array with the array of capacitive pens. 
Does this make sense? How could I improve the design? What materials would I need to build it myself. Assuming ready built arm? How could I build an array of capacitive touch micro pens?
",robotic-arm
On-board monocular odometry for quadcopter stabilization,"Has anyone done this with EKF/PID on a small microcontroller? Or know of code snippets to help implementing this?
","quadcopter, odometry, stability"
Highspeed with gearbox or low speed for brushless motor?,"I'm attempting to control a small vehicle at relatively slow (.5 m/s - 1 m/s) speeds, but with extreme accuracy (1mm). For the drive system, I'm considering using brushless motors as they have a much greater power / volume ratio than I am able to find with brushed motors, especially at this small size.
I will be using wheels between 1"" and 2"" diameter, so the RPM I will be looking for is between 150 - 500 RPM at max. This would suggest either driving the motors at a low speed directly, or driving them at a high speed and gearing them down. As I understand it, both setups will give high torques, as brushless motors decrease torque with speed. With brushed motors, it's quite obvious that a gearbox is necessary as otherwise there is no torque in the system, but here the choice isn't as clear, which is why I am asking.
tl;dr Use brushless motors at high speed with gearbox or low speed (ungeared) for high torque / low speed / high precision application?
","motor, brushless-motor"
Cognitive Architectures: how do you perform qualitative and quantitative comparisons?,"I couldn't find a sub stackexchange for artificial intelligence, but I think robotics comes close, and so I'm posting here.
I recently saw TED talks on AI and the Google car, with these being the most interesting to me:

Hod Lipson - Building ""self-aware"" robots 
Juan Enriquez - The next species of human
Ray Kurzweil - Get ready for hybrid thinking

The third one led me to the 'criticism' section (labeled Analysis on that wiki article, though it certainly at least partially reads as a criticism section as well) of Kurzweil 'theory' of the brain, namely ""Pattern Recognition Theory of Mind"" (PRTM).  After some link surfing on the people who have performed analysis of PRTM and their respective academic contributions, I came to learn about Cognitive Architecture:

""A cognitive architecture can refer to a theory about the structure of
  the human mind. One of the main goals of a cognitive architecture is
  to summarize the various results of cognitive psychology in a
  comprehensive computer model. However, the results need to be in a
  formalized form so far that they can be the basis of a computer
  program. By combining the individual results are so for a
  comprehensive theory of cognition and the other a commercially usable
  model arise. Successful cognitive architectures include ACT-R
  (Adaptive Control of Thought, ACT), SOAR and OpenCog.""

It appears that there are several interesting architectures, including the 3 mentioned above.  I read a bit about ACT-R, SOAR, OpenCog, DUAL, CHREST, and CLARION.  The list is not comprehensive.  It also appears that there are two main types of such architectures: Connectionism and Symbolic.
Though I have many questions, my main question is this:
What are some quantitative metrics and qualitative properties to measure and compare between the two architecture types?
Other questions

Can all architectures be categorized as one, the other, or some
combination of the two, or is there a third, fourth, etc? 
How are two main types alike? How are they different?
What are some recommended further readings on this topic. 
What centres and organizations are leading development in this?
What are some of the computer programming languages, related skill-sets, and
cross-domain knowledge set utilized in R&D and product offerings of
such systems?

",artificial-intelligence
How to read data from i2c using i2cget?,"I'm new to embedded devices and am trying to understand how to use i2cget (or the entire I2C protocol really).
I'm using an accelerometer MMA8452, and the datasheet says the Slave Address is 0x1D (if my SAO=1, which I believe is referring to the I2C bus being on channel 1 on my raspberrypi v2).
From the command line, I enter
sudo i2cget -y 1 0X1d

It returns
0X00

I think that means I'm attached to the correct device.
So now, I'm trying to figure out how do I get actual data back from the accelerometer?
The i2c spec says
i2cget [-y] i2cbus chip-address [data-address [mode]]

So I have tried
sudo i2cget -y 1 0x1D 0x01

where 0x01 is the OUT_X_MSB. I'm not sure entirely what I'm expecting to get back, but I figured if I saw some data other than 0x00, I might be able to figure that out.
Am I using ic2get wrong? Is there a better way to learn and get data from i2c?
The datasheet for my accelerometer chip is at http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Accelerometers/MMA8452Q.pdf
","raspberry-pi, i2c"
Improving Velocity estimation,"I have a sensor reduction model which gives me a velocity estimate of a suspension system(velocity 1) .
This suspension system estimate velocity is used to calculate another velocity(velocity 2) via a transfer function/plant model.
Can I use velocity 2 to improve my velocity estimate (velocity 1) through Kalman filtering or through some feedback system.??

V1 is ""estimated"" using these two sensors.That is fed into a geroter pump (Fs in diagram) which pumps fluid to manupulate the damper viscous fluid thereby applying resistance to the forces applied to the car body. There is no problem did I have an velocity sensor on the spring.I could measure it accurately but now I only have an estimate. I am trying to make the estimate better.Assume I have a model/plant or transfer function already that gives me the V2 given a V1.
","control, sensors, pid, kalman-filter"
Circuit Design and Simulation,"I want to design some circuits of my own. My area of expertise is in Computer Science Engineering. I have listed out the components which are essential in the circuit. I want a software which can be used to design and simulate circuits for real time projects. Please suggest me the best among them. Thank you.
@AkhilRajagopal
","software, electronics"
Can I use Bipolar stepper motor driver to drive Unipolar motor in Unipolar configuration?,"Can I use Bipolar stepper motor driver to drive Unipolar motor in Unipolar configuration ?
","stepper-motor, stepper-driver"
Discover vector/angle between stereo camera pose and vehicle body,"I have a calibrated stereo camera system that is mounted in a passenger car which means I am able to retrieve a point cloud from my stereo image. However, I need to find how well is the camera aligned with the vehicle - read: if the camera is perfectly facing forwards or not. I guess it will never perfectly face forwards so I need to get the angle (or rather 3D vector) between ""perfect forwards"" and ""actual camera pose"".
What came to my mind is to drive the vehicle possibly perfectly forwards and use stereo visual odometry to detect the angle of vehicle movement as seen by camera (which is the vector I am looking for). The LIBVISO library for visual odometry can output a 3D vector of movement change from one stereo frame to another which could be used to detect the needed vector.
The only problem may be to actually be able to drive perfectly forward with a car. Maybe an RTK GPS could be used to check for this or for correction. Will anyone have a suggestion on how to proceed?
The stereo camera I use consists of 2 separate Point Grey USB cameras. Each camera is mounted on a windshield inside the car with a mount like this one. The cameras were calibrated after mounting. The stereo baseline (distance between the cameras) is about 50 cm.
","stereo-vision, odometry"
Difference between planetary and precision gear motors,"i'm working on a building a rover and would like some advice on selecting motors. In particular, i want to understand the difference between precision and planetary gear motors.  My robot will way about 10-15lbs i think and would like it to be responsive and quick. I have two sabertooth 2x12 motor controllers (which can supply up to 12amps). I have been looking at these motors and i am not sure which is better choice for my application.  
These are the two sets of motors i am thinking about.
https://www.servocity.com/html/precision_robotzone_gear_motor.html
https://www.servocity.com/html/3-12v_precision_planetary_gear.html
googling does provide some info on planetary gears, but the application of these two is still is unclear to me.
Thanks
","mobile-robot, motor, gearing"
Provides a 10 Degree-Of-Freedom IMU reduntant data?,"Basic question concerning sensor fusion:
A standard 10 DoF IMU, I mean this cheap things for the tinkerer at home, provides 10 values:
3 Accelerometers
3 Gyroscope
3 Magnetic Field Measurements
1 Pressure sensor (+ 1 Temperature) 
I know that the accel-data provide long term stability, but are useless for short term and the gyroscope is more or less vice versa. 
So there are tons of strategies to ""marry"" this values, but how does the magnetic field measurement fit into this framework?
Basically the magnetic field measurement should provide an attitude, too. Like the other two sensors combined. I guess this measurement alone is neither reliable.
So how do all these sensors fit together?
BR 
","imu, sensor-fusion"
Human arm inverse kinematics,"Hi I want to implement an human arm robot and a task such as moving a glass between two points  using Robotic Toolbox for Matlab  by Peter Coorke. I'm a student and I'm a newbie in this kind of things so I would find a good reference for solving the inverse kinematics of the human arm and  an algorithm that implements some kind of obstacle avoidance exploiting the redundancy of the manipulator (7dof) using null space motion.  Anyone can suggest me a good reference to follow in this implementation with the toolbox? Thanks
","robotic-arm, inverse-kinematics, manipulator, matlab"
Spring with electronically adjustable stiffness,"I would like to build a mechanical module that acts like a spring with electronically controllable stiffness (spring rate).
For instance, let's imagine a solid, metallic cube, 0.5 m each side. On the top side of the cube, there is a chair sitting on top of a solid mechanical spring. When you sit on the chair, it would go down proportionally to your weight, and inversely proportional to the spring's rate. 
What I want is that this spring's rate be electronically adjustable in real time, for instance a microcontroller system might increase the spring's rate when it detects a larger weight.
I'm using this example to best describe what I want to achieve because I'm not a robotics specialist and I don't know the inside terms.
Is there already an electro-mechanic module as the one I'm describing? (obviously nevermind the cube and the chair, it's the spring I'm interested in).
",actuator
Wiring necessary to route power from any one of several rechargeable batteries,"I'm looking for my robotics project to draw its power from one of 3 rechargeable batteries; basically whichever has the most ""juice"" in it. From the initial research I've already done, I believe I could connect each rechargeable battery (probably LiPo) to a diode, and then wire each of the 3 diodes in series.
However, being so new to robotics/electronics, I guess I wanted to bounce this off the community as a sanity check, or to see if there is a better way of achieving this. Again, what I am looking for is a way for the circuit to automagically detect that battery #1 has more power than battery #2, and so it ""decides"" to draw power from #1. The instant #1 is depleted or deemed ""less powerful"" than #2, the #2 battery takes over. Thoughts/criticisms?
","power, battery, wiring"
Understanding how solar panels can supply power to robotic circuits,"Say I have this solar panel that outputs 6V at 330mA, or ~1.98 Watts. If I connect that to Arduino, which expects a 5V supply at (roughly) 50mA, then the Arduino as a whole requires 5V * .05A = 0.25 Watts to power it. To me, if I understand this correctly, then in perfect weather/sunlight, the solar panel will power Arduino all day long, no problem.
Now let's say we wire up 4 motors to the Arduino, each of which draw 250 Watts. Now the Arduino + 4 motors are drawing ~1.25 Watts. But since the panels are still outputting 1.98 Watts, I would think that (again, under perfect sunlight) the panel would power the Arduino and motors all day long, no problem.
Now we add 4 more motors to the Arduino circuit, for a total of 8 motors. The circuit is now drawing 1.25 Watts + 1 W = 2.25 Watts. I would expect the solar panel to no longer be capable of powering the circuit, at least properly.
My first concern here is: am I understanding these 3 scenarios correctly? If not, where is my understanding going awry?
Assuming I'm more or less on track, my next question is: can solar panels be ""daisy chained"" together to increase total power output? In the third case above, is there a way to add a second solar panel into the mix, effectively making the two panels output 1.98 Watts * 2 = 3.96 Watts, which would then make them capable of powering the Arduino and its 8 motors (yet again, assuming perfect weather/sunlight conditions)?
","power, circuit"
Matlab: System simulation with dynamic state matrix / input matrix,"I have the following system:
$$\dot{x} = A(t)x+B(t)u$$
$$y = x$$
$A(t)$ and $B(t)$ are actually scalar, but time-dependent. If they would be constant, I could simulate the system in Matlab using:
sys = ss(A,B,C,0);
lsim(sys,u,t,x0);
However, it would be nice to simulate the system with dynamic state and input matrix. The matrices are based on measurement data, this means I would have for each discrete time step $t_i$ another matrix $A(t_i)$. Any suggestions how to do that?
","dynamics, matlab, simulation"
How to determine how long a battery will power a robotic circuit for?,"Obviously robotic circuits draw different amounts of power/current. So given the same battery, say, a 9V, then connecting it to 2 different circuits will deplete it at two different rates. Robot/Circuit #1 might drain the battery in 5 minutes. Robot/Circuit #2 might drain the battery in 20 minutes.
What ratings do batteries have that allows us to figure out how long it will power a circuit for? Bonus points: does this same rating uphold for solar panels and, in deed, all power supplies (not just batteries)?
","power, battery, circuit"
Is ROS hard real time safe?,"I know that is a question that has been asked too many times, but still its not clear to me. I read online that it isn't but some people say that they control their robots under ROS in applications with hard real time constraints. So, because I need some technical arguments (rather than a plain ""ros is not real time"") I will be more specific (suppose we have ROS under a RTOS):

I read that ROS uses a TCP/IP-based communication for ROS topics and I know that TCP/IP is not reliable. That means I cannot use topics in a real time loop? For instance send a control signal to my system publishing it to a topic, and the system sending me some feedback via a topic?
If I have a RTOS (eg Linux+Xenomai) can I build a real time control loop for a robot using ROS, or ROS will be a bottleneck?

Maybe the above are naive or I lack some knowledge, so please enlighten me!
Note: I define as a hard real time system (eg in 1KHz), the system that can guarantee that we will not miss a thing (if the control loop fails to run every 1ms the system fails).
","ros, real-time"
Detect human in proximity?,"I'm looking for ways to detect human presense behind walls in close proximity (around 10 feet) in whatever way possible! Problem is I can't code! (I hope it's ok I'm posting here.) 
I know there are different sensors but they all seem to be for detecting by motion of target humans. 
How do you detect still persons?
Is there a sound amplification device that magnifies human breathing x 20?
Or detect body heat?
Or pick up radiation waves or something off humans?
","sensors, sensor-fusion, ultrasonic-sensors"
Can I control more than 18 servo motor with a Raspberry Pi,"I'm trying to make an hexapod with 18 servo motors and i'm asking how to control them with a Raspberry Pi. (Never used it). I saw lot's of stuff to control 1, but 18, 20...
Currently I'm working on an Arduino Mega, and a SSC-32 board, but I found the result to slow and jerky.
At this end, I want to add a camera and processing the image, I know an Arduino can't handle that process but a Raspberry Pi can ?
Thank for all information about that subject :) 
","arduino, raspberry-pi, cameras, servomotor"
How do I accurately calculate the speed of a rotary encoder at a high sample rate?,"I'm aiming to control a motorized joint at a specific speed. To do this, I'm planning on attaching a rotary encoder to do this.
I'll be controlling the motor with a PID controller. With this PID controller, I need to control the joints based on their velocity.
Since:
speed = distance / time

It would make sense to do something like this:
double getCurrentSpeed() {
    return (currentAngle - lastAngle) / samplingRate;
}

However, there's an issue; the encoder doesn't provide a high enough resolution to accurately calculate the speed (the sample rate is too high). I want to have updated data every 5-15 ms (somewhere in that range as my current motors seem to be able to respond to a change in that range)
Some more information:

14 bit precision (roughly 0.0219726562 degrees per ""step"" of encoder
I'd like to be able to calculate as small of speed differences as possible
As the motors will be going fairly fast (120+ degrees/second at highly variable speeds and directions), so the feedback has to be accurate and not delayed at all

So, a couple of ideas:

I can find encoders that I can sample at a very high rate. I was thinking about sampling the time between the changes of the encoder's value. However, this seems finicky and likely to be noise-prone
I could do some sort of rolling average, but that would cause the data values to ""lag"" because the previous values would ""hold back"" the output of the calculations somewhat and this would play with my PID loop some
Noise filter of some sort, although I don't know if that would work given the rapidly changing values of this application

However, none of these seem ideal. Is my only option to get a 16 bit (or higher!) encoder? Or is there another method/combination of methods that I could use to get the data I need?
","motor, pid, algorithm"
How do safety cages around quadcopter rotors/blades affect lift capabilities?,"I am interested in building a quadcopter from scratch.
Because I like to err on the side of caution, I'm considering adding ""safety cages"" around each propeller/rotor, to hopefully prevent (at least minimize) the chance of the spinning rotor blades coming into contact with someone. Without knowing much about the physics behind how ""lift"" works, I would have to imagine that cages present two main problems for rotors:

They add weight to the copter making it harder to lift the same payload; and
They're sheer presence/surface area makes it harder for the spinning rotor to generate lift and push down away from the ground

The former problem should be obvious and self-evident. For the latter problem, what I mean by ""surface area"" is that I imagine that the more caging around a spinning rotor, the more difficult it will be to lift effectively. For instance, a spinning rotor might have the ability to generate enough power to lift, say, 2kg. But if we were to construct an entire box (not cage) around the entire rotors, with 6 sides and no openings, I would imagine its lift capability would drop to 0kg.
So obviously, what I'm interested in is a cage design that provides adequate safety but doesn't ""box in"" the rotor so much that it causes the rotor to be ineffective or incapable of providing lift. So I'm looking for that optimal tradeoff of safety (boxing/caging around the spinning rotor) and lift performance.
I would imagine calculating and designing this is a pretty huge undertaking with a lot of math behind it. I'm just wondering if anyone has already figured all this stuff out, or if anyone knows of a way to model this safety-vs-lift-performance trade off in some way.
",quadcopter
Stewart platform as robotic wrist joint,"I'm planning the design of a wrist for a humanoid robot. I would like to choose a design that is sturdy while allowing for dexterity comparable to a human wrist.
One option that was presented to me was to use a Stewart platform. This setup appears to correctly recreate all possible movements of the human hand. My immediate concern is that this platform will use a total of six actuators which will require additional power and computational requirements. I don't want to commit to this design until I am certain that there isn't a better alternative.
Is a Stewart platform a good choice for replicating the dexterousness of the human wrist? If not, what is a better solution?
","robotic-arm, design, actuator, joint, humanoid"
Compound vision system or Megapixel camera reduction,"Are any commercially available compound vision sensors available?
Not a simple 8 sensor system using photo-diodes but a genuine sensor that can provide a >32x32 compound matrix. Would some form of reduction in the granularity of a megapixel camera be a better option? The real purpose is to reduce processing time to a minimum, while extracting the maximum basic information. 
",computer-vision
Which math course will be most beneficial?,"Let me know if this should be on Academia instead, but I posted it here to get responses specifically from people active in robotics development.
I'm currently an undergraduate student completing majors in both mechanical engineering and computer science. I'm still fairly new to the field, but my interest is firmly in electronic and mechanical systems. Next year I can take one of the courses below:
1. Multivariable Calc.
2. Linear Algebra
3. Differential Equations

I want to take all three and likely will eventually, but for the time being my schedule only allows for one. Therefore, I was wondering if you could explain a little bit about how each is applied to the robotics field and which you believe will be most helpful for me to learn now.Thanks in advance!
","beginner, theory"
Can a Jacobian matrix be used to derive joint angles from end-effector linear and rotational velocity (without a filter)?,"I have a 2-link, 2 degree of freedom robotic arm, that only measures linear acceleration at each link(through an accelerometer), and rotational velocity on each joint (through a gyroscope).
I know that through using the Jacobian matrix, I can compute link velocity and acceleration from joint angles, and through the inverse of the matrix I can compute joint velocities from joint angles and link acceleration.
However, I am not sure if I can compute joint angles using only the link linear and rotational acceleration? I am aware that the joint angle could be estimated by integrating the joint velocities (and applying some sort of filter), but is there an algebraic way this can be computed? It doesn't seem likely to me. 
","robotic-arm, accelerometer, gyroscope, jacobian"
Many to One Bluetooth Communication Link,"I have an application that requires data to be streamed from multiple Bluetooth modules to one host controller. Somewhat like multiple Clients and one Server. 
The throughput i am looking at is around 1920-bits per second per module. 
The SPBT2632C2A.AT2 module only supports SPP profile in which i can have a single link (One Client One Server). My application needs multiple modules ( Max 5) to send information to one server.  
Is there a way to have One Receiving Station and have multiple transmitting module using SPP? (All modules being the SPBT2632C2A), or i need a Different higher end module on the server side which supports multiple SPP Links?
It advisable to look into a module like the BCM2070 and have a driver run system?
",electronics
Why PD controllers for quadcopter angles control?,"my question is: in a lot of cases it is possible to find in Internet PD (instead PID) to control the euler angles of quadcopter? Why the integral part is often neglected in this kind of applications? thanks
","control, quadcopter, pid"
Programming the Odometry of Rover 5,"I have started in the programming stage of my project , and my first step is to made and test the odometry of my Rover 5 robot on Arduino Uno by using encoders to determine position and orientation .
I wrote this code and I don’t know if that code right or there are some mistakes,  because I am novice to Arduino and Robotic field so I need for some suggestions and corrections if  there were . 
thanks a lot
Arduino codes posted below.
#define encoder1A  0       //signal A of left encoder  (white wire)
#define encoder1B  1      //signal B of left encoder  (yellow wire)
#define encoder2A  2      //signal A of right encoder  (white wire)
#define encoder2B  3      //signal B of right encoder  (yellow wire)

volatile int encoderLeftPosition = 0;      // counts of left encoder
volatile int encoderRightPosition = 0;     // counts of right encoder

float  DIAMETER  = 61  ;         // wheel diameter (in mm)
float distanceLeftWheel, distanceRightWheel, Dc, Orientation_change;
float ENCODER_RESOLUTION = 333.3;      //encoder resolution (in pulses per revolution)  where in Rover 5,  1000 state changes per 3 wheel rotations

int x = 0;           // x initial coordinate of mobile robot
int y = 0;           // y initial coordinate of mobile robot
float Orientation  = 0;       // The initial orientation of mobile robot
float WHEELBASE=183  ;       //  the wheelbase of the mobile robot in mm
float CIRCUMSTANCE =PI * DIAMETER  ;

void setup()
{
  pinMode(encoder1A, INPUT);
  digitalWrite(encoder1A, HIGH);       // turn on pullup resistor
  pinMode(encoder1B, INPUT);
  digitalWrite(encoder1B, HIGH);       // turn on pullup resistor
  pinMode(encoder2A, INPUT);
  digitalWrite(encoder2A, HIGH);       // turn on pullup resistor
  pinMode(encoder2B, INPUT);
  digitalWrite(encoder2B, HIGH);       // turn on pullup resistor
  attachInterrupt(0, doEncoder, CHANGE);       // encoder pin on interrupt 0 - pin 3
  Serial.begin (9600);
}

void loop()
{
  distanceLeftWheel = CIRCUMSTANCE * (encoderLeftPosition / ENCODER_RESOLUTION);       //  travel distance for the left and right wheel respectively
  distanceRightWheel = CIRCUMSTANCE * (encoderRightPosition / ENCODER_RESOLUTION);     // which equal to pi * diameter of wheel * (encoder counts / encoder resolution )
  Dc=(distanceLeftWheel + distanceRightWheel) /2 ;            // incremental linear displacement of the robot's centerpoint C
  Orientation_change =(distanceRightWheel - distanceLeftWheel)/WHEELBASE;    // the robot's incremental change of orientation , where b is the wheelbase of the mobile robot ,
  Orientation = Orientation + Orientation_change ;          //  The robot's new relative orientation
  x = x + Dc * cos(Orientation);                            // the relative position of the centerpoint for mobile robot
  y = y + Dc * sin(Orientation);
}

void doEncoder(){
  //  ---------- For Encoder 1 (Left)  -----------
  if (digitalRead(encoder1A) == HIGH) {   // found a low-to-high on channel A
    if (digitalRead(encoder1B) == LOW) {  // check channel B to see which way
                                             // encoder is turning
      encoderLeftPosition = encoderLeftPosition - 1;         // CCW
    }
    else {
      encoderLeftPosition = encoderLeftPosition + 1;         // CW
    }
  }
  else                                        // found a high-to-low on channel A
  {
    if (digitalRead(encoder1B) == LOW) {   // check channel B to see which way
                                              // encoder is turning
     encoderLeftPosition = encoderLeftPosition + 1;          // CW
    }
    else {
      encoderLeftPosition = encoderLeftPosition - 1;          // CCW
    }
  }
  //  ------------ For Encoder 2 (Right)-------------
  if (digitalRead(encoder2A) == HIGH) {   // found a low-to-high on channel A
    if (digitalRead(encoder2B) == LOW) {  // check channel B to see which way  encoder is turning
      encoderRightPosition = encoderRightPosition - 1;         // CCW
    }
    else {
      encoderRightPosition = encoderRightPosition + 1;         // CW
    }
  }
  else                                        // found a high-to-low on channel A
  {
    if (digitalRead(encoder2B) == LOW) {   // check channel B to see which way  encoder is turning
     encoderRightPosition = encoderRightPosition + 1;          // CW
    }
    else {
     encoderRightPosition = encoderRightPosition - 1;          // CCW
    }
  }
}

",mobile-robot
Can we use this line sensor as a proximity sensor?,"I have an RSL Line Sensor which is designed to distinguish black and white lines. It detects white surface and gives me digital 1 as output, with 0 in case of black, but the surface needs to be close to it.
As it uses infra-red-sensors, I wanted to use this sensor as a proximity sensor, to tell me if there is a white surface near it. Is it possible to do this?
I think the only problem here is that we need to increase it's range of giving 1. Currently, it gives 1 only when white surface is too close to the sensors. I want 1 even if the white surface is there at a bit more distance.
Also there is an adjustable screw there to adjust something, under which POT is written. I am working with an Arduino.
","arduino, mobile-robot, sensors"
iRobot Create 2 and Open Interface 2 Spec not syncing up with incoming data,"I have the create 2 and have it hooked up to an arduino. Almost all the commands work fine except when retrieving sensor information. If i send a request for packet 18 I get back values that while consistent don't match up, unless I am missing something. So if I press the clean button I get 127 or 11111110 and if i then press spot I get something like 11111010. I might be messing up my endianness but regardless the data isnt formatted how I expected it to be according to the spec sheet. I have 3 create 2s and they all do the same thing. Any ideas? I am using a 2n7000 along with the tutorial from the site but i dont think that has anything to do with the formatting of the byte.
this is the library I am using: https://github.com/DomAmato/Create2
Sorry to take so long to get back on this, anyways the data we get is always formatted this way. It is not a baud rate issue since it understands the commands properly.

        day     hour    minute  schedule    clock   dock    spot    clean
day     3       x       x       x           x       x       x       x
hour    6       7       x       x           x       x       x       x
minute  13      14      15      x           x       x       x       x
schedule    x   x       x       x           x       x       x       x
clock   x       x       x       x           x       x       x       x
dock    27      29      30      x           x       31      x       x
spot    55      59      61      x           x       62      63      x
clean   111     119     123     x           x       125     126     127

Note that the schedule and clock buttons return nothing
","arduino, irobot-create"
Is GMIS really a breakthrough?,"The GMIS (General Machine Intelligence System) from a new article posted at codeproject.com looks interesting.  Do you think that it could be a breakthrough in the field of robotics
","mobile-robot, robotic-arm"
What is the reduced form of this block diagram?,"What is the reduced form of this block diagram? I can't see any solution way :(

",control
How to implement PID control for robotic arm?,"I'm wondering that, PID control is a linear control technique and the robot manipulator is a nonlinear system, so how it is possible to apply PID control, in this case. I found a paper named: PID control dynamics of a robotic arm manipulator with two
degrees of freedom. on slide share page, is this how we use PID control for robotic arm, is there any name for this approach? and how to remove the ambiguity that PID is linear control technique and the robot is nonlinear system. Any suggestions?
","control, pid, robotic-arm, industrial-robot, dynamics"
What is the technology used for no-resistance robot arms?,"I saw a high end robot arm once that I could move and bend it to any form I wanted with no resistance, as if the robot arm didn't have any weight. I'd like to know more about them. What is this class of robot arm design called? Where can I get some more information about its design and applications?
","control, robotic-arm, joint"
3D scanner from Phone Camera,"123D software can construct a 3D model from photos taken from your phone. It doesn't process the photos in your phone.  Instead, it sends them to the cloud to create 3d model. How can i construct a 3d model like this (only with one camera)? I searched it but i can only find information on laser/procetor scanners (simple and desktop use only). I think 123D uses only IMU sensors and camera why do they use the cloud?  Can a beaglebone or rasperry pi create 3d models like this?
","computer-vision, 3d-printing, 3d-reconstruction, 3d-model"
Quadcopter PID output and duty cycle conversion,"I'm trying to design two PD controllers to control the roll and pitch angle of my quadcopter and a P controller to control the yaw rate. I give to the system the reference roll, pitch and yaw rate from a smartphone controller (with WiFi).In the case of roll and pitch the feedback for the outer 'P' loop is given by my attitude estimation algorithm, while in the inner 'D' loop there is no reference angle rate, and the feedback is provived by a filtered version of the gyroscope data.
As far the yaw rate is concerned, is only a P controller, the reference yaw rate is given by the smartphone, and the feedback of the only loop is provived by the smartphone. This is to illustrate the situation. My sampling frequency is 100hz (imposed by the attitude estimation algorithm, that is a Kalman Filter, that I'm using). I have tuned my controller gains with matlab, imposing a rise time of 0.1 seconds and a maximum percent overshoot of 2% with root locus. Matlab is able to found me a solution, but with very large gains (like 8000 for P and 100 for D). I was doing the tuning, using a quadcopter model (for each euler angle) based on the linearized model for quadcopter or instance : $$\ddot \tau_\Phi = I_x\ddot \Phi    ->  G_\Phi(s) = \frac{I_x }{ s^2} $$ only in order to have a 'reasoned' starting point for my gains, and then re-tune it in the reality. (The transfer function above is continous, in my model I have obliviously used the discrete version at 100hz of sampling rate). 
This is to do a premise of my following questions.
Now, I have to map my controller outputs to duty cycle. Since I'm using a PWM at 25Khz frequency, my period (in the TIM channel configuration) is of 2879.
I have checked the activation threshold (after which the motor starts move) and the threshold after which it stops increasing its speeds, and the first is 202
and the second is 2389.
I was following the very good answer of Quadcopter PID output but I still have some questions.
1) As far the throttle mapping is concerned, I have to map it in such a way that the values coming from my smartphone controller (in the interval [0, 100]) are not
mapped in the whole [202, 2389] interval, but I have to 'reserve' some speed in order to allow the quadcopter to have an angular movement exploiting differences in the 4 motor speeds even with 100% throttle?
2) Coming back to the fact that matlab propose me huge gains for my controllers, this leads to the fact that I cannot directly sum the controller output to the duty cycle as stated in the metioned answer (because I will certainly go out of the [202, 2389] bound of my TIM pulse). Doing a proportion will result in altering the gains of the systems, so placing somewhere else the poles of my systems and the procedure done with matlab will became useless, right? So, what I'm doing wrong? I have tried to enforce matlab to bound the gainsm for instance in the [0,100] interval, but in this case it cannot find gains such that my constraints are verified.
Thank you
","control, quadcopter, pid, matlab"
"Introduction to Robotics Mechanics & Control, John J Craig., 3rd Ed., Forward transformation problem Examples 2.2 and 2.4","I am reading the book ""Introduction to Robotics Mechanics & Control"", John J Craig., 3rd Ed., Forward transformation problem Examples 2.2 and 2.4.
Ex. 2.2 (Page 29): Frame {B} is rotated relative to frame {A} about X axis by 60 degrees clockwise, translated 20 units along Y axis and 15 units along z axis. Find P in frame {A} where P in frame {b} = [0 8 7]
The book's answer is [0.18.062 3.572].
But my answer is [0 30.062 11.572].
Ex. 2.4 (Page 33): Vector P1 has to be rotated by 60 degrees clockwise, about X axis and translated 20 units along Y axis, and 15 units along Z axis. If P1 is given by [0 8 7], find P2.
Essentially Ex.2.2 and 2.4 are the same problem. However, the Transformation matrix for Ex 2.4, has [0 8 7] as translation vector (The 4th column of T) instead of [0 20 15]. And, the given answer is [0.18.062 3.572].
I am not sure if it is just typo, or I am missing some genuine operation. Please let me know your opinion.
Thanks.
","forward-kinematics, books"
Simple wireless connection between two circuits,"I'm relatively new to robotics, and I'm building a project for which I need a simple wireless connection between two circuits such that when the first circuit is switched on, the other circuit gets switched on too. I'm looking to preferably build something like this on my own, but I have no idea about wireless connections. I only know basic wired robotics. I also know C++ programming if that helps. Apologies if such a question has already been asked.
Regards,
Hanit Banga
",wireless
What is the cheapest way to detect and identify vehicles entering a gate in real time?,"I want to detect and identify each of the vehicles passing through a gate. 
I have the live video feed of the gate which I initially thought to process and detect the number plates with the help of OpenCV or any other graphics library freely available. The problem is, the size of number plates may vary very widely, and the language the number plates are written with(Bengali) does not have a good OCR performance at all.
The next idea was to put a QR code in the windshield of the vehicles. (Yes the vehicles supposed to enter the area are private and enlisted vehicles). But I am not confident that I will be able to detect and identify all the QR codes in real time with 100% accuracy, as the QR codes might get pixelated due to low resolution of video.
So can anyone suggest any other cheap way we can adopt to detect and identify the vehicles? Can NFC or any other cheap sensors be used for this purpose?
","sensors, design, computer-vision"
"Artificial Intelligence Software Packages: Professionals, University education is oft' a step behind. What's actually being used?","Currently using Windows 8, what software packages for artificial intelligence programming (robotics branch) are used in today's professional environment as standard. Lots of internet suggestions, but companies seem to keep this a closely guarded secret. And are the internet rumors true? Would switching to Ubuntu offer me more in terms of depth.
Context: Educational field: Computer Science and Artificial Intelligence, current focus (though obviously experience in others) in programming languages stands at c++, C and Python. Looking to build, program and develop a human-like bot (NOT aiming for singularity at this point ;))and am asking this question in order to build my toolbox a little. 
","design, software, artificial-intelligence, programming-languages"
Selecting hardware: stereo camera for beginners,"I'm looking for some cheap hardware that would offer me results decent enough to continue my experimentation.
I've been looking into how to obtain hardware for learning about stereo vision and 3D reconstruction, I found two basic ways: - buy 2 cheap webcams and DIY - buy a stereo camera
For what I understood little variations in distance and inclination can easily compromise the diff map and so the DIY version might end up requiring constant calibrations, however on the other end, so buying ""professional"" stereo camera range from 500 euro to infinite.
For the moment I trying something in between, like the minoru 3d, however the overall performance of the camera looks a bit poor also because it's a 2009 product, however I can't find any more recent product offering a similar solution.
Can you suggest me what would be the best way/product/guide to archive decent results without spending a fortune ?
Thank you very much :)
",stereo-vision
Using Armatures in Morse Robotic Simulator,"I'm trying to add my own robot in Morse 1.1 (using Ubuntu 12.04). I am struggling to add an armature actuator and armature pose sensor to an existing robot. Can someone please explain how this can be done (preferably with some sample code and using the socket interface).
Thanks. 
","mobile-robot, sensors, robotic-arm, simulator, python"
Libusb and arduino communication not working,"I am doing a line following robot based on opencv. I have my onboard computer(an old pandaboard) running opencv. It will calculate the offset from the required path and communicate it to the arduino via USB. Then it will do PID optimisation on the data, and adjust the speed of the left and right motors.
To my dismay the communication part is not working, and I've tried hard for a day to fix it with no result. Here is the relavent code running on the pandaboard:
while(1)
    {

        r = libusb_bulk_transfer(dev_handle, 131, recieve, 1, &actual, 0);
        cout<<""r=""<<r<<endl;
        int a;
        cin>>a;
        imgvalue=krish.calc_offset();
        send[0]=imgvalue&0xff; 
        send[1]=imgvalue>>8;

//make write

        cout<<""Data to send->""<<imgvalue<<""<-""<<endl; //just to see the data we want to write : abcd
        cout<<""Writing Data...""<<endl;
        r = libusb_bulk_transfer(dev_handle, (4 | LIBUSB_ENDPOINT_OUT), send, 2, &actual, 0); //my device's out endpoint was 2, found with trial- the device had 2 endpoints: 2 and 129
        if(r == 0 && actual == 2) //we wrote the 4 bytes successfully
            cout<<""Writing Successful!""<<endl;
        else
            cout<<""Write Error""<<endl;

    }

where imgvalue is the data to be send. This is the code running on the Arduino:
void loop()
{
  Serial.write('s');
  if(Serial.available()>0)
    Input_tmp = Serial.read();
  if(Serial.available()>0)
    Input_tmp = Input_tmp | (Serial.read() << 8);
  Input=Input_tmp;
  myPID.Compute();

// adjust the motor speed

}

What happens when I run is that it will pause at the libusb read operation as the timeout is zero(infinity). At this point I've tried resetting the arduino, but this doesn't help. So how do I make my program respond to this start byte send my the Arduino? Where did I go wrong?
","arduino, communication, usb"
ROBOTIC arm for playing chess,"I wish to build a chess playing robot with robot arm as shown on youtube, can anyone please tell me which robot arm would suit my purpose and whether it can be bought second hand or alternatively anybody willing to sell used chess arm robot? Please help out.
",robotic-arm
Fingerprinting/ model matching algorithms for localization,"This paper mentioned the fingerprinting/model matching case. But I could not find an image based algorithm. Any suggestion about image based localization
","mobile-robot, localization"
Matlab toolbox (Windows) for Sick lasers?,"Does anybody know where I can get a matlab toolbox or functions to work with a SICK laser-scanner (Windows OS)? I'm using a SICK-LDRS2110 with ethernet cable, but SOPAS software does not allow me to program recording times and other specific tasks. Any tips are more than welcome! 
Thanks!
","laser, matlab"
Quadcopter force/torques duty cycle conversion,"after having been determined my control loops for my quadcopter project, I'm going to determine the motor commands (PWM duty cycle) from the motor forces/torques. I was following the guidelines of this document but when I was trying to do the inverse of the matrix M (page 17) it has determinant equal to 0. The procedure is correct? Anyone can suggest me some other link for doing this conversion? I have searched in the Internet but I haven't found so much about that. Thanks
The part of the document that I'm referring is the following:

","control, quadcopter, pwm"
Determining transfer function of a PTU for visual tracking,"I have a PTU system whose transfer function I need to determine. The unit receives a velocity and position, and move towards that position with the given velocity. What kind of test would one perform for determining the transfer function...
I know Matlab provides a method. The problem, though, is that I am bit confused on what kind of test I should perform, and how I should use Matlab to determine the transfer function.
The unit which is being used is a Flir PTU D48E
---> More about the system 
The input to the system is pixel displacement of an object to the center of the frame. The controller I am using now converts pixel distances to angular distances multiplied by a gain $K_p$. This works fine. However, I can't seem to prove why that it works so well, I mean, I know servo motors cannot be modeled like that.
The controller is fed with angular displacement and its position now => added together give me angular position I have to go to. 
The angular displacement is used as the speed it has to move with, since a huge displacement gives a huge velocity.
By updating both elements at different frequency I'm able to step down the velocity such that the overshoot gets minimized. 
The problem here is: if I have to prove that the transfer function I found fits the system, I have to do tests somehow using the ident function in Matlab, and I'm quite unsure how to do that. I'm also a bit unsure whether the PTU already has a controller within it, since it moves so well, I mean, it's just simple math, so it makes no sense that I'll convert it like that.
",control
How can a memory alloy be used as an alternative to a compressor found in a refrigerator?,"I'm curious about this alloy and how they say it can be used as an alternative to a traditional compressor. Can anyone explain how this would work?
My goal is to understand that use case so I can adapt alloys in other robotic projects.  My gut tells me this is perfect for some kinematics, or other mechanisms, but I'm missing some pieces in this puzzle (how would it work?)
","kinematics, mechanism"
Determine the configuration space for a robotic arm,"I'm working with a 4DOF Parallel-Mechanism arm. I'm interested in writing planners for this arm (PRM or RRT) in the configuration space, but I'm not sure how to identify obstacles/collisions. 
When writing planners for mobile robots in a 2d workspace, it was easy to define and visualize the workspace and obstacles in which the planner/robot was operating. This website (link) shows a great example of visualizing the workspace and configuration space for a 2DOF arm, but how can I do this for higher dimensions?
","robotic-arm, motion-planning"
Electric piston (longitudinal electric motor)?,"Are there electric motors, which apply force not in rotational motion, but in longitudinal motion?
They should have electromagnetic design and no gears and worms.
Such motors would be great for linear actuators. They could transfer both force and feedback.
What is the name of such devices?
","motor, actuator"
Interfacing GPU image processing with motor control at 30+Hz,"I would like to make a robotic system which takes as input a video feed, runs some GPU-based image recognition on the video, and outputs commands to a set of motors. The goal is to have the motors react to the video with as little latency as possible, hopefully of the order of 10s of ms. Currently I have a GTX 770m on a laptop running Ubuntu 14.04, which is connected to the camera and doing the heavy image processing. This takes frames at 30Hz and will output motor commands at the same frequency.
After a few days of looking around on the web for how to design such a system, I'm still at a loose end whether (a) it is even feasible (b) if so, what the best approach is to interface the laptop with the motors? The image processing must run on Linux, so there is no leeway to change that part of things.
","control, real-time"
idea for web application in robotics,"I am learning and I am interested robotics, but also I need to update my web development skills so the question is - is there any idea for good web application that could be connected with robotics - service robots, industrial robots etc. Maybe there already is some open source ongoing web application projects for robotics in which I can make contribution.
Thanks!
","design, software"
Quadcopter frame design,"Quadcopter frames seem to consistently follow the same X design. For example:

I'm curious to know why that is. It certainly seems like the most efficient way to use space but is it the only frame design that would work for quadcopters?
For instance, would a design like this work?

Why or why not?
","quadcopter, design, frame"
Autonomous car steering using IR sensors,"I want to steer a RC car in a straight line.The car has 4 sharp IR sensors on each corner of the car to help it steer the corridor.The corridor is irregular and looks something similar to the picture below.
The car needs to be stay exactly at the middle(shown by lighter line) and take help of the IR sensors to correct its path.
The car has a servo on the front wheel to steer and another that controls the speed.
I tried running it using a algorithm where it summed the values on each side of the car and took the difference.THe difference was then fed to a pid control the output of which went to steer the car.The greater the value from the pid (on either sides), the greater the value of the steering angle till it reaches the middle.
It works for the part where the walls are at similar distance from the center and even then it oscillates a lot around the center and fails miserably around the bumps in the corridor.
I need to make changes to the algorithm and need some help in steering me  in the right direction.
The IR sensors are too finicky and is there a way to filter out the noise and make the readings more stable?
Any help regarding the changes that needs to be implemented is much appreciated.
Currently the car only uses 4 IR sensors to guide.I can also use 2 ultrasonic sensors.

","mobile-robot, sensors"
How many quadcopters would it take to lift a burrito?,"I am investigating a possible business opportunity in which quadcopters perform high-precision nutritional delivery via a burrito medium. I have never used a burrito, but I have read on the internet that they typically weigh 600-700 grams (1). This is much too heavy for commercially available platforms.
How many quadcopters would it take to lift a single burrito?
(1): https://www.facebook.com/chipotle/posts/390817319252
","quadcopter, distributed-systems"
Are LiPo really 100 times more energy dense than model rockets?,"Lately I've been interested in comparing the energy density of model rocket engines to lithium polymer batteries (attached to motors and propellers) for propelling things upwards.
To get a feel for this, I decided to compare an Estes C6-5 motor to a 3DR Iris + quadcopter.
Estes C6-5 has initial mass of 25.8g, and produces 10 N s total impulse. So, the ""Impulse density"" is about 10 N s / 25.8g = 0.38 N s g^-1.
3DR Iris+ weighs 1282g without battery. 3.5ah battery weighs 250g and will power hover for about 20 minutes (so about 10.5a draw). Thrust produced to hover on Earth is 9.8N kg^-1 * 1.532 kg = 14.7N. ""Impulse density"" is 14.7N * 1200s / 250g = 70.6 N s g^-1 .
So, according to my math here, the LiPo is about 0.38/70.6 = 186 times more energy dense than the model rocket engine.
Of course, the model rocket engine will lose 12.48g of propellant by the end of the flight so it will be effectively a little lighter, but that's not going to affect things by a factor of 100.
Does this seem right to you? Am I missing anything?
","quadcopter, rocket, lithium-polymer"
Quadcopter - is iPhone the ultimate flight controller?,"iPhone contains

Gyroscope
GPS
Two photo and video cameras
Self-sufficient battery that outlives the motor battery
Wifi
Backup connectivity (cellular, bluetooth)
Programmable computer
Real-time image processing capabilities and face detection
General purpose IO (with something like this)

and old models are available very cheap.
What is the main benefit of having a separate dedicated flight controller and camera on hobbyist rotorcraft rather than a general purpose device like the iPhone?
",quadcopter
Effect of adding a Pole and Zero to PID,"I am confused about how adding a D (which adds a zero to the complete system) decreases the speed of the system. But when we normally add a zero to the system, it causes the system to overshoot.
The same goes for the I part of the PID. Normally when we add a pole to the system, it has less overshoot, but at the same time the integrator increases the overshoot!
How can I make sense out of this inverse relation?
","control, pid"
